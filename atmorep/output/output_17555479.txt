0: Wandb run: atmorep-tge40j11-17555479
0: l40360:84505:84505 [0] NCCL INFO Bootstrap : Using ib0:10.128.9.102<0>
0: l40360:84505:84505 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l40360:84505:84505 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l40360:84505:84505 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l40360:84505:84505 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
0: l40360:84505:84748 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.9.102<0>
0: l40360:84505:84748 [0] NCCL INFO Using non-device net plugin version 0
0: l40360:84505:84748 [0] NCCL INFO Using network IB
0: l40360:84505:84748 [0] NCCL INFO ncclCommInitRank comm 0x55555f26a680 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0x43aacd038bf23638 - Init START
0: l40360:84505:84748 [0] NCCL INFO Setting affinity for GPU 0 to 0f0000,00000000,00000000,00000000,000f0000,00000000
0: l40360:84505:84748 [0] NCCL INFO comm 0x55555f26a680 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l40360:84505:84748 [0] NCCL INFO Channel 00/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 01/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 02/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 03/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 04/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 05/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 06/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 07/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 08/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 09/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 10/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 11/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 12/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 13/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 14/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 15/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 16/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 17/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 18/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 19/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 20/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 21/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 22/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 23/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 24/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 25/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 26/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 27/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 28/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 29/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 30/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Channel 31/32 :    0
0: l40360:84505:84748 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l40360:84505:84748 [0] NCCL INFO P2P Chunksize set to 131072
0: l40360:84505:84748 [0] NCCL INFO Connected all rings
0: l40360:84505:84748 [0] NCCL INFO Connected all trees
0: l40360:84505:84748 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l40360:84505:84748 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l40360:84505:84748 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l40360:84505:84748 [0] NCCL INFO ncclCommInitRank comm 0x55555f26a680 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0x43aacd038bf23638 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 4
0: par_rank : 0
0: par_size : 1
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 1, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 2, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 3, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 3, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17555479
0: wandb_id : tge40j11
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: mask_input_field : total_precip
0: mask_input_value : 0
0: years_test : [2021]
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 1
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 2
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded model id = wc5e2i3t at epoch = 0.
0: Loaded run 'wc5e2i3t' at epoch 0.
0: l40360:84505:84894 [1] NCCL INFO Using non-device net plugin version 0
0: l40360:84505:84894 [1] NCCL INFO Using network IB
0: l40360:84505:84894 [1] NCCL INFO ncclCommInitRank comm 0x5555a6f78c00 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0x4dfa01aeb8a65244 - Init START
0: l40360:84505:84894 [1] NCCL INFO Setting affinity for GPU 1 to 0f0000,00000000,00000000,00000000,000f0000
0: l40360:84505:84894 [1] NCCL INFO comm 0x5555a6f78c00 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l40360:84505:84894 [1] NCCL INFO Channel 00/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 01/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 02/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 03/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 04/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 05/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 06/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 07/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 08/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 09/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 10/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 11/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 12/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 13/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 14/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 15/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 16/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 17/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 18/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 19/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 20/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 21/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 22/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 23/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 24/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 25/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 26/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 27/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 28/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 29/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 30/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Channel 31/32 :    0
0: l40360:84505:84894 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l40360:84505:84894 [1] NCCL INFO P2P Chunksize set to 131072
0: l40360:84505:84894 [1] NCCL INFO Connected all rings
0: l40360:84505:84894 [1] NCCL INFO Connected all trees
0: l40360:84505:84894 [1] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l40360:84505:84894 [1] NCCL INFO ncclCommInitRank comm 0x5555a6f78c00 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0x4dfa01aeb8a65244 - Init COMPLETE
0: l40360:84505:84899 [2] NCCL INFO Using non-device net plugin version 0
0: l40360:84505:84899 [2] NCCL INFO Using network IB
0: l40360:84505:84899 [2] NCCL INFO ncclCommInitRank comm 0x55557eb732f0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0x5cfe3b2a1452b53d - Init START
0: l40360:84505:84899 [2] NCCL INFO Setting affinity for GPU 2 to 1f0000,00000000,00000000,00000000,001f0000,00000000,00000000,00000000
0: l40360:84505:84899 [2] NCCL INFO comm 0x55557eb732f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l40360:84505:84899 [2] NCCL INFO Channel 00/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 01/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 02/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 03/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 04/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 05/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 06/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 07/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 08/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 09/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 10/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 11/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 12/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 13/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 14/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 15/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 16/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 17/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 18/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 19/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 20/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 21/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 22/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 23/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 24/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 25/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 26/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 27/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 28/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 29/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 30/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Channel 31/32 :    0
0: l40360:84505:84899 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l40360:84505:84899 [2] NCCL INFO P2P Chunksize set to 131072
0: l40360:84505:84899 [2] NCCL INFO Connected all rings
0: l40360:84505:84899 [2] NCCL INFO Connected all trees
0: l40360:84505:84899 [2] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l40360:84505:84899 [2] NCCL INFO ncclCommInitRank comm 0x55557eb732f0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0x5cfe3b2a1452b53d - Init COMPLETE
0: l40360:84505:84904 [3] NCCL INFO Using non-device net plugin version 0
0: l40360:84505:84904 [3] NCCL INFO Using network IB
0: l40360:84505:84904 [3] NCCL INFO ncclCommInitRank comm 0x555587744780 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0x3412ee45587121c4 - Init START
0: l40360:84505:84904 [3] NCCL INFO Setting affinity for GPU 3 to 0f0000,00000000,00000000,00000000,000f0000,00000000,00000000
0: l40360:84505:84904 [3] NCCL INFO comm 0x555587744780 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l40360:84505:84904 [3] NCCL INFO Channel 00/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 01/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 02/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 03/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 04/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 05/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 06/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 07/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 08/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 09/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 10/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 11/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 12/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 13/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 14/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 15/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 16/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 17/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 18/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 19/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 20/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 21/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 22/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 23/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 24/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 25/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 26/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 27/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 28/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 29/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 30/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Channel 31/32 :    0
0: l40360:84505:84904 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l40360:84505:84904 [3] NCCL INFO P2P Chunksize set to 131072
0: l40360:84505:84904 [3] NCCL INFO Connected all rings
0: l40360:84505:84904 [3] NCCL INFO Connected all trees
0: l40360:84505:84904 [3] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l40360:84505:84904 [3] NCCL INFO ncclCommInitRank comm 0x555587744780 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0x3412ee45587121c4 - Init COMPLETE
0: Number of trainable parameters: 741,136,272
0: 1 : 20:30:42 :: batch_size = 96, lr = 1.5000000000000002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5108, -0.5137, -0.5149, -0.5157, -0.5176, -0.5220, -0.5292, -0.5391, -0.5500, -0.5602, -0.5683, -0.5742,
0:         -0.5781, -0.5812, -0.5844, -0.5879, -0.5917, -0.5949, -0.5451, -0.5499, -0.5504, -0.5475, -0.5431, -0.5397,
0:         -0.5394], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3379, 0.3396, 0.3411, 0.3435, 0.3476, 0.3541, 0.3629, 0.3730, 0.3825, 0.3896, 0.3922, 0.3900, 0.3827, 0.3715,
0:         0.3571, 0.3411, 0.3245, 0.3073, 0.3140, 0.3204, 0.3271, 0.3336, 0.3403, 0.3472, 0.3541], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4722, -0.4690, -0.4657, -0.4618, -0.4561, -0.4492, -0.4438, -0.4375, -0.4305, -0.4216, -0.4131, -0.4051,
0:         -0.3993, -0.3940, -0.3889, -0.3842, -0.3792, -0.3724, -0.4702, -0.4670, -0.4638, -0.4604, -0.4534, -0.4473,
0:         -0.4436], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1088, -0.1359, -0.1811, -0.2421, -0.3009, -0.3484, -0.3800, -0.4004, -0.4117, -0.4139, -0.4004, -0.3687,
0:         -0.3213, -0.2738, -0.2444, -0.2354, -0.2467, -0.2602, -0.1562, -0.1653, -0.1834, -0.2082, -0.2376, -0.2715,
0:         -0.3077], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.0854, -1.0584, -1.0285, -0.9953, -0.9592, -0.9205, -0.8805, -0.8403, -0.8005, -0.7616, -0.7239, -0.6871,
0:         -0.6516, -0.6168, -0.5821, -0.5472, -0.5104, -0.4712, -0.4289, -0.3841, -0.3380, -0.2925, -0.2496, -0.2107,
0:         -0.1775], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0702,
0:             nan,     nan,     nan,     nan, -0.1997,     nan,     nan,     nan,     nan,     nan, -0.1997,     nan,
0:             nan,     nan,     nan,     nan,  0.1783,     nan,     nan,     nan,     nan,     nan,     nan, -0.2409,
0:             nan,     nan, -0.0301,     nan,     nan,     nan,     nan,     nan, -0.2268,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2186,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2327, -0.2421,     nan,     nan,     nan,     nan,     nan, -0.2186,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2033,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0914,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0454,     nan,     nan, -0.2280,     nan, -0.2374,
0:          0.1701,     nan,     nan,     nan,     nan,     nan,     nan, -0.2292,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1962, -0.2292, -0.2209,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2327,     nan,     nan, -0.2033,     nan, -0.1526,     nan,     nan,     nan, -0.2362,
0:             nan,     nan, -0.1986,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0878,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1691,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1974, -0.2186, -0.1880,     nan,     nan,     nan,     nan,     nan,     nan, -0.1679,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1962,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1337, -0.1426, -0.1461, -0.1430, -0.1410, -0.1407, -0.1426, -0.1494, -0.1527, -0.1610, -0.1676, -0.1772,
0:         -0.1864, -0.1935, -0.1986, -0.2003, -0.2030, -0.2046, -0.0842, -0.0926, -0.0939, -0.0915, -0.0905, -0.0920,
0:         -0.0967], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5099, -0.4924, -0.4780, -0.4617, -0.4384, -0.4041, -0.3662, -0.3232, -0.2833, -0.2438, -0.2065, -0.1680,
0:         -0.1290, -0.0884, -0.0487, -0.0101,  0.0245,  0.0570, -0.5113, -0.4968, -0.4861, -0.4732, -0.4506, -0.4220,
0:         -0.3910], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6135, -0.6081, -0.6081, -0.6046, -0.6043, -0.6018, -0.6000, -0.5978, -0.5964, -0.5952, -0.5942, -0.5928,
0:         -0.5909, -0.5887, -0.5867, -0.5860, -0.5885, -0.5915, -0.6096, -0.6062, -0.6038, -0.6015, -0.6008, -0.5991,
0:         -0.5958], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0757,  0.0823,  0.0622,  0.0413,  0.0396,  0.0090, -0.0341, -0.0653, -0.0699, -0.0994, -0.1549, -0.1345,
0:         -0.1138, -0.1048, -0.0485, -0.0279, -0.0354, -0.0580,  0.0257,  0.0292,  0.0152, -0.0090,  0.0029, -0.0046,
0:         -0.0291], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.4825, -1.4859, -1.4849, -1.4814, -1.4776, -1.4791, -1.4857, -1.4962, -1.5027, -1.5030, -1.4994, -1.4959,
0:         -1.4940, -1.4934, -1.4945, -1.4956, -1.4971, -1.4988, -1.4989, -1.4960, -1.4896, -1.4802, -1.4670, -1.4510,
0:         -1.4331], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1712, -0.1734, -0.1815, -0.1823, -0.1835, -0.1874, -0.1882, -0.1828, -0.1819, -0.1732, -0.1707, -0.1760,
0:         -0.1798, -0.1843, -0.1825, -0.1874, -0.1853, -0.1783, -0.1726, -0.1718, -0.1730, -0.1745, -0.1772, -0.1765,
0:         -0.1790], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [1/5 (20%)]	Loss: 1.08883 : 0.30488 :: 0.14744 (1.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [2/5 (40%)]	Loss: 0.90529 : 0.27928 :: 0.14610 (15.59 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [3/5 (60%)]	Loss: 0.95140 : 0.25766 :: 0.14048 (12.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [4/5 (80%)]	Loss: 2.28993 : 0.49212 :: 0.16232 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -1.519, max = 5.586, mean = 0.794
0:          sample (first 20): tensor([1.4588, 1.5605, 1.7060, 2.0705, 2.1650, 2.0762, 1.8458, 1.8550, 1.5153, 0.8336, 1.1793, 1.4150, 1.6155, 1.6159,
0:         1.7269, 1.3936, 1.2867, 1.2021, 0.9815, 0.8737])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 1 : 0.4647250175476074
0: validation loss for velocity_u : 0.03569375351071358
0: validation loss for velocity_v : 0.06525867432355881
0: validation loss for specific_humidity : 0.027879923582077026
0: validation loss for velocity_z : 0.45258060097694397
0: validation loss for temperature : 0.07883954048156738
0: validation loss for total_precip : 2.1280972957611084
0: 2 : 20:35:01 :: batch_size = 96, lr = 2e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 2, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7639, -0.7566, -0.7472, -0.7353, -0.7216, -0.7071, -0.6912, -0.6737, -0.6542, -0.6336, -0.6126, -0.5907,
0:         -0.5679, -0.5453, -0.5246, -0.5063, -0.4906, -0.4797, -0.7021, -0.6942, -0.6835, -0.6699, -0.6544, -0.6377,
0:         -0.6194], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2570, -1.2514, -1.2472, -1.2447, -1.2441, -1.2456, -1.2499, -1.2561, -1.2641, -1.2722, -1.2791, -1.2826,
0:         -1.2806, -1.2722, -1.2575, -1.2367, -1.2106, -1.1797, -1.3054, -1.3029, -1.3011, -1.2991, -1.2976, -1.2969,
0:         -1.2980], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2535, -0.2353, -0.2161, -0.1991, -0.1817, -0.1617, -0.1414, -0.1242, -0.1101, -0.0920, -0.0789, -0.0694,
0:         -0.0669, -0.0625, -0.0644, -0.0679, -0.0765, -0.0827, -0.2173, -0.2017, -0.1851, -0.1695, -0.1529, -0.1374,
0:         -0.1211], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1179, -0.1774, -0.2479, -0.2931, -0.3030, -0.3163, -0.3593, -0.4077, -0.4188, -0.4033, -0.4144, -0.4628,
0:         -0.4992, -0.4948, -0.4926, -0.5257, -0.5466, -0.4838, -0.1465, -0.2005, -0.2678, -0.3196, -0.3405, -0.3559,
0:         -0.3901], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.7998, 1.7965, 1.7918, 1.7854, 1.7772, 1.7672, 1.7554, 1.7416, 1.7262, 1.7094, 1.6914, 1.6730, 1.6548, 1.6374,
0:         1.6199, 1.6019, 1.5841, 1.5676, 1.5531, 1.5391, 1.5262, 1.5160, 1.5088, 1.5031, 1.4972], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2064,     nan,     nan,     nan, -0.2403,     nan,     nan, -0.2312,     nan,     nan,     nan,     nan,
0:             nan, -0.2437,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2414, -0.2425,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2380,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1500,     nan,     nan,     nan,     nan,     nan, -0.1681,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2312,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2279,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2143,     nan,     nan, -0.2403, -0.2391,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2346,     nan, -0.2301,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2200,     nan,     nan,     nan,     nan, -0.2301, -0.2166,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 2, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6216, -0.6432, -0.6691, -0.6960, -0.7267, -0.7623, -0.7948, -0.8297, -0.8581, -0.8826, -0.9082, -0.9360,
0:         -0.9687, -0.9966, -1.0247, -1.0468, -1.0600, -1.0670, -0.5747, -0.6015, -0.6306, -0.6619, -0.6978, -0.7354,
0:         -0.7701], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4132, -0.4271, -0.4578, -0.5013, -0.5484, -0.5953, -0.6401, -0.6863, -0.7308, -0.7751, -0.8231, -0.8649,
0:         -0.8971, -0.9159, -0.9132, -0.8930, -0.8486, -0.7913, -0.4673, -0.4735, -0.4961, -0.5342, -0.5755, -0.6204,
0:         -0.6650], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6860, -0.6681, -0.6365, -0.5927, -0.5473, -0.5051, -0.4782, -0.4558, -0.4373, -0.4212, -0.3965, -0.3679,
0:         -0.3395, -0.3127, -0.2898, -0.2728, -0.2546, -0.2370, -0.6903, -0.6687, -0.6307, -0.5770, -0.5248, -0.4812,
0:         -0.4484], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0409, -0.0568, -0.0484, -0.0594, -0.0722, -0.0571, -0.0400, -0.0056,  0.0494,  0.0432,  0.0162,  0.0408,
0:          0.0404,  0.0104, -0.0293, -0.0655, -0.0716, -0.1251, -0.0384, -0.0642, -0.0546, -0.0556, -0.0549, -0.0439,
0:         -0.0484], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.0359, 1.0442, 1.0513, 1.0546, 1.0577, 1.0601, 1.0609, 1.0575, 1.0525, 1.0457, 1.0362, 1.0288, 1.0269, 1.0288,
0:         1.0390, 1.0522, 1.0663, 1.0781, 1.0892, 1.0990, 1.1087, 1.1167, 1.1203, 1.1184, 1.1119], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.6158,  0.7544,  0.9596,  1.2610,  1.4549,  1.5846,  1.6111,  1.7126,  1.5317,  0.0032,  0.2743,  0.5535,
0:          0.8313,  0.9510,  1.2268,  1.1577,  1.2191,  1.2391, -0.2079, -0.1363,  0.0529,  0.3213,  0.4509,  0.5575,
0:          0.5763], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 2 [1/5 (20%)]	Loss: 2.40762 : 0.48189 :: 0.14262 (2.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 2 [2/5 (40%)]	Loss: 2.27196 : 0.48926 :: 0.14970 (15.44 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 2 [3/5 (60%)]	Loss: 2.88438 : 0.58650 :: 0.14577 (15.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 2 [4/5 (80%)]	Loss: 2.47782 : 0.51124 :: 0.15621 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -1.320, max = 5.967, mean = 0.790
0:          sample (first 20): tensor([1.2168, 1.3007, 1.5540, 1.9747, 2.2727, 2.4245, 2.3969, 2.5633, 2.3044, 0.4596, 0.8357, 1.1517, 1.4753, 1.6906,
0:         2.0394, 1.8932, 1.9468, 1.8944, 0.4075, 0.3438])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 2 : 0.5100568532943726
0: validation loss for velocity_u : 0.041703108698129654
0: validation loss for velocity_v : 0.073368601500988
0: validation loss for specific_humidity : 0.024230899289250374
0: validation loss for velocity_z : 0.6250923871994019
0: validation loss for temperature : 0.07533206045627594
0: validation loss for total_precip : 2.220613718032837
0: 3 : 20:39:02 :: batch_size = 96, lr = 1.9512195121951222e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 3, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2348, 1.1944, 1.1689, 1.1725, 1.2096, 1.2471, 1.2577, 1.2485, 1.2296, 1.2103, 1.1895, 1.1638, 1.1433, 1.1205,
0:         1.0892, 1.0555, 1.0230, 1.0033, 1.2592, 1.2255, 1.2121, 1.2258, 1.2636, 1.2980, 1.3021], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5058, -0.4464, -0.4221, -0.4447, -0.4844, -0.5181, -0.5197, -0.4756, -0.4057, -0.3433, -0.3102, -0.2985,
0:         -0.2985, -0.3098, -0.3289, -0.3587, -0.4002, -0.4486, -0.5772, -0.5121, -0.4661, -0.4673, -0.5090, -0.5699,
0:         -0.6157], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6548, 0.7269, 0.8490, 0.8923, 0.9459, 0.9479, 1.0113, 1.1741, 1.2807, 1.3194, 1.3390, 1.3287, 1.3207, 1.2875,
0:         1.2479, 1.2103, 1.1689, 1.1416, 0.5378, 0.6256, 0.7626, 0.8523, 0.9255, 0.9385, 0.9324], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1893, -0.0563, -0.3430, -0.5019, -0.3319, -0.0452, -0.1307, -0.0774,  0.1015, -0.1874, -0.3585, -0.3241,
0:         -0.2585,  0.0726,  0.0093, -0.3752, -0.5352, -0.6430,  0.6171,  0.2904,  0.0393,  0.0926,  0.3804,  0.7393,
0:          0.9193], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.1438, -1.1141, -1.0893, -1.0991, -1.1283, -1.1255, -1.1319, -1.1906, -1.2546, -1.3132, -1.3677, -1.3935,
0:         -1.4006, -1.3960, -1.3932, -1.4083, -1.4321, -1.4602, -1.4842, -1.4943, -1.4900, -1.4616, -1.4066, -1.3251,
0:         -1.2361], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2474,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2462,     nan,     nan,     nan,     nan,     nan, -0.1818,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0745,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2307,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2474,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2474,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1532,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2415,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0256,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 3, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1489, 1.1253, 1.1115, 1.1021, 1.0954, 1.0939, 1.0921, 1.0878, 1.0811, 1.0654, 1.0363, 0.9921, 0.9402, 0.8901,
0:         0.8499, 0.8195, 0.8040, 0.7982, 1.2549, 1.2261, 1.1989, 1.1805, 1.1696, 1.1614, 1.1576], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8782, -0.9118, -0.9565, -1.0083, -1.0654, -1.1168, -1.1594, -1.1928, -1.2150, -1.2212, -1.2105, -1.1882,
0:         -1.1528, -1.1115, -1.0667, -1.0173, -0.9595, -0.9022, -0.8267, -0.8517, -0.8922, -0.9450, -0.9959, -1.0425,
0:         -1.0811], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.7686,  0.7353,  0.6755,  0.6014,  0.5031,  0.3937,  0.2663,  0.1347,  0.0013, -0.1235, -0.2222, -0.2844,
0:         -0.3061, -0.2757, -0.1971, -0.0732,  0.0712,  0.2204,  0.7079,  0.6793,  0.6313,  0.5709,  0.4922,  0.4030,
0:          0.3030], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8923, 0.9466, 1.0798, 1.2610, 1.3454, 1.4234, 1.4101, 1.3135, 1.2860, 1.1923, 1.0809, 1.0864, 1.0912, 1.0985,
0:         1.1482, 1.1829, 1.2183, 1.1735, 0.9921, 1.0444, 1.1324, 1.3013, 1.3681, 1.3981, 1.3277], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5487, -0.6200, -0.6883, -0.7413, -0.7751, -0.7774, -0.7410, -0.6638, -0.5502, -0.4125, -0.2703, -0.1508,
0:         -0.0819, -0.0920, -0.1866, -0.3559, -0.5668, -0.7816, -0.9603, -1.0794, -1.1348, -1.1401, -1.1205, -1.1045,
0:         -1.1082], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.8747,  1.2009,  1.7541,  2.5124,  3.1087,  3.3454,  3.3366,  3.4299,  3.0031, -0.0618,  0.5272,  1.1720,
0:          1.8280,  2.2367,  2.7296,  2.6917,  2.6120,  2.4153, -0.1700, -0.0462,  0.4005,  1.0256,  1.3668,  1.5376,
0:          1.6123], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 3 [1/5 (20%)]	Loss: 2.14217 : 0.45101 :: 0.14930 (2.58 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 3 [2/5 (40%)]	Loss: 0.83828 : 0.25313 :: 0.14129 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 3 [3/5 (60%)]	Loss: 0.52494 : 0.19555 :: 0.14824 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 3 [4/5 (80%)]	Loss: 1.32959 : 0.39613 :: 0.16449 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.168, max = 0.060, mean = -0.042
0:          sample (first 20): tensor([ 0.0111, -0.0034, -0.0177, -0.0221, -0.0359, -0.0396, -0.0397, -0.0208, -0.0136,  0.0219,  0.0074, -0.0069,
0:         -0.0261, -0.0308, -0.0385, -0.0284, -0.0156, -0.0058,  0.0220,  0.0141])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 3 : 0.293117493391037
0: validation loss for velocity_u : 0.05179252848029137
0: validation loss for velocity_v : 0.07118501514196396
0: validation loss for specific_humidity : 0.03708004578948021
0: validation loss for velocity_z : 0.636652946472168
0: validation loss for temperature : 0.1347360759973526
0: validation loss for total_precip : 0.827258288860321
0: 4 : 20:43:27 :: batch_size = 96, lr = 1.903628792385485e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 4, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1845, 0.1697, 0.1557, 0.1420, 0.1293, 0.1189, 0.1115, 0.1078, 0.1063, 0.1067, 0.1087, 0.1120, 0.1160, 0.1208,
0:         0.1269, 0.1341, 0.1411, 0.1486, 0.2422, 0.2277, 0.2150, 0.2036, 0.1936, 0.1858, 0.1793], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0413, -1.0215, -0.9972, -0.9694, -0.9375, -0.9011, -0.8613, -0.8175, -0.7691, -0.7144, -0.6517, -0.5824,
0:         -0.5082, -0.4290, -0.3467, -0.2623, -0.1768, -0.0906, -1.0538, -1.0304, -1.0028, -0.9718, -0.9382, -0.9000,
0:         -0.8574], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7077, -0.7078, -0.7075, -0.7069, -0.7054, -0.7037, -0.7008, -0.6978, -0.6949, -0.6922, -0.6912, -0.6899,
0:         -0.6909, -0.6924, -0.6953, -0.6998, -0.7047, -0.7068, -0.7091, -0.7089, -0.7087, -0.7081, -0.7072, -0.7055,
0:         -0.7037], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0790, -0.0833, -0.1093, -0.1224, -0.1028, -0.0790,  0.0144,  0.1077,  0.1337,  0.1771,  0.1728,  0.1489,
0:          0.1836,  0.1793,  0.1684,  0.2140,  0.2270,  0.1945, -0.0811, -0.1007, -0.1332, -0.1289, -0.0941, -0.0377,
0:          0.0664], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.9024, 0.8682, 0.8318, 0.7932, 0.7534, 0.7165, 0.6845, 0.6587, 0.6383, 0.6208, 0.6063, 0.5937, 0.5823, 0.5739,
0:         0.5662, 0.5584, 0.5544, 0.5534, 0.5531, 0.5544, 0.5566, 0.5598, 0.5651, 0.5685, 0.5694], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  6.5360,     nan,     nan,
0:             nan,     nan,     nan,  6.6452,  6.7857,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1742,     nan,     nan,  1.9376,     nan,
0:             nan,     nan,     nan, -0.1452,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.2695,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, 11.1298,     nan,     nan,     nan,  3.4875,     nan,
0:             nan,     nan,     nan,     nan, 10.1709,     nan,     nan,     nan,     nan,     nan,  7.4413,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  4.0628,     nan,     nan,  3.7529,     nan,     nan,     nan,     nan,  8.3378,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  4.7073,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  8.3757,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  6.3709,     nan,     nan,     nan,     nan,
0:             nan,     nan,  9.9546,     nan,     nan,     nan,     nan,  1.5853,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.1424,     nan,  4.0182,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.3498,  0.9341,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  5.2091,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 4, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.6074, 1.5497, 1.5082, 1.4750, 1.4518, 1.4321, 1.4174, 1.3900, 1.3819, 1.3686, 1.3555, 1.3516, 1.3403, 1.3298,
0:         1.3061, 1.2722, 1.2276, 1.1642, 1.6098, 1.5314, 1.4594, 1.4033, 1.3615, 1.3441, 1.3431], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1472, -1.1359, -1.1325, -1.1278, -1.1117, -1.0748, -1.0320, -0.9780, -0.9156, -0.8500, -0.7918, -0.7352,
0:         -0.7074, -0.6919, -0.6938, -0.6830, -0.6534, -0.5921, -1.2212, -1.1822, -1.1620, -1.1382, -1.1033, -1.0642,
0:         -1.0095], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 2.4153,  2.4296,  2.4019,  2.3577,  2.2769,  2.1819,  2.0439,  1.8848,  1.7161,  1.5342,  1.3619,  1.1866,
0:          0.9910,  0.7502,  0.4823,  0.2034, -0.0451, -0.2416,  2.4317,  2.4778,  2.4784,  2.4393,  2.3728,  2.2720,
0:          2.1442], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-2.6949, -3.1626, -3.6848, -3.8338, -3.7571, -3.1760, -2.2770, -1.7274, -1.4069, -0.9513, -0.4484, -0.1418,
0:         -0.0175, -0.1031, -0.3400, -0.3697, -0.2350, -0.2906, -2.6165, -2.9943, -3.5613, -3.8189, -3.8598, -3.3730,
0:         -2.5303], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.8150, 0.8074, 0.7991, 0.7945, 0.7818, 0.7669, 0.7447, 0.7301, 0.7289, 0.7349, 0.7375, 0.7290, 0.7168, 0.7035,
0:         0.6963, 0.6888, 0.6735, 0.6562, 0.6405, 0.6383, 0.6629, 0.7046, 0.7516, 0.7908, 0.8053], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0193,  0.0049, -0.0046, -0.0075, -0.0195, -0.0270, -0.0265, -0.0157, -0.0114,  0.0315,  0.0176,  0.0074,
0:         -0.0091, -0.0163, -0.0277, -0.0176, -0.0109, -0.0047,  0.0341,  0.0282,  0.0105,  0.0044, -0.0114, -0.0163,
0:         -0.0140], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 4 [1/5 (20%)]	Loss: 0.77798 : 0.29995 :: 0.17290 (2.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 4 [2/5 (40%)]	Loss: 0.98665 : 0.36831 :: 0.18462 (15.37 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 4 [3/5 (60%)]	Loss: 1.38403 : 0.46555 :: 0.17930 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 4 [4/5 (80%)]	Loss: 0.76194 : 0.35919 :: 0.18530 (15.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.152, max = 0.123, mean = 0.002
0:          sample (first 20): tensor([ 0.0330,  0.0209,  0.0024, -0.0092, -0.0230, -0.0305, -0.0323, -0.0139, -0.0051,  0.0535,  0.0310,  0.0196,
0:         -0.0025, -0.0126, -0.0267, -0.0146, -0.0050,  0.0033,  0.0545,  0.0489])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 4 : 0.40257927775382996
0: validation loss for velocity_u : 0.08856867998838425
0: validation loss for velocity_v : 0.10600163787603378
0: validation loss for specific_humidity : 0.045195143669843674
0: validation loss for velocity_z : 0.7887449264526367
0: validation loss for temperature : 0.37511298060417175
0: validation loss for total_precip : 1.0118523836135864
0: 5 : 20:47:56 :: batch_size = 96, lr = 1.857198821839498e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 5, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4127, -0.3630, -0.3123, -0.2629, -0.2186, -0.1807, -0.1484, -0.1215, -0.0964, -0.0741, -0.0538, -0.0342,
0:         -0.0190, -0.0069,  0.0012,  0.0066,  0.0120,  0.0158, -0.3285, -0.2795, -0.2342, -0.1944, -0.1617, -0.1355,
0:         -0.1122], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1625, 0.2318, 0.2977, 0.3556, 0.4045, 0.4488, 0.4906, 0.5316, 0.5746, 0.6170, 0.6576, 0.6931, 0.7194, 0.7339,
0:         0.7367, 0.7308, 0.7175, 0.6992, 0.2452, 0.3154, 0.3754, 0.4263, 0.4688, 0.5077, 0.5473], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4842, -0.4431, -0.4166, -0.4069, -0.3986, -0.3895, -0.3870, -0.3744, -0.3603, -0.3511, -0.3455, -0.3416,
0:         -0.3462, -0.3550, -0.3698, -0.3851, -0.3992, -0.4031, -0.4835, -0.4559, -0.4409, -0.4321, -0.4147, -0.4056,
0:         -0.3851], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1648, -0.2379, -0.3221, -0.4084, -0.4992, -0.5945, -0.6122, -0.5967, -0.5590, -0.4904, -0.4417, -0.3708,
0:         -0.3088, -0.2357, -0.1537, -0.1670, -0.2135, -0.2445, -0.2800, -0.3309, -0.3996, -0.4727, -0.5059, -0.5524,
0:         -0.5413], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0801, -0.1227, -0.1532, -0.1806, -0.2029, -0.2243, -0.2491, -0.2714, -0.2926, -0.3118, -0.3261, -0.3363,
0:         -0.3352, -0.3195, -0.2937, -0.2575, -0.2200, -0.1857, -0.1550, -0.1309, -0.1155, -0.1116, -0.1160, -0.1250,
0:         -0.1300], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2412,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2056,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0040,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0040, -0.0704,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1440,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2294,     nan,     nan,
0:             nan,     nan, -0.1511, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1487,     nan,
0:             nan, -0.1962,     nan,     nan,     nan,     nan,     nan,     nan, -0.1985,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0609,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1938,     nan,     nan,     nan, -0.1001,     nan,     nan,     nan, -0.0621,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2306,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1914,     nan,     nan,     nan,     nan,
0:         -0.2294,     nan,     nan,     nan,     nan,     nan, -0.0206,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1582,     nan,     nan,     nan,     nan, -0.2412,     nan,     nan,     nan,     nan,
0:         -0.1273, -0.1131,     nan,     nan,     nan,     nan,     nan,     nan, -0.2009,     nan,     nan,     nan,
0:         -0.2294,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 5, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0897, -0.1213, -0.1379, -0.1552, -0.1548, -0.1359, -0.1077, -0.0930, -0.0752, -0.0821, -0.0933, -0.0848,
0:         -0.0737, -0.0315,  0.0035,  0.0328,  0.0208, -0.0242, -0.1114, -0.1242, -0.1346, -0.1469, -0.1472, -0.1359,
0:         -0.1053], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2378, -0.2020, -0.1672, -0.1412, -0.1007, -0.0489, -0.0117,  0.0054, -0.0046, -0.0220, -0.0343, -0.0246,
0:         -0.0007,  0.0344,  0.0665,  0.1135,  0.1802,  0.2772, -0.2470, -0.1865, -0.1541, -0.1479, -0.1357, -0.1180,
0:         -0.0988], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2625, -0.2802, -0.3054, -0.3319, -0.3608, -0.3783, -0.3871, -0.3810, -0.3761, -0.3764, -0.3851, -0.3965,
0:         -0.4108, -0.4228, -0.4409, -0.4557, -0.4729, -0.4758, -0.2840, -0.2967, -0.3193, -0.3502, -0.3758, -0.3943,
0:         -0.3916], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0834,  0.0258, -0.0455, -0.0966, -0.1603, -0.1302, -0.0584, -0.0457, -0.0081,  0.0118, -0.0358, -0.0706,
0:         -0.0599, -0.0004, -0.0171, -0.0342,  0.0215, -0.1042,  0.0561,  0.0349, -0.0057, -0.0327, -0.0812, -0.0782,
0:         -0.0501], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0336,  0.0150,  0.0031, -0.0068, -0.0094, -0.0038, -0.0011, -0.0080, -0.0202, -0.0347, -0.0493, -0.0521,
0:         -0.0438, -0.0313, -0.0219, -0.0274, -0.0534, -0.0872, -0.1174, -0.1352, -0.1318, -0.1164, -0.0986, -0.0860,
0:         -0.0956], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0390,  0.0289,  0.0175,  0.0055, -0.0038, -0.0148, -0.0163, -0.0020,  0.0084,  0.0585,  0.0422,  0.0314,
0:          0.0146,  0.0032, -0.0082, -0.0004,  0.0092,  0.0140,  0.0616,  0.0566,  0.0424,  0.0352,  0.0193,  0.0126,
0:          0.0191], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 5 [1/5 (20%)]	Loss: 0.96776 : 0.36189 :: 0.18955 (2.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 5 [2/5 (40%)]	Loss: 1.47991 : 0.49698 :: 0.19516 (15.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 5 [3/5 (60%)]	Loss: 0.92140 : 0.41698 :: 0.20273 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 5 [4/5 (80%)]	Loss: 1.09524 : 0.49650 :: 0.21380 (15.23 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.094, max = 0.141, mean = 0.038
0:          sample (first 20): tensor([ 0.0699,  0.0618,  0.0453,  0.0245,  0.0152,  0.0037, -0.0076,  0.0055,  0.0139,  0.0800,  0.0626,  0.0616,
0:          0.0348,  0.0176,  0.0026,  0.0058,  0.0079,  0.0109,  0.0832,  0.0746])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 5 : 0.4096444845199585
0: validation loss for velocity_u : 0.14162535965442657
0: validation loss for velocity_v : 0.12253883481025696
0: validation loss for specific_humidity : 0.057331595569849014
0: validation loss for velocity_z : 0.6435633301734924
0: validation loss for temperature : 0.422003835439682
0: validation loss for total_precip : 1.0708041191101074
0: 6 : 20:51:58 :: batch_size = 96, lr = 1.8119012895995104e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 6, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3051, 0.3047, 0.3054, 0.3062, 0.3059, 0.3035, 0.2987, 0.2914, 0.2821, 0.2721, 0.2619, 0.2520, 0.2426, 0.2329,
0:         0.2212, 0.2075, 0.1916, 0.1723, 0.3173, 0.3240, 0.3294, 0.3334, 0.3352, 0.3344, 0.3309], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1398, 0.1659, 0.1939, 0.2227, 0.2556, 0.2925, 0.3292, 0.3664, 0.4048, 0.4413, 0.4759, 0.5124, 0.5479, 0.5794,
0:         0.6079, 0.6334, 0.6511, 0.6616, 0.1628, 0.1811, 0.2039, 0.2287, 0.2567, 0.2888, 0.3217], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7171, 0.7121, 0.7034, 0.6942, 0.6891, 0.6804, 0.6737, 0.6782, 0.6773, 0.6796, 0.6928, 0.7010, 0.7126, 0.7247,
0:         0.7328, 0.7366, 0.7388, 0.7422, 0.8109, 0.7977, 0.7872, 0.7770, 0.7693, 0.7568, 0.7441], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2292, 0.2954, 0.3440, 0.3970, 0.3860, 0.3285, 0.3109, 0.2756, 0.1828, 0.1276, 0.1232, 0.1011, 0.1188, 0.2049,
0:         0.2601, 0.2932, 0.3550, 0.3639, 0.1806, 0.2535, 0.3021, 0.3263, 0.2822, 0.2137, 0.2027], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.7142, -0.6952, -0.6756, -0.6566, -0.6363, -0.6156, -0.5970, -0.5797, -0.5644, -0.5550, -0.5490, -0.5444,
0:         -0.5411, -0.5374, -0.5309, -0.5213, -0.5094, -0.4919, -0.4707, -0.4497, -0.4285, -0.4050, -0.3812, -0.3588,
0:         -0.3375], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan, -0.1611,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2071,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2152,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1979,     nan, -0.2382, -0.2382,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2382,     nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2347,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2382,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan, -0.2382,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 6, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0729, -0.1372, -0.2198, -0.2856, -0.3204, -0.3175, -0.2987, -0.2832, -0.2828, -0.3085, -0.3230, -0.3181,
0:         -0.2802, -0.2325, -0.2007, -0.1937, -0.2258, -0.2745, -0.0415, -0.0771, -0.1479, -0.2256, -0.2790, -0.2886,
0:         -0.2806], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7526, 0.8132, 0.8623, 0.9026, 0.9840, 1.0887, 1.1616, 1.1875, 1.1504, 1.0978, 1.0424, 1.0159, 0.9727, 0.9058,
0:         0.8095, 0.7211, 0.6912, 0.7359, 0.7106, 0.7852, 0.8308, 0.8612, 0.9102, 0.9675, 0.9970], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0764, -0.0925, -0.1130, -0.1269, -0.1302, -0.1216, -0.1061, -0.0883, -0.0854, -0.0965, -0.1257, -0.1638,
0:         -0.2143, -0.2772, -0.3438, -0.4093, -0.4741, -0.5204, -0.1316, -0.1600, -0.1899, -0.2143, -0.2193, -0.2150,
0:         -0.1930], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1120,  0.0939,  0.0260, -0.0362, -0.1724, -0.1580, -0.0641, -0.0655, -0.0904, -0.1723, -0.2213, -0.2005,
0:         -0.1545, -0.0428, -0.0465, -0.0353,  0.0336, -0.1580,  0.0544,  0.0434, -0.0064,  0.0009, -0.0690, -0.0258,
0:          0.0542], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1703, -0.1709, -0.1356, -0.1018, -0.0859, -0.1084, -0.1429, -0.1606, -0.1379, -0.0757, -0.0054,  0.0448,
0:          0.0546,  0.0443,  0.0087, -0.0221, -0.0503, -0.0645, -0.0744, -0.0778, -0.0815, -0.0805, -0.0650, -0.0359,
0:         -0.0093], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0442,  0.0335,  0.0167, -0.0043, -0.0085, -0.0228, -0.0247, -0.0086,  0.0014,  0.0558,  0.0328,  0.0359,
0:          0.0051, -0.0071, -0.0206, -0.0095, -0.0062,  0.0029,  0.0575,  0.0458,  0.0347,  0.0228,  0.0062, -0.0054,
0:          0.0035], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 6 [1/5 (20%)]	Loss: 0.94350 : 0.41526 :: 0.20363 (2.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 6 [2/5 (40%)]	Loss: 0.70161 : 0.33353 :: 0.20008 (15.40 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 6 [3/5 (60%)]	Loss: 0.71344 : 0.32120 :: 0.20480 (15.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 6 [4/5 (80%)]	Loss: 1.05301 : 0.40157 :: 0.20520 (15.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.097, max = 0.142, mean = 0.020
0:          sample (first 20): tensor([ 0.0371,  0.0290,  0.0110, -0.0074, -0.0089, -0.0178, -0.0215, -0.0050,  0.0084,  0.0300,  0.0214,  0.0333,
0:          0.0040, -0.0097, -0.0178, -0.0077, -0.0043,  0.0088,  0.0363,  0.0275])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 6 : 0.43958497047424316
0: validation loss for velocity_u : 0.15971660614013672
0: validation loss for velocity_v : 0.1308295875787735
0: validation loss for specific_humidity : 0.058373335748910904
0: validation loss for velocity_z : 0.7123453617095947
0: validation loss for temperature : 0.3712119162082672
0: validation loss for total_precip : 1.2050327062606812
0: 7 : 20:56:00 :: batch_size = 96, lr = 1.7677085752190346e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 7, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2218, 0.2097, 0.1931, 0.2086, 0.2484, 0.2736, 0.2608, 0.2259, 0.2023, 0.2046, 0.2130, 0.2118, 0.2186, 0.2196,
0:         0.1717, 0.1130, 0.1067, 0.1204, 0.2241, 0.2319, 0.2164, 0.2062, 0.2276, 0.2630, 0.2777], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.1765, -2.0443, -1.9218, -1.8060, -1.6842, -1.5650, -1.4762, -1.4173, -1.3483, -1.2774, -1.2357, -1.2000,
0:         -1.1557, -1.1478, -1.1848, -1.1963, -1.1482, -1.0521, -2.2728, -2.1121, -1.9163, -1.7445, -1.6346, -1.5615,
0:         -1.4949], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6136, -0.5824, -0.5611, -0.5617, -0.5655, -0.5573, -0.5233, -0.4784, -0.4512, -0.4411, -0.4025, -0.3574,
0:         -0.3227, -0.3065, -0.3608, -0.4408, -0.5233, -0.5851, -0.6300, -0.5944, -0.5530, -0.5248, -0.5128, -0.5229,
0:         -0.5204], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.1019e+00,  1.7649e+00,  1.7340e+00,  1.1836e+00,  1.1063e+00,  1.1615e+00,  8.6317e-01,  9.2947e-01,
0:          1.0112e+00,  4.6534e-01, -9.9918e-04,  1.0509e-01,  3.6810e-01, -1.8680e-02, -6.3752e-01, -3.1042e-01,
0:          2.3312e-02, -3.4578e-01,  4.0125e-01,  1.0157e+00,  1.3494e+00,  8.9853e-01,  4.7860e-01,  3.1284e-01,
0:          4.0125e-01], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.1669, -1.2628, -1.3909, -1.5019, -1.5712, -1.5967, -1.5831, -1.5562, -1.5555, -1.5727, -1.5695, -1.5552,
0:         -1.5513, -1.5124, -1.3966, -1.2708, -1.2297, -1.2262, -1.2116, -1.2242, -1.2726, -1.3348, -1.4016, -1.4681,
0:         -1.5294], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,
0:         -0.2366,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan, -0.2366,
0:             nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2366,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan, -0.2366,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,
0:             nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2366,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2366,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 7, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6443, 0.5922, 0.5488, 0.5262, 0.5227, 0.5305, 0.5342, 0.5195, 0.4958, 0.4632, 0.4482, 0.4521, 0.4747, 0.5010,
0:         0.4977, 0.4776, 0.4441, 0.4025, 0.6149, 0.5763, 0.5358, 0.5086, 0.5019, 0.5204, 0.5458], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8839, -0.8897, -0.8898, -0.9002, -0.8969, -0.8764, -0.8678, -0.8729, -0.9022, -0.9606, -1.0268, -1.0804,
0:         -1.1287, -1.1508, -1.1555, -1.1484, -1.1376, -1.1183, -0.8833, -0.8583, -0.8676, -0.8963, -0.9243, -0.9352,
0:         -0.9333], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4768, -0.4936, -0.5035, -0.5087, -0.5034, -0.4900, -0.4723, -0.4509, -0.4430, -0.4398, -0.4518, -0.4568,
0:         -0.4593, -0.4570, -0.4542, -0.4531, -0.4667, -0.4827, -0.4888, -0.5069, -0.5198, -0.5304, -0.5312, -0.5252,
0:         -0.5067], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1773,  0.2020,  0.1349,  0.1276,  0.0539,  0.0692,  0.1356,  0.1138,  0.1302,  0.0476, -0.0803, -0.0212,
0:          0.1485,  0.2813,  0.1949,  0.2548,  0.4161,  0.1305,  0.1929,  0.2173,  0.1490,  0.1433,  0.0817,  0.1218,
0:          0.2101], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1173, -0.0971, -0.0598, -0.0590, -0.0874, -0.1436, -0.1815, -0.1689, -0.1140, -0.0315,  0.0402,  0.0655,
0:          0.0373,  0.0016, -0.0584, -0.0962, -0.1153, -0.1072, -0.0902, -0.0733, -0.0704, -0.0772, -0.0731, -0.0537,
0:         -0.0300], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0001, -0.0095, -0.0226, -0.0415, -0.0376, -0.0500, -0.0534, -0.0404, -0.0335, -0.0054, -0.0135, -0.0023,
0:         -0.0270, -0.0411, -0.0466, -0.0407, -0.0405, -0.0340,  0.0027, -0.0053, -0.0059, -0.0122, -0.0216, -0.0304,
0:         -0.0222], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 7 [1/5 (20%)]	Loss: 1.21332 : 0.47583 :: 0.21899 (2.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 7 [2/5 (40%)]	Loss: 1.26588 : 0.44660 :: 0.21026 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 7 [3/5 (60%)]	Loss: 1.06063 : 0.46271 :: 0.22125 (15.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 7 [4/5 (80%)]	Loss: 1.55349 : 0.51783 :: 0.21971 (15.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.069, max = 0.084, mean = 0.005
0:          sample (first 20): tensor([ 0.0079,  0.0070, -0.0025, -0.0136, -0.0058, -0.0124, -0.0226, -0.0169,  0.0015, -0.0062, -0.0069,  0.0146,
0:         -0.0088, -0.0246, -0.0273, -0.0239, -0.0255, -0.0153, -0.0085, -0.0169])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 7 : 0.48814690113067627
0: validation loss for velocity_u : 0.15953202545642853
0: validation loss for velocity_v : 0.15241818130016327
0: validation loss for specific_humidity : 0.05109000205993652
0: validation loss for velocity_z : 0.9942426085472107
0: validation loss for temperature : 0.4068460166454315
0: validation loss for total_precip : 1.1647528409957886
0: 8 : 20:59:56 :: batch_size = 96, lr = 1.7245937319210094e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 8, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.2013, 2.1378, 2.0635, 1.9852, 1.8964, 1.7845, 1.6462, 1.4949, 1.3500, 1.2248, 1.1228, 1.0437, 0.9882, 0.9566,
0:         0.9451, 0.9493, 0.9643, 0.9842, 2.1600, 2.1124, 2.0424, 1.9577, 1.8585, 1.7374, 1.5933], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7830, 0.6736, 0.5629, 0.4633, 0.3850, 0.3366, 0.3240, 0.3515, 0.4177, 0.5106, 0.6059, 0.6764, 0.7064, 0.6971,
0:         0.6617, 0.6159, 0.5701, 0.5304, 0.6929, 0.5968, 0.4924, 0.3923, 0.3159, 0.2793, 0.2864], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6409, -0.5699, -0.4500, -0.3532, -0.2117, -0.0603,  0.0427,  0.1741,  0.2043,  0.1950,  0.1226,  0.0787,
0:          0.0920,  0.0848,  0.1999,  0.3140,  0.4249,  0.5571, -0.4659, -0.3759, -0.2461, -0.1002,  0.0258,  0.1685,
0:          0.2381], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.8612,  1.9762,  0.6872, -1.5112, -2.6917, -2.1926, -1.2399, -1.0858, -1.6197, -2.0689, -2.2273, -2.4139,
0:         -2.7069, -2.8154, -2.6092, -2.2512, -1.8237, -1.2312,  3.6689,  3.8143,  2.3343, -0.3588, -2.1861, -2.1079,
0:         -1.1227], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([2.1087, 2.1745, 2.2249, 2.2545, 2.2704, 2.2833, 2.2970, 2.3077, 2.3109, 2.3056, 2.2912, 2.2657, 2.2272, 2.1792,
0:         2.1276, 2.0780, 2.0363, 2.0082, 1.9968, 1.9961, 1.9949, 1.9917, 1.9980, 2.0252, 2.0657], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2456,     nan,     nan,     nan,     nan, -0.2456,     nan, -0.2456,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2456,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1597,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1586,     nan, -0.2088,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1832,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2456,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2456,     nan, -0.2456,     nan,
0:             nan, -0.2266,     nan,     nan,     nan,     nan,     nan,     nan, -0.2456,     nan,     nan, -0.2378,
0:             nan, -0.2434,     nan,     nan,     nan,     nan, -0.2188,     nan, -0.2121,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1832,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2456,     nan, -0.2456,
0:             nan,     nan,     nan,     nan,     nan, -0.2456,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2289,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2456,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2333,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2422,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2411,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2445,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 8, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8254, 0.8263, 0.8451, 0.8738, 0.8975, 0.9035, 0.8887, 0.8428, 0.7830, 0.6962, 0.6064, 0.5335, 0.4913, 0.4866,
0:         0.4964, 0.5202, 0.5426, 0.5202, 0.8596, 0.8518, 0.8535, 0.8689, 0.8832, 0.9088, 0.9341], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0854, -1.0982, -1.1120, -1.1220, -1.1173, -1.0891, -1.0727, -1.0704, -1.0760, -1.0866, -1.0855, -1.0616,
0:         -1.0326, -0.9988, -0.9599, -0.8951, -0.8030, -0.6706, -1.1073, -1.1098, -1.1428, -1.1898, -1.2093, -1.2222,
0:         -1.2098], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3875, -0.4658, -0.5348, -0.5840, -0.6207, -0.6469, -0.6742, -0.6829, -0.6912, -0.6929, -0.6913, -0.6754,
0:         -0.6591, -0.6427, -0.6323, -0.6260, -0.6328, -0.6420, -0.2908, -0.3785, -0.4523, -0.5194, -0.5721, -0.6262,
0:         -0.6585], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1107,  0.1334,  0.0314,  0.0022, -0.0100,  0.0538,  0.1798,  0.2019,  0.2015,  0.2216,  0.2009,  0.2671,
0:          0.4659,  0.6749,  0.7042,  0.7116,  0.7724,  0.6632, -0.0663, -0.0448, -0.1386, -0.1198, -0.0803, -0.0314,
0:          0.0738], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0824, -0.0667,  0.0039,  0.0858,  0.1349,  0.1147,  0.0516, -0.0195, -0.0445, -0.0326, -0.0189, -0.0238,
0:         -0.0496, -0.0558, -0.0440, -0.0092,  0.0186,  0.0283,  0.0071, -0.0397, -0.0903, -0.1338, -0.1500, -0.1371,
0:         -0.1315], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0045,  0.0014, -0.0097, -0.0208, -0.0151, -0.0201, -0.0306, -0.0234, -0.0054, -0.0051, -0.0068,  0.0142,
0:         -0.0079, -0.0239, -0.0281, -0.0242, -0.0257, -0.0171, -0.0041, -0.0114, -0.0016, -0.0013, -0.0051, -0.0177,
0:         -0.0178], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 8 [1/5 (20%)]	Loss: 0.87885 : 0.39545 :: 0.22710 (2.73 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 8 [2/5 (40%)]	Loss: 0.66776 : 0.32220 :: 0.22189 (15.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 8 [3/5 (60%)]	Loss: 1.32930 : 0.46131 :: 0.22787 (15.46 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 8 [4/5 (80%)]	Loss: 1.88791 : 0.60402 :: 0.23163 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.084, max = 0.103, mean = 0.005
0:          sample (first 20): tensor([ 0.0130,  0.0138,  0.0084,  0.0007,  0.0085,  0.0064, -0.0082, -0.0040,  0.0191, -0.0006, -0.0002,  0.0236,
0:          0.0052, -0.0109, -0.0133, -0.0114, -0.0139,  0.0014, -0.0103, -0.0177])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 8 : 0.4400717616081238
0: validation loss for velocity_u : 0.16021685302257538
0: validation loss for velocity_v : 0.17144423723220825
0: validation loss for specific_humidity : 0.04212946817278862
0: validation loss for velocity_z : 0.8607649207115173
0: validation loss for temperature : 0.33613041043281555
0: validation loss for total_precip : 1.0697449445724487
0: 9 : 21:03:48 :: batch_size = 96, lr = 1.6825304701668386e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 9, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5220, -0.5328, -0.5434, -0.5539, -0.5640, -0.5740, -0.5837, -0.5930, -0.6021, -0.6104, -0.6184, -0.6258,
0:         -0.6330, -0.6401, -0.6472, -0.6543, -0.6612, -0.6678, -0.4485, -0.4596, -0.4709, -0.4821, -0.4932, -0.5040,
0:         -0.5144], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4704, -0.4602, -0.4505, -0.4413, -0.4329, -0.4248, -0.4174, -0.4103, -0.4037, -0.3977, -0.3928, -0.3889,
0:         -0.3862, -0.3844, -0.3836, -0.3836, -0.3846, -0.3866, -0.5197, -0.5101, -0.5000, -0.4902, -0.4803, -0.4706,
0:         -0.4614], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6366, -0.6327, -0.6286, -0.6246, -0.6209, -0.6174, -0.6155, -0.6143, -0.6125, -0.6099, -0.6077, -0.6069,
0:         -0.6060, -0.6067, -0.6083, -0.6100, -0.6101, -0.6102, -0.6508, -0.6473, -0.6431, -0.6385, -0.6339, -0.6299,
0:         -0.6261], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3080, 0.3102, 0.3014, 0.2838, 0.2685, 0.2619, 0.2685, 0.2860, 0.3036, 0.3167, 0.3211, 0.3167, 0.3102, 0.3080,
0:         0.3123, 0.3211, 0.3255, 0.3233, 0.3365, 0.3453, 0.3453, 0.3365, 0.3277, 0.3233, 0.3277], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.9212, -0.9278, -0.9340, -0.9394, -0.9442, -0.9485, -0.9520, -0.9548, -0.9568, -0.9578, -0.9579, -0.9573,
0:         -0.9558, -0.9534, -0.9510, -0.9479, -0.9446, -0.9407, -0.9364, -0.9321, -0.9275, -0.9233, -0.9192, -0.9156,
0:         -0.9120], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2456,     nan, -0.2433,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2433,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2456,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2480,     nan, -0.2421,     nan,     nan,     nan,     nan,     nan, -0.2468,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2386,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2468,     nan,
0:             nan,     nan,     nan,     nan, -0.2456,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2456,     nan, -0.2398,     nan,     nan,     nan,     nan,     nan,     nan, -0.2468,
0:             nan,     nan,     nan,     nan,     nan, -0.2433, -0.2445, -0.2456,     nan,     nan,     nan, -0.2410,
0:             nan,     nan,     nan,     nan, -0.2445,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 9, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0729, -0.0931, -0.1053, -0.1202, -0.1307, -0.1369, -0.1501, -0.1808, -0.2047, -0.2466, -0.2791, -0.2750,
0:         -0.2474, -0.1966, -0.1579, -0.1334, -0.1474, -0.1933, -0.1128, -0.1193, -0.1385, -0.1591, -0.1763, -0.1818,
0:         -0.1777], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1096, -0.0963, -0.0835, -0.0824, -0.0616, -0.0173,  0.0059,  0.0033, -0.0278, -0.0766, -0.1229, -0.1338,
0:         -0.1338, -0.1194, -0.1074, -0.0889, -0.0512,  0.0212, -0.1436, -0.1071, -0.0981, -0.1142, -0.1235, -0.1277,
0:         -0.1200], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5728, -0.6036, -0.6348, -0.6577, -0.6666, -0.6700, -0.6557, -0.6317, -0.6139, -0.6037, -0.6075, -0.6074,
0:         -0.6149, -0.6237, -0.6418, -0.6645, -0.7050, -0.7405, -0.6145, -0.6459, -0.6774, -0.6964, -0.7028, -0.7015,
0:         -0.6826], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0141,  0.0803,  0.0143,  0.0092, -0.0484, -0.1044, -0.0665, -0.0966, -0.1041, -0.0873, -0.1485, -0.1502,
0:         -0.0449,  0.0524,  0.0437,  0.0654,  0.0985, -0.0546, -0.0868, -0.0339, -0.0687,  0.0004, -0.0261, -0.0613,
0:         -0.0070], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.2531, -0.2067, -0.1293, -0.0753, -0.0503, -0.0770, -0.1124, -0.1273, -0.1068, -0.0604, -0.0207, -0.0101,
0:         -0.0360, -0.0528, -0.0859, -0.1024, -0.1277, -0.1510, -0.1775, -0.1956, -0.2020, -0.1926, -0.1579, -0.1036,
0:         -0.0511], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0466, -0.0462, -0.0480, -0.0508, -0.0382, -0.0373, -0.0525, -0.0457, -0.0244, -0.0611, -0.0598, -0.0355,
0:         -0.0487, -0.0561, -0.0568, -0.0517, -0.0590, -0.0472, -0.0700, -0.0772, -0.0626, -0.0603, -0.0456, -0.0601,
0:         -0.0612], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 9 [1/5 (20%)]	Loss: 1.00272 : 0.41991 :: 0.23174 (2.49 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 9 [2/5 (40%)]	Loss: 1.04444 : 0.43902 :: 0.22948 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 9 [3/5 (60%)]	Loss: 1.03065 : 0.42844 :: 0.23384 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 9 [4/5 (80%)]	Loss: 1.26849 : 0.45316 :: 0.22613 (15.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.115, max = 0.076, mean = -0.023
0:          sample (first 20): tensor([-0.0144, -0.0113, -0.0072, -0.0045,  0.0062,  0.0077, -0.0061, -0.0091,  0.0106, -0.0315, -0.0234, -0.0019,
0:         -0.0061, -0.0139, -0.0188, -0.0149, -0.0246, -0.0107, -0.0450, -0.0417])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 9 : 0.4053158164024353
0: validation loss for velocity_u : 0.1448308378458023
0: validation loss for velocity_v : 0.1329612284898758
0: validation loss for specific_humidity : 0.04313640668988228
0: validation loss for velocity_z : 0.8511003851890564
0: validation loss for temperature : 0.41638633608818054
0: validation loss for total_precip : 0.8434802889823914
0: 10 : 21:07:51 :: batch_size = 96, lr = 1.6414931416261842e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 10, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6647, -0.6517, -0.6484, -0.6535, -0.6589, -0.6694, -0.6887, -0.7109, -0.7253, -0.7294, -0.7266, -0.7274,
0:         -0.7358, -0.7443, -0.7445, -0.7346, -0.7224, -0.7107, -0.6351, -0.6292, -0.6342, -0.6455, -0.6504, -0.6563,
0:         -0.6722], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1493, -0.1308, -0.1197, -0.1226, -0.1421, -0.1699, -0.1958, -0.2159, -0.2231, -0.2200, -0.2126, -0.2102,
0:         -0.2155, -0.2239, -0.2281, -0.2215, -0.2071, -0.1867, -0.1454, -0.1415, -0.1419, -0.1526, -0.1744, -0.2005,
0:         -0.2250], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 2.5208e-01,  1.4186e-01,  7.6836e-02, -5.4025e-05, -2.2975e-01, -2.2220e-01, -1.7422e-01, -5.3004e-02,
0:         -5.3435e-02, -1.6861e-01, -2.5240e-01, -3.7803e-01, -3.5819e-01, -3.1559e-01, -2.6016e-01, -2.5466e-01,
0:         -2.5909e-01, -2.5833e-01,  1.1161e-02, -1.0099e-01, -1.8036e-01, -2.2415e-01, -3.4363e-01, -3.5722e-01,
0:         -3.4902e-01], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0420,  0.0044, -0.0791, -0.1649, -0.2378, -0.2603, -0.2699, -0.2613, -0.2035, -0.1499, -0.0824, -0.0063,
0:          0.0023,  0.0162,  0.0162,  0.0730,  0.1684,  0.2413,  0.0205,  0.0827,  0.1138,  0.1298,  0.0730,  0.0130,
0:         -0.0138], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.1786,  0.1468,  0.1563,  0.1992,  0.2480,  0.2531,  0.2116,  0.1118, -0.0047, -0.0938, -0.1639, -0.2234,
0:         -0.2954, -0.3529, -0.3798, -0.3760, -0.3436, -0.3154, -0.2790, -0.2288, -0.1654, -0.0827, -0.0057,  0.0562,
0:          0.1082], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 10, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2461,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2426,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2461, -0.2461,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2450,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2010,     nan,     nan,     nan,     nan,     nan, -0.2195,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2461,     nan,     nan,
0:             nan, -0.2461,     nan,     nan,     nan,     nan, -0.2438,     nan, -0.2288, -0.2045,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1952,     nan,     nan,     nan,     nan, -0.2461,     nan,
0:             nan,     nan,     nan, -0.1987,     nan,     nan, -0.2450,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2033,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2461,     nan, -0.2450,     nan,     nan,     nan,     nan,
0:         -0.2438,     nan,     nan,     nan,     nan,     nan, -0.2461, -0.2415, -0.2426,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,
0:             nan,     nan,     nan, -0.1373,     nan, -0.1640,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2461,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2126,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 10, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4687, -0.4485, -0.5081, -0.6030, -0.6811, -0.7075, -0.7004, -0.7046, -0.6855, -0.7105, -0.7240, -0.6705,
0:         -0.5966, -0.5052, -0.4455, -0.4512, -0.5265, -0.6248, -0.4608, -0.4124, -0.4491, -0.5546, -0.6498, -0.6959,
0:         -0.6984], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1116,  0.1489,  0.1557,  0.1391,  0.1463,  0.1906,  0.2008,  0.1640,  0.0798, -0.0233, -0.1011, -0.1269,
0:         -0.1433, -0.1543, -0.2014, -0.2346, -0.2404, -0.1830,  0.0342,  0.0850,  0.0831,  0.0363,  0.0135,  0.0137,
0:          0.0199], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2449, -0.2802, -0.3006, -0.3057, -0.2908, -0.2503, -0.1998, -0.1552, -0.1448, -0.1650, -0.2101, -0.2533,
0:         -0.2814, -0.2915, -0.2960, -0.2960, -0.3170, -0.3396, -0.3639, -0.4032, -0.4364, -0.4593, -0.4496, -0.4134,
0:         -0.3531], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1742, 0.1386, 0.0821, 0.0827, 0.0200, 0.0404, 0.1214, 0.1109, 0.1087, 0.0859, 0.0268, 0.0656, 0.1749, 0.2827,
0:         0.2024, 0.1412, 0.2145, 0.0419, 0.1493, 0.1045, 0.0334, 0.0438, 0.0031, 0.0308, 0.0927], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.2413, -0.2296, -0.1766, -0.1308, -0.0997, -0.1002, -0.1125, -0.1154, -0.1010, -0.0806, -0.0813, -0.1081,
0:         -0.1510, -0.1766, -0.1917, -0.1885, -0.1885, -0.1865, -0.1786, -0.1575, -0.1135, -0.0596, -0.0046,  0.0349,
0:          0.0445], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0224, -0.0223, -0.0164, -0.0147, -0.0079, -0.0110, -0.0226, -0.0262, -0.0074, -0.0431, -0.0346, -0.0120,
0:         -0.0206, -0.0323, -0.0401, -0.0355, -0.0417, -0.0251, -0.0571, -0.0568, -0.0458, -0.0406, -0.0214, -0.0325,
0:         -0.0447], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 10 [1/5 (20%)]	Loss: 1.07500 : 0.46125 :: 0.22916 (2.81 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 10 [2/5 (40%)]	Loss: 1.70377 : 0.58666 :: 0.24042 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 10 [3/5 (60%)]	Loss: 1.16208 : 0.45751 :: 0.23650 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 10 [4/5 (80%)]	Loss: 1.18932 : 0.46280 :: 0.22780 (15.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.163, max = 0.308, mean = -0.023
0:          sample (first 20): tensor([-3.5981e-02, -3.1186e-02, -1.9536e-02, -1.2485e-02,  9.8759e-04,  6.9878e-05, -1.0809e-02, -2.1033e-02,
0:         -1.4466e-02, -5.8470e-02, -4.3194e-02, -2.9260e-02, -2.3106e-02, -2.4169e-02, -3.2826e-02, -3.2144e-02,
0:         -4.7777e-02, -4.3529e-02, -7.3169e-02, -6.3736e-02])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 10 : 0.4309636354446411
0: validation loss for velocity_u : 0.18383115530014038
0: validation loss for velocity_v : 0.15416964888572693
0: validation loss for specific_humidity : 0.04285421594977379
0: validation loss for velocity_z : 0.7566190361976624
0: validation loss for temperature : 0.41918230056762695
0: validation loss for total_precip : 1.0291261672973633
0: 11 : 21:11:45 :: batch_size = 96, lr = 1.601456723537741e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 11, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.8718, 1.8475, 1.8242, 1.8020, 1.7806, 1.7592, 1.7371, 1.7137, 1.6880, 1.6603, 1.6304, 1.5992, 1.5681, 1.5385,
0:         1.5119, 1.4889, 1.4701, 1.4553, 1.9024, 1.8759, 1.8492, 1.8229, 1.7965, 1.7700, 1.7427], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2727,  0.2539,  0.2371,  0.2222,  0.2093,  0.1982,  0.1882,  0.1780,  0.1665,  0.1519,  0.1329,  0.1080,
0:          0.0762,  0.0371, -0.0085, -0.0589, -0.1120, -0.1650,  0.2166,  0.1940,  0.1726,  0.1526,  0.1340,  0.1169,
0:          0.1007], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6033, -0.6040, -0.6052, -0.6068, -0.6085, -0.6086, -0.6087, -0.6086, -0.6068, -0.6050, -0.6016, -0.5953,
0:         -0.5893, -0.5831, -0.5746, -0.5652, -0.5561, -0.5471, -0.6137, -0.6155, -0.6160, -0.6164, -0.6181, -0.6182,
0:         -0.6177], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7762, 0.7560, 0.7279, 0.6920, 0.6426, 0.5752, 0.4876, 0.3798, 0.2630, 0.1541, 0.0721, 0.0283, 0.0227, 0.0474,
0:         0.0867, 0.1327, 0.1754, 0.2102, 0.6594, 0.6707, 0.6695, 0.6426, 0.5853, 0.5033, 0.4056], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.2876, 0.2945, 0.3020, 0.3094, 0.3163, 0.3220, 0.3264, 0.3286, 0.3289, 0.3263, 0.3214, 0.3139, 0.3046, 0.2941,
0:         0.2827, 0.2716, 0.2615, 0.2536, 0.2490, 0.2487, 0.2536, 0.2630, 0.2766, 0.2925, 0.3095], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 11, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1683,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.2581,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.0797,  0.0843,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1069,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1780,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2396,
0:             nan,     nan,     nan,     nan,     nan,  0.1984,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0676,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0136,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0937,     nan,     nan,     nan,     nan,     nan,     nan, -0.1644, -0.1632,
0:             nan,     nan,     nan, -0.1597,     nan,     nan,     nan, -0.1726,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1196,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0396,     nan,     nan, -0.0136,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0782,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0370,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0125,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1043, -0.1903,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 11, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.5166, 1.5876, 1.6660, 1.7190, 1.7290, 1.7034, 1.6483, 1.5603, 1.4943, 1.4317, 1.3991, 1.4061, 1.4167, 1.4556,
0:         1.4748, 1.5094, 1.5743, 1.6526, 1.4492, 1.5086, 1.5618, 1.5991, 1.6141, 1.6198, 1.6158], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7649, -0.7690, -0.7774, -0.7960, -0.8005, -0.7897, -0.8025, -0.8365, -0.9025, -0.9849, -1.0503, -1.0837,
0:         -1.1198, -1.1451, -1.1748, -1.1655, -1.1200, -1.0082, -0.7686, -0.7441, -0.7568, -0.7939, -0.8284, -0.8518,
0:         -0.8684], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3591, -0.3787, -0.3874, -0.3859, -0.3725, -0.3536, -0.3382, -0.3172, -0.3136, -0.3132, -0.3229, -0.3229,
0:         -0.3337, -0.3425, -0.3673, -0.4000, -0.4466, -0.4845, -0.3710, -0.3934, -0.4109, -0.4075, -0.3946, -0.3806,
0:         -0.3558], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1197,  0.0726, -0.0247, -0.0672, -0.2088, -0.1944, -0.0964, -0.1057, -0.1326, -0.1946, -0.2450, -0.2245,
0:         -0.1333, -0.0800, -0.1849, -0.1782, -0.1478, -0.3092,  0.0651,  0.0463, -0.0541, -0.0794, -0.1780, -0.1255,
0:         -0.0353], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3244, -0.2965, -0.2494, -0.2373, -0.2518, -0.3074, -0.3593, -0.3851, -0.3804, -0.3465, -0.3020, -0.2792,
0:         -0.2767, -0.2702, -0.2852, -0.2867, -0.2822, -0.2597, -0.2349, -0.2187, -0.2190, -0.2345, -0.2447, -0.2492,
0:         -0.2524], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1106, -0.1064, -0.0906, -0.0865, -0.0710, -0.0753, -0.0887, -0.1043, -0.1028, -0.1394, -0.1263, -0.1081,
0:         -0.0985, -0.1009, -0.1060, -0.1133, -0.1361, -0.1365, -0.1596, -0.1475, -0.1402, -0.1301, -0.1034, -0.1153,
0:         -0.1428], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 11 [1/5 (20%)]	Loss: 1.04417 : 0.51163 :: 0.23128 (2.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 11 [2/5 (40%)]	Loss: 0.56849 : 0.31177 :: 0.23800 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 11 [3/5 (60%)]	Loss: 0.91489 : 0.40795 :: 0.23609 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 11 [4/5 (80%)]	Loss: 0.94224 : 0.40483 :: 0.23244 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.232, max = 0.014, mean = -0.132
0:          sample (first 20): tensor([-0.1869, -0.1733, -0.1594, -0.1566, -0.1420, -0.1403, -0.1483, -0.1618, -0.1708, -0.2065, -0.1895, -0.1796,
0:         -0.1629, -0.1565, -0.1649, -0.1696, -0.1934, -0.1982, -0.2153, -0.2007])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 11 : 0.4080739915370941
0: validation loss for velocity_u : 0.21656449139118195
0: validation loss for velocity_v : 0.15124426782131195
0: validation loss for specific_humidity : 0.038145847618579865
0: validation loss for velocity_z : 0.7900095582008362
0: validation loss for temperature : 0.34288010001182556
0: validation loss for total_precip : 0.9095997214317322
0: 12 : 21:15:46 :: batch_size = 96, lr = 1.5623968034514547e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 12, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7683, -0.7683, -0.7682, -0.7682, -0.7680, -0.7680, -0.7678, -0.7677, -0.7675, -0.7673, -0.7671, -0.7670,
0:         -0.7668, -0.7666, -0.7663, -0.7661, -0.7658, -0.7656, -0.7487, -0.7490, -0.7494, -0.7497, -0.7501, -0.7502,
0:         -0.7506], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2206, 0.2236, 0.2267, 0.2297, 0.2327, 0.2357, 0.2389, 0.2420, 0.2452, 0.2482, 0.2514, 0.2547, 0.2579, 0.2611,
0:         0.2646, 0.2678, 0.2711, 0.2745, 0.1661, 0.1693, 0.1726, 0.1758, 0.1790, 0.1823, 0.1857], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5608, -0.5603, -0.5599, -0.5594, -0.5590, -0.5585, -0.5581, -0.5576, -0.5572, -0.5567, -0.5563, -0.5559,
0:         -0.5555, -0.5550, -0.5547, -0.5545, -0.5542, -0.5540, -0.5626, -0.5620, -0.5616, -0.5612, -0.5608, -0.5603,
0:         -0.5599], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0595, 0.0595, 0.0595, 0.0606, 0.0606, 0.0606, 0.0606, 0.0606, 0.0617, 0.0617, 0.0629, 0.0629, 0.0640, 0.0651,
0:         0.0663, 0.0674, 0.0685, 0.0697, 0.0437, 0.0425, 0.0425, 0.0414, 0.0403, 0.0403, 0.0391], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.0198, 0.0219, 0.0240, 0.0261, 0.0281, 0.0302, 0.0326, 0.0349, 0.0369, 0.0393, 0.0416, 0.0442, 0.0465, 0.0488,
0:         0.0513, 0.0539, 0.0564, 0.0590, 0.0615, 0.0640, 0.0668, 0.0692, 0.0720, 0.0745, 0.0772], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 12, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2386,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2398,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2386,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2386,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2374,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2339,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2303,     nan,     nan, -0.2292,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2327,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2374,     nan,     nan,     nan, -0.2362,     nan,
0:             nan,     nan,     nan,     nan, -0.2362,     nan, -0.2374,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2398,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2315,     nan, -0.2315,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2327,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2315,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2327,     nan,     nan, -0.2351,     nan, -0.2351,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2362, -0.2362,     nan,     nan, -0.2303,     nan,
0:             nan, -0.2315,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 12, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2513, -0.2869, -0.3246, -0.3503, -0.3695, -0.3698, -0.3694, -0.3830, -0.3929, -0.4231, -0.4455, -0.4306,
0:         -0.3988, -0.3483, -0.3071, -0.2847, -0.2960, -0.3420, -0.2236, -0.2368, -0.2624, -0.2913, -0.3148, -0.3192,
0:         -0.3201], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2474, -0.2047, -0.1659, -0.1343, -0.0888, -0.0316,  0.0045,  0.0111, -0.0213, -0.0869, -0.1472, -0.1813,
0:         -0.2054, -0.2072, -0.2088, -0.2003, -0.1895, -0.1533, -0.3011, -0.2359, -0.2139, -0.2176, -0.2157, -0.2124,
0:         -0.1947], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4428, -0.4827, -0.5107, -0.5268, -0.5262, -0.5148, -0.4996, -0.4775, -0.4728, -0.4727, -0.4822, -0.4840,
0:         -0.4945, -0.5054, -0.5252, -0.5483, -0.5834, -0.6053, -0.4567, -0.5014, -0.5358, -0.5478, -0.5434, -0.5339,
0:         -0.5116], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1956,  0.2430,  0.1613,  0.1738,  0.1346,  0.0901,  0.1539,  0.0810, -0.0011,  0.0015, -0.0814, -0.0510,
0:          0.1196,  0.1876,  0.1168,  0.1538,  0.2160,  0.0920,  0.2053,  0.2479,  0.1607,  0.1763,  0.1400,  0.1389,
0:          0.2610], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1131, -0.1054, -0.0709, -0.0431, -0.0276, -0.0422, -0.0737, -0.1018, -0.1118, -0.1001, -0.0803, -0.0660,
0:         -0.0694, -0.0672, -0.0721, -0.0610, -0.0502, -0.0340, -0.0318, -0.0446, -0.0679, -0.0851, -0.0751, -0.0379,
0:         -0.0019], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1834, -0.1721, -0.1583, -0.1575, -0.1443, -0.1432, -0.1517, -0.1688, -0.1770, -0.2046, -0.1890, -0.1786,
0:         -0.1625, -0.1575, -0.1666, -0.1740, -0.1988, -0.2049, -0.2145, -0.1991, -0.2008, -0.1897, -0.1617, -0.1706,
0:         -0.1986], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 12 [1/5 (20%)]	Loss: 0.91869 : 0.44082 :: 0.24015 (2.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 12 [2/5 (40%)]	Loss: 0.92041 : 0.40171 :: 0.23458 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 12 [3/5 (60%)]	Loss: 0.89087 : 0.38605 :: 0.23502 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 12 [4/5 (80%)]	Loss: 1.05980 : 0.45614 :: 0.23759 (15.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.242, max = -0.008, mean = -0.150
0:          sample (first 20): tensor([-0.2172, -0.1982, -0.1899, -0.1885, -0.1750, -0.1730, -0.1796, -0.1889, -0.2011, -0.2310, -0.2163, -0.2121,
0:         -0.1971, -0.1845, -0.1925, -0.1966, -0.2161, -0.2249, -0.2284, -0.2196])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 12 : 0.3784324526786804
0: validation loss for velocity_u : 0.23718947172164917
0: validation loss for velocity_v : 0.15430645644664764
0: validation loss for specific_humidity : 0.038833703845739365
0: validation loss for velocity_z : 0.8199152946472168
0: validation loss for temperature : 0.3506801128387451
0: validation loss for total_precip : 0.6696704030036926
0: 13 : 21:19:45 :: batch_size = 96, lr = 1.5242895643428828e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 13, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8129, -0.8377, -0.8382, -0.8307, -0.8413, -0.8743, -0.8992, -0.9074, -0.9178, -0.9368, -0.9529, -0.9503,
0:         -0.9394, -0.9311, -0.9214, -0.9034, -0.8772, -0.8405, -0.7934, -0.8426, -0.8587, -0.8667, -0.8963, -0.9480,
0:         -0.9848], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2750, 0.3065, 0.3210, 0.3151, 0.3130, 0.3337, 0.3740, 0.4139, 0.4423, 0.4697, 0.4978, 0.5167, 0.5282, 0.5346,
0:         0.5335, 0.5215, 0.5057, 0.4900, 0.2444, 0.2556, 0.2675, 0.2643, 0.2722, 0.3089, 0.3570], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5824, -0.5771, -0.6067, -0.6227, -0.6291, -0.6304, -0.6425, -0.6267, -0.6062, -0.6208, -0.6661, -0.6676,
0:         -0.6224, -0.5463, -0.4231, -0.2739, -0.1318,  0.0600, -0.4756, -0.4698, -0.5432, -0.6276, -0.6333, -0.6420,
0:         -0.6564], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.7647,  0.4767,  0.2503,  0.1250,  0.2019,  0.1843,  0.2261,  0.1206, -0.0707, -0.0421, -0.0092, -0.0179,
0:         -0.1345, -0.1367, -0.0993, -0.1389, -0.1059, -0.1652,  0.5361,  0.3360, -0.0619, -0.1191,  0.0590, -0.0641,
0:          0.0040], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-2.2919, -2.3394, -2.3435, -2.3165, -2.2926, -2.2817, -2.2631, -2.2234, -2.1713, -2.1198, -2.0786, -2.0478,
0:         -2.0183, -1.9921, -1.9766, -1.9581, -1.9288, -1.8856, -1.8185, -1.7271, -1.6199, -1.5187, -1.4274, -1.3392,
0:         -1.2599], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 13, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2466,     nan,     nan,     nan,     nan, -0.1439,     nan, -0.1428,     nan, -0.1105, -0.1935,     nan,
0:             nan,     nan,     nan,  0.0464,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1981,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0078,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0989,     nan, -0.1128,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1774,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1555,  0.3130,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.0476,  0.1953,     nan,     nan,     nan,     nan,     nan, -0.0424,
0:             nan,     nan,     nan,  0.2322,     nan,     nan,     nan,     nan,     nan, -0.0562,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0291,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2443,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1808,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1220,     nan,
0:         -0.0251,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1151,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2466,     nan,     nan,     nan,     nan, -0.1324,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 13, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5525, -0.5680, -0.6082, -0.6480, -0.6787, -0.6918, -0.7161, -0.7529, -0.7672, -0.7978, -0.8033, -0.7547,
0:         -0.7084, -0.6669, -0.6462, -0.6666, -0.7070, -0.7297, -0.5851, -0.5688, -0.5952, -0.6251, -0.6503, -0.6646,
0:         -0.6892], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1247,  0.1729,  0.1819,  0.1544,  0.1498,  0.1676,  0.1637,  0.1235,  0.0540, -0.0394, -0.1068, -0.1402,
0:         -0.1745, -0.2040, -0.2477, -0.2588, -0.2211, -0.1073,  0.0422,  0.1137,  0.1363,  0.1181,  0.0941,  0.0761,
0:          0.0606], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4022, -0.4227, -0.4213, -0.3870, -0.3255, -0.2331, -0.1360, -0.0431,  0.0200,  0.0573,  0.0758,  0.0888,
0:          0.1029,  0.1171,  0.1191,  0.1015,  0.0532, -0.0014, -0.4592, -0.4871, -0.4932, -0.4771, -0.4216, -0.3418,
0:         -0.2431], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3081,  0.2741,  0.1200,  0.0708, -0.0242,  0.0321,  0.2084,  0.1699,  0.1031,  0.0664, -0.0191,  0.0106,
0:          0.1513,  0.3479,  0.2840,  0.1882,  0.2781,  0.0150,  0.2648,  0.2155,  0.0350, -0.0159, -0.0774,  0.0335,
0:          0.2388], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0431, -0.0482, -0.0080,  0.0403,  0.0740,  0.0715,  0.0545,  0.0394,  0.0345,  0.0396,  0.0410,  0.0339,
0:          0.0171,  0.0162,  0.0049, -0.0117, -0.0405, -0.0699, -0.0889, -0.0863, -0.0673, -0.0312,  0.0186,  0.0708,
0:          0.1123], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1859, -0.1677, -0.1602, -0.1583, -0.1442, -0.1447, -0.1502, -0.1604, -0.1739, -0.1983, -0.1828, -0.1800,
0:         -0.1693, -0.1560, -0.1643, -0.1681, -0.1853, -0.1974, -0.1955, -0.1879, -0.1995, -0.1863, -0.1603, -0.1655,
0:         -0.1878], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 13 [1/5 (20%)]	Loss: 0.80984 : 0.41209 :: 0.23712 (2.71 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 13 [2/5 (40%)]	Loss: 0.67452 : 0.37674 :: 0.23438 (15.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 13 [3/5 (60%)]	Loss: 0.88244 : 0.42811 :: 0.22584 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 13 [4/5 (80%)]	Loss: 0.88231 : 0.42891 :: 0.22361 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.204, max = 0.020, mean = -0.122
0:          sample (first 20): tensor([-0.1839, -0.1651, -0.1637, -0.1654, -0.1552, -0.1520, -0.1572, -0.1624, -0.1733, -0.1845, -0.1788, -0.1817,
0:         -0.1775, -0.1613, -0.1709, -0.1719, -0.1865, -0.1923, -0.1783, -0.1783])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 13 : 0.38523954153060913
0: validation loss for velocity_u : 0.2596201002597809
0: validation loss for velocity_v : 0.14925463497638702
0: validation loss for specific_humidity : 0.0373150072991848
0: validation loss for velocity_z : 0.7945296764373779
0: validation loss for temperature : 0.4395703077316284
0: validation loss for total_precip : 0.6311478018760681
0: 14 : 21:23:47 :: batch_size = 96, lr = 1.4871117700906175e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 14, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2481, 0.2453, 0.2419, 0.2379, 0.2334, 0.2282, 0.2225, 0.2162, 0.2095, 0.2025, 0.1951, 0.1875, 0.1798, 0.1719,
0:         0.1642, 0.1567, 0.1493, 0.1420, 0.2032, 0.1967, 0.1898, 0.1824, 0.1745, 0.1662, 0.1577], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9963, 0.9919, 0.9864, 0.9794, 0.9714, 0.9619, 0.9516, 0.9403, 0.9281, 0.9154, 0.9020, 0.8884, 0.8746, 0.8606,
0:         0.8468, 0.8330, 0.8198, 0.8069, 0.9193, 0.9092, 0.8981, 0.8863, 0.8736, 0.8604, 0.8468], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6869, -0.6878, -0.6883, -0.6885, -0.6887, -0.6889, -0.6891, -0.6894, -0.6890, -0.6886, -0.6883, -0.6879,
0:         -0.6876, -0.6874, -0.6877, -0.6873, -0.6866, -0.6854, -0.6879, -0.6881, -0.6883, -0.6885, -0.6884, -0.6881,
0:         -0.6878], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1807, 0.1730, 0.1665, 0.1610, 0.1566, 0.1522, 0.1489, 0.1457, 0.1435, 0.1402, 0.1369, 0.1336, 0.1303, 0.1270,
0:         0.1238, 0.1205, 0.1183, 0.1172, 0.1391, 0.1413, 0.1424, 0.1435, 0.1446, 0.1457, 0.1468], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.9538, 0.9581, 0.9613, 0.9634, 0.9648, 0.9657, 0.9659, 0.9655, 0.9646, 0.9636, 0.9616, 0.9599, 0.9576, 0.9552,
0:         0.9524, 0.9497, 0.9470, 0.9444, 0.9416, 0.9392, 0.9368, 0.9346, 0.9328, 0.9315, 0.9302], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 14, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1532,     nan, -0.1497,     nan,     nan,     nan,
0:         -0.1438,     nan,     nan, -0.1626,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1685, -0.1650,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1603,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1826,     nan,     nan,     nan, -0.1802,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1791,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1791,     nan,     nan, -0.1861,     nan,     nan,
0:             nan,     nan,     nan, -0.1767,     nan,     nan, -0.1896,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1767,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1626,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2096,     nan,     nan, -0.2108,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2061,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2002,
0:             nan, -0.2026,     nan,     nan,     nan, -0.1979,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1955,     nan,     nan,     nan,     nan,     nan,     nan, -0.1920,     nan,
0:             nan,     nan, -0.1896])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 14, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5127, -0.5308, -0.5727, -0.6076, -0.6335, -0.6343, -0.6295, -0.6336, -0.6263, -0.6364, -0.6359, -0.6009,
0:         -0.5564, -0.5196, -0.4938, -0.4936, -0.5190, -0.5540, -0.5194, -0.5182, -0.5484, -0.5805, -0.5993, -0.5948,
0:         -0.5825], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5601, 0.6107, 0.6495, 0.6653, 0.7073, 0.7607, 0.7738, 0.7346, 0.6589, 0.5671, 0.5038, 0.4845, 0.4657, 0.4314,
0:         0.3601, 0.2910, 0.2465, 0.2694, 0.4598, 0.5354, 0.5718, 0.5703, 0.5713, 0.5708, 0.5623], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5397, -0.5897, -0.6285, -0.6483, -0.6462, -0.6270, -0.6027, -0.5747, -0.5628, -0.5558, -0.5590, -0.5553,
0:         -0.5614, -0.5689, -0.5897, -0.6168, -0.6548, -0.6855, -0.5651, -0.6243, -0.6705, -0.6898, -0.6789, -0.6570,
0:         -0.6226], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1785, 0.2380, 0.1681, 0.2007, 0.1043, 0.0376, 0.1718, 0.1694, 0.0738, 0.0610, 0.0347, 0.0277, 0.1325, 0.2193,
0:         0.1318, 0.1387, 0.2333, 0.1399, 0.1834, 0.2327, 0.1378, 0.1744, 0.0843, 0.0465, 0.2023], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.1450, 0.1861, 0.2517, 0.2886, 0.2952, 0.2605, 0.2185, 0.1897, 0.1888, 0.2052, 0.2189, 0.2216, 0.2047, 0.1995,
0:         0.1775, 0.1653, 0.1492, 0.1371, 0.1236, 0.1091, 0.0974, 0.1015, 0.1397, 0.2057, 0.2663], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1826, -0.1657, -0.1673, -0.1696, -0.1584, -0.1514, -0.1547, -0.1563, -0.1657, -0.1862, -0.1799, -0.1836,
0:         -0.1790, -0.1599, -0.1666, -0.1640, -0.1795, -0.1836, -0.1760, -0.1754, -0.1986, -0.1903, -0.1685, -0.1743,
0:         -0.1820], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 14 [1/5 (20%)]	Loss: 0.82210 : 0.44559 :: 0.22361 (2.82 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 14 [2/5 (40%)]	Loss: 1.00172 : 0.50318 :: 0.22167 (14.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 14 [3/5 (60%)]	Loss: 0.80619 : 0.42546 :: 0.20972 (14.88 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 14 [4/5 (80%)]	Loss: 1.02503 : 0.47073 :: 0.21053 (14.82 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.130, max = 4.052, mean = 0.160
0:          sample (first 20): tensor([-0.0622, -0.0443, -0.0513, -0.0518, -0.0430, -0.0427, -0.0441, -0.0462, -0.0488, -0.0507, -0.0523, -0.0611,
0:         -0.0676, -0.0490, -0.0635, -0.0569, -0.0620, -0.0664, -0.0487, -0.0509])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 14 : 0.4848424196243286
0: validation loss for velocity_u : 0.30939069390296936
0: validation loss for velocity_v : 0.1579357236623764
0: validation loss for specific_humidity : 0.04289773479104042
0: validation loss for velocity_z : 1.0396987199783325
0: validation loss for temperature : 0.3974442481994629
0: validation loss for total_precip : 0.9616879820823669
0: 15 : 21:27:48 :: batch_size = 96, lr = 1.4508407513079195e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 15, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9004, -0.9219, -0.9422, -0.9633, -0.9828, -0.9986, -1.0136, -1.0287, -1.0433, -1.0585, -1.0738, -1.0893,
0:         -1.1042, -1.1184, -1.1322, -1.1451, -1.1587, -1.1728, -0.9589, -0.9797, -0.9989, -1.0175, -1.0352, -1.0498,
0:         -1.0631], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2999, -0.3106, -0.3224, -0.3362, -0.3528, -0.3714, -0.3919, -0.4138, -0.4365, -0.4591, -0.4802, -0.4992,
0:         -0.5156, -0.5310, -0.5474, -0.5642, -0.5828, -0.6021, -0.3011, -0.3108, -0.3243, -0.3403, -0.3603, -0.3835,
0:         -0.4066], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3195, -0.3171, -0.3170, -0.3209, -0.3293, -0.3445, -0.3613, -0.3792, -0.3974, -0.4140, -0.4265, -0.4338,
0:         -0.4375, -0.4377, -0.4361, -0.4319, -0.4243, -0.4135, -0.3229, -0.3226, -0.3242, -0.3304, -0.3432, -0.3589,
0:         -0.3745], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2995,  0.2844,  0.2780,  0.2393,  0.1577,  0.0804,  0.0074, -0.0054,  0.0332,  0.0847,  0.1663,  0.2350,
0:          0.3016,  0.3402,  0.3531,  0.4132,  0.4433,  0.4562,  0.1921,  0.1341,  0.1062,  0.0869,  0.0654,  0.0418,
0:          0.0139], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.6333, 1.6233, 1.6134, 1.6036, 1.5894, 1.5773, 1.5723, 1.5683, 1.5632, 1.5620, 1.5619, 1.5606, 1.5601, 1.5587,
0:         1.5576, 1.5541, 1.5478, 1.5425, 1.5397, 1.5414, 1.5519, 1.5693, 1.5913, 1.6200, 1.6473], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 15, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2204,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.1546,     nan,     nan,     nan,     nan,     nan,
0:         -0.1710,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2339,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1074,     nan,
0:             nan,     nan,     nan, -0.2182,     nan, -0.1081,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0879,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1710, -0.1261,     nan, -0.2227,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2294,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1755,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1778,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1620,     nan, -0.1890,     nan,     nan,     nan, -0.2092,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1912, -0.1317,     nan,     nan,     nan,     nan,     nan, -0.0486,
0:         -0.1317, -0.1025,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 15, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2667, -0.2805, -0.3185, -0.3695, -0.4182, -0.4461, -0.4859, -0.5280, -0.5528, -0.5638, -0.5396, -0.4638,
0:         -0.3909, -0.3150, -0.2720, -0.2619, -0.2833, -0.3155, -0.2754, -0.2671, -0.2913, -0.3368, -0.3662, -0.3870,
0:         -0.3993], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2925, -0.2627, -0.2415, -0.2327, -0.2099, -0.1691, -0.1406, -0.1348, -0.1585, -0.2073, -0.2506, -0.2691,
0:         -0.2807, -0.2711, -0.2638, -0.2366, -0.1991, -0.1279, -0.2650, -0.2098, -0.2003, -0.2176, -0.2320, -0.2317,
0:         -0.2102], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4909, -0.5371, -0.5761, -0.6011, -0.6053, -0.5889, -0.5569, -0.5168, -0.4964, -0.4928, -0.5120, -0.5290,
0:         -0.5432, -0.5496, -0.5571, -0.5615, -0.5787, -0.5962, -0.5018, -0.5448, -0.5874, -0.6254, -0.6397, -0.6313,
0:         -0.6006], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1605,  0.1610,  0.0169,  0.0134, -0.0519, -0.0308,  0.1491,  0.1573,  0.1009,  0.0858,  0.0456,  0.0699,
0:          0.1812,  0.2030,  0.0616,  0.0700,  0.1820,  0.0927,  0.1527,  0.1968,  0.0423,  0.0412, -0.0070,  0.0079,
0:          0.1115], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.2174, 0.2063, 0.2220, 0.2172, 0.1793, 0.1034, 0.0319, 0.0011, 0.0365, 0.1099, 0.1691, 0.1802, 0.1316, 0.0884,
0:         0.0454, 0.0320, 0.0228, 0.0278, 0.0162, 0.0109, 0.0495, 0.1369, 0.2739, 0.4099, 0.4512], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1181, -0.1004, -0.1054, -0.1104, -0.1018, -0.1015, -0.1036, -0.1058, -0.1128, -0.1104, -0.1099, -0.1182,
0:         -0.1243, -0.1084, -0.1190, -0.1163, -0.1221, -0.1269, -0.1062, -0.1089, -0.1321, -0.1299, -0.1186, -0.1270,
0:         -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 15 [1/5 (20%)]	Loss: 0.75242 : 0.44419 :: 0.21190 (2.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 15 [2/5 (40%)]	Loss: 0.58924 : 0.34577 :: 0.20046 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 15 [3/5 (60%)]	Loss: 0.83753 : 0.43184 :: 0.20045 (15.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 15 [4/5 (80%)]	Loss: 0.82877 : 0.41742 :: 0.19167 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.153, max = 7.693, mean = 0.409
0:          sample (first 20): tensor([-0.1055, -0.0903, -0.0968, -0.0974, -0.0855, -0.0869, -0.0839, -0.0807, -0.0834, -0.0971, -0.0982, -0.1093,
0:         -0.1168, -0.0963, -0.1067, -0.0963, -0.0993, -0.1013, -0.1001, -0.1038])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 15 : 0.43511664867401123
0: validation loss for velocity_u : 0.2984450161457062
0: validation loss for velocity_v : 0.1268167346715927
0: validation loss for specific_humidity : 0.04736277833580971
0: validation loss for velocity_z : 0.9253632426261902
0: validation loss for temperature : 0.4415459632873535
0: validation loss for total_precip : 0.7711656093597412
0: 16 : 21:31:50 :: batch_size = 96, lr = 1.4154543915199217e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 16, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.0203, -2.0134, -2.0053, -1.9959, -1.9852, -1.9730, -1.9595, -1.9442, -1.9273, -1.9090, -1.8894, -1.8688,
0:         -1.8473, -1.8247, -1.8012, -1.7764, -1.7501, -1.7228, -2.1458, -2.1412, -2.1342, -2.1250, -2.1135, -2.0999,
0:         -2.0840], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5552, 0.5977, 0.6392, 0.6799, 0.7196, 0.7585, 0.7966, 0.8336, 0.8696, 0.9046, 0.9386, 0.9716, 1.0040, 1.0355,
0:         1.0664, 1.0968, 1.1263, 1.1550, 0.5619, 0.6130, 0.6632, 0.7125, 0.7611, 0.8088, 0.8554], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6135, -0.6139, -0.6143, -0.6147, -0.6151, -0.6156, -0.6159, -0.6158, -0.6163, -0.6168, -0.6172, -0.6176,
0:         -0.6179, -0.6182, -0.6185, -0.6186, -0.6188, -0.6190, -0.6185, -0.6194, -0.6199, -0.6206, -0.6213, -0.6221,
0:         -0.6226], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5340, -0.5146, -0.4973, -0.4756, -0.4562, -0.4518, -0.4735, -0.5254, -0.5925, -0.6552, -0.6963, -0.7071,
0:         -0.6898, -0.6682, -0.6595, -0.6812, -0.7309, -0.8002, -0.5340, -0.5557, -0.5903, -0.6184, -0.6314, -0.6314,
0:         -0.6292], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.3231, -0.3337, -0.3451, -0.3571, -0.3697, -0.3828, -0.3964, -0.4103, -0.4238, -0.4373, -0.4501, -0.4628,
0:         -0.4752, -0.4874, -0.5000, -0.5126, -0.5251, -0.5369, -0.5480, -0.5585, -0.5678, -0.5768, -0.5849, -0.5920,
0:         -0.5980], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 16, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.1785,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1976,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2302,     nan,     nan,     nan,     nan,
0:             nan, -0.2302,     nan, -0.2313,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2302,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2290,     nan,
0:         -0.2290,     nan,     nan,     nan,     nan,     nan,     nan, -0.2223,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2055,     nan,     nan, -0.2100,     nan,     nan,
0:             nan,     nan, -0.1897,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2302,     nan,     nan,
0:             nan,     nan, -0.2313,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2212,
0:         -0.2189,     nan,     nan, -0.2178,     nan, -0.2201,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2055,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2212,     nan,     nan,     nan,     nan,
0:             nan, -0.2302,     nan,     nan,     nan,     nan,     nan, -0.2302,     nan, -0.2313,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2212,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 16, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0759, -1.0603, -1.0796, -1.0881, -1.0914, -1.0795, -1.0703, -1.0788, -1.0778, -1.0968, -1.1012, -1.0618,
0:         -0.9955, -0.9113, -0.8258, -0.7686, -0.7330, -0.7212, -1.1479, -1.0789, -1.0640, -1.0635, -1.0698, -1.0600,
0:         -1.0657], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6288, 0.7472, 0.9068, 1.0553, 1.2624, 1.4738, 1.5986, 1.6553, 1.6770, 1.6851, 1.7453, 1.8604, 1.9471, 1.9779,
0:         1.9537, 1.8534, 1.8145, 1.8472, 0.5051, 0.6538, 0.8202, 0.9920, 1.1224, 1.2242, 1.2833], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6640, -0.6846, -0.7026, -0.7138, -0.7144, -0.7100, -0.6967, -0.6809, -0.6713, -0.6688, -0.6748, -0.6811,
0:         -0.6858, -0.6914, -0.6957, -0.7034, -0.7140, -0.7172, -0.6847, -0.7089, -0.7287, -0.7411, -0.7400, -0.7364,
0:         -0.7161], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3386,  0.0818, -0.1496, -0.1451, -0.2181, -0.1535,  0.0166, -0.0244, -0.0712, -0.0678, -0.0793, -0.1038,
0:         -0.0484,  0.0737, -0.0161, -0.0039,  0.1869,  0.0984,  0.3161,  0.1214, -0.1657, -0.1696, -0.2301, -0.1519,
0:          0.0488], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1874, -0.1390, -0.0979, -0.1119, -0.1321, -0.1547, -0.1477, -0.1208, -0.0888, -0.0640, -0.0622, -0.0794,
0:         -0.1075, -0.1150, -0.1282, -0.1363, -0.1605, -0.1831, -0.1970, -0.1724, -0.1033, -0.0004,  0.1075,  0.1805,
0:          0.1774], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0002,  0.0116, -0.0008, -0.0044,  0.0031,  0.0007,  0.0025, -0.0005, -0.0006,  0.0117,  0.0102, -0.0136,
0:         -0.0252, -0.0064, -0.0247, -0.0123, -0.0152, -0.0200,  0.0101,  0.0032, -0.0267, -0.0315, -0.0292, -0.0370,
0:         -0.0261], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 16 [1/5 (20%)]	Loss: 1.02890 : 0.49680 :: 0.19319 (2.56 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 16 [2/5 (40%)]	Loss: 0.72562 : 0.41744 :: 0.19495 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 16 [3/5 (60%)]	Loss: 0.99941 : 0.46890 :: 0.18789 (15.23 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 16 [4/5 (80%)]	Loss: 0.58459 : 0.37291 :: 0.19003 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.181, max = 1.561, mean = 0.156
0:          sample (first 20): tensor([-0.1404, -0.1247, -0.1285, -0.1327, -0.1193, -0.1218, -0.1171, -0.1105, -0.1152, -0.1320, -0.1329, -0.1426,
0:         -0.1476, -0.1319, -0.1382, -0.1280, -0.1299, -0.1295, -0.1430, -0.1443])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 16 : 0.3950332701206207
0: validation loss for velocity_u : 0.31244271993637085
0: validation loss for velocity_v : 0.1391698271036148
0: validation loss for specific_humidity : 0.045607175678014755
0: validation loss for velocity_z : 0.8081813454627991
0: validation loss for temperature : 0.40151169896125793
0: validation loss for total_precip : 0.663287341594696
0: 17 : 21:35:46 :: batch_size = 96, lr = 1.3809311136779726e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 17, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5410, -0.5432, -0.5427, -0.5397, -0.5342, -0.5267, -0.5177, -0.5079, -0.4979, -0.4882, -0.4792, -0.4714,
0:         -0.4646, -0.4589, -0.4541, -0.4504, -0.4476, -0.4454, -0.5295, -0.5320, -0.5322, -0.5302, -0.5262, -0.5205,
0:         -0.5136], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7449, -0.7121, -0.6769, -0.6397, -0.6020, -0.5642, -0.5273, -0.4914, -0.4572, -0.4242, -0.3931, -0.3635,
0:         -0.3353, -0.3090, -0.2840, -0.2604, -0.2381, -0.2168, -0.7706, -0.7393, -0.7050, -0.6689, -0.6320, -0.5951,
0:         -0.5590], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7603, -0.7596, -0.7589, -0.7581, -0.7574, -0.7567, -0.7560, -0.7553, -0.7547, -0.7542, -0.7535, -0.7529,
0:         -0.7522, -0.7515, -0.7506, -0.7499, -0.7490, -0.7480, -0.7614, -0.7610, -0.7604, -0.7596, -0.7590, -0.7582,
0:         -0.7576], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5925,  0.5903,  0.5773,  0.5534,  0.5164,  0.4686,  0.4100,  0.3404,  0.2600,  0.1709,  0.0797, -0.0051,
0:         -0.0789, -0.1376, -0.1745, -0.1941, -0.2006, -0.2006,  0.6229,  0.6186,  0.5990,  0.5686,  0.5273,  0.4773,
0:          0.4187], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.3489, -0.3443, -0.3400, -0.3366, -0.3331, -0.3298, -0.3259, -0.3219, -0.3170, -0.3114, -0.3047, -0.2970,
0:         -0.2873, -0.2754, -0.2612, -0.2439, -0.2239, -0.2019, -0.1778, -0.1533, -0.1283, -0.1046, -0.0819, -0.0610,
0:         -0.0417], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 17, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2392, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan, -0.2392,     nan,
0:             nan, -0.2392, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392, -0.2392,     nan,
0:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392, -0.2392])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 17, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3040, -0.3350, -0.3760, -0.4133, -0.4481, -0.4589, -0.4788, -0.5038, -0.5248, -0.5573, -0.5651, -0.5332,
0:         -0.4904, -0.4174, -0.3497, -0.2978, -0.2630, -0.2455, -0.3201, -0.3339, -0.3672, -0.4060, -0.4291, -0.4323,
0:         -0.4284], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9431, -0.9741, -0.9966, -1.0056, -0.9801, -0.9258, -0.8941, -0.8979, -0.9429, -1.0120, -1.0713, -1.1005,
0:         -1.1285, -1.1359, -1.1534, -1.1358, -1.0798, -0.9792, -0.8675, -0.8678, -0.8988, -0.9324, -0.9354, -0.9162,
0:         -0.8905], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7359, -0.7566, -0.7778, -0.7935, -0.7968, -0.7932, -0.7806, -0.7610, -0.7480, -0.7378, -0.7378, -0.7356,
0:         -0.7390, -0.7463, -0.7595, -0.7748, -0.8021, -0.8189, -0.7454, -0.7731, -0.7967, -0.8114, -0.8126, -0.8080,
0:         -0.7934], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1252,  0.1956,  0.1108,  0.1277,  0.0562, -0.0067,  0.0530,  0.0307, -0.0122,  0.0156,  0.0455,  0.0989,
0:          0.1883,  0.2377,  0.1579,  0.1378,  0.2213,  0.1017,  0.2539,  0.3372,  0.2358,  0.2506,  0.1933,  0.1723,
0:          0.2356], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.6320, -0.5052, -0.3860, -0.3213, -0.3006, -0.3275, -0.3765, -0.4258, -0.4496, -0.4486, -0.4402, -0.4411,
0:         -0.4666, -0.4729, -0.4746, -0.4500, -0.4210, -0.3983, -0.3967, -0.3994, -0.3916, -0.3653, -0.3251, -0.2837,
0:         -0.2704], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1573, -0.1450, -0.1412, -0.1486, -0.1344, -0.1381, -0.1325, -0.1269, -0.1356, -0.1480, -0.1486, -0.1602,
0:         -0.1640, -0.1469, -0.1535, -0.1432, -0.1512, -0.1501, -0.1645, -0.1638, -0.1726, -0.1727, -0.1673, -0.1727,
0:         -0.1627], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 17 [1/5 (20%)]	Loss: 0.78110 : 0.43970 :: 0.18663 (2.78 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 17 [2/5 (40%)]	Loss: 0.82526 : 0.44026 :: 0.18162 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 17 [3/5 (60%)]	Loss: 0.77335 : 0.42754 :: 0.18141 (15.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 17 [4/5 (80%)]	Loss: 0.66030 : 0.42831 :: 0.17875 (15.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.150, max = 8.493, mean = 0.795
0:          sample (first 20): tensor([0.1043, 0.1223, 0.1134, 0.1090, 0.1130, 0.1062, 0.1025, 0.0989, 0.0960, 0.1086, 0.1150, 0.1089, 0.0957, 0.1015,
0:         0.0841, 0.0916, 0.0880, 0.0823, 0.1025, 0.1034])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 17 : 0.49172157049179077
0: validation loss for velocity_u : 0.3659970760345459
0: validation loss for velocity_v : 0.14388452470302582
0: validation loss for specific_humidity : 0.047516923397779465
0: validation loss for velocity_z : 0.9622802734375
0: validation loss for temperature : 0.5719847083091736
0: validation loss for total_precip : 0.858665943145752
0: 18 : 21:39:38 :: batch_size = 96, lr = 1.3472498670029002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 18, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.4450, -1.4448, -1.4427, -1.4400, -1.4372, -1.4344, -1.4309, -1.4257, -1.4184, -1.4082, -1.3956, -1.3806,
0:         -1.3641, -1.3466, -1.3285, -1.3102, -1.2919, -1.2739, -1.4573, -1.4578, -1.4545, -1.4490, -1.4420, -1.4344,
0:         -1.4259], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7460, 0.8151, 0.8826, 0.9517, 1.0234, 1.0964, 1.1687, 1.2378, 1.3011, 1.3572, 1.4046, 1.4438, 1.4752, 1.5001,
0:         1.5192, 1.5328, 1.5415, 1.5463, 0.7410, 0.8203, 0.8974, 0.9749, 1.0531, 1.1302, 1.2045], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3620,  0.3757,  0.3875,  0.3877,  0.3876,  0.3914,  0.3805,  0.3641,  0.3277,  0.2951,  0.2625,  0.2170,
0:          0.1648,  0.1098,  0.0504, -0.0063, -0.0588, -0.0968,  0.3185,  0.3323,  0.3452,  0.3587,  0.3727,  0.3766,
0:          0.3656], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4390, -0.3238, -0.2794, -0.2679, -0.2520, -0.2155, -0.1710, -0.1334, -0.1037, -0.0650, -0.0034,  0.0867,
0:          0.1985,  0.3216,  0.4459,  0.5543,  0.6227,  0.6284, -0.3181, -0.2121, -0.1950, -0.2406, -0.2953, -0.3215,
0:         -0.3215], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2818, -0.2760, -0.2627, -0.2414, -0.2125, -0.1764, -0.1341, -0.0863, -0.0342,  0.0186,  0.0691,  0.1154,
0:          0.1555,  0.1878,  0.2110,  0.2232,  0.2241,  0.2148,  0.1979,  0.1759,  0.1508,  0.1251,  0.1004,  0.0810,
0:          0.0687], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 18, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2541,     nan,     nan,     nan,     nan, -0.2400,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2447,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2342,     nan,     nan,     nan,     nan, -0.0308,
0:             nan,     nan, -0.1154,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2306,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2271,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2530, -0.2553,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2530,     nan,     nan,     nan, -0.2541,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2153,
0:             nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2248,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2083,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2530,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2541,     nan,     nan, -0.2400,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2318,     nan,     nan,     nan,     nan,     nan,
0:         -0.2459,     nan,     nan,     nan, -0.2377, -0.2424,     nan,     nan,     nan,     nan,     nan, -0.1519,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2365,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 18, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7053, -0.6933, -0.6980, -0.7207, -0.7447, -0.7498, -0.7716, -0.8029, -0.8288, -0.8695, -0.8855, -0.8564,
0:         -0.8105, -0.7461, -0.6987, -0.6762, -0.6854, -0.7207, -0.6840, -0.6497, -0.6484, -0.6727, -0.6998, -0.7177,
0:         -0.7293], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0093, 1.0797, 1.1494, 1.2047, 1.2864, 1.4024, 1.4776, 1.4871, 1.4440, 1.3547, 1.2738, 1.2289, 1.1469, 1.0644,
0:         0.9352, 0.7904, 0.6814, 0.6615, 0.8433, 0.9439, 1.0344, 1.0808, 1.1254, 1.1685, 1.1926], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1411, -0.1766, -0.2080, -0.2239, -0.2230, -0.2042, -0.1771, -0.1379, -0.1028, -0.0675, -0.0315,  0.0085,
0:          0.0474,  0.0744,  0.0851,  0.0783,  0.0508,  0.0159, -0.1485, -0.1945, -0.2425, -0.2758, -0.2888, -0.2879,
0:         -0.2662], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3006,  0.1656,  0.0021, -0.0312, -0.2256, -0.2821, -0.1894, -0.2129, -0.2231, -0.1909, -0.1027,  0.0256,
0:          0.1874,  0.3205,  0.2393,  0.2896,  0.4403,  0.2391,  0.2428,  0.1813,  0.0500,  0.0348, -0.1562, -0.1753,
0:         -0.1058], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.2255, -0.2165, -0.1269, -0.0271,  0.0480,  0.0517,  0.0143, -0.0254, -0.0412, -0.0276, -0.0015,  0.0109,
0:          0.0009,  0.0036, -0.0091, -0.0243, -0.0607, -0.1130, -0.1778, -0.2251, -0.2388, -0.2045, -0.1325, -0.0608,
0:         -0.0263], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1326, -0.1276, -0.1292, -0.1355, -0.1270, -0.1263, -0.1229, -0.1148, -0.1227, -0.1317, -0.1354, -0.1431,
0:         -0.1508, -0.1418, -0.1472, -0.1359, -0.1428, -0.1382, -0.1503, -0.1484, -0.1591, -0.1643, -0.1644, -0.1677,
0:         -0.1594], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 18 [1/5 (20%)]	Loss: 0.88761 : 0.44750 :: 0.17681 (2.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 18 [2/5 (40%)]	Loss: 0.88320 : 0.45518 :: 0.18515 (15.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 18 [3/5 (60%)]	Loss: 0.74911 : 0.45445 :: 0.17615 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 18 [4/5 (80%)]	Loss: 0.59146 : 0.43064 :: 0.18116 (15.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.239, max = 3.719, mean = 0.150
0:          sample (first 20): tensor([0.1922, 0.1990, 0.1939, 0.1843, 0.1784, 0.1742, 0.1623, 0.1527, 0.1535, 0.1861, 0.1909, 0.1859, 0.1703, 0.1703,
0:         0.1501, 0.1583, 0.1484, 0.1442, 0.1770, 0.1760])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 18 : 0.4533378779888153
0: validation loss for velocity_u : 0.3712432384490967
0: validation loss for velocity_v : 0.13018716871738434
0: validation loss for specific_humidity : 0.05485537648200989
0: validation loss for velocity_z : 0.7437145113945007
0: validation loss for temperature : 0.3813052475452423
0: validation loss for total_precip : 1.038722038269043
0: 19 : 21:43:37 :: batch_size = 96, lr = 1.3143901141491711e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 19, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.2023, 2.2433, 2.2856, 2.3281, 2.3727, 2.4218, 2.4788, 2.5453, 2.6227, 2.7100, 2.8033, 2.8959, 2.9823, 3.0607,
0:         3.1324, 3.1991, 3.2603, 3.3131, 2.1318, 2.1667, 2.2067, 2.2482, 2.2892, 2.3308, 2.3757], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2373, -0.3021, -0.3692, -0.4392, -0.5117, -0.5881, -0.6688, -0.7545, -0.8466, -0.9461, -1.0508, -1.1564,
0:         -1.2587, -1.3529, -1.4346, -1.5040, -1.5644, -1.6185, -0.1849, -0.2478, -0.3126, -0.3784, -0.4451, -0.5130,
0:         -0.5820], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8304, -0.8326, -0.8342, -0.8354, -0.8364, -0.8373, -0.8383, -0.8392, -0.8403, -0.8414, -0.8422, -0.8423,
0:         -0.8423, -0.8419, -0.8407, -0.8393, -0.8363, -0.8326, -0.8229, -0.8261, -0.8285, -0.8306, -0.8319, -0.8328,
0:         -0.8333], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2321, -0.2498, -0.2543, -0.3273, -0.4357, -0.4999, -0.4755, -0.3649, -0.2211, -0.0905,  0.0289,  0.1108,
0:          0.0466, -0.1945, -0.4446, -0.5286, -0.4556, -0.3516,  0.0621, -0.0817, -0.2344, -0.3848, -0.4932, -0.5463,
0:         -0.5131], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.9465, -1.9523, -1.9561, -1.9586, -1.9595, -1.9587, -1.9556, -1.9494, -1.9391, -1.9223, -1.8976, -1.8650,
0:         -1.8271, -1.7885, -1.7520, -1.7165, -1.6780, -1.6345, -1.5864, -1.5353, -1.4827, -1.4306, -1.3812, -1.3349,
0:         -1.2895], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 19, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0678, -0.1671,     nan,     nan,
0:             nan, -0.1960,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0447,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0170,     nan,     nan,     nan,
0:         -0.0159,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0574,     nan,     nan,     nan,     nan, -0.1764,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2018,     nan, -0.1140,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1648,
0:             nan, -0.0436,     nan,     nan,     nan,     nan, -0.1198,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1083,     nan,     nan,     nan,     nan, -0.0805,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1302,     nan,     nan,     nan, -0.0944,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0852,     nan,     nan,     nan, -0.0736,     nan,     nan,     nan,     nan,     nan, -0.1221,     nan,
0:             nan,     nan,     nan,     nan, -0.1256,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1775,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1868,     nan, -0.1614,
0:             nan, -0.2376,     nan,     nan,     nan,     nan,     nan, -0.1556,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1718,     nan,     nan,     nan,     nan, -0.2491,     nan,     nan, -0.2237,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 19, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3575, 0.3617, 0.4429, 0.5843, 0.7159, 0.8016, 0.8250, 0.7813, 0.7165, 0.6507, 0.6149, 0.5973, 0.6139, 0.6662,
0:         0.7264, 0.8357, 0.9731, 1.0935, 0.3874, 0.3376, 0.3459, 0.4159, 0.5118, 0.6115, 0.6603], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6319, 0.6350, 0.6629, 0.6958, 0.7544, 0.8125, 0.8256, 0.7779, 0.6909, 0.5785, 0.4990, 0.4596, 0.4330, 0.4055,
0:         0.3554, 0.2971, 0.2570, 0.2624, 0.5401, 0.5681, 0.6117, 0.6439, 0.6660, 0.6732, 0.6541], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6354, -0.6662, -0.6933, -0.7134, -0.7242, -0.7278, -0.7230, -0.7138, -0.7118, -0.7071, -0.7096, -0.7102,
0:         -0.7150, -0.7196, -0.7300, -0.7413, -0.7612, -0.7749, -0.6430, -0.6748, -0.7038, -0.7253, -0.7365, -0.7441,
0:         -0.7378], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2295, 0.2675, 0.1886, 0.1833, 0.1264, 0.1506, 0.2430, 0.2219, 0.1810, 0.1529, 0.1568, 0.2083, 0.2866, 0.2702,
0:         0.1152, 0.1331, 0.1926, 0.0534, 0.2829, 0.2966, 0.1861, 0.1526, 0.0867, 0.1544, 0.2299], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4120, -0.3972, -0.2914, -0.1959, -0.1332, -0.1450, -0.1733, -0.1880, -0.1770, -0.1446, -0.1037, -0.0678,
0:         -0.0338,  0.0337,  0.0681,  0.0736,  0.0278, -0.0407, -0.1156, -0.1771, -0.2343, -0.2846, -0.3147, -0.3130,
0:         -0.2858], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2002, -0.1990, -0.1909, -0.1994, -0.1928, -0.1853, -0.1844, -0.1730, -0.1747, -0.2114, -0.2119, -0.2134,
0:         -0.2174, -0.2026, -0.2017, -0.1912, -0.1956, -0.1839, -0.2300, -0.2274, -0.2319, -0.2315, -0.2234, -0.2221,
0:         -0.2106], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 19 [1/5 (20%)]	Loss: 0.86554 : 0.46330 :: 0.17533 (2.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 19 [2/5 (40%)]	Loss: 0.74075 : 0.46201 :: 0.18476 (15.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 19 [3/5 (60%)]	Loss: 0.68848 : 0.46044 :: 0.18445 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 19 [4/5 (80%)]	Loss: 0.64584 : 0.39066 :: 0.17576 (14.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.265, max = 1.241, mean = 0.108
0:          sample (first 20): tensor([-0.0655, -0.0658, -0.0632, -0.0619, -0.0543, -0.0474, -0.0453, -0.0405, -0.0361, -0.0775, -0.0737, -0.0700,
0:         -0.0762, -0.0689, -0.0763, -0.0551, -0.0629, -0.0493, -0.0887, -0.0883])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 19 : 0.38760408759117126
0: validation loss for velocity_u : 0.404121994972229
0: validation loss for velocity_v : 0.13463126122951508
0: validation loss for specific_humidity : 0.05223356559872627
0: validation loss for velocity_z : 0.7501661777496338
0: validation loss for temperature : 0.4727800190448761
0: validation loss for total_precip : 0.5116913318634033
0: 20 : 21:47:35 :: batch_size = 96, lr = 1.2823318186821183e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 20, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1157, 0.1138, 0.1111, 0.1080, 0.1044, 0.1001, 0.0957, 0.0909, 0.0857, 0.0802, 0.0746, 0.0687, 0.0625, 0.0559,
0:         0.0492, 0.0419, 0.0345, 0.0270, 0.1309, 0.1318, 0.1322, 0.1317, 0.1307, 0.1290, 0.1267], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0634, -0.0739, -0.0827, -0.0894, -0.0945, -0.0980, -0.1001, -0.1008, -0.1005, -0.0993, -0.0973, -0.0945,
0:         -0.0911, -0.0872, -0.0827, -0.0775, -0.0720, -0.0657, -0.0455, -0.0621, -0.0765, -0.0889, -0.0995, -0.1083,
0:         -0.1154], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2634, -0.2771, -0.2919, -0.3068, -0.3217, -0.3366, -0.3513, -0.3629, -0.3764, -0.3896, -0.4030, -0.4164,
0:         -0.4298, -0.4433, -0.4563, -0.4665, -0.4725, -0.4785, -0.2150, -0.2274, -0.2390, -0.2509, -0.2626, -0.2745,
0:         -0.2863], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1227, -0.1183, -0.1139, -0.1095, -0.1040, -0.0996, -0.0940, -0.0907, -0.0874, -0.0852, -0.0863, -0.0885,
0:         -0.0929, -0.0996, -0.1062, -0.1128, -0.1183, -0.1216, -0.1702, -0.1691, -0.1669, -0.1647, -0.1614, -0.1570,
0:         -0.1526], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.3445, 0.3461, 0.3478, 0.3490, 0.3502, 0.3512, 0.3522, 0.3528, 0.3538, 0.3543, 0.3552, 0.3562, 0.3571, 0.3581,
0:         0.3591, 0.3603, 0.3615, 0.3625, 0.3637, 0.3647, 0.3658, 0.3668, 0.3675, 0.3682, 0.3686], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 20, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2304, -0.2304,     nan, -0.2304,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2490,     nan,     nan,     nan,     nan,     nan, -0.2456,     nan,     nan,     nan,
0:         -0.2432,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2514, -0.2514,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2467,     nan,     nan,
0:         -0.2525,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2118,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2258,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2479,     nan,     nan,     nan,     nan, -0.2514,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2479,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2490,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1931,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2490,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2479,
0:             nan,     nan, -0.2479])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 20, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1406, -0.1324, -0.1304, -0.1296, -0.1355, -0.1309, -0.1560, -0.1894, -0.2251, -0.2705, -0.2918, -0.2795,
0:         -0.2464, -0.1914, -0.1383, -0.0937, -0.0698, -0.0661, -0.1141, -0.1033, -0.1065, -0.1235, -0.1405, -0.1525,
0:         -0.1646], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2023, 0.2204, 0.2652, 0.3224, 0.4112, 0.5163, 0.5575, 0.5342, 0.4549, 0.3324, 0.2333, 0.1838, 0.1500, 0.1438,
0:         0.1251, 0.1133, 0.1194, 0.1815, 0.0923, 0.1367, 0.1863, 0.2315, 0.2763, 0.3116, 0.3309], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1399, -0.1673, -0.1984, -0.2184, -0.2253, -0.2202, -0.2046, -0.1784, -0.1631, -0.1452, -0.1328, -0.1222,
0:         -0.1205, -0.1255, -0.1482, -0.1797, -0.2262, -0.2705, -0.1592, -0.1927, -0.2315, -0.2542, -0.2605, -0.2617,
0:         -0.2405], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0512,  0.0110, -0.0182,  0.0563, -0.0379, -0.0493,  0.0446,  0.0356,  0.0215, -0.0103, -0.0532, -0.0029,
0:          0.0932,  0.1386,  0.0251,  0.0198,  0.1386,  0.0305,  0.1393,  0.1700,  0.1309,  0.1612,  0.0696,  0.1034,
0:          0.1720], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0029,  0.0019,  0.0343,  0.0500,  0.0439,  0.0055, -0.0370, -0.0544, -0.0414, -0.0084,  0.0172,  0.0207,
0:          0.0009, -0.0044, -0.0051,  0.0154,  0.0321,  0.0413,  0.0283,  0.0074, -0.0111, -0.0053,  0.0341,  0.0893,
0:          0.1235], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2153, -0.2176, -0.2078, -0.2127, -0.2062, -0.1970, -0.1944, -0.1877, -0.1879, -0.2290, -0.2317, -0.2265,
0:         -0.2266, -0.2173, -0.2171, -0.1988, -0.2127, -0.1984, -0.2442, -0.2467, -0.2439, -0.2438, -0.2358, -0.2318,
0:         -0.2272], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 20 [1/5 (20%)]	Loss: 0.89379 : 0.46290 :: 0.17043 (2.57 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 20 [2/5 (40%)]	Loss: 0.60800 : 0.39925 :: 0.17768 (14.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 20 [3/5 (60%)]	Loss: 0.95082 : 0.46728 :: 0.17116 (14.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 20 [4/5 (80%)]	Loss: 0.50392 : 0.37188 :: 0.16956 (14.92 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.287, max = 2.771, mean = -0.115
0:          sample (first 20): tensor([-0.2282, -0.2331, -0.2273, -0.2329, -0.2234, -0.2168, -0.2174, -0.2110, -0.2084, -0.2514, -0.2513, -0.2461,
0:         -0.2429, -0.2377, -0.2385, -0.2232, -0.2361, -0.2189, -0.2681, -0.2688])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 20 : 0.4414505958557129
0: validation loss for velocity_u : 0.3827005922794342
0: validation loss for velocity_v : 0.16536621749401093
0: validation loss for specific_humidity : 0.05235454440116882
0: validation loss for velocity_z : 0.8102388978004456
0: validation loss for temperature : 0.4950312077999115
0: validation loss for total_precip : 0.7430123686790466
0: 21 : 21:51:37 :: batch_size = 96, lr = 1.2510554328606033e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 21, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.6209, 1.6109, 1.5986, 1.5841, 1.5675, 1.5489, 1.5283, 1.5060, 1.4821, 1.4568, 1.4302, 1.4026, 1.3739, 1.3443,
0:         1.3131, 1.2802, 1.2453, 1.2085, 1.7149, 1.7034, 1.6893, 1.6725, 1.6537, 1.6329, 1.6102], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.8515, 1.8853, 1.9177, 1.9481, 1.9757, 1.9994, 2.0188, 2.0332, 2.0418, 2.0445, 2.0418, 2.0336, 2.0205, 2.0026,
0:         1.9798, 1.9517, 1.9179, 1.8780, 1.8734, 1.9008, 1.9256, 1.9473, 1.9652, 1.9788, 1.9878], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4179, -0.4297, -0.4428, -0.4573, -0.4741, -0.4903, -0.5058, -0.5213, -0.5381, -0.5551, -0.5716, -0.5875,
0:         -0.5978, -0.6100, -0.6238, -0.6376, -0.6453, -0.6530, -0.4573, -0.4717, -0.4860, -0.5004, -0.5168, -0.5334,
0:         -0.5500], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6559,  0.6294,  0.5609,  0.4615,  0.3445,  0.2231,  0.1104,  0.0089, -0.0728, -0.1347, -0.1766, -0.1965,
0:         -0.1965, -0.1788, -0.1457, -0.1038, -0.0596, -0.0221,  0.5256,  0.5057,  0.4395,  0.3467,  0.2385,  0.1303,
0:          0.0309], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.3196,  0.3115,  0.3028,  0.2937,  0.2839,  0.2731,  0.2615,  0.2490,  0.2353,  0.2208,  0.2053,  0.1887,
0:          0.1713,  0.1531,  0.1342,  0.1144,  0.0943,  0.0735,  0.0523,  0.0306,  0.0090, -0.0124, -0.0330, -0.0524,
0:         -0.0701], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 21, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2336,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2382,     nan, -0.2382,     nan,     nan,     nan,     nan, -0.2336,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2347,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan, -0.2359,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382, -0.2382,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,
0:             nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2336,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2359, -0.2370,     nan, -0.2370,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2370,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2382,     nan,     nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2382,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 21, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7104, 0.6827, 0.6789, 0.6781, 0.6765, 0.6615, 0.6143, 0.5503, 0.4868, 0.4238, 0.3936, 0.3887, 0.3957, 0.4155,
0:         0.4280, 0.4537, 0.4923, 0.5323, 0.7109, 0.6690, 0.6379, 0.6188, 0.6121, 0.6034, 0.5838], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2180, 1.2947, 1.3920, 1.4780, 1.5817, 1.7023, 1.7477, 1.7145, 1.6285, 1.5151, 1.4530, 1.4587, 1.4740, 1.4921,
0:         1.4486, 1.3500, 1.2554, 1.2312, 1.0852, 1.1730, 1.2576, 1.3263, 1.3845, 1.4168, 1.4302], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6640, -0.6903, -0.7119, -0.7274, -0.7349, -0.7349, -0.7277, -0.7109, -0.6958, -0.6820, -0.6771, -0.6748,
0:         -0.6810, -0.6888, -0.7025, -0.7169, -0.7380, -0.7553, -0.6778, -0.7050, -0.7281, -0.7458, -0.7520, -0.7551,
0:         -0.7458], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0737, 0.1189, 0.0820, 0.1083, 0.0535, 0.0604, 0.1564, 0.1220, 0.0207, 0.0017, 0.0051, 0.0101, 0.0900, 0.1112,
0:         0.0049, 0.0425, 0.1289, 0.0070, 0.0236, 0.1076, 0.0993, 0.1386, 0.0857, 0.1206, 0.2217], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1821, -0.1741, -0.1114, -0.0567, -0.0168, -0.0234, -0.0395, -0.0427, -0.0165,  0.0325,  0.0756,  0.0924,
0:          0.0720,  0.0671,  0.0388,  0.0295,  0.0143,  0.0071, -0.0132, -0.0383, -0.0677, -0.0850, -0.0698, -0.0241,
0:          0.0199], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2253, -0.2285, -0.2180, -0.2272, -0.2171, -0.2072, -0.2076, -0.2047, -0.2052, -0.2505, -0.2473, -0.2415,
0:         -0.2348, -0.2282, -0.2265, -0.2144, -0.2305, -0.2148, -0.2665, -0.2686, -0.2588, -0.2515, -0.2432, -0.2354,
0:         -0.2383], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 21 [1/5 (20%)]	Loss: 0.68007 : 0.41248 :: 0.17373 (2.78 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 21 [2/5 (40%)]	Loss: 1.16462 : 0.54953 :: 0.17974 (15.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 21 [3/5 (60%)]	Loss: 0.77088 : 0.43555 :: 0.17569 (14.94 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 21 [4/5 (80%)]	Loss: 1.04869 : 0.54238 :: 0.17838 (14.91 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.269, max = 2.668, mean = 0.005
0:          sample (first 20): tensor([-0.1776, -0.1847, -0.1842, -0.1861, -0.1798, -0.1736, -0.1721, -0.1660, -0.1591, -0.2006, -0.2032, -0.1999,
0:         -0.1988, -0.1950, -0.1958, -0.1813, -0.1909, -0.1719, -0.2151, -0.2202])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 21 : 0.5081369876861572
0: validation loss for velocity_u : 0.4147254228591919
0: validation loss for velocity_v : 0.1550915539264679
0: validation loss for specific_humidity : 0.05102336406707764
0: validation loss for velocity_z : 0.9043095111846924
0: validation loss for temperature : 0.43298426270484924
0: validation loss for total_precip : 1.0906867980957031
0: 22 : 21:55:37 :: batch_size = 96, lr = 1.2205418857176618e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 22, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9296, -0.9414, -0.9708, -1.0341, -1.1200, -1.2033, -1.2650, -1.2938, -1.3069, -1.3269, -1.3543, -1.3766,
0:         -1.3853, -1.3822, -1.3740, -1.3650, -1.3556, -1.3440, -0.9775, -0.9706, -0.9783, -1.0293, -1.1087, -1.1825,
0:         -1.2379], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1435, -0.1313, -0.0661, -0.0175, -0.0259, -0.0490, -0.0346,  0.0051,  0.0413,  0.0652,  0.0794,  0.0890,
0:          0.0948,  0.0970,  0.0950,  0.0914,  0.0883,  0.0874, -0.2414, -0.2343, -0.1719, -0.1067, -0.0756, -0.0610,
0:         -0.0348], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 2.3468,  1.3421,  0.0216, -0.6068, -0.7184, -0.6431, -0.6164, -0.6322, -0.6498, -0.6567, -0.6625, -0.6674,
0:         -0.6661, -0.6603, -0.6541, -0.6475, -0.6424, -0.6386,  1.9782,  0.8575, -0.2516, -0.7094, -0.6911, -0.6140,
0:         -0.6237], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3448,  0.2522, -0.1002, -0.4448, -0.3511, -0.0645,  0.0359,  0.0593,  0.0191, -0.0656, -0.1091, -0.1047,
0:         -0.0924, -0.1180, -0.1247, -0.1504, -0.1816, -0.2641,  0.2422,  0.1217, -0.2485, -0.5742, -0.5128, -0.2262,
0:          0.0203], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.4085,  0.9083,  1.3289,  1.4534,  1.4286,  1.4332,  1.4510,  1.4281,  1.3783,  1.3065,  1.2072,  1.0862,
0:          0.9526,  0.8397,  0.7309,  0.6167,  0.4967,  0.3860,  0.3166,  0.2789,  0.2500,  0.1802,  0.0701, -0.0643,
0:         -0.2008], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 22, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([        nan,         nan,         nan,         nan,         nan,         nan, -2.5361e-01,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  1.6729e-01,
0:                 nan,  6.8546e-05,         nan,         nan,         nan,         nan,         nan,         nan,
0:          5.6885e-01,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,  3.7546e-01,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,  2.8508e-02,         nan,
0:                 nan,         nan,  3.2882e-01,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,  1.5136e-01, -2.0408e-02,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  1.7980e-01,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan, -3.2921e-02,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan, -8.1836e-02,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan, -9.7762e-02,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,  1.2861e-01,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan, -2.0014e-01,         nan,         nan,         nan,
0:         -2.5361e-01,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:         -2.4678e-01,         nan,         nan,         nan,         nan,         nan,         nan, -8.8661e-02,
0:                 nan,         nan, -2.4109e-01,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan, -1.3758e-01,
0:                 nan,         nan,         nan,         nan,         nan, -9.5486e-02,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,  6.8602e-01,         nan,         nan,  5.4114e+00,  3.8291e+00,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  4.8813e+00,
0:                 nan,         nan,         nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 22, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0650, -0.0904, -0.1501, -0.2277, -0.2939, -0.3264, -0.3451, -0.3544, -0.3503, -0.3644, -0.3610, -0.3464,
0:         -0.3201, -0.2935, -0.2883, -0.3078, -0.3487, -0.3960, -0.1096, -0.1164, -0.1695, -0.2552, -0.3315, -0.3748,
0:         -0.3899], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3654, -0.3572, -0.3452, -0.3141, -0.2518, -0.1692, -0.1130, -0.1002, -0.1361, -0.2005, -0.2556, -0.2794,
0:         -0.2913, -0.2788, -0.2854, -0.2884, -0.2947, -0.2718, -0.3559, -0.3290, -0.3274, -0.3291, -0.3076, -0.2783,
0:         -0.2436], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6020, -0.7571, -0.8690, -0.8989, -0.8671, -0.8067, -0.7409, -0.7018, -0.6905, -0.6833, -0.6853, -0.6678,
0:         -0.6531, -0.6449, -0.6566, -0.6826, -0.7415, -0.7917, -0.3986, -0.6144, -0.7954, -0.8930, -0.9138, -0.8773,
0:         -0.8235], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0878, -0.0367, -0.1480, -0.0624, -0.1518, -0.1473,  0.0175, -0.0046, -0.0208, -0.0192, -0.0465,  0.0474,
0:          0.1720,  0.2102,  0.0876,  0.0874,  0.1765, -0.0226,  0.2174,  0.1392, -0.0310, -0.0086, -0.0854, -0.0330,
0:          0.0873], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.1290,  0.1546,  0.1854,  0.1803,  0.1515,  0.0950,  0.0512,  0.0317,  0.0483,  0.0757,  0.0805,  0.0475,
0:         -0.0223, -0.0771, -0.1248, -0.1541, -0.1796, -0.1980, -0.2180, -0.2246, -0.2087, -0.1657, -0.0935, -0.0153,
0:          0.0410], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0493,  0.0371,  0.0276,  0.0170,  0.0087,  0.0089, -0.0024, -0.0020,  0.0088,  0.0444,  0.0327,  0.0306,
0:          0.0169,  0.0062, -0.0056,  0.0057, -0.0057,  0.0066,  0.0396,  0.0313,  0.0218,  0.0122,  0.0007,  0.0050,
0:          0.0007], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 22 [1/5 (20%)]	Loss: 0.78428 : 0.43367 :: 0.16870 (2.39 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 22 [2/5 (40%)]	Loss: 0.64882 : 0.49358 :: 0.17451 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 22 [3/5 (60%)]	Loss: 0.79590 : 0.43551 :: 0.17179 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 22 [4/5 (80%)]	Loss: 0.62262 : 0.39701 :: 0.17343 (15.04 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.287, max = 0.444, mean = -0.146
0:          sample (first 20): tensor([-0.2297, -0.2356, -0.2355, -0.2399, -0.2367, -0.2297, -0.2291, -0.2213, -0.2154, -0.2485, -0.2527, -0.2525,
0:         -0.2486, -0.2459, -0.2420, -0.2338, -0.2435, -0.2265, -0.2593, -0.2649])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 22 : 0.4501710534095764
0: validation loss for velocity_u : 0.41863247752189636
0: validation loss for velocity_v : 0.15368320047855377
0: validation loss for specific_humidity : 0.05558628961443901
0: validation loss for velocity_z : 0.8649213314056396
0: validation loss for temperature : 0.43004631996154785
0: validation loss for total_precip : 0.7781572341918945
0: 23 : 21:59:43 :: batch_size = 96, lr = 1.1907725714318652e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 23, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3180, -0.3159, -0.3136, -0.3115, -0.3094, -0.3071, -0.3050, -0.3029, -0.3006, -0.2985, -0.2964, -0.2943,
0:         -0.2922, -0.2901, -0.2879, -0.2858, -0.2837, -0.2816, -0.3471, -0.3453, -0.3434, -0.3416, -0.3396, -0.3378,
0:         -0.3359], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0844, -0.0810, -0.0776, -0.0741, -0.0709, -0.0677, -0.0645, -0.0615, -0.0585, -0.0556, -0.0528, -0.0500,
0:         -0.0472, -0.0446, -0.0422, -0.0395, -0.0371, -0.0349, -0.0120, -0.0098, -0.0073, -0.0051, -0.0031, -0.0009,
0:          0.0011], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6361, -0.6370, -0.6380, -0.6390, -0.6399, -0.6408, -0.6417, -0.6427, -0.6436, -0.6446, -0.6454, -0.6464,
0:         -0.6473, -0.6483, -0.6488, -0.6489, -0.6495, -0.6500, -0.6508, -0.6507, -0.6506, -0.6505, -0.6503, -0.6502,
0:         -0.6502], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2409, 0.2409, 0.2432, 0.2454, 0.2476, 0.2476, 0.2499, 0.2521, 0.2521, 0.2543, 0.2543, 0.2565, 0.2565, 0.2565,
0:         0.2588, 0.2588, 0.2588, 0.2588, 0.2856, 0.2878, 0.2900, 0.2923, 0.2945, 0.2967, 0.2990], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.4430, -1.4436, -1.4441, -1.4447, -1.4453, -1.4458, -1.4464, -1.4470, -1.4476, -1.4482, -1.4486, -1.4492,
0:         -1.4495, -1.4500, -1.4503, -1.4509, -1.4511, -1.4514, -1.4519, -1.4523, -1.4525, -1.4529, -1.4531, -1.4536,
0:         -1.4539], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 23, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2351,     nan, -0.2351,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2210,     nan,
0:             nan,     nan,     nan,     nan, -0.2221, -0.2210,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2210,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2245,
0:             nan,     nan,     nan, -0.2245,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2363,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2351,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2233,
0:             nan,     nan,     nan,     nan,     nan, -0.2233,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2292,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2363,     nan,     nan,     nan,     nan,
0:             nan, -0.2375,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2221,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2339,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 23, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2098, -0.2571, -0.3182, -0.3719, -0.3911, -0.3712, -0.3578, -0.3582, -0.3788, -0.4132, -0.4437, -0.4224,
0:         -0.3881, -0.3463, -0.3159, -0.3005, -0.2959, -0.2753, -0.1502, -0.1891, -0.2572, -0.3262, -0.3553, -0.3440,
0:         -0.3148], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2221, -0.2191, -0.2013, -0.1715, -0.1006, -0.0052,  0.0549,  0.0616,  0.0367, -0.0289, -0.0810, -0.1064,
0:         -0.1453, -0.1682, -0.2010, -0.2095, -0.1892, -0.1124, -0.1993, -0.1681, -0.1450, -0.1245, -0.0937, -0.0661,
0:         -0.0348], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5945, -0.6160, -0.6373, -0.6527, -0.6595, -0.6608, -0.6518, -0.6374, -0.6241, -0.6116, -0.6067, -0.6025,
0:         -0.6047, -0.6115, -0.6253, -0.6390, -0.6589, -0.6678, -0.5932, -0.6194, -0.6453, -0.6653, -0.6742, -0.6808,
0:         -0.6720], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1293,  0.1360,  0.0782,  0.1410,  0.0553, -0.0060,  0.1194,  0.1173,  0.0029, -0.0385, -0.0581, -0.0682,
0:         -0.0119,  0.0303, -0.1179, -0.1218,  0.0704, -0.0591,  0.1656,  0.1878,  0.0951,  0.1738,  0.1476,  0.1401,
0:          0.2343], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3174, -0.2727, -0.2095, -0.1781, -0.1570, -0.1515, -0.1394, -0.1266, -0.1201, -0.1309, -0.1648, -0.2070,
0:         -0.2459, -0.2341, -0.2039, -0.1656, -0.1606, -0.1846, -0.2286, -0.2596, -0.2586, -0.2294, -0.1786, -0.1276,
0:         -0.1058], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2371, -0.2416, -0.2426, -0.2504, -0.2490, -0.2433, -0.2440, -0.2377, -0.2327, -0.2561, -0.2610, -0.2623,
0:         -0.2577, -0.2554, -0.2526, -0.2478, -0.2595, -0.2416, -0.2684, -0.2745, -0.2707, -0.2719, -0.2660, -0.2542,
0:         -0.2653], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 23 [1/5 (20%)]	Loss: 0.76540 : 0.45771 :: 0.16547 (2.80 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 23 [2/5 (40%)]	Loss: 0.41930 : 0.33923 :: 0.16731 (14.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 23 [3/5 (60%)]	Loss: 0.46196 : 0.34237 :: 0.16067 (15.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 23 [4/5 (80%)]	Loss: 0.76211 : 0.43526 :: 0.16689 (14.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.281, max = 1.734, mean = -0.113
0:          sample (first 20): tensor([-0.2410, -0.2428, -0.2457, -0.2503, -0.2493, -0.2476, -0.2464, -0.2393, -0.2321, -0.2556, -0.2612, -0.2614,
0:         -0.2581, -0.2562, -0.2506, -0.2481, -0.2565, -0.2420, -0.2647, -0.2703])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 23 : 0.4689137041568756
0: validation loss for velocity_u : 0.39313414692878723
0: validation loss for velocity_v : 0.1550140380859375
0: validation loss for specific_humidity : 0.06044384837150574
0: validation loss for velocity_z : 0.8395566940307617
0: validation loss for temperature : 0.4718876779079437
0: validation loss for total_precip : 0.893446147441864
0: 24 : 22:03:39 :: batch_size = 96, lr = 1.1617293379823076e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 24, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0476, -0.0636, -0.0788, -0.0900, -0.1034, -0.1221, -0.1446, -0.1748, -0.2136, -0.2535, -0.2868, -0.3113,
0:         -0.3292, -0.3485, -0.3705, -0.3911, -0.4056, -0.4090,  0.0062, -0.0078, -0.0252, -0.0403, -0.0553, -0.0707,
0:         -0.0862], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6189, -0.6321, -0.6444, -0.6574, -0.6763, -0.6942, -0.7078, -0.7205, -0.7299, -0.7303, -0.7220, -0.7042,
0:         -0.6776, -0.6446, -0.6053, -0.5617, -0.5125, -0.4596, -0.6518, -0.6608, -0.6708, -0.6846, -0.7059, -0.7231,
0:         -0.7337], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0136,  0.0435,  0.1216,  0.2700,  0.4398,  0.5411,  0.6174,  0.6813,  0.7405,  0.7844,  0.8347,  0.8749,
0:          0.8827,  0.8827,  0.8459,  0.8128,  0.7712,  0.7404, -0.0108,  0.0342,  0.1023,  0.2106,  0.3802,  0.5497,
0:          0.6308], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1321,  0.0633,  0.0744, -0.0143,  0.0389,  0.1276,  0.3095,  0.4935,  0.6288,  0.7707,  0.8017,  0.8727,
0:          0.7995,  0.7463,  0.6931,  0.5423,  0.4492,  0.2806,  0.1520,  0.0833,  0.0788,  0.0034,  0.0034,  0.0700,
0:          0.2097], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2013, -0.2780, -0.3496, -0.3982, -0.3965, -0.3413, -0.2864, -0.2478, -0.2163, -0.2013, -0.1975, -0.2020,
0:         -0.2030, -0.1881, -0.1624, -0.1304, -0.0992, -0.0711, -0.0468, -0.0165,  0.0162,  0.0439,  0.0660,  0.0736,
0:          0.0683], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 24, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,
0:         -0.2451,     nan,     nan,     nan,     nan,     nan, -0.2451, -0.2451,     nan, -0.2451,     nan, -0.2451,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan, -0.2451,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451, -0.2451,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,
0:         -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,
0:             nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2428, -0.2393,
0:             nan, -0.2324,     nan, -0.2451,     nan,     nan, -0.2451, -0.2428,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2254,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2451,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,
0:             nan, -0.2451,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 24, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1850, -0.1782, -0.1692, -0.1772, -0.1904, -0.1883, -0.1950, -0.1973, -0.2057, -0.2371, -0.2614, -0.2775,
0:         -0.2839, -0.2768, -0.2704, -0.2637, -0.2646, -0.2720, -0.1977, -0.1786, -0.1747, -0.1959, -0.2110, -0.2133,
0:         -0.1941], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1087,  0.1599,  0.2002,  0.2222,  0.2503,  0.2893,  0.3026,  0.2809,  0.2296,  0.1435,  0.0615,  0.0046,
0:         -0.0499, -0.0828, -0.1269, -0.1514, -0.1676, -0.1555,  0.0782,  0.1436,  0.1623,  0.1471,  0.1318,  0.1163,
0:          0.1164], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1874, -0.2406, -0.2821, -0.3001, -0.2998, -0.2854, -0.2636, -0.2354, -0.2111, -0.1951, -0.1921, -0.1861,
0:         -0.1865, -0.1922, -0.2106, -0.2363, -0.2801, -0.3288, -0.1776, -0.2360, -0.2879, -0.3168, -0.3174, -0.3037,
0:         -0.2776], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0997, -0.0391, -0.1166,  0.0119, -0.0459, -0.0866, -0.0395, -0.0894, -0.1334, -0.1025, -0.0344, -0.0034,
0:          0.0180,  0.0517, -0.0458, -0.0257,  0.1136, -0.0161,  0.1667,  0.0654, -0.0126,  0.0793,  0.0209,  0.0462,
0:          0.1102], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.5431, 0.6424, 0.7572, 0.8011, 0.7771, 0.6803, 0.5683, 0.5164, 0.5307, 0.5901, 0.6234, 0.5938, 0.5041, 0.4382,
0:         0.3938, 0.3852, 0.3542, 0.2964, 0.1923, 0.1028, 0.0593, 0.0775, 0.1409, 0.2282, 0.3034], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2204, -0.2244, -0.2292, -0.2369, -0.2342, -0.2331, -0.2336, -0.2265, -0.2164, -0.2354, -0.2401, -0.2444,
0:         -0.2432, -0.2388, -0.2366, -0.2334, -0.2391, -0.2264, -0.2455, -0.2500, -0.2463, -0.2502, -0.2462, -0.2367,
0:         -0.2492], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 24 [1/5 (20%)]	Loss: 0.84839 : 0.47223 :: 0.17636 (2.56 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 24 [2/5 (40%)]	Loss: 0.80512 : 0.46392 :: 0.16677 (15.24 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 24 [3/5 (60%)]	Loss: 1.26008 : 0.52377 :: 0.16535 (15.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 24 [4/5 (80%)]	Loss: 0.85389 : 0.47717 :: 0.17199 (15.03 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.237, max = 6.669, mean = 0.280
0:          sample (first 20): tensor([-0.1968, -0.1941, -0.2041, -0.2035, -0.2038, -0.2055, -0.2026, -0.1950, -0.1807, -0.2093, -0.2128, -0.2099,
0:         -0.2131, -0.2110, -0.2084, -0.2031, -0.2050, -0.1918, -0.2165, -0.2217])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 24 : 0.42083966732025146
0: validation loss for velocity_u : 0.3842644989490509
0: validation loss for velocity_v : 0.14967933297157288
0: validation loss for specific_humidity : 0.0493074506521225
0: validation loss for velocity_z : 0.8112220764160156
0: validation loss for temperature : 0.43995144963264465
0: validation loss for total_precip : 0.6906137466430664
0: 25 : 22:07:40 :: batch_size = 96, lr = 1.1333944760803001e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 25, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2107, -0.2090, -0.2082, -0.2115, -0.2142, -0.2218, -0.2308, -0.2382, -0.2418, -0.2388, -0.2375, -0.2375,
0:         -0.2422, -0.2523, -0.2690, -0.2921, -0.3169, -0.3416, -0.2168, -0.2120, -0.2082, -0.2063, -0.2043, -0.2058,
0:         -0.2063], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3101, -0.2879, -0.2683, -0.2500, -0.2322, -0.2164, -0.1991, -0.1811, -0.1627, -0.1454, -0.1329, -0.1256,
0:         -0.1212, -0.1152, -0.1064, -0.0895, -0.0563, -0.0131, -0.3222, -0.3080, -0.2942, -0.2781, -0.2598, -0.2427,
0:         -0.2247], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2180, 0.2423, 0.3039, 0.3917, 0.5071, 0.6342, 0.7646, 0.9515, 1.1445, 1.4323, 1.7962, 2.1679, 2.4304, 2.5885,
0:         2.6354, 2.6728, 2.7077, 2.6650, 0.2492, 0.2736, 0.3418, 0.4443, 0.5644, 0.7048, 0.8449], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1861, 0.2817, 0.3687, 0.4078, 0.3839, 0.3774, 0.3013, 0.2557, 0.2100, 0.1818, 0.2709, 0.3404, 0.4447, 0.5686,
0:         0.6055, 0.6707, 0.7033, 0.6881, 0.2231, 0.2252, 0.2731, 0.3361, 0.2904, 0.3035, 0.2926], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.6589,  0.7643,  0.8602,  0.9306,  0.9719,  1.0244,  1.0355,  0.9867,  0.8865,  0.7085,  0.4476,  0.1453,
0:         -0.1532, -0.4062, -0.6026, -0.7921, -0.9107, -0.9325, -0.8810, -0.7782, -0.6589, -0.5245, -0.4381, -0.3919,
0:         -0.3339], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 25, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2392,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1460,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.0736,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan, -0.2392,
0:             nan,     nan,     nan,     nan, -0.1897,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1162,  0.0127,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2024,     nan,     nan,     nan, -0.2288,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0161,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0046,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 25, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1405, -0.1697, -0.2114, -0.2419, -0.2635, -0.2728, -0.2868, -0.3009, -0.3184, -0.3366, -0.3430, -0.3410,
0:         -0.3281, -0.3070, -0.3014, -0.2976, -0.3095, -0.3181, -0.1705, -0.1866, -0.2219, -0.2629, -0.2990, -0.3216,
0:         -0.3393], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2992, -0.2918, -0.2782, -0.2575, -0.2169, -0.1584, -0.1209, -0.1241, -0.1637, -0.2312, -0.2917, -0.3153,
0:         -0.3303, -0.3197, -0.3148, -0.2829, -0.2340, -0.1510, -0.3007, -0.2800, -0.2828, -0.2972, -0.2898, -0.2670,
0:         -0.2337], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.7416, 1.6995, 1.6799, 1.6655, 1.6685, 1.6687, 1.6878, 1.7275, 1.8082, 1.9296, 2.0732, 2.1954, 2.3114, 2.3964,
0:         2.4580, 2.5352, 2.6080, 2.6637, 1.6915, 1.6739, 1.6642, 1.6496, 1.6670, 1.6677, 1.6752], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1478,  0.1544,  0.0852,  0.1403,  0.0756,  0.0062,  0.0454,  0.0090, -0.0776, -0.0998, -0.0352,  0.0246,
0:          0.0613,  0.0783, -0.0472, -0.0694,  0.0782, -0.0289,  0.1747,  0.1937,  0.0955,  0.1379,  0.1169,  0.1213,
0:          0.1625], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1836, -0.2019, -0.1708, -0.1328, -0.1212, -0.1433, -0.1820, -0.1875, -0.1283, -0.0265,  0.0533,  0.0702,
0:          0.0139, -0.0520, -0.1092, -0.1250, -0.1253, -0.1064, -0.1100, -0.1110, -0.0770,  0.0122,  0.1573,  0.3376,
0:          0.4778], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1790, -0.1774, -0.1904, -0.1910, -0.1931, -0.1939, -0.1876, -0.1749, -0.1573, -0.1895, -0.1943, -0.1951,
0:         -0.2029, -0.2033, -0.2018, -0.1938, -0.1935, -0.1707, -0.1908, -0.1975, -0.2014, -0.2024, -0.2052, -0.2017,
0:         -0.2114], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 25 [1/5 (20%)]	Loss: 0.68845 : 0.44873 :: 0.16485 (2.50 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 25 [2/5 (40%)]	Loss: 0.80984 : 0.46554 :: 0.16996 (15.04 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 25 [3/5 (60%)]	Loss: 0.63467 : 0.36977 :: 0.16496 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 25 [4/5 (80%)]	Loss: 0.67216 : 0.47647 :: 0.17071 (14.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.242, max = 0.313, mean = -0.170
0:          sample (first 20): tensor([-0.1955, -0.1910, -0.1995, -0.1992, -0.2018, -0.2039, -0.2022, -0.2008, -0.1844, -0.2022, -0.2091, -0.2015,
0:         -0.2091, -0.2068, -0.2075, -0.2057, -0.2062, -0.1962, -0.2094, -0.2147])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 25 : 0.4152873754501343
0: validation loss for velocity_u : 0.38127151131629944
0: validation loss for velocity_v : 0.15513190627098083
0: validation loss for specific_humidity : 0.043144408613443375
0: validation loss for velocity_z : 0.8343172073364258
0: validation loss for temperature : 0.478085994720459
0: validation loss for total_precip : 0.5997737050056458
0: 26 : 22:11:43 :: batch_size = 96, lr = 1.1057507083710246e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 26, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -1.3787,
0:         -1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -1.3787, -0.2536, -0.2574, -0.2612, -0.2651, -0.2689, -0.2727,
0:         -0.2767], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -0.6728,
0:         -0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -0.6728, -1.0904, -1.0927, -1.0947, -1.0968, -1.0989, -1.1010,
0:         -1.1031], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5778,
0:         -0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5778, -0.5791, -0.5791, -0.5791, -0.5791, -0.5792, -0.5792,
0:         -0.5792], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1639,
0:         -0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1639, -0.1551, -0.1551, -0.1551, -0.1551, -0.1551, -0.1551,
0:         -0.1551], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966,
0:         0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966, 0.0966], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 26, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2285, -0.2285,     nan,
0:         -0.2285,     nan, -0.2285,     nan,     nan,     nan,     nan, -0.2240,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2274,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2262,
0:             nan,     nan,     nan, -0.2240,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2262,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2262,     nan,     nan,     nan,     nan,
0:         -0.2308,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2262, -0.2262,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2240,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2240, -0.2228,     nan, -0.2228,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2240,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2320, -0.2320,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2274,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2217,
0:             nan, -0.2217,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2183,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2183,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2217,
0:         -0.2217,     nan, -0.2228,     nan,     nan,     nan,     nan, -0.2228,     nan,     nan,     nan,     nan,
0:         -0.2240,     nan, -0.2240])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 26, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4617, -0.4720, -0.5037, -0.5437, -0.5731, -0.5849, -0.5995, -0.6148, -0.6108, -0.6126, -0.6021, -0.5440,
0:         -0.4927, -0.4360, -0.3949, -0.3800, -0.3786, -0.4001, -0.3869, -0.3625, -0.3810, -0.4218, -0.4662, -0.4966,
0:         -0.5233], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3649,  0.3818,  0.3989,  0.4183,  0.4535,  0.5001,  0.5264,  0.5286,  0.5033,  0.4584,  0.4167,  0.4048,
0:          0.4063,  0.4322,  0.4538,  0.4904,  0.5326,  0.6181, -0.0346, -0.0201, -0.0296, -0.0432, -0.0450, -0.0355,
0:         -0.0206], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6460, -0.6710, -0.6932, -0.7064, -0.7073, -0.6982, -0.6817, -0.6633, -0.6520, -0.6483, -0.6530, -0.6563,
0:         -0.6628, -0.6688, -0.6818, -0.6972, -0.7218, -0.7407, -0.6491, -0.6782, -0.7076, -0.7267, -0.7318, -0.7309,
0:         -0.7147], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1447,  0.0864, -0.0117,  0.0289, -0.0199, -0.0251,  0.0974,  0.0480, -0.0581, -0.0771, -0.0942, -0.0121,
0:          0.1120,  0.0960, -0.0975, -0.1428, -0.0092, -0.0725,  0.2675,  0.2452,  0.0949,  0.1181,  0.1072,  0.1218,
0:          0.2026], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0037,  0.0183,  0.0405,  0.0336,  0.0163, -0.0141, -0.0274, -0.0209,  0.0053,  0.0298,  0.0346,  0.0207,
0:         -0.0105, -0.0143, -0.0089,  0.0157,  0.0330,  0.0386,  0.0266,  0.0081, -0.0079, -0.0098,  0.0140,  0.0554,
0:          0.0851], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2091, -0.2052, -0.2135, -0.2164, -0.2203, -0.2242, -0.2245, -0.2246, -0.2111, -0.2139, -0.2235, -0.2161,
0:         -0.2257, -0.2240, -0.2247, -0.2237, -0.2313, -0.2189, -0.2207, -0.2286, -0.2296, -0.2325, -0.2313, -0.2278,
0:         -0.2360], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 26 [1/5 (20%)]	Loss: 0.77687 : 0.46239 :: 0.16400 (2.44 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 26 [2/5 (40%)]	Loss: 0.80792 : 0.43858 :: 0.17098 (15.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 26 [3/5 (60%)]	Loss: 0.93915 : 0.51013 :: 0.17417 (15.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 26 [4/5 (80%)]	Loss: 1.01611 : 0.50634 :: 0.17490 (15.02 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.252, max = 0.920, mean = -0.104
0:          sample (first 20): tensor([-0.2160, -0.2103, -0.2151, -0.2163, -0.2185, -0.2168, -0.2128, -0.2146, -0.1968, -0.2184, -0.2278, -0.2172,
0:         -0.2215, -0.2244, -0.2223, -0.2216, -0.2221, -0.2113, -0.2248, -0.2327])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 26 : 0.4589994251728058
0: validation loss for velocity_u : 0.38664016127586365
0: validation loss for velocity_v : 0.16279585659503937
0: validation loss for specific_humidity : 0.05749627947807312
0: validation loss for velocity_z : 0.8024960160255432
0: validation loss for temperature : 0.48201438784599304
0: validation loss for total_precip : 0.8625544905662537
0: 27 : 22:15:47 :: batch_size = 96, lr = 1.0787811788985607e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 27, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.5666, 2.5869, 2.6126, 2.6259, 2.6329, 2.6519, 2.6843, 2.7139, 2.7310, 2.7402, 2.7486, 2.7566, 2.7652, 2.7782,
0:         2.7964, 2.8194, 2.8529, 2.8985, 2.5519, 2.5714, 2.5941, 2.6006, 2.5978, 2.6075, 2.6326], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8215, -0.8203, -0.8213, -0.8261, -0.8306, -0.8310, -0.8304, -0.8374, -0.8552, -0.8783, -0.8975, -0.9070,
0:         -0.9085, -0.9048, -0.8975, -0.8870, -0.8742, -0.8595, -0.8091, -0.8075, -0.8058, -0.8035, -0.8025, -0.8039,
0:         -0.8075], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2497, -0.2656, -0.2775, -0.2917, -0.3080, -0.3172, -0.3271, -0.3371, -0.3642, -0.4014, -0.4464, -0.4871,
0:         -0.5185, -0.5418, -0.5529, -0.5586, -0.5605, -0.5564, -0.2222, -0.2316, -0.2415, -0.2554, -0.2726, -0.2866,
0:         -0.2986], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3565, -0.4720, -0.0534,  0.3809,  0.1654, -0.4193, -0.6269, -0.4923, -0.6561, -1.1308, -1.2195, -0.6572,
0:          0.0442,  0.4617,  0.6559,  0.6559,  0.3820,  0.0812, -0.5517, -0.6662, -0.1623,  0.4381,  0.2889, -0.4114,
0:         -0.7133], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4974, -0.5075, -0.5187, -0.5222, -0.5230, -0.5351, -0.5583, -0.5732, -0.5656, -0.5442, -0.5275, -0.5264,
0:         -0.5391, -0.5573, -0.5746, -0.5907, -0.6092, -0.6317, -0.6541, -0.6714, -0.6832, -0.6962, -0.7142, -0.7315,
0:         -0.7407], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 27, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1970,     nan,     nan,     nan,  0.0615,     nan,     nan,     nan, -0.0183, -0.0617,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.3936,     nan,     nan,     nan,     nan,  1.8126,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          2.2474,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  5.4817,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  7.4793,     nan,     nan,     nan,     nan, -0.0774,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.3839,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.9298,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  2.2064,     nan,  1.6870,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  5.5662,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  8.0602,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  8.2063,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 27, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5595, 0.5870, 0.6495, 0.7235, 0.7829, 0.8132, 0.8035, 0.7527, 0.6931, 0.6387, 0.6330, 0.6560, 0.7060, 0.7799,
0:         0.8254, 0.8851, 0.9762, 1.0660, 0.5186, 0.5348, 0.5541, 0.5885, 0.6307, 0.6793, 0.6984], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7510, 0.8475, 0.9554, 1.0319, 1.0945, 1.1324, 1.1086, 1.0516, 0.9697, 0.8807, 0.8204, 0.8328, 0.8645, 0.9339,
0:         0.9836, 0.9959, 0.9753, 0.9421, 0.7883, 0.9126, 1.0205, 1.0884, 1.1192, 1.0881, 1.0446], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2373, 0.2310, 0.2333, 0.2401, 0.2536, 0.2742, 0.2888, 0.2982, 0.3029, 0.3127, 0.3218, 0.3433, 0.3577, 0.3562,
0:         0.3317, 0.2909, 0.2246, 0.1443, 0.2911, 0.2830, 0.2764, 0.2761, 0.2896, 0.3043, 0.3191], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.8563, -2.2029, -2.1417, -1.7923, -1.7454, -1.7861, -1.4844, -1.3232, -1.1909, -0.8766, -0.7607, -0.6743,
0:         -0.5133, -0.6794, -0.8548, -0.6097, -0.5771, -0.7923, -2.1270, -2.3927, -2.4503, -2.2924, -2.3366, -2.4269,
0:         -2.1831], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1967, -0.1372, -0.0438,  0.0016, -0.0084, -0.0842, -0.1512, -0.1519, -0.0961, -0.0173,  0.0357,  0.0309,
0:         -0.0113, -0.0114, -0.0230, -0.0296, -0.0818, -0.1492, -0.2170, -0.2502, -0.2532, -0.2356, -0.1981, -0.1454,
0:         -0.0854], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.4637, 0.4792, 0.4698, 0.4512, 0.4343, 0.4269, 0.4064, 0.3825, 0.3903, 0.4649, 0.4600, 0.4678, 0.4485, 0.4402,
0:         0.4197, 0.4226, 0.4185, 0.4005, 0.4541, 0.4487, 0.4368, 0.4344, 0.4235, 0.4175, 0.4175], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 27 [1/5 (20%)]	Loss: 0.74587 : 0.44125 :: 0.18179 (2.70 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 27 [2/5 (40%)]	Loss: 0.76394 : 0.42883 :: 0.17919 (15.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 27 [3/5 (60%)]	Loss: 1.11852 : 0.47124 :: 0.17116 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 27 [4/5 (80%)]	Loss: 0.69015 : 0.41979 :: 0.17710 (15.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.259, max = 2.261, mean = -0.042
0:          sample (first 20): tensor([-0.1845, -0.1825, -0.1867, -0.1841, -0.1864, -0.1836, -0.1804, -0.1837, -0.1639, -0.1919, -0.2008, -0.1864,
0:         -0.1908, -0.1950, -0.1946, -0.1927, -0.1900, -0.1798, -0.1965, -0.2084])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 27 : 0.4466763734817505
0: validation loss for velocity_u : 0.360519140958786
0: validation loss for velocity_v : 0.15521620213985443
0: validation loss for specific_humidity : 0.056021708995103836
0: validation loss for velocity_z : 0.8365967869758606
0: validation loss for temperature : 0.48143887519836426
0: validation loss for total_precip : 0.7902655005455017
0: 28 : 22:19:42 :: batch_size = 96, lr = 1.0524694428278642e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 28, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8353, 0.8500, 0.8655, 0.8794, 0.8916, 0.9027, 0.9110, 0.9140, 0.9090, 0.8944, 0.8696, 0.8338, 0.7898, 0.7437,
0:         0.7016, 0.6671, 0.6406, 0.6233, 0.8660, 0.8872, 0.9083, 0.9230, 0.9293, 0.9277, 0.9193], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.9292, 2.8891, 2.8407, 2.7848, 2.7235, 2.6584, 2.5908, 2.5236, 2.4615, 2.4094, 2.3713, 2.3517, 2.3558, 2.3869,
0:         2.4416, 2.5117, 2.5854, 2.6512, 2.8382, 2.8010, 2.7536, 2.6991, 2.6412, 2.5822, 2.5234], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3805, -0.3659, -0.3564, -0.3463, -0.3352, -0.3238, -0.3116, -0.2991, -0.2829, -0.2615, -0.2353, -0.2005,
0:         -0.1620, -0.1123, -0.0619, -0.0094,  0.0401,  0.0829, -0.4153, -0.3971, -0.3855, -0.3748, -0.3612, -0.3440,
0:         -0.3235], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.0971, 1.9908, 1.8803, 1.7514, 1.6548, 1.6376, 1.6774, 1.7514, 1.8942, 2.1272, 2.3773, 2.5287, 2.5147, 2.3172,
0:         1.9200, 1.3639, 0.7863, 0.3333, 2.1937, 2.0596, 1.8534, 1.6076, 1.4186, 1.3489, 1.3725], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.2685, 0.2724, 0.2697, 0.2644, 0.2607, 0.2615, 0.2679, 0.2803, 0.2973, 0.3176, 0.3398, 0.3644, 0.3919, 0.4231,
0:         0.4564, 0.4906, 0.5251, 0.5606, 0.5984, 0.6401, 0.6869, 0.7379, 0.7896, 0.8378, 0.8797], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 28, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan, -0.2339,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan, -0.2339,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2339,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan,     nan,
0:         -0.2339,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan,     nan,
0:         -0.2339,     nan,     nan, -0.2339, -0.2339,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan, -0.2339,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,
0:             nan,     nan, -0.2339,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2339,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 28, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0071, 0.9563, 0.9218, 0.8925, 0.8928, 0.9030, 0.8876, 0.8431, 0.7892, 0.7069, 0.6471, 0.6198, 0.5973, 0.5870,
0:         0.5488, 0.5176, 0.5072, 0.5045, 1.0917, 1.0409, 0.9746, 0.9222, 0.9103, 0.9151, 0.9231], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8100, 0.8796, 0.9568, 1.0222, 1.1079, 1.1900, 1.2041, 1.1524, 1.0781, 0.9769, 0.9542, 0.9978, 1.0547, 1.1457,
0:         1.2220, 1.3100, 1.4094, 1.5473, 0.7160, 0.7901, 0.8381, 0.8866, 0.9362, 0.9760, 0.9841], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5324, -0.5463, -0.5630, -0.5799, -0.6005, -0.6193, -0.6391, -0.6541, -0.6624, -0.6677, -0.6683, -0.6664,
0:         -0.6651, -0.6687, -0.6752, -0.6853, -0.6981, -0.7027, -0.5387, -0.5569, -0.5749, -0.5962, -0.6142, -0.6316,
0:         -0.6471], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 2.0987e-01,  2.7492e-01,  9.9094e-02,  2.5490e-02, -1.8043e-02,  8.2904e-02,  2.7428e-01,  2.0711e-01,
0:          5.3473e-02,  1.2315e-02, -3.3476e-02, -2.1252e-02,  1.0118e-01,  1.2576e-01, -1.1551e-01, -1.2769e-01,
0:          1.1046e-01, -1.3488e-04,  4.0202e-01,  3.5153e-01,  8.2673e-02,  1.5710e-02,  7.5607e-02,  1.9678e-01,
0:          3.2857e-01], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0990,  0.0314,  0.1225,  0.1183,  0.0636, -0.0313, -0.1138, -0.1680, -0.1759, -0.1471, -0.1062, -0.0701,
0:         -0.0607, -0.0489, -0.0394, -0.0225, -0.0126, -0.0193, -0.0543, -0.0924, -0.1038, -0.0652,  0.0261,  0.1341,
0:          0.2081], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2342, -0.2304, -0.2320, -0.2297, -0.2302, -0.2264, -0.2254, -0.2322, -0.2151, -0.2425, -0.2489, -0.2343,
0:         -0.2306, -0.2372, -0.2347, -0.2375, -0.2375, -0.2306, -0.2447, -0.2563, -0.2492, -0.2464, -0.2497, -0.2432,
0:         -0.2398], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 28 [1/5 (20%)]	Loss: 0.80600 : 0.43177 :: 0.16969 (2.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 28 [2/5 (40%)]	Loss: 1.45613 : 0.59907 :: 0.17529 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 28 [3/5 (60%)]	Loss: 0.69765 : 0.39494 :: 0.16444 (15.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 28 [4/5 (80%)]	Loss: 0.72213 : 0.42348 :: 0.18564 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.253, max = 0.020, mean = -0.184
0:          sample (first 20): tensor([-0.1985, -0.1993, -0.1991, -0.1979, -0.1987, -0.1950, -0.1935, -0.1979, -0.1822, -0.2086, -0.2123, -0.1997,
0:         -0.2009, -0.2064, -0.2013, -0.2052, -0.2046, -0.1948, -0.2094, -0.2197])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 28 : 0.46269428730010986
0: validation loss for velocity_u : 0.2949084937572479
0: validation loss for velocity_v : 0.14813335239887238
0: validation loss for specific_humidity : 0.05170829966664314
0: validation loss for velocity_z : 0.923283576965332
0: validation loss for temperature : 0.4290114939212799
0: validation loss for total_precip : 0.9291202425956726
0: 29 : 22:23:35 :: batch_size = 96, lr = 1.0267994564174285e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 29, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7126, -0.7285, -0.7420, -0.7554, -0.7708, -0.7913, -0.8188, -0.8494, -0.8763, -0.8900, -0.8857, -0.8747,
0:         -0.8722, -0.8865, -0.9155, -0.9495, -0.9832, -1.0130, -0.6968, -0.7145, -0.7282, -0.7398, -0.7531, -0.7721,
0:         -0.8006], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2510, -0.2371, -0.2188, -0.1960, -0.1646, -0.1151, -0.0311,  0.0996,  0.2714,  0.4548,  0.5988,  0.6677,
0:          0.6640,  0.5868,  0.4565,  0.3437,  0.2941,  0.3026, -0.2727, -0.2564, -0.2358, -0.2108, -0.1773, -0.1270,
0:         -0.0444], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8156, -0.8153, -0.8107, -0.7949, -0.7382, -0.6370, -0.5140, -0.3962, -0.3199, -0.3090, -0.3566, -0.4538,
0:         -0.5854, -0.7116, -0.7926, -0.7947, -0.7684, -0.7429, -0.8174, -0.8184, -0.8141, -0.7986, -0.7401, -0.6352,
0:         -0.5025], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1063, -0.0686, -0.0144,  0.0663,  0.1947,  0.2423,  0.2434,  0.2478,  0.2290,  0.2622,  0.2754,  0.2268,
0:          0.2091,  0.2013,  0.1659,  0.1106,  0.0409, -0.0421, -0.1262, -0.0576,  0.0376,  0.1372,  0.1969,  0.2124,
0:          0.1803], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.0978, 0.0924, 0.1058, 0.1347, 0.1725, 0.2112, 0.2490, 0.2847, 0.3126, 0.3285, 0.3306, 0.3195, 0.3073, 0.3085,
0:         0.3196, 0.3385, 0.3643, 0.3904, 0.4158, 0.4395, 0.4623, 0.4763, 0.4745, 0.4669, 0.4607], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 29, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2259,     nan, -0.2438, -0.2438,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan, -0.2169,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2237,
0:             nan,     nan, -0.2349,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan, -0.2438,     nan,     nan,
0:             nan,     nan, -0.1855,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
0:             nan, -0.1900,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,  0.1576,     nan, -0.2438,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2416,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2438,     nan, -0.1766,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.2226,     nan,     nan, -0.1945,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0151,  0.0365,     nan,     nan,     nan, -0.2438,
0:             nan,     nan,     nan, -0.0017,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 29, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3431, -0.3670, -0.4277, -0.5062, -0.5646, -0.5944, -0.6094, -0.6256, -0.6367, -0.6694, -0.6969, -0.6921,
0:         -0.6766, -0.6419, -0.6124, -0.5944, -0.5968, -0.6088, -0.3425, -0.3525, -0.4132, -0.4945, -0.5680, -0.5996,
0:         -0.6043], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7080, -0.7148, -0.7176, -0.7201, -0.6819, -0.6213, -0.5812, -0.5799, -0.6202, -0.7043, -0.7733, -0.8071,
0:         -0.8301, -0.8312, -0.8363, -0.8118, -0.7686, -0.6784, -0.6971, -0.6879, -0.7080, -0.7319, -0.7248, -0.6895,
0:         -0.6447], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5854, -0.6095, -0.6391, -0.6576, -0.6710, -0.6734, -0.6713, -0.6643, -0.6654, -0.6719, -0.6848, -0.6960,
0:         -0.7035, -0.7095, -0.7122, -0.7202, -0.7388, -0.7513, -0.5859, -0.6131, -0.6403, -0.6647, -0.6776, -0.6838,
0:         -0.6793], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0453, -0.0619, -0.2019, -0.1514, -0.2865, -0.3344, -0.1460, -0.2050, -0.3466, -0.3476, -0.4037, -0.4368,
0:         -0.3321, -0.2866, -0.4394, -0.3744, -0.1255, -0.2117,  0.2974,  0.2118, -0.0302, -0.0393, -0.1417, -0.1799,
0:         -0.0810], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0236,  0.0427,  0.1044,  0.0953,  0.0342, -0.0695, -0.1382, -0.1534, -0.0982, -0.0211,  0.0327,  0.0619,
0:          0.0742,  0.1276,  0.1908,  0.2467,  0.2435,  0.1862,  0.0822, -0.0056, -0.0219,  0.0537,  0.2179,  0.4068,
0:          0.5259], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2097, -0.2133, -0.2146, -0.2136, -0.2127, -0.2100, -0.2101, -0.2169, -0.2018, -0.2212, -0.2259, -0.2134,
0:         -0.2173, -0.2235, -0.2194, -0.2239, -0.2246, -0.2124, -0.2190, -0.2334, -0.2279, -0.2233, -0.2325, -0.2288,
0:         -0.2284], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 29 [1/5 (20%)]	Loss: 0.79321 : 0.44330 :: 0.18608 (2.49 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 29 [2/5 (40%)]	Loss: 0.85762 : 0.45229 :: 0.17990 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 29 [3/5 (60%)]	Loss: 0.66883 : 0.43622 :: 0.19193 (15.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 29 [4/5 (80%)]	Loss: 0.59001 : 0.37645 :: 0.18058 (15.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.245, max = 1.789, mean = -0.017
0:          sample (first 20): tensor([-0.1941, -0.1939, -0.1982, -0.1968, -0.1966, -0.1931, -0.1900, -0.1911, -0.1793, -0.2133, -0.2079, -0.2015,
0:         -0.1992, -0.2096, -0.2012, -0.2041, -0.2027, -0.1881, -0.2106, -0.2187])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 29 : 0.3883974850177765
0: validation loss for velocity_u : 0.3352900445461273
0: validation loss for velocity_v : 0.14456970989704132
0: validation loss for specific_humidity : 0.058320727199316025
0: validation loss for velocity_z : 0.670432984828949
0: validation loss for temperature : 0.45960450172424316
0: validation loss for total_precip : 0.6621670722961426
0: 30 : 22:27:39 :: batch_size = 96, lr = 1.0017555672365157e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 30, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2215, 1.2116, 1.1977, 1.1797, 1.1591, 1.1422, 1.1377, 1.1487, 1.1686, 1.1844, 1.1849, 1.1628, 1.1186, 1.0633,
0:         1.0124, 0.9748, 0.9529, 0.9457, 1.2215, 1.2213, 1.2222, 1.2177, 1.2064, 1.1924, 1.1800], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.6607, -1.6301, -1.6009, -1.5743, -1.5468, -1.5149, -1.4722, -1.4119, -1.3372, -1.2631, -1.2037, -1.1697,
0:         -1.1647, -1.1787, -1.1928, -1.1947, -1.1791, -1.1491, -1.6775, -1.6486, -1.6229, -1.6038, -1.5894, -1.5757,
0:         -1.5552], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1707, 0.1940, 0.2125, 0.2342, 0.2483, 0.2537, 0.2582, 0.2580, 0.2609, 0.2630, 0.2750, 0.2972, 0.3243, 0.3577,
0:         0.3833, 0.4001, 0.3973, 0.3894, 0.1150, 0.1402, 0.1657, 0.1905, 0.2072, 0.2249, 0.2324], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6443, -0.6020, -0.6510, -0.6800, -0.6109, -0.5263, -0.4416, -0.2612,  0.0439,  0.4426,  0.8546,  1.1241,
0:          1.1419,  0.9281,  0.6096,  0.3490,  0.2087,  0.1085, -0.7468, -0.5931, -0.5062, -0.4639, -0.4372, -0.4840,
0:         -0.5820], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.6101, -0.5775, -0.5485, -0.5230, -0.4981, -0.4773, -0.4647, -0.4584, -0.4529, -0.4387, -0.4073, -0.3593,
0:         -0.3072, -0.2655, -0.2437, -0.2430, -0.2563, -0.2752, -0.2923, -0.3021, -0.3047, -0.3039, -0.2996, -0.2921,
0:         -0.2857], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 30, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.1889,     nan,     nan, -0.1222,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1329,     nan,     nan,     nan, -0.2175,     nan, -0.2390, -0.2366,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan, -0.1722,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1925,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0471, -0.0376,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0424,     nan, -0.1139,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1258,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1734,     nan, -0.0281,     nan,     nan,     nan, -0.2116,     nan,
0:             nan,     nan,     nan, -0.2330,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1568,     nan, -0.2473,     nan,     nan,     nan,     nan,     nan,     nan, -0.0853,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1758,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1282,     nan,     nan, -0.0614,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0220,     nan,     nan, -0.0019,     nan, -0.1496,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1067,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 30, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4879, -0.4877, -0.4648, -0.4202, -0.3856, -0.3849, -0.4063, -0.4529, -0.4895, -0.5148, -0.5047, -0.4895,
0:         -0.4524, -0.4072, -0.3787, -0.3126, -0.2395, -0.1735, -0.4494, -0.4638, -0.4643, -0.4538, -0.4467, -0.4378,
0:         -0.4500], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.1340, 2.2282, 2.3375, 2.4224, 2.5242, 2.6306, 2.6839, 2.7016, 2.6818, 2.6502, 2.6532, 2.7400, 2.8653, 3.0223,
0:         3.1304, 3.1737, 3.1632, 3.1068, 2.0932, 2.2206, 2.3241, 2.3897, 2.4588, 2.4924, 2.5174], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4410, -0.4664, -0.4937, -0.5186, -0.5365, -0.5457, -0.5481, -0.5464, -0.5481, -0.5480, -0.5482, -0.5499,
0:         -0.5495, -0.5528, -0.5604, -0.5647, -0.5733, -0.5745, -0.4512, -0.4817, -0.5127, -0.5458, -0.5646, -0.5762,
0:         -0.5775], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1319, -0.0368, -0.2037, -0.2096, -0.3033, -0.1838,  0.0462,  0.0311, -0.0112,  0.0057,  0.0338,  0.0979,
0:          0.2399,  0.3317,  0.2162,  0.2393,  0.3690,  0.2231,  0.1592, -0.0060, -0.1962, -0.2105, -0.2613, -0.1055,
0:          0.0747], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5077, -0.4047, -0.2528, -0.1680, -0.1419, -0.1946, -0.2399, -0.2449, -0.2112, -0.1664, -0.1333, -0.1175,
0:         -0.1036, -0.0393, -0.0125, -0.0266, -0.1158, -0.2351, -0.3552, -0.4260, -0.4668, -0.4925, -0.5077, -0.4980,
0:         -0.4703], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1687, -0.1693, -0.1758, -0.1744, -0.1760, -0.1746, -0.1687, -0.1689, -0.1529, -0.1870, -0.1834, -0.1775,
0:         -0.1822, -0.1930, -0.1836, -0.1842, -0.1787, -0.1646, -0.1854, -0.1944, -0.1934, -0.1862, -0.1963, -0.1966,
0:         -0.1926], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 30 [1/5 (20%)]	Loss: 0.71257 : 0.45836 :: 0.18250 (2.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 30 [2/5 (40%)]	Loss: 0.92315 : 0.48264 :: 0.17762 (15.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 30 [3/5 (60%)]	Loss: 0.95899 : 0.48458 :: 0.18532 (15.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 30 [4/5 (80%)]	Loss: 0.95765 : 0.51412 :: 0.18877 (15.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.253, max = -0.135, mean = -0.203
0:          sample (first 20): tensor([-0.2088, -0.2114, -0.2108, -0.2126, -0.2093, -0.2064, -0.2022, -0.2072, -0.2038, -0.2249, -0.2205, -0.2195,
0:         -0.2135, -0.2219, -0.2095, -0.2164, -0.2218, -0.2126, -0.2218, -0.2303])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 30 : 0.4112469553947449
0: validation loss for velocity_u : 0.27593275904655457
0: validation loss for velocity_v : 0.17680172622203827
0: validation loss for specific_humidity : 0.057028502225875854
0: validation loss for velocity_z : 0.8025546669960022
0: validation loss for temperature : 0.5082616806030273
0: validation loss for total_precip : 0.6469029784202576
0: 31 : 22:31:37 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 31, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5239, -0.4448, -0.3835, -0.3704, -0.3704, -0.3741, -0.3803, -0.4387, -0.4946, -0.5004, -0.5224, -0.5247,
0:         -0.5528, -0.6495, -0.7467, -0.8160, -0.8176, -0.7855, -0.4716, -0.4156, -0.3781, -0.3643, -0.3682, -0.3805,
0:         -0.3791], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7521, 0.8243, 0.8896, 0.8983, 0.9108, 0.9233, 0.9317, 0.9396, 0.9171, 0.8689, 0.8349, 0.7654, 0.6315, 0.5466,
0:         0.5781, 0.6384, 0.6580, 0.6809, 0.7921, 0.8426, 0.8906, 0.9194, 0.9421, 0.9446, 0.9392], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.9059, 2.6982, 2.4869, 2.3910, 2.2550, 2.2519, 2.2300, 2.2917, 2.4342, 2.5934, 2.5875, 2.1840, 1.5621, 1.1937,
0:         1.7768, 2.3270, 2.6829, 3.0056, 2.7887, 2.6844, 2.5251, 2.4029, 2.3430, 2.4105, 2.4290], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2518, -0.1156,  0.1906,  0.4033,  0.4202,  0.5463,  0.7523,  0.4641,  0.2367,  0.3786,  0.2469, -0.0244,
0:         -0.3003, -0.2597,  0.1590,  0.1241, -0.1145, -0.3138, -0.2811, -0.1336,  0.1253,  0.3673,  0.4833,  0.4517,
0:          0.4529], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.7307, -1.3612, -1.1451, -0.9894, -0.8946, -1.0220, -0.9586, -0.9880, -1.3684, -1.4093, -1.1781, -0.6990,
0:          0.0959,  0.2877, -0.4922, -1.7269, -2.5926, -2.9711, -3.0558, -2.8805, -2.5945, -2.1526, -1.7798, -1.5960,
0:         -1.4763], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 31, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2484,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,     nan, -0.2484,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2377,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2221,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,     nan,     nan,
0:         -0.1791,     nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,     nan,     nan,  0.0670,
0:             nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1039,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,
0:             nan, -0.0836,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1684, -0.0441,     nan,     nan,     nan,
0:         -0.2257,     nan,     nan, -0.1564,     nan,     nan,     nan,     nan, -0.0179,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1911,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1528,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0716,     nan,     nan,     nan,     nan, -0.1648,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1218,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 31, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2889, -0.3259, -0.4029, -0.4958, -0.5573, -0.5794, -0.5764, -0.5725, -0.5715, -0.5928, -0.6179, -0.6138,
0:         -0.6043, -0.5758, -0.5419, -0.5152, -0.5082, -0.5228, -0.2910, -0.2926, -0.3467, -0.4306, -0.5036, -0.5362,
0:         -0.5401], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0697, 0.1197, 0.1706, 0.2003, 0.2369, 0.2787, 0.2960, 0.2806, 0.2341, 0.1695, 0.1127, 0.0930, 0.0900, 0.1131,
0:         0.1268, 0.1426, 0.1602, 0.1978, 0.0460, 0.1256, 0.1767, 0.1781, 0.1695, 0.1570, 0.1512], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.2698, 2.2353, 2.1758, 2.1070, 2.0444, 1.9872, 1.9624, 1.9752, 2.0320, 2.1010, 2.1813, 2.2361, 2.3026, 2.3634,
0:         2.4501, 2.5646, 2.7126, 2.8413, 2.3501, 2.3278, 2.2984, 2.2392, 2.2046, 2.1660, 2.1650], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1792,  0.1074, -0.0662, -0.0404, -0.0762, -0.1040,  0.0628,  0.1035,  0.0344,  0.0120, -0.0226, -0.0248,
0:          0.0524,  0.1220,  0.0141,  0.0160,  0.2066,  0.0574,  0.3429,  0.2459,  0.0296,  0.0316,  0.0163,  0.0168,
0:          0.1212], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5731, -0.5881, -0.5348, -0.4818, -0.4659, -0.5234, -0.6182, -0.6987, -0.7376, -0.7383, -0.7271, -0.7392,
0:         -0.7791, -0.7758, -0.7517, -0.7031, -0.6708, -0.6578, -0.6800, -0.6899, -0.6683, -0.6084, -0.5433, -0.4920,
0:         -0.4749], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1091, -0.1088, -0.1153, -0.1119, -0.1077, -0.1083, -0.0994, -0.1030, -0.0909, -0.1284, -0.1203, -0.1217,
0:         -0.1194, -0.1298, -0.1227, -0.1173, -0.1185, -0.1067, -0.1280, -0.1368, -0.1334, -0.1290, -0.1391, -0.1388,
0:         -0.1368], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 31 [1/5 (20%)]	Loss: 1.08896 : 0.47413 :: 0.17929 (2.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 31 [2/5 (40%)]	Loss: 1.12111 : 0.53905 :: 0.19704 (15.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 31 [3/5 (60%)]	Loss: 0.64399 : 0.37139 :: 0.17948 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 31 [4/5 (80%)]	Loss: 0.66849 : 0.43203 :: 0.18094 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.249, max = 8.629, mean = 0.375
0:          sample (first 20): tensor([-0.2112, -0.2173, -0.2123, -0.2115, -0.2047, -0.2058, -0.1983, -0.2017, -0.1975, -0.2313, -0.2216, -0.2243,
0:         -0.2144, -0.2209, -0.2094, -0.2150, -0.2173, -0.2113, -0.2296, -0.2344])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 31 : 0.41570815443992615
0: validation loss for velocity_u : 0.3198483884334564
0: validation loss for velocity_v : 0.16725637018680573
0: validation loss for specific_humidity : 0.042227551341056824
0: validation loss for velocity_z : 0.7841640114784241
0: validation loss for temperature : 0.47437945008277893
0: validation loss for total_precip : 0.7063725590705872
0: 32 : 22:35:45 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 32, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.9197, 2.8352, 2.7980, 2.8279, 2.9172, 3.0173, 3.0868, 3.1213, 3.1183, 3.0658, 2.9789, 2.8958, 2.8382, 2.8100,
0:         2.8110, 2.8382, 2.8888, 2.9567, 2.8749, 2.8314, 2.8394, 2.9001, 2.9867, 3.0542, 3.0818], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.0827, 2.1280, 2.2209, 2.2882, 2.2814, 2.2081, 2.1085, 2.0162, 1.9421, 1.8919, 1.8853, 1.9266, 1.9870, 2.0327,
0:         2.0372, 1.9740, 1.8416, 1.6790, 2.0489, 2.1766, 2.3016, 2.3440, 2.2999, 2.2147, 2.1243], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8305, 0.8856, 0.9378, 0.9929, 1.0315, 1.0802, 1.1085, 1.1366, 1.1577, 1.1517, 1.1181, 1.1246, 1.1667, 1.2165,
0:         1.2850, 1.2876, 1.2511, 1.1501, 0.9147, 0.9695, 1.0138, 1.0614, 1.0920, 1.0986, 1.1215], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.4473, -2.0672, -1.4472, -0.9272, -0.7627, -0.9316, -1.1694, -1.3428, -1.6806, -2.3495, -3.0606, -3.5696,
0:         -3.9562, -4.0340, -3.4718, -2.5450, -1.7872, -1.3094, -1.9895, -0.7716,  0.3129,  0.6529,  0.2351, -0.4472,
0:         -1.0739], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.4726, 0.5129, 0.5657, 0.6323, 0.6935, 0.7240, 0.7240, 0.7117, 0.7024, 0.7027, 0.7073, 0.7112, 0.7230, 0.7443,
0:         0.7661, 0.7933, 0.8265, 0.8387, 0.8126, 0.7675, 0.7242, 0.6933, 0.6840, 0.6933, 0.7089], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 32, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2490,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.3474,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0738,     nan,     nan,     nan,     nan,     nan,     nan,  0.2742,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2070,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.3426,     nan,     nan,     nan,  0.3558,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1098,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0642,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0618,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.1914,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.4350,  0.4338,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1410,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.0882,  0.0354,     nan,  0.1674,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0078,     nan,  0.0882,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0102,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0510,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1314,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2706,     nan,     nan,  0.2322,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 32, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.3151, 2.3945, 2.5330, 2.6808, 2.7685, 2.7875, 2.7367, 2.6444, 2.5593, 2.4938, 2.4659, 2.4451, 2.4438, 2.4718,
0:         2.5017, 2.5985, 2.7444, 2.8725, 2.3065, 2.3161, 2.3721, 2.4713, 2.5610, 2.6343, 2.6426], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1336, 0.2770, 0.4225, 0.5568, 0.7064, 0.8466, 0.9042, 0.8901, 0.8641, 0.8542, 0.9288, 1.0677, 1.2035, 1.2857,
0:         1.2949, 1.2680, 1.2511, 1.2760, 0.1133, 0.2518, 0.3838, 0.5117, 0.6305, 0.7172, 0.7498], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6590, -0.6878, -0.7172, -0.7437, -0.7604, -0.7722, -0.7738, -0.7686, -0.7630, -0.7608, -0.7628, -0.7655,
0:         -0.7663, -0.7699, -0.7727, -0.7760, -0.7818, -0.7870, -0.6968, -0.7251, -0.7540, -0.7782, -0.7923, -0.8011,
0:         -0.7959], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.5094,  0.4266,  0.4295,  0.3997,  0.1155, -0.0106, -0.0987, -0.4395, -0.7972, -1.0303, -1.1877, -1.2815,
0:         -1.2200, -1.1089, -1.1649, -1.0358, -0.7488, -0.7305,  0.6787,  0.6365,  0.6273,  0.5769,  0.3371,  0.2593,
0:          0.1252], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3212, -0.1952, -0.0844, -0.0549, -0.0799, -0.1516, -0.2155, -0.2520, -0.2529, -0.2170, -0.1659, -0.1159,
0:         -0.0878, -0.0375, -0.0145, -0.0022, -0.0168, -0.0451, -0.0904, -0.1209, -0.1254, -0.0876, -0.0141,  0.0626,
0:          0.1155], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0852, -0.0932, -0.0950, -0.0924, -0.0926, -0.0930, -0.0845, -0.0819, -0.0763, -0.1043, -0.0991, -0.1023,
0:         -0.1044, -0.1078, -0.1025, -0.0997, -0.0941, -0.0848, -0.1025, -0.1123, -0.1125, -0.1125, -0.1177, -0.1153,
0:         -0.1130], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 32 [1/5 (20%)]	Loss: 0.92443 : 0.51068 :: 0.18585 (2.83 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 32 [2/5 (40%)]	Loss: 0.77120 : 0.42754 :: 0.18282 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 32 [3/5 (60%)]	Loss: 0.54003 : 0.42503 :: 0.19301 (15.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 32 [4/5 (80%)]	Loss: 0.48544 : 0.36113 :: 0.18420 (15.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.254, max = 0.165, mean = -0.181
0:          sample (first 20): tensor([-0.2182, -0.2282, -0.2194, -0.2228, -0.2169, -0.2162, -0.2109, -0.2141, -0.2121, -0.2357, -0.2314, -0.2359,
0:         -0.2253, -0.2278, -0.2170, -0.2249, -0.2288, -0.2271, -0.2346, -0.2397])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 32 : 0.46241962909698486
0: validation loss for velocity_u : 0.32396838068962097
0: validation loss for velocity_v : 0.16793228685855865
0: validation loss for specific_humidity : 0.06157124415040016
0: validation loss for velocity_z : 0.7367949485778809
0: validation loss for temperature : 0.5641182065010071
0: validation loss for total_precip : 0.920132577419281
0: 33 : 22:39:40 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 33, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.4245, -1.4601, -1.4943, -1.5233, -1.5448, -1.5600, -1.5693, -1.5741, -1.5739, -1.5674, -1.5568, -1.5430,
0:         -1.5278, -1.5132, -1.4998, -1.4900, -1.4835, -1.4783, -1.4378, -1.4738, -1.5051, -1.5288, -1.5430, -1.5482,
0:         -1.5464], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1722, -0.1782, -0.1899, -0.2067, -0.2258, -0.2425, -0.2499, -0.2412, -0.2138, -0.1714, -0.1190, -0.0617,
0:         -0.0054,  0.0462,  0.0921,  0.1303,  0.1603,  0.1842, -0.2086, -0.2225, -0.2418, -0.2655, -0.2855, -0.2921,
0:         -0.2788], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5306, -0.4973, -0.4718, -0.4525, -0.4436, -0.4398, -0.4367, -0.4380, -0.4390, -0.4510, -0.4682, -0.4800,
0:         -0.4956, -0.5084, -0.5190, -0.5256, -0.5293, -0.5299, -0.5132, -0.4845, -0.4651, -0.4572, -0.4553, -0.4646,
0:         -0.4797], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5176,  0.5677,  0.6069,  0.5830,  0.5590,  0.5176,  0.4325,  0.3432,  0.2472,  0.1382,  0.0096, -0.1234,
0:         -0.2346, -0.3152, -0.3283, -0.2869, -0.2258, -0.1757,  0.4151,  0.4129,  0.4042,  0.3911,  0.3541,  0.3017,
0:          0.2363], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.0159, -1.0752, -1.1434, -1.2137, -1.2774, -1.3317, -1.3757, -1.4111, -1.4390, -1.4573, -1.4644, -1.4639,
0:         -1.4574, -1.4443, -1.4252, -1.4068, -1.3927, -1.3855, -1.3849, -1.3855, -1.3833, -1.3759, -1.3621, -1.3436,
0:         -1.3227], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 33, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.1346,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0996,     nan,     nan,     nan,     nan,     nan, -0.1816,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,
0:             nan,     nan, -0.2261,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0715,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2097,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2378,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2261,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1369,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2144,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan, -0.2449,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2402,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2355,     nan,     nan,     nan,     nan,     nan, -0.2027,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 33, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5945, -0.5923, -0.6276, -0.6813, -0.7211, -0.7391, -0.7487, -0.7636, -0.7880, -0.8363, -0.8802, -0.8802,
0:         -0.8705, -0.8216, -0.7704, -0.7320, -0.7210, -0.7271, -0.5722, -0.5458, -0.5686, -0.6345, -0.6916, -0.7308,
0:         -0.7449], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3316, -0.3030, -0.2810, -0.2655, -0.2224, -0.1686, -0.1292, -0.1340, -0.1692, -0.2228, -0.2567, -0.2477,
0:         -0.2198, -0.1675, -0.1327, -0.1175, -0.1127, -0.0666, -0.4031, -0.3545, -0.3261, -0.3315, -0.3320, -0.3239,
0:         -0.3088], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6278, -0.6601, -0.6896, -0.7101, -0.7188, -0.7123, -0.6934, -0.6709, -0.6626, -0.6641, -0.6751, -0.6832,
0:         -0.6799, -0.6649, -0.6470, -0.6254, -0.6124, -0.6080, -0.6098, -0.6487, -0.6907, -0.7272, -0.7430, -0.7434,
0:         -0.7243], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4463, 0.3942, 0.3219, 0.2874, 0.1125, 0.0687, 0.2076, 0.1831, 0.1068, 0.1026, 0.0289, 0.0308, 0.1992, 0.3224,
0:         0.1822, 0.1461, 0.2918, 0.1225, 0.4852, 0.4417, 0.3486, 0.3251, 0.2378, 0.2715, 0.3887], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3488, -0.3566, -0.2913, -0.2692, -0.3274, -0.4770, -0.6038, -0.6172, -0.4991, -0.3142, -0.1557, -0.0942,
0:         -0.1261, -0.1417, -0.1906, -0.2310, -0.3110, -0.4080, -0.5099, -0.5639, -0.5595, -0.5051, -0.4164, -0.3311,
0:         -0.2841], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1971, -0.2062, -0.2001, -0.2030, -0.2021, -0.2014, -0.2020, -0.2067, -0.2027, -0.2144, -0.2080, -0.2173,
0:         -0.2102, -0.2144, -0.2070, -0.2140, -0.2201, -0.2183, -0.2135, -0.2210, -0.2150, -0.2206, -0.2193, -0.2178,
0:         -0.2265], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 33 [1/5 (20%)]	Loss: 0.73746 : 0.46666 :: 0.18025 (2.86 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 33 [2/5 (40%)]	Loss: 0.86214 : 0.46211 :: 0.17876 (15.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 33 [3/5 (60%)]	Loss: 0.80258 : 0.49446 :: 0.17272 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 33 [4/5 (80%)]	Loss: 0.53548 : 0.37736 :: 0.19950 (15.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.232, max = 1.533, mean = 0.014
0:          sample (first 20): tensor([-0.1831, -0.1962, -0.1911, -0.1973, -0.1934, -0.1933, -0.1897, -0.1912, -0.1874, -0.1974, -0.1982, -0.2079,
0:         -0.2029, -0.2050, -0.1979, -0.2047, -0.2076, -0.2084, -0.2001, -0.2078])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 33 : 0.45334291458129883
0: validation loss for velocity_u : 0.31849583983421326
0: validation loss for velocity_v : 0.16286157071590424
0: validation loss for specific_humidity : 0.06461423635482788
0: validation loss for velocity_z : 0.764682948589325
0: validation loss for temperature : 0.5434744358062744
0: validation loss for total_precip : 0.8659288883209229
0: 34 : 22:43:37 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 34, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0627, 1.0585, 1.0570, 1.0570, 1.0573, 1.0570, 1.0549, 1.0506, 1.0445, 1.0371, 1.0296, 1.0222, 1.0155, 1.0084,
0:         0.9999, 0.9884, 0.9724, 0.9496, 1.0711, 1.0703, 1.0708, 1.0714, 1.0711, 1.0693, 1.0655], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0089,  0.0976,  0.2042,  0.3090,  0.4100,  0.5057,  0.5953,  0.6779,  0.7523,  0.8175,  0.8713,  0.9130,
0:          0.9423,  0.9600,  0.9670,  0.9640,  0.9496,  0.9205, -0.0356,  0.0743,  0.1829,  0.2894,  0.3932,  0.4937,
0:          0.5901], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5112, -0.5199, -0.5343, -0.5486, -0.5618, -0.5763, -0.5902, -0.5952, -0.5969, -0.5976, -0.5596, -0.5179,
0:         -0.4715, -0.3846, -0.2976, -0.2173, -0.1416, -0.0653, -0.5014, -0.5149, -0.5322, -0.5478, -0.5620, -0.5767,
0:         -0.5914], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2254, -1.2319, -1.1777, -1.0411, -0.8330, -0.5794, -0.3193, -0.0916,  0.0796,  0.1815,  0.2227,  0.2162,
0:          0.1837,  0.1360,  0.0731, -0.0071, -0.1068, -0.2260, -1.2514, -1.1473, -0.9761, -0.7550, -0.5100, -0.2716,
0:         -0.0721], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([2.1513, 2.1620, 2.1697, 2.1751, 2.1791, 2.1822, 2.1853, 2.1880, 2.1900, 2.1906, 2.1888, 2.1840, 2.1762, 2.1648,
0:         2.1501, 2.1325, 2.1126, 2.0904, 2.0656, 2.0369, 2.0035, 1.9654, 1.9237, 1.8806, 1.8389], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 34, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,
0:             nan, -0.2396,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,
0:             nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2396, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2396,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,
0:         -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,
0:             nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2396,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan, -0.2396,
0:         -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 34, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4027, 1.3770, 1.4015, 1.4584, 1.5141, 1.5418, 1.5204, 1.4589, 1.3983, 1.3386, 1.3294, 1.3580, 1.4025, 1.4711,
0:         1.5223, 1.6066, 1.7245, 1.8549, 1.4817, 1.4384, 1.4339, 1.4784, 1.5455, 1.6043, 1.6311], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2469, 0.3034, 0.3926, 0.4673, 0.5551, 0.6301, 0.6415, 0.6073, 0.5605, 0.5088, 0.5063, 0.5682, 0.6313, 0.7126,
0:         0.7747, 0.8320, 0.8990, 1.0142, 0.3276, 0.4069, 0.4987, 0.5776, 0.6329, 0.6409, 0.6245], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5010, -0.5154, -0.5353, -0.5524, -0.5690, -0.5778, -0.5787, -0.5730, -0.5676, -0.5602, -0.5576, -0.5588,
0:         -0.5676, -0.5802, -0.6011, -0.6248, -0.6516, -0.6673, -0.5379, -0.5581, -0.5822, -0.6003, -0.6148, -0.6229,
0:         -0.6197], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1287, 0.1506, 0.0609, 0.0398, 0.0761, 0.1876, 0.3294, 0.2722, 0.1172, 0.1016, 0.1689, 0.3040, 0.4906, 0.5878,
0:         0.5345, 0.5701, 0.6903, 0.5627, 0.3057, 0.3784, 0.2788, 0.2571, 0.3015, 0.4299, 0.6141], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0428,  0.0706,  0.1118,  0.1257,  0.1161,  0.0631, -0.0019, -0.0646, -0.0937, -0.0876, -0.0638, -0.0381,
0:         -0.0247,  0.0223,  0.0554,  0.0761,  0.0611,  0.0134, -0.0710, -0.1376, -0.1539, -0.0984,  0.0297,  0.1899,
0:          0.3269], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1900, -0.2021, -0.1967, -0.1994, -0.1941, -0.1925, -0.1872, -0.1870, -0.1807, -0.2052, -0.2043, -0.2139,
0:         -0.2058, -0.2083, -0.1979, -0.2043, -0.2063, -0.2052, -0.2046, -0.2116, -0.2062, -0.2155, -0.2099, -0.2067,
0:         -0.2173], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 34 [1/5 (20%)]	Loss: 0.67826 : 0.37675 :: 0.17581 (2.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 34 [2/5 (40%)]	Loss: 0.84350 : 0.43199 :: 0.17839 (15.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 34 [3/5 (60%)]	Loss: 0.47191 : 0.36202 :: 0.18099 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 34 [4/5 (80%)]	Loss: 0.78006 : 0.38589 :: 0.17843 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.273, max = 0.626, mean = -0.146
0:          sample (first 20): tensor([-0.2363, -0.2452, -0.2437, -0.2464, -0.2398, -0.2386, -0.2354, -0.2337, -0.2316, -0.2484, -0.2512, -0.2572,
0:         -0.2468, -0.2453, -0.2372, -0.2459, -0.2520, -0.2518, -0.2532, -0.2550])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 34 : 0.49795734882354736
0: validation loss for velocity_u : 0.2973713278770447
0: validation loss for velocity_v : 0.1460556834936142
0: validation loss for specific_humidity : 0.06579243391752243
0: validation loss for velocity_z : 0.721813976764679
0: validation loss for temperature : 0.4712575674057007
0: validation loss for total_precip : 1.2854540348052979
0: 35 : 22:47:41 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 35, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8938, -0.9158, -0.9389, -0.9618, -0.9843, -1.0065, -1.0286, -1.0507, -1.0716, -1.0906, -1.1071, -1.1216,
0:         -1.1356, -1.1492, -1.1622, -1.1737, -1.1828, -1.1904, -0.9447, -0.9699, -0.9964, -1.0223, -1.0473, -1.0707,
0:         -1.0933], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4855, -0.4760, -0.4676, -0.4596, -0.4525, -0.4467, -0.4423, -0.4389, -0.4355, -0.4304, -0.4226, -0.4115,
0:         -0.3977, -0.3820, -0.3655, -0.3487, -0.3316, -0.3145, -0.5448, -0.5365, -0.5295, -0.5235, -0.5180, -0.5128,
0:         -0.5074], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6013, -0.6050, -0.6088, -0.6124, -0.6156, -0.6189, -0.6222, -0.6256, -0.6292, -0.6327, -0.6358, -0.6384,
0:         -0.6406, -0.6434, -0.6466, -0.6494, -0.6521, -0.6551, -0.6133, -0.6168, -0.6198, -0.6229, -0.6261, -0.6289,
0:         -0.6322], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3557, 0.4116, 0.4913, 0.5709, 0.6355, 0.6850, 0.7323, 0.7905, 0.8550, 0.9153, 0.9540, 0.9799, 1.0035, 1.0315,
0:         1.0552, 1.0595, 1.0358, 0.9928, 0.3492, 0.4568, 0.5709, 0.6678, 0.7345, 0.7840, 0.8357], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.0463,  0.0243,  0.0008, -0.0236, -0.0489, -0.0748, -0.1016, -0.1287, -0.1555, -0.1816, -0.2065, -0.2305,
0:         -0.2538, -0.2768, -0.2996, -0.3218, -0.3437, -0.3652, -0.3863, -0.4075, -0.4288, -0.4496, -0.4694, -0.4879,
0:         -0.5041], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 35, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2401, -0.2343,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2483, -0.2483,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,
0:             nan,     nan, -0.2471,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan, -0.2448,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan, -0.2483,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 35, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5185, -0.5508, -0.6137, -0.6796, -0.7216, -0.7236, -0.7192, -0.7226, -0.7397, -0.7869, -0.8279, -0.8120,
0:         -0.7580, -0.6682, -0.5791, -0.5246, -0.5359, -0.6020, -0.5478, -0.5602, -0.6190, -0.7042, -0.7764, -0.8033,
0:         -0.8087], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0514,  0.2166,  0.3933,  0.5254,  0.6740,  0.8297,  0.9196,  0.9663,  0.9832,  0.9762,  0.9893,  1.0624,
0:          1.1298,  1.1918,  1.2203,  1.2144,  1.2082,  1.2500, -0.1296,  0.0663,  0.2462,  0.3817,  0.4827,  0.5745,
0:          0.6491], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5258, -0.5650, -0.5970, -0.6103, -0.6064, -0.5779, -0.5336, -0.4790, -0.4277, -0.3784, -0.3384, -0.3026,
0:         -0.2752, -0.2578, -0.2487, -0.2511, -0.2708, -0.2882, -0.5347, -0.5862, -0.6327, -0.6582, -0.6611, -0.6431,
0:         -0.6001], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0605, -0.1688, -0.3049, -0.2422, -0.3188, -0.3597, -0.2206, -0.1614, -0.1492, -0.1379, -0.1387, -0.1151,
0:         -0.0684, -0.0448, -0.1196, -0.1698, -0.1416, -0.2913, -0.0398, -0.1065, -0.2655, -0.2335, -0.2885, -0.2595,
0:         -0.1344], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1962, -0.0671,  0.0495,  0.0527, -0.0096, -0.1206, -0.1798, -0.1756, -0.1168, -0.0358,  0.0356,  0.0900,
0:          0.1312,  0.1931,  0.2091,  0.1950,  0.1271,  0.0445, -0.0293, -0.0623, -0.0522, -0.0023,  0.0858,  0.1780,
0:          0.2357], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1995, -0.2101, -0.2156, -0.2211, -0.2194, -0.2233, -0.2247, -0.2235, -0.2209, -0.2098, -0.2199, -0.2301,
0:         -0.2268, -0.2283, -0.2273, -0.2368, -0.2410, -0.2439, -0.2186, -0.2228, -0.2177, -0.2356, -0.2330, -0.2344,
0:         -0.2440], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 35 [1/5 (20%)]	Loss: 0.64668 : 0.39558 :: 0.18146 (2.67 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 35 [2/5 (40%)]	Loss: 0.63412 : 0.41933 :: 0.17308 (15.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 35 [3/5 (60%)]	Loss: 0.54463 : 0.39336 :: 0.18646 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 35 [4/5 (80%)]	Loss: 0.57189 : 0.34646 :: 0.17258 (15.04 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.271, max = -0.072, mean = -0.204
0:          sample (first 20): tensor([-0.2363, -0.2443, -0.2436, -0.2466, -0.2399, -0.2385, -0.2385, -0.2317, -0.2310, -0.2423, -0.2490, -0.2561,
0:         -0.2454, -0.2403, -0.2350, -0.2461, -0.2511, -0.2498, -0.2506, -0.2513])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 35 : 0.440243124961853
0: validation loss for velocity_u : 0.2861587107181549
0: validation loss for velocity_v : 0.16108611226081848
0: validation loss for specific_humidity : 0.06430397182703018
0: validation loss for velocity_z : 0.7631611227989197
0: validation loss for temperature : 0.501229465007782
0: validation loss for total_precip : 0.8655192852020264
0: 36 : 22:51:43 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 36, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3075,  0.1810,  0.0599, -0.0391, -0.1101, -0.1559, -0.1860, -0.2154, -0.2573, -0.3154, -0.3802, -0.4366,
0:         -0.4729, -0.4885, -0.4933, -0.4968, -0.4978, -0.4852,  0.0661, -0.0593, -0.1540, -0.2099, -0.2342, -0.2435,
0:         -0.2544], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2607,  0.1029, -0.0681, -0.2168, -0.3228, -0.3875, -0.4296, -0.4771, -0.5539, -0.6593, -0.7675, -0.8557,
0:         -0.9301, -1.0127, -1.1085, -1.2021, -1.2837, -1.3601, -0.0797, -0.2590, -0.4048, -0.4930, -0.5308, -0.5430,
0:         -0.5525], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6347, -0.6138, -0.5933, -0.5438, -0.4718, -0.4058, -0.3426, -0.3351, -0.3396, -0.3416, -0.3252, -0.2827,
0:         -0.2644, -0.2516, -0.2451, -0.2346, -0.2254, -0.2126, -0.6082, -0.5745, -0.5075, -0.4281, -0.3411, -0.2780,
0:         -0.2303], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.2496,  1.2238,  1.0646,  0.8582,  0.6626,  0.5421,  0.5529,  0.6368,  0.6518,  0.5722,  0.4905,  0.4303,
0:          0.3293,  0.2562,  0.3121,  0.2626, -0.4555, -2.0230,  1.2238,  1.0861,  0.8367,  0.5873,  0.4110,  0.3228,
0:          0.3336], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.7640, -0.8863, -1.0083, -1.1088, -1.1793, -1.2310, -1.2730, -1.3020, -1.3202, -1.3467, -1.3875, -1.4192,
0:         -1.4185, -1.3926, -1.3606, -1.3251, -1.2766, -1.2155, -1.1486, -1.0743, -0.9909, -0.9128, -0.8649, -0.8591,
0:         -0.8828], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 36, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,  0.7601,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.5776,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.7098,     nan,     nan,     nan,  1.9914,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  1.7587,     nan,     nan,     nan,     nan,     nan,     nan,  0.4559,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1435,     nan,     nan,  0.8283,  1.1772,     nan,     nan,
0:         -0.2475,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.1750,     nan,     nan,     nan,
0:             nan,  0.8909,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.8876,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3720,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1558,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  1.0855,     nan,  0.9625,  2.3425,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.5127,     nan,     nan,
0:             nan,     nan,  0.3161,     nan,     nan,     nan,     nan,     nan,     nan,  0.7299,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.2557,     nan,     nan,  0.4235,     nan,     nan, -0.1603,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 36, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1559, 0.1664, 0.1954, 0.2418, 0.2776, 0.2844, 0.2649, 0.2203, 0.1814, 0.1583, 0.1717, 0.1906, 0.2221, 0.2646,
0:         0.2926, 0.3542, 0.4387, 0.5197, 0.1150, 0.1036, 0.1111, 0.1350, 0.1645, 0.1906, 0.1870], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.5232, -1.6072, -1.6579, -1.6719, -1.6412, -1.5839, -1.5488, -1.5449, -1.5660, -1.6179, -1.6754, -1.7223,
0:         -1.7813, -1.8327, -1.8856, -1.9000, -1.8844, -1.8160, -1.4922, -1.5523, -1.6010, -1.6286, -1.6126, -1.5741,
0:         -1.5224], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3563, -0.3675, -0.3660, -0.3406, -0.2928, -0.2270, -0.1635, -0.0989, -0.0503, -0.0172, -0.0009,  0.0081,
0:          0.0105,  0.0053, -0.0105, -0.0355, -0.0760, -0.1107, -0.3773, -0.3955, -0.3997, -0.3736, -0.3238, -0.2601,
0:         -0.1853], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1049, -0.2855, -0.4266, -0.5369, -0.7679, -0.8839, -0.9114, -1.0928, -1.2588, -1.2362, -1.1939, -1.1746,
0:         -1.0890, -1.0572, -1.2192, -1.3278, -1.3592, -1.5909, -0.2550, -0.3585, -0.4318, -0.4694, -0.6113, -0.6548,
0:         -0.7164], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-2.6015e-01, -2.1334e-01, -1.0253e-01, -2.8394e-02,  1.2468e-03, -4.9263e-02, -1.0761e-01, -1.2584e-01,
0:         -1.0258e-01, -5.1558e-02, -9.9209e-03,  4.5416e-03, -2.3259e-04,  4.8095e-02,  8.8280e-02,  1.4514e-01,
0:          1.4978e-01,  1.2426e-01,  6.8701e-02,  1.8292e-02, -2.1719e-02, -4.1467e-02, -2.7792e-02,  1.4026e-02,
0:          5.8909e-02], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.2162, 0.2117, 0.2015, 0.1938, 0.1871, 0.1901, 0.1806, 0.1857, 0.1955, 0.1972, 0.1908, 0.1828, 0.1752, 0.1705,
0:         0.1645, 0.1694, 0.1818, 0.1777, 0.1730, 0.1653, 0.1662, 0.1504, 0.1444, 0.1560, 0.1582], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 36 [1/5 (20%)]	Loss: 0.67228 : 0.43166 :: 0.17508 (2.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 36 [2/5 (40%)]	Loss: 0.96455 : 0.49044 :: 0.18306 (15.24 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 36 [3/5 (60%)]	Loss: 0.63158 : 0.40288 :: 0.17730 (15.23 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 36 [4/5 (80%)]	Loss: 0.67861 : 0.42404 :: 0.18205 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.267, max = 0.678, mean = -0.189
0:          sample (first 20): tensor([-0.2364, -0.2418, -0.2424, -0.2420, -0.2357, -0.2334, -0.2352, -0.2260, -0.2259, -0.2403, -0.2456, -0.2534,
0:         -0.2402, -0.2359, -0.2320, -0.2425, -0.2468, -0.2440, -0.2495, -0.2483])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 36 : 0.4446103274822235
0: validation loss for velocity_u : 0.2898120582103729
0: validation loss for velocity_v : 0.19494469463825226
0: validation loss for specific_humidity : 0.05579957738518715
0: validation loss for velocity_z : 0.7474024295806885
0: validation loss for temperature : 0.5865673422813416
0: validation loss for total_precip : 0.793135941028595
0: 37 : 22:55:51 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 37, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8047, -0.8258, -0.8444, -0.8607, -0.8746, -0.8861, -0.8953, -0.9026, -0.9078, -0.9110, -0.9118, -0.9103,
0:         -0.9060, -0.8991, -0.8896, -0.8777, -0.8640, -0.8492, -0.6956, -0.7162, -0.7353, -0.7529, -0.7690, -0.7837,
0:         -0.7968], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.3002, -1.2936, -1.2839, -1.2718, -1.2574, -1.2409, -1.2224, -1.2021, -1.1803, -1.1570, -1.1324, -1.1070,
0:         -1.0812, -1.0555, -1.0303, -1.0060, -0.9835, -0.9630, -1.3413, -1.3398, -1.3353, -1.3286, -1.3197, -1.3087,
0:         -1.2962], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7580, -0.7581, -0.7582, -0.7583, -0.7581, -0.7579, -0.7576, -0.7573, -0.7564, -0.7557, -0.7550, -0.7542,
0:         -0.7537, -0.7532, -0.7525, -0.7518, -0.7525, -0.7533, -0.7571, -0.7572, -0.7574, -0.7575, -0.7573, -0.7571,
0:         -0.7571], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0743, 0.0834, 0.1028, 0.1335, 0.1768, 0.2280, 0.2826, 0.3339, 0.3726, 0.3965, 0.4022, 0.3896, 0.3612, 0.3202,
0:         0.2701, 0.2166, 0.1620, 0.1084, 0.1062, 0.1176, 0.1358, 0.1631, 0.2007, 0.2451, 0.2917], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.1694, 0.1776, 0.1843, 0.1897, 0.1932, 0.1956, 0.1964, 0.1966, 0.1961, 0.1952, 0.1950, 0.1959, 0.1984, 0.2039,
0:         0.2127, 0.2250, 0.2417, 0.2624, 0.2862, 0.3132, 0.3419, 0.3715, 0.4013, 0.4305, 0.4579], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 37, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan, -0.2503,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,     nan,
0:             nan,     nan, -0.2550,     nan,     nan,     nan,     nan,     nan, -0.2550,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2562,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2503,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2574,     nan, -0.2550,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,
0:         -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2479,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2550,     nan,     nan,     nan,     nan,     nan, -0.2550,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 37, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4902, -0.5093, -0.5773, -0.6519, -0.7036, -0.7092, -0.6882, -0.6684, -0.6673, -0.6931, -0.7217, -0.7326,
0:         -0.6934, -0.6348, -0.5982, -0.5897, -0.6540, -0.7658, -0.5563, -0.5467, -0.5920, -0.6764, -0.7612, -0.7903,
0:         -0.7846], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2018, 0.2585, 0.3241, 0.3622, 0.4125, 0.4670, 0.4789, 0.4437, 0.3798, 0.2992, 0.2378, 0.2236, 0.2094, 0.1884,
0:         0.1359, 0.0797, 0.0328, 0.0287, 0.0956, 0.1537, 0.2112, 0.2377, 0.2551, 0.2718, 0.2770], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7356, -0.7630, -0.7828, -0.7957, -0.7994, -0.7933, -0.7817, -0.7680, -0.7579, -0.7531, -0.7592, -0.7630,
0:         -0.7745, -0.7869, -0.7974, -0.8052, -0.8165, -0.8159, -0.7509, -0.7794, -0.8037, -0.8161, -0.8206, -0.8181,
0:         -0.8065], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0136, -0.0905, -0.1968, -0.1307, -0.1414, -0.1623, -0.0647, -0.0799, -0.1487, -0.0975, -0.0807, -0.0677,
0:          0.0394,  0.0537, -0.0404,  0.0201,  0.1132, -0.0223,  0.1677,  0.1006, -0.0532, -0.0229, -0.0218,  0.0081,
0:          0.1083], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.2872, -0.2071, -0.1310, -0.1125, -0.1250, -0.1778, -0.2225, -0.2446, -0.2360, -0.2031, -0.1698, -0.1473,
0:         -0.1419, -0.1017, -0.0708, -0.0336, -0.0369, -0.0781, -0.1599, -0.2404, -0.2928, -0.2894, -0.2190, -0.1136,
0:         -0.0124], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2002, -0.2064, -0.2063, -0.2069, -0.2022, -0.2015, -0.1975, -0.1888, -0.1868, -0.2005, -0.2092, -0.2180,
0:         -0.2111, -0.2046, -0.2024, -0.2055, -0.2095, -0.2039, -0.2147, -0.2129, -0.2027, -0.2239, -0.2125, -0.2061,
0:         -0.2112], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 37 [1/5 (20%)]	Loss: 0.69592 : 0.46651 :: 0.17367 (2.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 37 [2/5 (40%)]	Loss: 0.80912 : 0.42115 :: 0.18556 (15.23 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 37 [3/5 (60%)]	Loss: 0.78500 : 0.43562 :: 0.18112 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 37 [4/5 (80%)]	Loss: 0.85105 : 0.48750 :: 0.17977 (15.58 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.253, max = 2.011, mean = 0.048
0:          sample (first 20): tensor([0.7733, 0.7885, 0.8072, 0.8147, 0.8164, 0.8409, 0.8314, 0.8365, 0.8217, 0.7853, 0.7877, 0.8030, 0.8062, 0.8297,
0:         0.8421, 0.8706, 0.8564, 0.8631, 0.7619, 0.7826])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 37 : 0.43892115354537964
0: validation loss for velocity_u : 0.2788073718547821
0: validation loss for velocity_v : 0.14177940785884857
0: validation loss for specific_humidity : 0.06655603647232056
0: validation loss for velocity_z : 0.8366329073905945
0: validation loss for temperature : 0.5752190351486206
0: validation loss for total_precip : 0.7345316410064697
0: 38 : 22:59:56 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 38, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2983, -1.2699, -1.2292, -1.1872, -1.1501, -1.1220, -1.1014, -1.0867, -1.0740, -1.0632, -1.0563, -1.0543,
0:         -1.0609, -1.0757, -1.0966, -1.1202, -1.1451, -1.1682, -1.3064, -1.2770, -1.2345, -1.1903, -1.1504, -1.1210,
0:         -1.0988], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5422, -0.5611, -0.5755, -0.5808, -0.5766, -0.5609, -0.5330, -0.4944, -0.4488, -0.4028, -0.3627, -0.3320,
0:         -0.3106, -0.3001, -0.2968, -0.2999, -0.3053, -0.3088, -0.5378, -0.5618, -0.5784, -0.5814, -0.5725, -0.5515,
0:         -0.5186], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.7223, 1.6152, 1.4850, 1.3601, 1.2588, 1.1847, 1.1099, 1.0344, 0.9169, 0.8122, 0.7042, 0.6281, 0.5642, 0.5296,
0:         0.5308, 0.5628, 0.6050, 0.6736, 1.7070, 1.6120, 1.5253, 1.4145, 1.3160, 1.2286, 1.1480], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2314,  0.1214, -0.0524, -0.1745, -0.2250, -0.2503, -0.2206, -0.1789, -0.1371, -0.0986, -0.0557, -0.0161,
0:          0.0917,  0.1973,  0.2754,  0.3887,  0.4415,  0.4888,  0.2600,  0.1401, -0.0007, -0.1206, -0.1591, -0.1613,
0:         -0.1415], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.4490, -1.4489, -1.4080, -1.3387, -1.2624, -1.1987, -1.1513, -1.1181, -1.0850, -1.0481, -1.0068, -0.9615,
0:         -0.9267, -0.9110, -0.9259, -0.9653, -1.0124, -1.0593, -1.1007, -1.1368, -1.1619, -1.1794, -1.1860, -1.1899,
0:         -1.1889], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 38, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2459,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2459,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2459,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2459,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2459,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2459,     nan,     nan,     nan,     nan, -0.2459,     nan,
0:         -0.2459, -0.2459,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,
0:             nan, -0.2459,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2437,
0:         -0.2358,     nan,     nan,     nan, -0.2168,     nan,     nan,     nan,     nan, -0.2011,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2459, -0.2459,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2459,     nan,     nan, -0.2437,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2459,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2459, -0.2459,     nan,     nan,     nan,     nan,     nan,     nan, -0.2459,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 38, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3787, -0.3852, -0.4361, -0.4927, -0.5234, -0.5268, -0.5349, -0.5644, -0.6051, -0.6655, -0.7142, -0.7021,
0:         -0.6644, -0.6109, -0.5589, -0.5341, -0.5371, -0.5637, -0.3808, -0.3732, -0.4089, -0.4618, -0.4940, -0.4992,
0:         -0.4972], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2737, -0.2551, -0.2279, -0.1898, -0.1227, -0.0389,  0.0048, -0.0059, -0.0547, -0.1321, -0.1891, -0.2124,
0:         -0.2351, -0.2454, -0.2687, -0.2764, -0.2754, -0.2346, -0.2679, -0.2379, -0.2163, -0.2062, -0.1760, -0.1418,
0:         -0.1113], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7193, -0.7568, -0.7927, -0.8142, -0.8216, -0.8217, -0.8102, -0.7960, -0.7887, -0.7898, -0.7978, -0.8032,
0:         -0.8009, -0.7936, -0.7795, -0.7664, -0.7603, -0.7568, -0.7284, -0.7661, -0.8026, -0.8303, -0.8415, -0.8426,
0:         -0.8321], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1632,  0.0732, -0.0199,  0.0660, -0.0338, -0.0690,  0.0660,  0.0428, -0.0146,  0.0133,  0.0291,  0.0809,
0:          0.1873,  0.2582,  0.1594,  0.1286,  0.2056,  0.0034,  0.2364,  0.2138,  0.0886,  0.1390,  0.0868,  0.1112,
0:          0.2028], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0358,  0.0009,  0.0175,  0.0227, -0.0038, -0.0727, -0.1338, -0.1527, -0.1189, -0.0682, -0.0475, -0.0650,
0:         -0.0921, -0.0500,  0.0042,  0.0575,  0.0492, -0.0055, -0.0874, -0.1373, -0.1255, -0.0577,  0.0448,  0.1501,
0:          0.2078], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2495, -0.2487, -0.2490, -0.2440, -0.2414, -0.2395, -0.2429, -0.2400, -0.2439, -0.2545, -0.2524, -0.2610,
0:         -0.2435, -0.2373, -0.2403, -0.2525, -0.2619, -0.2596, -0.2613, -0.2549, -0.2467, -0.2551, -0.2468, -0.2439,
0:         -0.2528], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 38 [1/5 (20%)]	Loss: 0.93800 : 0.49565 :: 0.18543 (2.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 38 [2/5 (40%)]	Loss: 0.63628 : 0.40471 :: 0.18147 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 38 [3/5 (60%)]	Loss: 0.77228 : 0.41985 :: 0.17890 (15.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 38 [4/5 (80%)]	Loss: 1.00574 : 0.46696 :: 0.18328 (15.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.257, max = 0.950, mean = -0.107
0:          sample (first 20): tensor([-0.1757, -0.1813, -0.1818, -0.1831, -0.1829, -0.1825, -0.1849, -0.1793, -0.1785, -0.1835, -0.1782, -0.1928,
0:         -0.1873, -0.1830, -0.1889, -0.1929, -0.1969, -0.1900, -0.1846, -0.1842])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 38 : 0.4027964174747467
0: validation loss for velocity_u : 0.26934510469436646
0: validation loss for velocity_v : 0.1363985687494278
0: validation loss for specific_humidity : 0.06251929700374603
0: validation loss for velocity_z : 0.6078751683235168
0: validation loss for temperature : 0.5709949731826782
0: validation loss for total_precip : 0.7696452140808105
0: 39 : 23:04:00 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 39, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2198,  0.1612,  0.1174,  0.0890,  0.0642,  0.0420,  0.0089, -0.0370, -0.0797, -0.1088, -0.1162, -0.1101,
0:         -0.1078, -0.1119, -0.1291, -0.1605, -0.1853, -0.1946,  0.2032,  0.1568,  0.1167,  0.0852,  0.0574,  0.0356,
0:          0.0108], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.7072, 1.6451, 1.6188, 1.6091, 1.5941, 1.5669, 1.5352, 1.4958, 1.4450, 1.3893, 1.3304, 1.2702, 1.2098, 1.1440,
0:         1.0750, 1.0056, 0.9276, 0.8411, 1.6166, 1.5778, 1.5688, 1.5635, 1.5444, 1.5131, 1.4782], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.2867, 2.0283, 2.2005, 2.6363, 3.1161, 3.4345, 3.3737, 3.1245, 2.7743, 2.4540, 2.2185, 2.1039, 2.0764, 2.0690,
0:         2.0404, 1.9450, 1.7867, 1.5924, 2.2641, 2.0702, 2.2774, 2.7202, 3.1888, 3.4633, 3.3943], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5678, -0.8008, -0.8807, -0.8186, -0.7531, -0.7697, -0.6843, -0.4280, -0.0141,  0.4653,  0.7605,  0.7849,
0:          0.6151,  0.3776,  0.2301,  0.1391,  0.0403, -0.0141, -0.1772, -0.5001, -0.6943, -0.7187, -0.8019, -0.8951,
0:         -0.7176], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.0917, 1.1127, 1.0298, 0.8470, 0.6144, 0.4764, 0.5188, 0.6998, 0.9345, 1.0896, 1.0782, 0.9604, 0.8046, 0.6865,
0:         0.6701, 0.7106, 0.7598, 0.8256, 0.9003, 0.9723, 1.0281, 1.0338, 0.9954, 0.9575, 0.9511], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 39, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0392,     nan, -0.0838,     nan,     nan,     nan,     nan,     nan, -0.0104,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1109,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1097,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1718,     nan, -0.1391, -0.1515,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1368,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1255, -0.1357,     nan,     nan,     nan,
0:             nan,     nan, -0.2079,     nan,     nan,     nan,     nan, -0.1312, -0.1188,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1064,     nan,     nan, -0.2079,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1312,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1605,     nan,     nan,     nan, -0.0646,     nan,     nan, -0.1853,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1774,
0:             nan,     nan,     nan, -0.1774,     nan,     nan,     nan,     nan, -0.2079,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1899,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0759,     nan,     nan,     nan,     nan,     nan,     nan, -0.0420,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 39, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0397,  0.0159, -0.0072, -0.0165, -0.0078,  0.0105,  0.0197,  0.0103, -0.0128, -0.0430, -0.0524, -0.0356,
0:         -0.0061,  0.0358,  0.0556,  0.0663,  0.0617,  0.0464,  0.0408,  0.0310,  0.0002, -0.0274, -0.0418, -0.0377,
0:         -0.0338], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6452, 0.6863, 0.7248, 0.7373, 0.7640, 0.8016, 0.8046, 0.7606, 0.6879, 0.5780, 0.4855, 0.4218, 0.3731, 0.3411,
0:         0.2989, 0.2443, 0.1848, 0.1457, 0.5080, 0.5827, 0.6278, 0.6272, 0.6064, 0.5694, 0.5301], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2190, 0.1397, 0.0884, 0.0700, 0.0685, 0.0594, 0.0733, 0.0937, 0.1177, 0.1409, 0.1528, 0.1468, 0.1666, 0.1879,
0:         0.2239, 0.3133, 0.4290, 0.5494, 0.2950, 0.2292, 0.2069, 0.1797, 0.2046, 0.2166, 0.2579], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1242, -0.0336, -0.2282, -0.2539, -0.3487, -0.3953, -0.3065, -0.3890, -0.4874, -0.4549, -0.4524, -0.4134,
0:         -0.2793, -0.1990, -0.3088, -0.2735, -0.1281, -0.2931,  0.1550,  0.1338, -0.0542, -0.1028, -0.1962, -0.2857,
0:         -0.2836], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.3915,  0.4835,  0.5857,  0.6020,  0.5199,  0.3431,  0.1797,  0.1058,  0.1591,  0.2753,  0.3688,  0.3812,
0:          0.3169,  0.2645,  0.2129,  0.1802,  0.1194,  0.0144, -0.1499, -0.3064, -0.3962, -0.3743, -0.2308, -0.0244,
0:          0.1305], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0309, -0.0358, -0.0453, -0.0422, -0.0438, -0.0367, -0.0403, -0.0327, -0.0219, -0.0459, -0.0412, -0.0502,
0:         -0.0468, -0.0487, -0.0544, -0.0477, -0.0423, -0.0339, -0.0411, -0.0443, -0.0559, -0.0656, -0.0628, -0.0589,
0:         -0.0578], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 39 [1/5 (20%)]	Loss: 0.76779 : 0.47500 :: 0.18809 (2.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 39 [2/5 (40%)]	Loss: 0.55094 : 0.37442 :: 0.16993 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 39 [3/5 (60%)]	Loss: 0.97249 : 0.42427 :: 0.17594 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 39 [4/5 (80%)]	Loss: 0.64162 : 0.40963 :: 0.17797 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.237, max = 0.750, mean = -0.110
0:          sample (first 20): tensor([-0.2017, -0.2082, -0.2049, -0.2085, -0.2078, -0.2058, -0.2120, -0.2049, -0.2025, -0.2125, -0.2029, -0.2180,
0:         -0.2095, -0.2062, -0.2086, -0.2136, -0.2166, -0.2066, -0.2073, -0.2045])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 39 : 0.3955841064453125
0: validation loss for velocity_u : 0.2556987702846527
0: validation loss for velocity_v : 0.13383682072162628
0: validation loss for specific_humidity : 0.06866772472858429
0: validation loss for velocity_z : 0.701892077922821
0: validation loss for temperature : 0.567475438117981
0: validation loss for total_precip : 0.6459346413612366
0: 40 : 23:08:01 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 40, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1132, -1.0946, -1.0840, -1.1038, -1.1266, -1.1375, -1.1443, -1.1543, -1.1694, -1.1856, -1.1963, -1.2008,
0:         -1.2069, -1.2188, -1.2363, -1.2453, -1.2457, -1.2601, -1.1243, -1.1053, -1.0984, -1.1211, -1.1431, -1.1523,
0:         -1.1622], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3287, 0.3258, 0.3177, 0.3118, 0.3024, 0.2929, 0.2914, 0.2951, 0.2954, 0.2813, 0.2524, 0.2145, 0.1735, 0.1373,
0:         0.1103, 0.0974, 0.0858, 0.0608, 0.2688, 0.2649, 0.2695, 0.2794, 0.2804, 0.2712, 0.2636], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.6394, 2.7658, 2.8213, 2.7758, 2.8036, 2.8933, 2.9698, 2.9912, 2.9832, 2.9690, 2.9606, 2.9759, 2.9860, 2.9326,
0:         2.8555, 2.8408, 2.8793, 2.9207, 2.6462, 2.7624, 2.7491, 2.6679, 2.7166, 2.8720, 2.9903], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7052, -0.8175, -0.4602, -0.1834, -0.0915, -0.0949, -0.0949, -0.1017, -0.1324, -0.2242, -0.3184, -0.3422,
0:         -0.3694, -0.2356,  0.0786, -0.1312, -0.4114, -0.1142, -0.7063, -0.6065, -0.2605, -0.1210, -0.2061, -0.2900,
0:         -0.1981], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.8422, -0.9553, -0.9915, -0.9192, -0.8316, -0.8003, -0.8590, -0.9619, -1.0856, -1.1980, -1.3131, -1.4499,
0:         -1.5533, -1.5508, -1.4989, -1.4646, -1.3976, -1.2962, -1.2515, -1.2575, -1.1885, -1.0048, -0.7814, -0.6105,
0:         -0.5283], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 40, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2494,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2553,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1917,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1270,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1423,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1118,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2600,     nan,     nan,     nan,     nan, -0.2306,     nan,     nan,
0:             nan,     nan, -0.2600,     nan,     nan,     nan, -0.2411,     nan,     nan,     nan, -0.2600,     nan,
0:             nan,     nan,     nan, -0.2517,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2270,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1223,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2576,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2600,     nan,     nan,     nan,     nan,     nan,
0:         -0.2564,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2600, -0.2588,     nan,     nan, -0.2576,     nan,     nan,     nan,     nan, -0.2600, -0.2588,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2106,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 40, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9594, -0.8977, -0.8830, -0.8864, -0.8976, -0.9012, -0.9194, -0.9532, -1.0086, -1.0778, -1.1273, -1.1041,
0:         -1.0308, -0.9090, -0.7827, -0.7072, -0.7049, -0.7734, -0.8675, -0.7983, -0.7830, -0.8086, -0.8543, -0.8916,
0:         -0.9171], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-7.0642e-02, -8.3383e-02, -9.4556e-02, -9.8268e-02, -6.7688e-02,  2.8289e-04,  3.9499e-02,  2.8487e-02,
0:         -1.9937e-02, -1.0473e-01, -1.9030e-01, -2.4345e-01, -2.8294e-01, -2.9332e-01, -3.0306e-01, -2.9411e-01,
0:         -2.8475e-01, -2.5500e-01, -1.1899e-01, -1.2797e-01, -1.4892e-01, -1.7637e-01, -1.7371e-01, -1.5614e-01,
0:         -1.2516e-01], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6349, 0.6036, 0.5579, 0.5214, 0.5004, 0.4887, 0.5022, 0.5185, 0.5281, 0.5228, 0.5040, 0.4704, 0.4490, 0.4261,
0:         0.4006, 0.3766, 0.3387, 0.2935, 0.4759, 0.4316, 0.3740, 0.3133, 0.2795, 0.2616, 0.2712], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1973, 0.1141, 0.0246, 0.1096, 0.0554, 0.0575, 0.2111, 0.1802, 0.0901, 0.0889, 0.0773, 0.0990, 0.1771, 0.2633,
0:         0.1764, 0.1368, 0.2724, 0.1462, 0.2322, 0.2185, 0.1334, 0.2069, 0.1854, 0.1847, 0.2443], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.8624, -0.7907, -0.6854, -0.6683, -0.7282, -0.8498, -0.9369, -0.9205, -0.8078, -0.6662, -0.5688, -0.5561,
0:         -0.5972, -0.5874, -0.5513, -0.4887, -0.4629, -0.4893, -0.5647, -0.6248, -0.6358, -0.5659, -0.4516, -0.3308,
0:         -0.2479], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2378, -0.2410, -0.2424, -0.2429, -0.2409, -0.2416, -0.2485, -0.2443, -0.2452, -0.2488, -0.2357, -0.2516,
0:         -0.2402, -0.2378, -0.2411, -0.2466, -0.2562, -0.2449, -0.2401, -0.2376, -0.2392, -0.2521, -0.2416, -0.2408,
0:         -0.2461], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 40 [1/5 (20%)]	Loss: 0.76990 : 0.43902 :: 0.18123 (2.23 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 40 [2/5 (40%)]	Loss: 0.71989 : 0.40389 :: 0.18214 (15.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 40 [3/5 (60%)]	Loss: 0.63912 : 0.38248 :: 0.18250 (15.37 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 40 [4/5 (80%)]	Loss: 1.10022 : 0.45964 :: 0.17554 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.261, max = 0.066, mean = -0.198
0:          sample (first 20): tensor([-0.1981, -0.2057, -0.2018, -0.2083, -0.2078, -0.2084, -0.2181, -0.2097, -0.2075, -0.2069, -0.1980, -0.2136,
0:         -0.2048, -0.2044, -0.2084, -0.2151, -0.2181, -0.2113, -0.1982, -0.1983])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 40 : 0.4040137529373169
0: validation loss for velocity_u : 0.2923043668270111
0: validation loss for velocity_v : 0.17103005945682526
0: validation loss for specific_humidity : 0.06505034118890762
0: validation loss for velocity_z : 0.6351547837257385
0: validation loss for temperature : 0.634768009185791
0: validation loss for total_precip : 0.6257744431495667
0: 41 : 23:12:05 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 41, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3922, 1.3664, 1.3479, 1.3340, 1.3163, 1.2933, 1.2660, 1.2319, 1.1969, 1.1655, 1.1352, 1.1059, 1.0733, 1.0335,
0:         0.9887, 0.9429, 0.9092, 0.9006, 1.4780, 1.4411, 1.4171, 1.4061, 1.4008, 1.3961, 1.3902], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6650, -0.6546, -0.6393, -0.6226, -0.6064, -0.5930, -0.5828, -0.5737, -0.5632, -0.5483, -0.5259, -0.4959,
0:         -0.4609, -0.4212, -0.3765, -0.3269, -0.2729, -0.2213, -0.7033, -0.6958, -0.6739, -0.6437, -0.6122, -0.5868,
0:         -0.5706], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0007, -0.0017, -0.0457, -0.1326, -0.2083, -0.3146, -0.4273, -0.4916, -0.4847, -0.5194, -0.4788, -0.4585,
0:         -0.4360, -0.4211, -0.3879, -0.3694, -0.3394, -0.3022,  0.0037,  0.0052,  0.0070, -0.0076, -0.0830, -0.1279,
0:         -0.2441], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1375, 1.1887, 1.1269, 1.0545, 0.8862, 0.6582, 0.5197, 0.4643, 0.5560, 0.7754, 0.8819, 0.8606, 0.7860, 0.6028,
0:         0.4004, 0.2684, 0.1917, 0.1874, 1.0566, 1.2909, 1.3250, 1.1844, 0.8563, 0.4281, 0.1405], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.8687, 0.8748, 0.8768, 0.8754, 0.8661, 0.8513, 0.8309, 0.8052, 0.7811, 0.7618, 0.7449, 0.7277, 0.7027, 0.6640,
0:         0.6143, 0.5531, 0.4852, 0.4196, 0.3589, 0.3048, 0.2573, 0.2164, 0.1835, 0.1620, 0.1551], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 41, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([ 1.0492,     nan,     nan, -0.2385,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1759,     nan,
0:             nan,     nan, -0.2246,     nan,     nan, -0.2408,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2408,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2408,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2060,     nan,     nan,     nan,     nan,     nan, -0.2408,     nan,     nan, -0.1574,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2153,     nan,     nan,     nan,     nan, -0.2385,     nan,     nan,     nan, -0.1968,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2408, -0.2385,     nan, -0.2408,     nan, -0.2408,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2408,     nan,     nan,     nan,
0:         -0.2408,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2408,     nan, -0.2408,     nan,
0:             nan, -0.2408,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2361,     nan,     nan, -0.0254,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1620,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2408,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 41, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9284, 0.9468, 1.0061, 1.0733, 1.1453, 1.1800, 1.1728, 1.1256, 1.0596, 0.9853, 0.9373, 0.9103, 0.9100, 0.9478,
0:         0.9943, 1.0836, 1.2077, 1.3104, 0.9360, 0.9327, 0.9527, 0.9954, 1.0507, 1.1007, 1.1160], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3149,  0.3347,  0.3502,  0.3445,  0.3356,  0.3172,  0.2696,  0.1847,  0.0744, -0.0520, -0.1687, -0.2552,
0:         -0.3396, -0.4025, -0.4722, -0.5358, -0.5869, -0.6092,  0.2956,  0.3499,  0.3667,  0.3399,  0.2860,  0.2126,
0:          0.1263], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2859, 0.2964, 0.3084, 0.3141, 0.3273, 0.3499, 0.3848, 0.4432, 0.5056, 0.5652, 0.6156, 0.6501, 0.6634, 0.6465,
0:         0.5965, 0.5243, 0.4331, 0.3449, 0.2539, 0.2742, 0.2863, 0.2921, 0.3017, 0.3113, 0.3441], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5648, -0.6997, -0.6371, -0.1800, -0.0935, -0.0968,  0.0071, -0.2101, -0.3186, -0.2386, -0.1434,  0.0545,
0:          0.2069,  0.1897,  0.0794,  0.1334,  0.2398,  0.1100, -0.6510, -0.8158, -0.9656, -0.5396, -0.2285, -0.0033,
0:          0.1374], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0728,  0.1943,  0.2695,  0.2274,  0.1042, -0.0615, -0.2073, -0.2948, -0.3092, -0.2937, -0.2861, -0.3100,
0:         -0.3576, -0.3662, -0.3445, -0.2969, -0.2612, -0.2645, -0.3297, -0.4086, -0.4442, -0.3952, -0.2612, -0.0893,
0:          0.0357], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0721, -0.0845, -0.0889, -0.0950, -0.1024, -0.1037, -0.1145, -0.1088, -0.1000, -0.0814, -0.0783, -0.0968,
0:         -0.0975, -0.1029, -0.1146, -0.1136, -0.1137, -0.1052, -0.0766, -0.0790, -0.0941, -0.1109, -0.1119, -0.1088,
0:         -0.1172], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 41 [1/5 (20%)]	Loss: 0.66332 : 0.39296 :: 0.18427 (2.63 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 41 [2/5 (40%)]	Loss: 0.76287 : 0.39520 :: 0.18737 (15.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 41 [3/5 (60%)]	Loss: 0.64493 : 0.37673 :: 0.18306 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 41 [4/5 (80%)]	Loss: 0.93310 : 0.46341 :: 0.18551 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.280, max = 8.191, mean = 0.465
0:          sample (first 20): tensor([0.3133, 0.2965, 0.2741, 0.2452, 0.2172, 0.2073, 0.1749, 0.1672, 0.1735, 0.2926, 0.2884, 0.2529, 0.2275, 0.2024,
0:         0.1750, 0.1647, 0.1661, 0.1574, 0.2876, 0.2690])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 41 : 0.4424983263015747
0: validation loss for velocity_u : 0.26185691356658936
0: validation loss for velocity_v : 0.14952977001667023
0: validation loss for specific_humidity : 0.06743832677602768
0: validation loss for velocity_z : 0.761092483997345
0: validation loss for temperature : 0.6330435872077942
0: validation loss for total_precip : 0.7820283770561218
0: 42 : 23:16:03 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 42, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3093, -0.2650, -0.2212, -0.1779, -0.1328, -0.0810, -0.0178,  0.0586,  0.1464,  0.2433,  0.3491,  0.4585,
0:          0.5618,  0.6546,  0.7384,  0.8145,  0.8878,  0.9620, -0.3976, -0.3588, -0.3238, -0.2941, -0.2694, -0.2438,
0:         -0.2095], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.0102,  1.0292,  1.0409,  1.0423,  1.0252,  0.9812,  0.9029,  0.7898,  0.6482,  0.4773,  0.2793,  0.0705,
0:         -0.1315, -0.3116, -0.4599, -0.5790, -0.6735, -0.7399,  1.0430,  1.0603,  1.0732,  1.0800,  1.0749,  1.0478,
0:          0.9863], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2352, -0.2884, -0.3721, -0.4604, -0.5371, -0.5704, -0.5615, -0.5272, -0.5079, -0.5189, -0.5567, -0.5736,
0:         -0.5726, -0.5641, -0.5731, -0.5902, -0.6163, -0.6606, -0.2115, -0.2434, -0.3247, -0.3840, -0.4628, -0.5109,
0:         -0.5316], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.8588,  0.8711,  0.8611,  0.9332,  1.0376,  1.1664,  1.2508,  1.1864,  1.0576,  0.8777,  0.5879,  0.2982,
0:          0.0184, -0.3114, -0.5345, -0.6078, -0.6411, -0.5246,  1.0143,  0.9665,  0.8810,  0.8810,  0.9399,  1.0642,
0:          1.2430], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.3262, -0.3363, -0.3651, -0.4153, -0.4880, -0.5789, -0.6751, -0.7587, -0.8143, -0.8384, -0.8438, -0.8463,
0:         -0.8538, -0.8660, -0.8822, -0.9003, -0.9156, -0.9219, -0.9152, -0.8931, -0.8600, -0.8237, -0.7877, -0.7486,
0:         -0.7002], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 42, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1152,     nan,     nan, -0.2592,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1988,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1385,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2128,
0:             nan,     nan,     nan,     nan, -0.2592,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2290,     nan, -0.2453,
0:             nan,     nan, -0.2604, -0.1838, -0.1536,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1036,     nan,     nan, -0.2221,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2581,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1849,     nan,     nan,     nan, -0.1048,     nan,     nan,     nan,     nan, -0.2569,     nan,
0:         -0.2209,     nan,     nan,     nan, -0.1617,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1036,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2604,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1292,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 42, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0926,  0.0517,  0.0265,  0.0371,  0.0580,  0.0809,  0.0783,  0.0458,  0.0010, -0.0444, -0.0483, -0.0357,
0:         -0.0047,  0.0279,  0.0383,  0.0580,  0.0864,  0.1162,  0.1052,  0.0621,  0.0212,  0.0069,  0.0232,  0.0603,
0:          0.0704], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6897, 0.7569, 0.8304, 0.9008, 1.0011, 1.1050, 1.1504, 1.1268, 1.0726, 1.0193, 1.0145, 1.0650, 1.1082, 1.1220,
0:         1.0734, 0.9441, 0.8139, 0.7327, 0.4633, 0.5519, 0.6338, 0.6978, 0.7590, 0.8006, 0.8104], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6246, -0.6453, -0.6589, -0.6611, -0.6549, -0.6370, -0.6224, -0.6044, -0.5977, -0.5892, -0.5878, -0.5797,
0:         -0.5832, -0.5875, -0.6031, -0.6217, -0.6426, -0.6579, -0.6425, -0.6744, -0.6940, -0.7007, -0.6958, -0.6872,
0:         -0.6678], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0942,  0.0641, -0.0285, -0.0206, -0.0595, -0.0541,  0.0705,  0.0785, -0.0071, -0.0195,  0.0397,  0.1708,
0:          0.2974,  0.3350,  0.2853,  0.3254,  0.3990,  0.2246,  0.1697,  0.1860,  0.0653,  0.0565,  0.0747,  0.1474,
0:          0.2619], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0928, -0.0729, -0.0649, -0.0878, -0.1106, -0.1369, -0.1446, -0.1378, -0.1218, -0.1030, -0.0931, -0.0886,
0:         -0.1023, -0.0947, -0.0954, -0.0878, -0.0925, -0.1032, -0.1316, -0.1621, -0.1816, -0.1686, -0.1060, -0.0184,
0:          0.0523], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2029, -0.2115, -0.2070, -0.2090, -0.2108, -0.2079, -0.2161, -0.2060, -0.1974, -0.2147, -0.2041, -0.2167,
0:         -0.2116, -0.2107, -0.2107, -0.2168, -0.2123, -0.2048, -0.2017, -0.2036, -0.2109, -0.2188, -0.2102, -0.2141,
0:         -0.2196], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 42 [1/5 (20%)]	Loss: 0.58707 : 0.39760 :: 0.17145 (2.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 42 [2/5 (40%)]	Loss: 0.65582 : 0.37824 :: 0.17548 (15.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 42 [3/5 (60%)]	Loss: 0.74224 : 0.38966 :: 0.17584 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 42 [4/5 (80%)]	Loss: 0.86550 : 0.41367 :: 0.17127 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.235, max = 0.640, mean = -0.104
0:          sample (first 20): tensor([0.2593, 0.2443, 0.2400, 0.2175, 0.1969, 0.1946, 0.1728, 0.1806, 0.1930, 0.2549, 0.2535, 0.2251, 0.2015, 0.1843,
0:         0.1691, 0.1753, 0.1801, 0.1814, 0.2472, 0.2373])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 42 : 0.467460572719574
0: validation loss for velocity_u : 0.24683155119419098
0: validation loss for velocity_v : 0.16980963945388794
0: validation loss for specific_humidity : 0.0766487568616867
0: validation loss for velocity_z : 0.7873208522796631
0: validation loss for temperature : 0.6039913892745972
0: validation loss for total_precip : 0.920161783695221
0: 43 : 23:20:02 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 43, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0190, -1.0170, -1.0088, -1.0011, -0.9995, -1.0063, -1.0182, -1.0302, -1.0383, -1.0431, -1.0461, -1.0486,
0:         -1.0494, -1.0478, -1.0434, -1.0361, -1.0302, -1.0271, -1.0071, -1.0014, -0.9897, -0.9792, -0.9741, -0.9782,
0:         -0.9904], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9117, -0.8944, -0.8702, -0.8421, -0.8137, -0.7839, -0.7549, -0.7237, -0.6921, -0.6626, -0.6364, -0.6153,
0:         -0.5939, -0.5706, -0.5420, -0.5135, -0.4903, -0.4776, -0.9491, -0.9401, -0.9223, -0.8974, -0.8714, -0.8447,
0:         -0.8169], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1603, 0.2982, 0.4361, 0.5782, 0.7098, 0.8064, 0.8656, 0.8962, 0.9175, 0.9444, 0.9913, 1.0505, 1.1105, 1.1501,
0:         1.2072, 1.2541, 1.3070, 1.3696, 0.0906, 0.2280, 0.3624, 0.5041, 0.6255, 0.7219, 0.7943], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2871, -0.4032, -0.4300, -0.4233, -0.2961, -0.0818,  0.0499,  0.2731,  0.3423,  0.4718,  0.4986,  0.4361,
0:          0.3178,  0.1526,  0.0387, -0.0796, -0.0461, -0.1175, -0.1621, -0.3050, -0.3496, -0.3876, -0.3206, -0.1287,
0:          0.0075], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4145, -0.3651, -0.3190, -0.2687, -0.2055, -0.1188, -0.0103,  0.1223,  0.2430,  0.3332,  0.3800,  0.3773,
0:          0.3418,  0.2930,  0.2423,  0.1909,  0.1410,  0.0736,  0.0126, -0.0205, -0.0096,  0.0510,  0.1062,  0.1187,
0:          0.0610], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 43, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.0616,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1844,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1619,     nan,     nan,     nan, -0.2162,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2080,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2139,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2092,
0:         -0.2363,     nan, -0.2068,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1549,     nan,     nan,     nan, -0.1879,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.4519,     nan,     nan,     nan,     nan, -0.1985,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1407,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1820,     nan,     nan,     nan,     nan,     nan,
0:         -0.1915,     nan,     nan, -0.1844,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2139,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1076,     nan,     nan,     nan,     nan,     nan, -0.1360,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1076,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 43, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8950, -0.9105, -0.9806, -1.0683, -1.1339, -1.1640, -1.1727, -1.1888, -1.1994, -1.2418, -1.2841, -1.2675,
0:         -1.2169, -1.1338, -1.0446, -0.9855, -0.9641, -0.9865, -0.8671, -0.8549, -0.9143, -1.0124, -1.1096, -1.1546,
0:         -1.1753], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0546, -0.0767, -0.0960, -0.1046, -0.0744, -0.0065,  0.0496,  0.0737,  0.0638,  0.0140, -0.0508, -0.1022,
0:         -0.1526, -0.1758, -0.1943, -0.1887, -0.1737, -0.1388, -0.0899, -0.1025, -0.1291, -0.1575, -0.1624, -0.1418,
0:         -0.0981], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.5590, 1.5887, 1.6095, 1.6323, 1.6488, 1.6447, 1.6230, 1.5774, 1.5222, 1.4599, 1.4062, 1.3461, 1.2970, 1.2449,
0:         1.1809, 1.1252, 1.0740, 1.0254, 1.4905, 1.5305, 1.5604, 1.5871, 1.6198, 1.6180, 1.5947], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0234, -0.0873, -0.1914, -0.1386, -0.2383, -0.2790, -0.1562, -0.1627, -0.1892, -0.1510, -0.1741, -0.1624,
0:         -0.0724, -0.0414, -0.2048, -0.2201, -0.0097, -0.1135,  0.1658,  0.0587, -0.0949, -0.0943, -0.1556, -0.1511,
0:         -0.0785], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1596, -0.1446, -0.1161, -0.1314, -0.1776, -0.2657, -0.3386, -0.3685, -0.3546, -0.3196, -0.3027, -0.3066,
0:         -0.3251, -0.2935, -0.2670, -0.2438, -0.2798, -0.3524, -0.4441, -0.5041, -0.5075, -0.4493, -0.3319, -0.1949,
0:         -0.0901], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1413, -0.1447, -0.1366, -0.1488, -0.1492, -0.1438, -0.1589, -0.1515, -0.1396, -0.1460, -0.1403, -0.1528,
0:         -0.1511, -0.1509, -0.1525, -0.1599, -0.1551, -0.1488, -0.1432, -0.1448, -0.1481, -0.1481, -0.1473, -0.1475,
0:         -0.1559], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 43 [1/5 (20%)]	Loss: 0.73673 : 0.43875 :: 0.18404 (2.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 43 [2/5 (40%)]	Loss: 1.03760 : 0.48152 :: 0.18118 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 43 [3/5 (60%)]	Loss: 0.63681 : 0.44871 :: 0.19358 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 43 [4/5 (80%)]	Loss: 0.71244 : 0.40905 :: 0.17766 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.234, max = 0.779, mean = -0.096
0:          sample (first 20): tensor([0.7582, 0.7519, 0.7564, 0.7236, 0.6868, 0.6797, 0.6414, 0.6350, 0.6306, 0.7788, 0.7731, 0.7433, 0.7183, 0.6814,
0:         0.6619, 0.6717, 0.6489, 0.6459, 0.7520, 0.7518])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 43 : 0.4901830852031708
0: validation loss for velocity_u : 0.2317989617586136
0: validation loss for velocity_v : 0.16982431709766388
0: validation loss for specific_humidity : 0.06887376308441162
0: validation loss for velocity_z : 0.8420184254646301
0: validation loss for temperature : 0.6526660919189453
0: validation loss for total_precip : 0.9759175777435303
0: 44 : 23:24:05 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 44, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.3941, 2.4188, 2.4406, 2.4593, 2.4751, 2.4888, 2.5000, 2.5093, 2.5161, 2.5206, 2.5235, 2.5252, 2.5260, 2.5257,
0:         2.5236, 2.5193, 2.5122, 2.5029, 2.2505, 2.2867, 2.3211, 2.3534, 2.3834, 2.4107, 2.4352], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.4823, -1.5038, -1.5253, -1.5472, -1.5697, -1.5928, -1.6165, -1.6404, -1.6647, -1.6891, -1.7132, -1.7367,
0:         -1.7598, -1.7817, -1.8020, -1.8199, -1.8347, -1.8466, -1.4626, -1.4831, -1.5026, -1.5217, -1.5410, -1.5609,
0:         -1.5812], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3411,  0.3381,  0.3372,  0.3324,  0.3208,  0.3092,  0.2825,  0.2557,  0.2425,  0.2158,  0.1727,  0.1294,
0:          0.0764,  0.0229, -0.0164, -0.0644, -0.1209, -0.1779,  0.3515,  0.3518,  0.3521,  0.3520,  0.3519,  0.3519,
0:          0.3498], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2551, 0.2960, 0.3133, 0.3219, 0.3348, 0.3692, 0.4273, 0.4941, 0.5457, 0.5651, 0.5457, 0.5005, 0.4489, 0.4101,
0:         0.3864, 0.3671, 0.3305, 0.2638, 0.4596, 0.4489, 0.4338, 0.4316, 0.4489, 0.4811, 0.5134], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.9329, 0.9339, 0.9336, 0.9319, 0.9290, 0.9246, 0.9188, 0.9118, 0.9032, 0.8930, 0.8816, 0.8687, 0.8545, 0.8391,
0:         0.8221, 0.8032, 0.7826, 0.7600, 0.7357, 0.7100, 0.6835, 0.6565, 0.6292, 0.6018, 0.5742], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 44, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2460,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2343,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2471,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan, -0.2483,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2389,     nan,     nan, -0.2354,     nan,
0:         -0.2319,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2389,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2471,
0:             nan,     nan,     nan, -0.2471,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2483,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 44, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6916, 0.6920, 0.7195, 0.7684, 0.8001, 0.8163, 0.7904, 0.7551, 0.7138, 0.6822, 0.6770, 0.6712, 0.6713, 0.6833,
0:         0.7066, 0.7849, 0.9277, 1.0788, 0.7608, 0.7248, 0.7024, 0.7075, 0.7247, 0.7509, 0.7501], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-2.6403, -2.6966, -2.7244, -2.7107, -2.6650, -2.6048, -2.5453, -2.5258, -2.5591, -2.6446, -2.7569, -2.8404,
0:         -2.8949, -2.8771, -2.8361, -2.7965, -2.7834, -2.8105, -2.5148, -2.5576, -2.5913, -2.6228, -2.6059, -2.5633,
0:         -2.4822], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6097, -0.6408, -0.6700, -0.6885, -0.6993, -0.7049, -0.7020, -0.6937, -0.6877, -0.6811, -0.6799, -0.6743,
0:         -0.6718, -0.6775, -0.6943, -0.7124, -0.7394, -0.7614, -0.6230, -0.6634, -0.6972, -0.7186, -0.7322, -0.7389,
0:         -0.7325], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0646, -0.0846, -0.1472, -0.1052, -0.1482, -0.0977,  0.0832,  0.0558, -0.0113,  0.0582,  0.1353,  0.2428,
0:          0.3811,  0.3942,  0.2279,  0.2043,  0.2511, -0.0212,  0.0645,  0.0642, -0.0313, -0.0528, -0.0777, -0.0016,
0:          0.1163], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0830,  0.1460,  0.1972,  0.1920,  0.1443,  0.0730,  0.0270,  0.0213,  0.0461,  0.0763,  0.0900,  0.0915,
0:          0.0892,  0.1141,  0.1399,  0.1547,  0.1392,  0.0977,  0.0249, -0.0453, -0.0974, -0.1006, -0.0497,  0.0265,
0:          0.0883], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2084, -0.2113, -0.2043, -0.2162, -0.2156, -0.2168, -0.2213, -0.2130, -0.2033, -0.2116, -0.2095, -0.2224,
0:         -0.2200, -0.2193, -0.2135, -0.2232, -0.2152, -0.2107, -0.2095, -0.2126, -0.2143, -0.2149, -0.2106, -0.2155,
0:         -0.2166], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 44 [1/5 (20%)]	Loss: 0.65690 : 0.43381 :: 0.18843 (2.23 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 44 [2/5 (40%)]	Loss: 0.61904 : 0.39930 :: 0.18817 (15.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 44 [3/5 (60%)]	Loss: 0.50475 : 0.36014 :: 0.17380 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 44 [4/5 (80%)]	Loss: 0.64884 : 0.41588 :: 0.18464 (15.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.252, max = 4.024, mean = 0.185
0:          sample (first 20): tensor([0.1124, 0.1029, 0.0905, 0.0732, 0.0599, 0.0566, 0.0459, 0.0408, 0.0530, 0.1094, 0.0949, 0.0795, 0.0614, 0.0481,
0:         0.0383, 0.0329, 0.0423, 0.0432, 0.1016, 0.0826])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 44 : 0.46121644973754883
0: validation loss for velocity_u : 0.23869507014751434
0: validation loss for velocity_v : 0.1646888107061386
0: validation loss for specific_humidity : 0.06393478810787201
0: validation loss for velocity_z : 0.7981024384498596
0: validation loss for temperature : 0.6190317273139954
0: validation loss for total_precip : 0.8828456401824951
0: 45 : 23:28:15 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 45, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2251,  0.2095,  0.1986,  0.1812,  0.1501,  0.1246,  0.1061,  0.0744,  0.0208, -0.0573, -0.1096, -0.0966,
0:         -0.0872, -0.1137, -0.1650, -0.2547, -0.3125, -0.3285,  0.2154,  0.2010,  0.1885,  0.1707,  0.1433,  0.1188,
0:          0.1086], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6871, -0.7386, -0.7863, -0.8326, -0.8735, -0.9077, -0.9298, -0.9403, -0.9412, -0.9111, -0.8891, -0.9063,
0:         -0.9065, -0.9007, -0.8623, -0.7390, -0.6603, -0.6086, -0.6661, -0.7209, -0.7730, -0.8214, -0.8658, -0.9115,
0:         -0.9453], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4636, -0.4859, -0.5044, -0.5398, -0.5713, -0.5957, -0.6131, -0.6284, -0.6473, -0.6654, -0.6749, -0.6747,
0:         -0.6763, -0.6779, -0.6792, -0.6699, -0.6680, -0.6673, -0.4392, -0.4528, -0.4729, -0.5098, -0.5464, -0.5708,
0:         -0.5800], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.7968,  0.6460,  0.5323,  0.3334,  0.2241,  0.1870,  0.0514, -0.0185,  0.0777,  0.2219,  0.3356,  0.3400,
0:          0.2263,  0.4034,  0.5935,  0.4383,  0.7619,  0.9499,  0.6854,  0.5149,  0.3640,  0.2613,  0.1717,  0.0733,
0:          0.0186], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 1.2237,  1.2834,  1.3295,  1.3779,  1.4237,  1.4458,  1.4369,  1.3979,  1.3236,  1.2114,  1.1260,  1.1019,
0:          1.0711,  1.0201,  0.9145,  0.7281,  0.6000,  0.4548,  0.0820, -0.3080, -0.4817, -0.5630, -0.6188, -0.6099,
0:         -0.5716], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 45, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2359, -0.2359, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2359,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,
0:         -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,
0:             nan,     nan,     nan, -0.2359,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan, -0.2359, -0.2359,     nan,     nan,
0:         -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,
0:         -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,
0:             nan,     nan, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan, -0.2359,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan, -0.2359,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 45, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0868, 0.0892, 0.0855, 0.0788, 0.0887, 0.1086, 0.1283, 0.1340, 0.1290, 0.1118, 0.1010, 0.1181, 0.1332, 0.1570,
0:         0.1656, 0.1649, 0.1568, 0.1455, 0.0931, 0.0978, 0.0827, 0.0634, 0.0606, 0.0833, 0.1168], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4783, -0.4242, -0.3697, -0.3296, -0.2729, -0.2006, -0.1348, -0.0986, -0.0879, -0.0975, -0.1130, -0.1122,
0:         -0.1048, -0.0875, -0.0775, -0.0731, -0.0774, -0.0633, -0.4452, -0.3770, -0.3264, -0.3091, -0.2924, -0.2704,
0:         -0.2302], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9813, 0.9374, 0.8810, 0.8313, 0.7817, 0.7509, 0.7165, 0.7237, 0.7575, 0.8364, 0.9516, 1.0603, 1.1326, 1.1558,
0:         1.1361, 1.0893, 1.0225, 0.9646, 1.0515, 0.9955, 0.9245, 0.8565, 0.8068, 0.7528, 0.7095], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0050, -0.0478, -0.1754, -0.1753, -0.2595, -0.3274, -0.2447, -0.3046, -0.3389, -0.1574, -0.0448, -0.0921,
0:         -0.1393, -0.1597, -0.3493, -0.4818, -0.3971, -0.5450,  0.0454, -0.0043, -0.1976, -0.2100, -0.2486, -0.3126,
0:         -0.2340], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4973, -0.4339, -0.3512, -0.3286, -0.3542, -0.4252, -0.4737, -0.4661, -0.4162, -0.3509, -0.3065, -0.2862,
0:         -0.2871, -0.2430, -0.2012, -0.1414, -0.1089, -0.0899, -0.0988, -0.1245, -0.1647, -0.1962, -0.1911, -0.1507,
0:         -0.0937], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1692, -0.1713, -0.1641, -0.1768, -0.1730, -0.1747, -0.1794, -0.1717, -0.1672, -0.1604, -0.1657, -0.1731,
0:         -0.1715, -0.1697, -0.1694, -0.1791, -0.1769, -0.1728, -0.1581, -0.1615, -0.1625, -0.1619, -0.1620, -0.1668,
0:         -0.1672], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 45 [1/5 (20%)]	Loss: 0.58718 : 0.40568 :: 0.18891 (2.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 45 [2/5 (40%)]	Loss: 0.58678 : 0.37405 :: 0.17409 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 45 [3/5 (60%)]	Loss: 0.48502 : 0.33471 :: 0.17076 (15.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 45 [4/5 (80%)]	Loss: 0.79608 : 0.43636 :: 0.17138 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.257, max = 8.549, mean = 0.468
0:          sample (first 20): tensor([-0.2199, -0.2184, -0.2137, -0.2180, -0.2115, -0.2145, -0.2090, -0.2053, -0.2002, -0.2146, -0.2213, -0.2249,
0:         -0.2201, -0.2162, -0.2057, -0.2123, -0.2089, -0.2049, -0.2220, -0.2234])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 45 : 0.44118964672088623
0: validation loss for velocity_u : 0.24985551834106445
0: validation loss for velocity_v : 0.14932262897491455
0: validation loss for specific_humidity : 0.0708775743842125
0: validation loss for velocity_z : 0.6968563199043274
0: validation loss for temperature : 0.6367164850234985
0: validation loss for total_precip : 0.8435094952583313
0: 46 : 23:32:19 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 46, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3939, -0.3948, -0.3956, -0.3966, -0.3979, -0.3995, -0.4015, -0.4038, -0.4064, -0.4093, -0.4124, -0.4154,
0:         -0.4185, -0.4211, -0.4236, -0.4257, -0.4272, -0.4283, -0.4165, -0.4164, -0.4167, -0.4174, -0.4185, -0.4200,
0:         -0.4218], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4368, 0.4379, 0.4381, 0.4373, 0.4353, 0.4318, 0.4266, 0.4198, 0.4109, 0.4002, 0.3873, 0.3727, 0.3559, 0.3374,
0:         0.3173, 0.2955, 0.2726, 0.2486, 0.4440, 0.4447, 0.4445, 0.4432, 0.4405, 0.4364, 0.4307], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2770, -0.2699, -0.2572, -0.2424, -0.2281, -0.2137, -0.1995, -0.1865, -0.1781, -0.1721, -0.1662, -0.1602,
0:         -0.1548, -0.1560, -0.1597, -0.1634, -0.1672, -0.1709, -0.2924, -0.2760, -0.2596, -0.2432, -0.2268, -0.2111,
0:         -0.2008], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1021, -0.1088, -0.1066, -0.0999, -0.0866, -0.0688, -0.0489, -0.0289, -0.0089,  0.0111,  0.0288,  0.0444,
0:          0.0621,  0.0821,  0.1021,  0.1220,  0.1442,  0.1642, -0.1465, -0.1465, -0.1443, -0.1399, -0.1332, -0.1266,
0:         -0.1177], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0855, -0.0849, -0.0847, -0.0848, -0.0845, -0.0845, -0.0836, -0.0822, -0.0803, -0.0773, -0.0734, -0.0692,
0:         -0.0638, -0.0578, -0.0514, -0.0443, -0.0370, -0.0291, -0.0216, -0.0143, -0.0074, -0.0010,  0.0046,  0.0090,
0:          0.0123], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 46, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2428,     nan,     nan,     nan, -0.2346,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2299,     nan,     nan,     nan, -0.2311,     nan,     nan,
0:             nan,     nan,     nan, -0.2475,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2334,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2276,     nan,     nan,     nan,     nan,
0:             nan, -0.2533,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1971,
0:             nan, -0.2287,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan, -0.2264,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2252,     nan, -0.2416,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2463,     nan,     nan,     nan,
0:             nan,     nan, -0.2357,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2486,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2545,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2521,     nan,     nan, -0.2381,     nan,     nan,     nan,
0:         -0.2381,     nan,     nan,     nan,     nan, -0.2428, -0.2217,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2498,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2311,
0:             nan,     nan,     nan, -0.2545,     nan,     nan,     nan,     nan, -0.2545,     nan,     nan,     nan,
0:             nan,     nan, -0.2428])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 46, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2396, -0.2294, -0.2478, -0.2679, -0.2699, -0.2466, -0.2329, -0.2504, -0.3131, -0.4085, -0.4957, -0.5271,
0:         -0.5192, -0.4631, -0.4074, -0.3758, -0.3826, -0.4159, -0.2217, -0.2137, -0.2499, -0.3175, -0.3668, -0.3805,
0:         -0.3652], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0751,  0.0737,  0.0782,  0.0825,  0.1168,  0.1791,  0.2340,  0.2475,  0.2162,  0.1517,  0.0696,  0.0200,
0:         -0.0190, -0.0274, -0.0339, -0.0326, -0.0310, -0.0137,  0.0643,  0.0818,  0.0793,  0.0561,  0.0494,  0.0696,
0:          0.1010], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4979, -0.5235, -0.5442, -0.5619, -0.5714, -0.5714, -0.5617, -0.5424, -0.5243, -0.5051, -0.4891, -0.4703,
0:         -0.4567, -0.4472, -0.4492, -0.4589, -0.4830, -0.5048, -0.5117, -0.5403, -0.5680, -0.5896, -0.5980, -0.5979,
0:         -0.5821], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0028, -0.0988, -0.2420, -0.1814, -0.2084, -0.1899, -0.0630, -0.1373, -0.1954, -0.1619, -0.1991, -0.1278,
0:          0.0323,  0.0513, -0.0861, -0.0634,  0.1153, -0.0368,  0.1115,  0.0934, -0.0573, -0.0117, -0.0538, -0.0342,
0:          0.0769], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0088,  0.0342,  0.1082,  0.1616,  0.1823,  0.1463,  0.1012,  0.0843,  0.0997,  0.1367,  0.1615,  0.1525,
0:          0.1126,  0.0940,  0.0623,  0.0379, -0.0161, -0.0804, -0.1459, -0.1754, -0.1622, -0.1015, -0.0118,  0.0749,
0:          0.1338], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2379, -0.2345, -0.2279, -0.2321, -0.2248, -0.2270, -0.2233, -0.2214, -0.2195, -0.2313, -0.2386, -0.2419,
0:         -0.2340, -0.2313, -0.2205, -0.2295, -0.2292, -0.2264, -0.2379, -0.2412, -0.2338, -0.2289, -0.2244, -0.2262,
0:         -0.2241], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 46 [1/5 (20%)]	Loss: 0.92903 : 0.42516 :: 0.17817 (2.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 46 [2/5 (40%)]	Loss: 0.78794 : 0.48366 :: 0.17947 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 46 [3/5 (60%)]	Loss: 0.65450 : 0.38909 :: 0.17548 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 46 [4/5 (80%)]	Loss: 0.76609 : 0.40411 :: 0.17678 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.270, max = 9.896, mean = 0.290
0:          sample (first 20): tensor([-0.2549, -0.2500, -0.2467, -0.2486, -0.2387, -0.2424, -0.2395, -0.2373, -0.2419, -0.2482, -0.2580, -0.2554,
0:         -0.2460, -0.2436, -0.2307, -0.2395, -0.2471, -0.2464, -0.2552, -0.2531])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 46 : 0.4317299425601959
0: validation loss for velocity_u : 0.27367302775382996
0: validation loss for velocity_v : 0.1712496429681778
0: validation loss for specific_humidity : 0.054070208221673965
0: validation loss for velocity_z : 0.6697127223014832
0: validation loss for temperature : 0.6494923233985901
0: validation loss for total_precip : 0.7721818089485168
0: 47 : 23:36:19 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 47, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6165, -0.6246, -0.6313, -0.6390, -0.6496, -0.6648, -0.6853, -0.7113, -0.7424, -0.7779, -0.8165, -0.8567,
0:         -0.8970, -0.9360, -0.9728, -1.0068, -1.0372, -1.0647, -0.6458, -0.6550, -0.6646, -0.6768, -0.6933, -0.7154,
0:         -0.7441], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.6592, 1.7012, 1.7442, 1.7859, 1.8235, 1.8545, 1.8771, 1.8892, 1.8902, 1.8810, 1.8631, 1.8389, 1.8109, 1.7813,
0:         1.7519, 1.7239, 1.6981, 1.6736, 1.6332, 1.6766, 1.7198, 1.7600, 1.7945, 1.8203, 1.8351], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7021, -0.6977, -0.6937, -0.6869, -0.6771, -0.6685, -0.6582, -0.6480, -0.6403, -0.6381, -0.6332, -0.6338,
0:         -0.6354, -0.6361, -0.6366, -0.6383, -0.6395, -0.6405, -0.6975, -0.6923, -0.6879, -0.6778, -0.6673, -0.6569,
0:         -0.6485], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6854,  0.5724,  0.4355,  0.2921,  0.1552,  0.0248, -0.0968, -0.2207, -0.3445, -0.4575, -0.5466, -0.6031,
0:         -0.6249, -0.6140, -0.5814, -0.5336, -0.4858, -0.4423,  0.5898,  0.4659,  0.3247,  0.1791,  0.0357, -0.1012,
0:         -0.2359], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.5913, -1.6016, -1.6049, -1.6003, -1.5887, -1.5718, -1.5512, -1.5280, -1.5031, -1.4777, -1.4531, -1.4319,
0:         -1.4152, -1.4041, -1.3987, -1.3988, -1.4040, -1.4135, -1.4266, -1.4428, -1.4618, -1.4838, -1.5095, -1.5373,
0:         -1.5670], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 47, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.2512,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan, 1.1270,    nan,    nan,    nan,    nan, 0.7337,    nan,    nan,
0:         1.0258,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.9453,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan, 0.4554, 0.8763,    nan,    nan, 0.7866,    nan, 0.6739,    nan,    nan, 0.4140,    nan,    nan,
0:            nan, 0.7751,    nan,    nan,    nan,    nan,    nan,    nan, 0.8418, 0.8257,    nan,    nan,    nan,    nan,
0:            nan, 0.4140,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.8556,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.7544,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.5474,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.4853,    nan,    nan, 0.4669,    nan,    nan,    nan,
0:         0.4393,    nan,    nan,    nan,    nan,    nan,    nan, 0.3358,    nan, 0.4531,    nan,    nan,    nan,    nan,
0:            nan,    nan, 0.2990, 0.2484,    nan,    nan,    nan,    nan,    nan,    nan, 0.3450,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan, 0.4278,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.5635,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.4048,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan, 0.3289,    nan,    nan,    nan,    nan, 0.3634,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.4140,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan, 0.4669,    nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 47, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1777, 1.1501, 1.1498, 1.1864, 1.2257, 1.2501, 1.2410, 1.1925, 1.1375, 1.0865, 1.0770, 1.0821, 1.0958, 1.1066,
0:         1.0960, 1.1157, 1.1802, 1.2494, 1.1251, 1.0756, 1.0419, 1.0628, 1.1064, 1.1622, 1.1862], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7615, 0.8711, 0.9646, 1.0198, 1.0922, 1.1639, 1.1578, 1.0777, 0.9723, 0.8578, 0.8360, 0.8812, 0.9102, 0.9035,
0:         0.8278, 0.7496, 0.7050, 0.7452, 0.5992, 0.7373, 0.8346, 0.8920, 0.9388, 0.9638, 0.9364], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2336, -0.2443, -0.2532, -0.2584, -0.2573, -0.2457, -0.2330, -0.2160, -0.2058, -0.1927, -0.1826, -0.1700,
0:         -0.1677, -0.1694, -0.1863, -0.2142, -0.2599, -0.2993, -0.2473, -0.2567, -0.2711, -0.2752, -0.2766, -0.2696,
0:         -0.2547], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4820, -0.5532, -0.6930, -0.7434, -0.8783, -0.8951, -0.7543, -0.8290, -0.9654, -0.9054, -0.8529, -0.8266,
0:         -0.6830, -0.6157, -0.6655, -0.5629, -0.4130, -0.4150, -0.4989, -0.5070, -0.6483, -0.6872, -0.7487, -0.6990,
0:         -0.5670], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.2511,  0.2802,  0.3576,  0.4103,  0.4184,  0.3518,  0.2679,  0.2185,  0.2235,  0.2610,  0.2794,  0.2474,
0:          0.1666,  0.1041,  0.0524,  0.0445,  0.0391,  0.0325,  0.0065, -0.0248, -0.0420, -0.0270,  0.0357,  0.1195,
0:          0.1913], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.3597, 0.3464, 0.3468, 0.3259, 0.2960, 0.2724, 0.2535, 0.2425, 0.2265, 0.3653, 0.3450, 0.3274, 0.3100, 0.2865,
0:         0.2655, 0.2663, 0.2457, 0.2343, 0.3321, 0.3295, 0.3185, 0.3162, 0.2919, 0.2823, 0.2527], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 47 [1/5 (20%)]	Loss: 0.65304 : 0.41976 :: 0.17671 (2.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 47 [2/5 (40%)]	Loss: 0.73802 : 0.43360 :: 0.16835 (15.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 47 [3/5 (60%)]	Loss: 0.44168 : 0.36974 :: 0.18249 (15.24 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 47 [4/5 (80%)]	Loss: 0.69980 : 0.39401 :: 0.16966 (15.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.251, max = 0.123, mean = -0.167
0:          sample (first 20): tensor([-0.0470, -0.0606, -0.0690, -0.0776, -0.0814, -0.0828, -0.0818, -0.0696, -0.0569, -0.0326, -0.0573, -0.0607,
0:         -0.0761, -0.0809, -0.0763, -0.0670, -0.0635, -0.0533, -0.0395, -0.0495])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 47 : 0.37679317593574524
0: validation loss for velocity_u : 0.24448545277118683
0: validation loss for velocity_v : 0.13858799636363983
0: validation loss for specific_humidity : 0.06381412595510483
0: validation loss for velocity_z : 0.6444620490074158
0: validation loss for temperature : 0.5754755735397339
0: validation loss for total_precip : 0.5939337015151978
0: 48 : 23:40:26 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 48, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1724, 0.1776, 0.1794, 0.1807, 0.1820, 0.1810, 0.1776, 0.1744, 0.1731, 0.1734, 0.1765, 0.1856, 0.2026, 0.2272,
0:         0.2611, 0.3072, 0.3618, 0.4135, 0.1532, 0.1588, 0.1617, 0.1635, 0.1652, 0.1658, 0.1658], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4342, 0.4472, 0.4509, 0.4428, 0.4211, 0.3894, 0.3530, 0.3187, 0.2938, 0.2829, 0.2850, 0.2996, 0.3309, 0.3859,
0:         0.4654, 0.5649, 0.6798, 0.8058, 0.4677, 0.4759, 0.4768, 0.4677, 0.4466, 0.4178, 0.3884], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7058, -0.7046, -0.7024, -0.6995, -0.6965, -0.6936, -0.6922, -0.6911, -0.6923, -0.6940, -0.6979, -0.7028,
0:         -0.7085, -0.7139, -0.7200, -0.7251, -0.7294, -0.7332, -0.7081, -0.7064, -0.7042, -0.7012, -0.6990, -0.6962,
0:         -0.6948], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1693,  0.3206,  0.3248,  0.1896, -0.0149, -0.2227, -0.3782, -0.4346, -0.3878, -0.2770, -0.1417, -0.0043,
0:          0.1044,  0.1608,  0.1864,  0.2300,  0.2812,  0.2545,  0.2205,  0.3312,  0.3153,  0.1779, -0.0480, -0.2844,
0:         -0.4378], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.4438, -1.4064, -1.3683, -1.3300, -1.2926, -1.2567, -1.2222, -1.1875, -1.1510, -1.1124, -1.0715, -1.0281,
0:         -0.9827, -0.9384, -0.8981, -0.8620, -0.8283, -0.8002, -0.7834, -0.7834, -0.8024, -0.8414, -0.8984, -0.9624,
0:         -1.0167], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 48, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2188,
0:         -0.2304,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.2294,     nan,     nan,     nan,     nan, -0.2385,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0902,  0.1112,     nan,     nan,     nan, -0.2153,
0:             nan,     nan, -0.2408,     nan,  0.1552,  0.0962,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.0545,     nan,     nan, -0.2246,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2385, -0.2396, -0.2408, -0.2408,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1840,     nan,     nan,     nan,
0:             nan,     nan, -0.2408,     nan, -0.1424,     nan,     nan, -0.2280,     nan,     nan,     nan, -0.2408,
0:         -0.1053,     nan,     nan,     nan,     nan,     nan, -0.2361,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1910,     nan, -0.2315,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2408,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2408,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2408,     nan,     nan,     nan, -0.2269,     nan, -0.2361,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2014, -0.2327, -0.2373,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2338,     nan,     nan, -0.2408,     nan, -0.1366,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2396,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 48, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6615, -0.6709, -0.6900, -0.7151, -0.7369, -0.7528, -0.7631, -0.7814, -0.8154, -0.8670, -0.9109, -0.9405,
0:         -0.9309, -0.8866, -0.8357, -0.7871, -0.7635, -0.7740, -0.6562, -0.6333, -0.6511, -0.7020, -0.7698, -0.8058,
0:         -0.8336], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0587, -0.1945, -0.3285, -0.4540, -0.5560, -0.6339, -0.7314, -0.8438, -0.9644, -1.1077, -1.2492, -1.3830,
0:         -1.5439, -1.6919, -1.8293, -1.9157, -1.9496, -1.9183,  0.0556, -0.0729, -0.2154, -0.3577, -0.4784, -0.5869,
0:         -0.6839], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6827, -0.6929, -0.7011, -0.7086, -0.7152, -0.7159, -0.7119, -0.7048, -0.6998, -0.6968, -0.7013, -0.7029,
0:         -0.7060, -0.7105, -0.7158, -0.7222, -0.7337, -0.7421, -0.6880, -0.7015, -0.7149, -0.7288, -0.7383, -0.7445,
0:         -0.7395], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2299, -0.5211, -1.2337, -1.5920, -1.8917, -1.7494, -1.2638, -0.8985, -0.6038, -0.4128, -0.3531, -0.2192,
0:          0.2000,  0.8581,  1.6238,  2.5281,  3.1613,  3.0566,  0.6702,  0.0741, -0.6763, -1.2367, -1.7710, -1.7851,
0:         -1.4555], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3805, -0.3589, -0.2799, -0.2363, -0.2265, -0.2849, -0.3345, -0.3379, -0.2754, -0.1680, -0.0656, -0.0159,
0:         -0.0343, -0.0533, -0.1141, -0.1763, -0.2623, -0.3528, -0.4355, -0.4864, -0.4938, -0.4668, -0.4165, -0.3722,
0:         -0.3497], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.1393, 0.1285, 0.1179, 0.1030, 0.0960, 0.0853, 0.0743, 0.0680, 0.0790, 0.1396, 0.1161, 0.1125, 0.0989, 0.0935,
0:         0.0880, 0.0883, 0.0890, 0.0846, 0.1208, 0.1126, 0.1069, 0.1155, 0.1011, 0.1008, 0.1017], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 48 [1/5 (20%)]	Loss: 0.48957 : 0.37502 :: 0.16920 (2.59 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 48 [2/5 (40%)]	Loss: 0.60506 : 0.38489 :: 0.16517 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 48 [3/5 (60%)]	Loss: 0.61553 : 0.41413 :: 0.18027 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 48 [4/5 (80%)]	Loss: 0.65793 : 0.40083 :: 0.16910 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.276, max = 0.810, mean = -0.161
0:          sample (first 20): tensor([-0.2216, -0.2285, -0.2297, -0.2324, -0.2198, -0.2285, -0.2197, -0.2222, -0.2185, -0.2147, -0.2308, -0.2315,
0:         -0.2303, -0.2264, -0.2188, -0.2207, -0.2251, -0.2269, -0.2222, -0.2273])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 48 : 0.4644611179828644
0: validation loss for velocity_u : 0.2945307791233063
0: validation loss for velocity_v : 0.17879976332187653
0: validation loss for specific_humidity : 0.05900437757372856
0: validation loss for velocity_z : 0.7795171141624451
0: validation loss for temperature : 0.6504059433937073
0: validation loss for total_precip : 0.8245086073875427
0: 49 : 23:44:26 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 49, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4812,  0.3510,  0.2412,  0.1586,  0.1013,  0.0638,  0.0432,  0.0370,  0.0368,  0.0289,  0.0010, -0.0489,
0:         -0.1120, -0.1756, -0.2325, -0.2858, -0.3434, -0.4062,  0.5204,  0.3900,  0.2784,  0.1933,  0.1325,  0.0907,
0:          0.0656], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1852, 0.2064, 0.2369, 0.2829, 0.3475, 0.4284, 0.5139, 0.5836, 0.6253, 0.6458, 0.6645, 0.6946, 0.7330, 0.7625,
0:         0.7579, 0.6940, 0.5594, 0.3675, 0.1006, 0.1174, 0.1397, 0.1748, 0.2267, 0.2956, 0.3745], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6035, -0.6428, -0.6655, -0.6547, -0.6542, -0.6585, -0.6777, -0.6917, -0.6956, -0.6877, -0.6737, -0.6591,
0:         -0.6431, -0.6242, -0.6091, -0.5944, -0.6348, -0.6712, -0.5985, -0.6445, -0.6698, -0.6562, -0.6501, -0.6493,
0:         -0.6635], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0270,  0.0566,  0.2150,  0.3579,  0.4305,  0.5295,  0.8947,  1.6184,  2.3928,  2.6963,  2.2850,  1.4007,
0:          0.5317,  0.0544,  0.0368,  0.2809,  0.5581,  0.8243, -0.0886, -0.0182,  0.1358,  0.3095,  0.4217,  0.5097,
0:          0.7869], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.3650, 1.3557, 1.3479, 1.3420, 1.3389, 1.3373, 1.3314, 1.3132, 1.2788, 1.2330, 1.1871, 1.1538, 1.1378, 1.1303,
0:         1.1166, 1.0897, 1.0563, 1.0265, 1.0054, 0.9906, 0.9774, 0.9651, 0.9558, 0.9516, 0.9511], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 49, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2213,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1809,     nan, -0.2033,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1966,     nan,     nan, -0.1776,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1518,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0431,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1428,     nan,     nan,  0.2975,     nan,  1.1433,     nan,     nan, -0.2044,     nan,     nan,     nan,
0:             nan,     nan,  0.1653,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1540,     nan,     nan,     nan,     nan,
0:         -0.1664, -0.1944, -0.2033,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1798,     nan,     nan,     nan,     nan,     nan,     nan, -0.1641,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1249,     nan,     nan,     nan, -0.1630,     nan,     nan,     nan, -0.0644,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2044,     nan,     nan, -0.2201,     nan,     nan,     nan,     nan, -0.1451, -0.1596,
0:             nan,     nan,     nan, -0.1372,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1910,     nan,     nan, -0.1529,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1204,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 49, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7714, 0.8368, 0.9531, 1.0990, 1.2069, 1.2324, 1.1732, 1.0526, 0.9280, 0.8363, 0.8108, 0.7985, 0.8069, 0.8165,
0:         0.8080, 0.8486, 0.9486, 1.0612, 0.7166, 0.7317, 0.7997, 0.9114, 1.0253, 1.1126, 1.1101], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.5047, -1.4574, -1.3951, -1.3263, -1.2608, -1.1877, -1.1260, -1.0682, -1.0349, -1.0320, -1.0472, -1.0516,
0:         -1.0493, -1.0185, -0.9627, -0.8957, -0.8263, -0.7679, -1.4890, -1.4101, -1.3490, -1.3072, -1.2714, -1.2321,
0:         -1.1650], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2363, 0.2533, 0.2835, 0.3232, 0.3654, 0.4021, 0.4226, 0.4376, 0.4450, 0.4549, 0.4771, 0.4988, 0.5122, 0.5105,
0:         0.4894, 0.4596, 0.4182, 0.3838, 0.2049, 0.2050, 0.2186, 0.2426, 0.2827, 0.3140, 0.3403], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2186,  0.2900,  0.2737,  0.2818,  0.1682,  0.1038,  0.1976,  0.1941,  0.1526,  0.1756,  0.1898,  0.2534,
0:          0.3153,  0.2552,  0.0654, -0.0113,  0.0575, -0.1196,  0.0039,  0.1338,  0.1374,  0.1853,  0.1927,  0.2445,
0:          0.3382], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.7188, 0.8545, 0.8918, 0.8261, 0.6954, 0.5472, 0.4292, 0.3545, 0.3473, 0.3767, 0.3961, 0.3794, 0.3306, 0.3356,
0:         0.3930, 0.4813, 0.5295, 0.4842, 0.3186, 0.1278, 0.0276, 0.0971, 0.3199, 0.5807, 0.7232], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0578, -0.0705, -0.0834, -0.0866, -0.0864, -0.0953, -0.0916, -0.0849, -0.0763, -0.0612, -0.0786, -0.0871,
0:         -0.1038, -0.1067, -0.1010, -0.0962, -0.0943, -0.0885, -0.0791, -0.0884, -0.0935, -0.0885, -0.1051, -0.1021,
0:         -0.1045], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 49 [1/5 (20%)]	Loss: 0.66735 : 0.41909 :: 0.18658 (2.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 49 [2/5 (40%)]	Loss: 0.73333 : 0.41938 :: 0.17914 (15.24 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 49 [3/5 (60%)]	Loss: 0.69737 : 0.40599 :: 0.17167 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 49 [4/5 (80%)]	Loss: 0.56430 : 0.36774 :: 0.17145 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.260, max = 2.938, mean = 0.063
0:          sample (first 20): tensor([0.5268, 0.5164, 0.5167, 0.5173, 0.5162, 0.5239, 0.5340, 0.5477, 0.5562, 0.5021, 0.4885, 0.4793, 0.4626, 0.4744,
0:         0.4772, 0.5252, 0.5327, 0.5434, 0.4487, 0.4427])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 49 : 0.45271411538124084
0: validation loss for velocity_u : 0.2500518560409546
0: validation loss for velocity_v : 0.18478505313396454
0: validation loss for specific_humidity : 0.07358565926551819
0: validation loss for velocity_z : 0.7368981242179871
0: validation loss for temperature : 0.6146349310874939
0: validation loss for total_precip : 0.8563289642333984
0: Finished training at 23:48:34 with test loss = 0.45271411538124084.
0: [1;34mwandb[0m:
0: [1;34mwandb[0m: You can sync this run to the cloud by running:
0: [1;34mwandb[0m: [1mwandb sync /work/ab1412/atmorep/wandb/offline-run-20250610_202950-tge40j11[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250610_202950-tge40j11/logs[0m
0: l40360:84505:84900 [2] NCCL INFO [Service thread] Connection closed by localRank 0
0: l40360:84505:96894 [2] NCCL INFO comm 0x55557eb732f0 rank 0 nranks 1 cudaDev 2 busId 84000 - Abort COMPLETE
0: l40360:84505:84905 [3] NCCL INFO [Service thread] Connection closed by localRank 0
0: l40360:84505:96894 [3] NCCL INFO comm 0x555587744780 rank 0 nranks 1 cudaDev 3 busId c4000 - Abort COMPLETE
0: l40360:84505:84895 [1] NCCL INFO [Service thread] Connection closed by localRank 0
0: l40360:84505:96894 [1] NCCL INFO comm 0x5555a6f78c00 rank 0 nranks 1 cudaDev 1 busId 44000 - Abort COMPLETE
0: l40360:84505:84751 [0] NCCL INFO [Service thread] Connection closed by localRank 0
0: l40360:84505:96894 [0] NCCL INFO comm 0x55555f26a680 rank 0 nranks 1 cudaDev 0 busId 3000 - Abort COMPLETE
