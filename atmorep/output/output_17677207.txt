0: Wandb run: atmorep-qfvmwu56-17677207
0: l50112:1422082:1422082 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.170<0>
0: l50112:1422082:1422082 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50112:1422082:1422082 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50112:1422082:1422082 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50112:1422082:1422082 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
1: l50112:1422083:1422083 [1] NCCL INFO cudaDriverVersion 12050
2: l50112:1422084:1422084 [2] NCCL INFO cudaDriverVersion 12050
3: l50112:1422085:1422085 [3] NCCL INFO cudaDriverVersion 12050
1: l50112:1422083:1422083 [1] NCCL INFO Bootstrap : Using ib0:10.128.11.170<0>
2: l50112:1422084:1422084 [2] NCCL INFO Bootstrap : Using ib0:10.128.11.170<0>
3: l50112:1422085:1422085 [3] NCCL INFO Bootstrap : Using ib0:10.128.11.170<0>
1: l50112:1422083:1422083 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
1: l50112:1422083:1422083 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
1: l50112:1422083:1422083 [1] NCCL INFO NET/Plugin: Using internal network plugin.
2: l50112:1422084:1422084 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
2: l50112:1422084:1422084 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
2: l50112:1422084:1422084 [2] NCCL INFO NET/Plugin: Using internal network plugin.
3: l50112:1422085:1422085 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
3: l50112:1422085:1422085 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
3: l50112:1422085:1422085 [3] NCCL INFO NET/Plugin: Using internal network plugin.
1: l50112:1422083:1422556 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.170<0>
1: l50112:1422083:1422556 [1] NCCL INFO Using non-device net plugin version 0
1: l50112:1422083:1422556 [1] NCCL INFO Using network IB
3: l50112:1422085:1422558 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.170<0>
0: l50112:1422082:1422539 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.170<0>
3: l50112:1422085:1422558 [3] NCCL INFO Using non-device net plugin version 0
3: l50112:1422085:1422558 [3] NCCL INFO Using network IB
0: l50112:1422082:1422539 [0] NCCL INFO Using non-device net plugin version 0
0: l50112:1422082:1422539 [0] NCCL INFO Using network IB
2: l50112:1422084:1422557 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.170<0>
2: l50112:1422084:1422557 [2] NCCL INFO Using non-device net plugin version 0
2: l50112:1422084:1422557 [2] NCCL INFO Using network IB
3: l50112:1422085:1422558 [3] NCCL INFO ncclCommInitRank comm 0x55555ee283e0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c4000 commId 0xdbf96a021c013528 - Init START
2: l50112:1422084:1422557 [2] NCCL INFO ncclCommInitRank comm 0x55555ee280b0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 84000 commId 0xdbf96a021c013528 - Init START
1: l50112:1422083:1422556 [1] NCCL INFO ncclCommInitRank comm 0x55555ee28790 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 44000 commId 0xdbf96a021c013528 - Init START
0: l50112:1422082:1422539 [0] NCCL INFO ncclCommInitRank comm 0x55555f26c9c0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3000 commId 0xdbf96a021c013528 - Init START
2: l50112:1422084:1422557 [2] NCCL INFO NVLS multicast support is not available on dev 2
0: l50112:1422082:1422539 [0] NCCL INFO NVLS multicast support is not available on dev 0
3: l50112:1422085:1422558 [3] NCCL INFO NVLS multicast support is not available on dev 3
1: l50112:1422083:1422556 [1] NCCL INFO NVLS multicast support is not available on dev 1
0: l50112:1422082:1422539 [0] NCCL INFO comm 0x55555f26c9c0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
0: l50112:1422082:1422539 [0] NCCL INFO Channel 00/24 :    0   1   2   3
0: l50112:1422082:1422539 [0] NCCL INFO Channel 01/24 :    0   1   3   2
2: l50112:1422084:1422557 [2] NCCL INFO comm 0x55555ee280b0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
3: l50112:1422085:1422558 [3] NCCL INFO comm 0x55555ee283e0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
3: l50112:1422085:1422558 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->1 [5] -1/-1/-1->3->1 [6] -1/-1/-1->3->1 [7] -1/-1/-1->3->1 [8] 2/-1/-1->3->0 [9] 2/-1/-1->3->0 [10] 2/-1/-1->3->0 [11] 2/-1/-1->3->0 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->1 [17] -1/-1/-1->3->1 [18] -1/-1/-1->3->1 [19] -1/-1/-1->3->1 [20] 2/-1/-1->3->0 [21] 2/-1/-1->3->0 [22] 2/-1/-1->3->0 [23] 2/-1/-1->3->0
0: l50112:1422082:1422539 [0] NCCL INFO Channel 02/24 :    0   2   3   1
0: l50112:1422082:1422539 [0] NCCL INFO Channel 03/24 :    0   2   1   3
0: l50112:1422082:1422539 [0] NCCL INFO Channel 04/24 :    0   3   1   2
0: l50112:1422082:1422539 [0] NCCL INFO Channel 05/24 :    0   3   2   1
0: l50112:1422082:1422539 [0] NCCL INFO Channel 06/24 :    0   1   2   3
0: l50112:1422082:1422539 [0] NCCL INFO Channel 07/24 :    0   1   3   2
0: l50112:1422082:1422539 [0] NCCL INFO Channel 08/24 :    0   2   3   1
0: l50112:1422082:1422539 [0] NCCL INFO Channel 09/24 :    0   2   1   3
0: l50112:1422082:1422539 [0] NCCL INFO Channel 10/24 :    0   3   1   2
0: l50112:1422082:1422539 [0] NCCL INFO Channel 11/24 :    0   3   2   1
0: l50112:1422082:1422539 [0] NCCL INFO Channel 12/24 :    0   1   2   3
0: l50112:1422082:1422539 [0] NCCL INFO Channel 13/24 :    0   1   3   2
0: l50112:1422082:1422539 [0] NCCL INFO Channel 14/24 :    0   2   3   1
0: l50112:1422082:1422539 [0] NCCL INFO Channel 15/24 :    0   2   1   3
0: l50112:1422082:1422539 [0] NCCL INFO Channel 16/24 :    0   3   1   2
0: l50112:1422082:1422539 [0] NCCL INFO Channel 17/24 :    0   3   2   1
0: l50112:1422082:1422539 [0] NCCL INFO Channel 18/24 :    0   1   2   3
0: l50112:1422082:1422539 [0] NCCL INFO Channel 19/24 :    0   1   3   2
0: l50112:1422082:1422539 [0] NCCL INFO Channel 20/24 :    0   2   3   1
0: l50112:1422082:1422539 [0] NCCL INFO Channel 21/24 :    0   2   1   3
0: l50112:1422082:1422539 [0] NCCL INFO Channel 22/24 :    0   3   1   2
1: l50112:1422083:1422556 [1] NCCL INFO comm 0x55555ee28790 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
1: l50112:1422083:1422556 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 3/-1/-1->1->2 [5] 3/-1/-1->1->2 [6] 3/-1/-1->1->2 [7] 3/-1/-1->1->2 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 3/-1/-1->1->2 [17] 3/-1/-1->1->2 [18] 3/-1/-1->1->2 [19] 3/-1/-1->1->2 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1
2: l50112:1422084:1422557 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->0 [5] 1/-1/-1->2->0 [6] 1/-1/-1->2->0 [7] 1/-1/-1->2->0 [8] -1/-1/-1->2->3 [9] -1/-1/-1->2->3 [10] -1/-1/-1->2->3 [11] -1/-1/-1->2->3 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 1/-1/-1->2->0 [17] 1/-1/-1->2->0 [18] 1/-1/-1->2->0 [19] 1/-1/-1->2->0 [20] -1/-1/-1->2->3 [21] -1/-1/-1->2->3 [22] -1/-1/-1->2->3 [23] -1/-1/-1->2->3
2: l50112:1422084:1422557 [2] NCCL INFO P2P Chunksize set to 524288
3: l50112:1422085:1422558 [3] NCCL INFO P2P Chunksize set to 524288
0: l50112:1422082:1422539 [0] NCCL INFO Channel 23/24 :    0   3   2   1
0: l50112:1422082:1422539 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 2/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 2/-1/-1->0->-1 [7] 2/-1/-1->0->-1 [8] 3/-1/-1->0->1 [9] 3/-1/-1->0->1 [10] 3/-1/-1->0->1 [11] 3/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 2/-1/-1->0->-1 [17] 2/-1/-1->0->-1 [18] 2/-1/-1->0->-1 [19] 2/-1/-1->0->-1 [20] 3/-1/-1->0->1 [21] 3/-1/-1->0->1 [22] 3/-1/-1->0->1 [23] 3/-1/-1->0->1
0: l50112:1422082:1422539 [0] NCCL INFO P2P Chunksize set to 524288
1: l50112:1422083:1422556 [1] NCCL INFO P2P Chunksize set to 524288
3: l50112:1422085:1422558 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 04/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 08/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 10/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 09/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 14/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 14/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 13/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 13/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 16/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 15/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 16/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 15/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 20/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 20/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 19/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 22/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 21/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 21/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 04/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 16/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 17/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 22/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 23/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Connected all rings
1: l50112:1422083:1422556 [1] NCCL INFO Connected all rings
3: l50112:1422085:1422558 [3] NCCL INFO Connected all rings
2: l50112:1422084:1422557 [2] NCCL INFO Connected all rings
0: l50112:1422082:1422539 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 04/0 : 0[0] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 06/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 05/0 : 3[3] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 04/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 07/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 05/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 16/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 07/0 : 3[3] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 17/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 17/0 : 3[3] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 16/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 18/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 17/0 : 2[2] -> 0[0] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 18/0 : 3[3] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 17/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 19/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 18/0 : 2[2] -> 0[0] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 19/0 : 3[3] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 18/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 08/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 20/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50112:1422082:1422539 [0] NCCL INFO Channel 21/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50112:1422085:1422558 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50112:1422083:1422556 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read
2: l50112:1422084:1422557 [2] NCCL INFO Connected all trees
0: l50112:1422082:1422539 [0] NCCL INFO Connected all trees
1: l50112:1422083:1422556 [1] NCCL INFO Connected all trees
3: l50112:1422085:1422558 [3] NCCL INFO Connected all trees
1: l50112:1422083:1422556 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
1: l50112:1422083:1422556 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
2: l50112:1422084:1422557 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
2: l50112:1422084:1422557 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
3: l50112:1422085:1422558 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
3: l50112:1422085:1422558 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
0: l50112:1422082:1422539 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
0: l50112:1422082:1422539 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
0: l50112:1422082:1422539 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50112:1422082:1422539 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50112:1422082:1422539 [0] NCCL INFO ncclCommInitRank comm 0x55555f26c9c0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3000 commId 0xdbf96a021c013528 - Init COMPLETE
1: l50112:1422083:1422556 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
1: l50112:1422083:1422556 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
1: l50112:1422083:1422556 [1] NCCL INFO ncclCommInitRank comm 0x55555ee28790 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 44000 commId 0xdbf96a021c013528 - Init COMPLETE
2: l50112:1422084:1422557 [2] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
2: l50112:1422084:1422557 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
2: l50112:1422084:1422557 [2] NCCL INFO ncclCommInitRank comm 0x55555ee280b0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 84000 commId 0xdbf96a021c013528 - Init COMPLETE
3: l50112:1422085:1422558 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
3: l50112:1422085:1422558 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
3: l50112:1422085:1422558 [3] NCCL INFO ncclCommInitRank comm 0x55555ee283e0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c4000 commId 0xdbf96a021c013528 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 1
0: par_rank : 0
0: par_size : 4
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 0, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 0, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17677207
0: wandb_id : qfvmwu56
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: mask_input_field : total_precip
0: mask_input_value : 0
0: years_test : [2021]
3: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
2: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
3: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
3: self.lats : (721,)
3: self.lons : (1440,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
2: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.lats : (721,)
2: self.lons : (1440,)
0: self.lons : (1440,)
2: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
3: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
2: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.lats : (721,)
2: self.lons : (1440,)
3: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
3: self.lats : (721,)
3: self.lons : (1440,)
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
2: ['cuda:2'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
0: ['cuda:0'] 0
2: ['cuda:2'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
0: ['cuda:0'] 0
2: Loaded model id = wc5e2i3t.
2: Loaded run 'wc5e2i3t' at epoch -2.
1: Loaded model id = wc5e2i3t.
1: Loaded run 'wc5e2i3t' at epoch -2.
3: Loaded model id = wc5e2i3t.
3: Loaded run 'wc5e2i3t' at epoch -2.
0: Loaded model id = wc5e2i3t.
0: Loaded run 'wc5e2i3t' at epoch -2.
1: -1 : 20:47:23 :: batch_size = 96, lr = 1e-05
2: -1 : 20:47:23 :: batch_size = 96, lr = 1e-05
3: -1 : 20:47:23 :: batch_size = 96, lr = 1e-05
0: Number of trainable parameters: 741,136,272
0: -1 : 20:47:23 :: batch_size = 96, lr = 1e-05
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch -1, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch -1, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch -1, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] INPUT BATCH
0: Epoch -1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.0449,  0.0506,  0.0561,  0.0616,  0.0669,  0.0722,  0.0772,  0.0821,
1:          0.0866,  0.0909,  0.0950,  0.0988,  0.1024,  0.1056,  0.1087,  0.1113,
1:          0.1137,  0.1158, -0.0298, -0.0256, -0.0214, -0.0172, -0.0130, -0.0088,
3:      first 25 values: tensor([ 0.0449,  0.0506,  0.0561,  0.0616,  0.0669,  0.0722,  0.0772,  0.0821,
3:          0.0866,  0.0909,  0.0950,  0.0988,  0.1024,  0.1056,  0.1087,  0.1113,
3:          0.1137,  0.1158, -0.0298, -0.0256, -0.0214, -0.0172, -0.0130, -0.0088,
0:      first 25 values: tensor([ 0.0449,  0.0506,  0.0561,  0.0616,  0.0669,  0.0722,  0.0772,  0.0821,
0:          0.0866,  0.0909,  0.0950,  0.0988,  0.1024,  0.1056,  0.1087,  0.1113,
0:          0.1137,  0.1158, -0.0298, -0.0256, -0.0214, -0.0172, -0.0130, -0.0088,
2:      first 25 values: tensor([ 0.0449,  0.0506,  0.0561,  0.0616,  0.0669,  0.0722,  0.0772,  0.0821,
2:          0.0866,  0.0909,  0.0950,  0.0988,  0.1024,  0.1056,  0.1087,  0.1113,
2:          0.1137,  0.1158, -0.0298, -0.0256, -0.0214, -0.0172, -0.0130, -0.0088,
1:         -0.0048], device='cuda:1')
3:         -0.0048], device='cuda:3')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:         -0.0048], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:         -0.0048], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.8011, -0.7921, -0.7828, -0.7736, -0.7643, -0.7548, -0.7453, -0.7354,
3:         -0.7255, -0.7151, -0.7048, -0.6939, -0.6831, -0.6721, -0.6609, -0.6496,
3:         -0.6381, -0.6265, -0.7519, -0.7438, -0.7356, -0.7275, -0.7195, -0.7111,
3:         -0.7028], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.8011, -0.7921, -0.7828, -0.7736, -0.7643, -0.7548, -0.7453, -0.7354,
1:         -0.7255, -0.7151, -0.7048, -0.6939, -0.6831, -0.6721, -0.6609, -0.6496,
1:         -0.6381, -0.6265, -0.7519, -0.7438, -0.7356, -0.7275, -0.7195, -0.7111,
1:         -0.7028], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.8011, -0.7921, -0.7828, -0.7736, -0.7643, -0.7548, -0.7453, -0.7354,
2:         -0.7255, -0.7151, -0.7048, -0.6939, -0.6831, -0.6721, -0.6609, -0.6496,
2:         -0.6381, -0.6265, -0.7519, -0.7438, -0.7356, -0.7275, -0.7195, -0.7111,
2:         -0.7028], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2793, -0.2747, -0.2710, -0.2673, -0.2635, -0.2599, -0.2571, -0.2543,
3:         -0.2513, -0.2486, -0.2467, -0.2446, -0.2426, -0.2406, -0.2402, -0.2394,
3:         -0.2386, -0.2379, -0.3242, -0.3186, -0.3141, -0.3084, -0.3025, -0.2967,
3:         -0.2909], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8011, -0.7921, -0.7828, -0.7736, -0.7643, -0.7548, -0.7453, -0.7354,
0:         -0.7255, -0.7151, -0.7048, -0.6939, -0.6831, -0.6721, -0.6609, -0.6496,
0:         -0.6381, -0.6265, -0.7519, -0.7438, -0.7356, -0.7275, -0.7195, -0.7111,
0:         -0.7028], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.2326,  0.2293,  0.2270,  0.2226,  0.2148,  0.2025,  0.1857,  0.1679,
3:          0.1489,  0.1332,  0.1176,  0.1042,  0.0886,  0.0707,  0.0495,  0.0260,
3:          0.0037, -0.0153,  0.3856,  0.3733,  0.3644,  0.3566,  0.3454,  0.3309,
3:          0.3097], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.2793, -0.2747, -0.2710, -0.2673, -0.2635, -0.2599, -0.2571, -0.2543,
1:         -0.2513, -0.2486, -0.2467, -0.2446, -0.2426, -0.2406, -0.2402, -0.2394,
1:         -0.2386, -0.2379, -0.3242, -0.3186, -0.3141, -0.3084, -0.3025, -0.2967,
1:         -0.2909], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2793, -0.2747, -0.2710, -0.2673, -0.2635, -0.2599, -0.2571, -0.2543,
2:         -0.2513, -0.2486, -0.2467, -0.2446, -0.2426, -0.2406, -0.2402, -0.2394,
2:         -0.2386, -0.2379, -0.3242, -0.3186, -0.3141, -0.3084, -0.3025, -0.2967,
2:         -0.2909], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.6351, -0.6524, -0.6696, -0.6859, -0.7017, -0.7165, -0.7308, -0.7438,
3:         -0.7559, -0.7679, -0.7788, -0.7892, -0.7995, -0.8088, -0.8178, -0.8261,
3:         -0.8338, -0.8406, -0.8468, -0.8524, -0.8575, -0.8616, -0.8650, -0.8676,
3:         -0.8692], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
0:      first 25 values: tensor([-0.2793, -0.2747, -0.2710, -0.2673, -0.2635, -0.2599, -0.2571, -0.2543,
0:         -0.2513, -0.2486, -0.2467, -0.2446, -0.2426, -0.2406, -0.2402, -0.2394,
0:         -0.2386, -0.2379, -0.3242, -0.3186, -0.3141, -0.3084, -0.3025, -0.2967,
0:         -0.2909], device='cuda:0')
1:      first 25 values: tensor([ 0.2326,  0.2293,  0.2270,  0.2226,  0.2148,  0.2025,  0.1857,  0.1679,
1:          0.1489,  0.1332,  0.1176,  0.1042,  0.0886,  0.0707,  0.0495,  0.0260,
1:          0.0037, -0.0153,  0.3856,  0.3733,  0.3644,  0.3566,  0.3454,  0.3309,
1:          0.3097], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.2326,  0.2293,  0.2270,  0.2226,  0.2148,  0.2025,  0.1857,  0.1679,
2:          0.1489,  0.1332,  0.1176,  0.1042,  0.0886,  0.0707,  0.0495,  0.0260,
2:          0.0037, -0.0153,  0.3856,  0.3733,  0.3644,  0.3566,  0.3454,  0.3309,
2:          0.3097], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.6351, -0.6524, -0.6696, -0.6859, -0.7017, -0.7165, -0.7308, -0.7438,
1:         -0.7559, -0.7679, -0.7788, -0.7892, -0.7995, -0.8088, -0.8178, -0.8261,
1:         -0.8338, -0.8406, -0.8468, -0.8524, -0.8575, -0.8616, -0.8650, -0.8676,
1:         -0.8692], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
0:      first 25 values: tensor([ 0.2326,  0.2293,  0.2270,  0.2226,  0.2148,  0.2025,  0.1857,  0.1679,
0:          0.1489,  0.1332,  0.1176,  0.1042,  0.0886,  0.0707,  0.0495,  0.0260,
0:          0.0037, -0.0153,  0.3856,  0.3733,  0.3644,  0.3566,  0.3454,  0.3309,
0:          0.3097], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.6351, -0.6524, -0.6696, -0.6859, -0.7017, -0.7165, -0.7308, -0.7438,
2:         -0.7559, -0.7679, -0.7788, -0.7892, -0.7995, -0.8088, -0.8178, -0.8261,
2:         -0.8338, -0.8406, -0.8468, -0.8524, -0.8575, -0.8616, -0.8650, -0.8676,
2:         -0.8692], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
3: [DEBUG] TARGET BATCH
3: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0:      first 25 values: tensor([-0.6351, -0.6524, -0.6696, -0.6859, -0.7017, -0.7165, -0.7308, -0.7438,
0:         -0.7559, -0.7679, -0.7788, -0.7892, -0.7995, -0.8088, -0.8178, -0.8261,
0:         -0.8338, -0.8406, -0.8468, -0.8524, -0.8575, -0.8616, -0.8650, -0.8676,
0:         -0.8692], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
2: [DEBUG] TARGET BATCH
2: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] TARGET BATCH
1: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] TARGET BATCH
0: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2306,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2395,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2074,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2240,     nan, -0.2207,     nan,
3:             nan,     nan,     nan,     nan, -0.2295,     nan,     nan,     nan,
3:             nan, -0.2273,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2340,     nan,     nan, -0.2351,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2373,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2406,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2395,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2173,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2229,     nan,     nan,     nan, -0.2063, -0.2340,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2395,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2406,     nan,
3:             nan, -0.2406, -0.2406])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch -1, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2306,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2351,
2:             nan, -0.2351,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2328, -0.2351,     nan,     nan,     nan, -0.2284,     nan,
2:             nan,     nan,     nan,     nan, -0.2373,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2395,     nan,     nan,     nan,     nan,     nan,
2:         -0.2439,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2030,     nan,     nan,     nan,     nan, -0.2196,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2173,     nan, -0.2295, -0.2295,     nan,     nan,
2:         -0.2273,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2384,
2:             nan,     nan,     nan, -0.2406, -0.2417,     nan,     nan,     nan,
2:             nan,     nan, -0.2395,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2406,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1974,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2328,     nan,     nan,     nan,     nan, -0.2207,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2262,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2328, -0.2351,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2395,     nan,     nan, -0.2395,     nan,     nan,     nan,     nan,
1:         -0.2395,     nan,     nan,     nan,     nan, -0.2439,     nan,     nan,
1:             nan, -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2162,     nan,     nan,     nan,     nan,     nan,     nan, -0.2185,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2273,
1:         -0.2273,     nan,     nan,     nan, -0.2284,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2362,
1:             nan, -0.2373,     nan,     nan,     nan,     nan, -0.2373,     nan,
1:             nan, -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2395, -0.2395,     nan,     nan,     nan,     nan,
1:         -0.2406,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2096,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2395,     nan,     nan,     nan,     nan,     nan, -0.2373,     nan,
1:             nan,     nan,     nan, -0.2395,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2406,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch -1, first predictions sample:
2: Epoch -1, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.2328,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2306,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2395,     nan, -0.2284,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2395,     nan, -0.2395,     nan,     nan,     nan,     nan, -0.2439,
0:             nan, -0.2207,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2030,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2295,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2340,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2373,     nan,     nan,     nan,     nan,     nan,
0:         -0.2384,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2406, -0.2118,     nan,     nan,     nan,     nan, -0.2173,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2218,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2328,     nan,
0:         -0.2328,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2384,     nan,     nan, -0.2317,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2362,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2406,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch -1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.0055, -0.0114, -0.0156, -0.0225, -0.0337, -0.0473, -0.0577, -0.0702,
3:         -0.0785, -0.0868, -0.0985, -0.1145, -0.1336, -0.1536, -0.1760, -0.1960,
3:         -0.2159, -0.2341,  0.0730,  0.0662,  0.0593,  0.0517,  0.0397,  0.0270,
3:          0.0138], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.0165,  0.0089,  0.0010, -0.0079, -0.0155, -0.0225, -0.0278, -0.0352,
1:         -0.0416, -0.0505, -0.0616, -0.0798, -0.1007, -0.1205, -0.1429, -0.1615,
1:         -0.1813, -0.1981,  0.0799,  0.0729,  0.0654,  0.0549,  0.0446,  0.0345,
1:          0.0277], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0312,  0.0223,  0.0150,  0.0073, -0.0040, -0.0164, -0.0275, -0.0408,
2:         -0.0524, -0.0650, -0.0813, -0.1051, -0.1298, -0.1530, -0.1784, -0.1986,
2:         -0.2207, -0.2352,  0.0956,  0.0817,  0.0730,  0.0611,  0.0504,  0.0386,
2:          0.0288], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.9446, -0.9628, -0.9853, -1.0142, -1.0478, -1.0847, -1.1249, -1.1655,
3:         -1.2063, -1.2502, -1.2961, -1.3390, -1.3787, -1.4100, -1.4325, -1.4556,
3:         -1.4775, -1.5051, -0.9896, -1.0035, -1.0266, -1.0544, -1.0898, -1.1309,
3:         -1.1730], device='cuda:3', grad_fn=<SliceBackward0>)
0:      first 25 pred values: tensor([-0.0097, -0.0058, -0.0052, -0.0083, -0.0161, -0.0270, -0.0393, -0.0529,
0:         -0.0658, -0.0770, -0.0906, -0.1089, -0.1278, -0.1522, -0.1779, -0.2020,
0:         -0.2230, -0.2365,  0.0707,  0.0664,  0.0631,  0.0565,  0.0474,  0.0354,
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:          0.0247], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.9580, -0.9803, -1.0061, -1.0412, -1.0800, -1.1188, -1.1578, -1.1961,
1:         -1.2320, -1.2689, -1.3099, -1.3506, -1.3891, -1.4220, -1.4467, -1.4638,
1:         -1.4791, -1.4953, -0.9987, -1.0212, -1.0470, -1.0823, -1.1181, -1.1579,
1:         -1.1942], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5754, -0.5820, -0.5810, -0.5726, -0.5678, -0.5578, -0.5562, -0.5505,
3:         -0.5497, -0.5475, -0.5392, -0.5338, -0.5274, -0.5222, -0.5189, -0.5184,
3:         -0.5168, -0.5146, -0.5626, -0.5849, -0.5908, -0.5905, -0.5829, -0.5735,
3:         -0.5651], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.9199, -0.9500, -0.9842, -1.0229, -1.0631, -1.1013, -1.1424, -1.1819,
2:         -1.2195, -1.2572, -1.3001, -1.3392, -1.3756, -1.4091, -1.4385, -1.4674,
2:         -1.4987, -1.5385, -0.9608, -0.9949, -1.0318, -1.0688, -1.1071, -1.1472,
2:         -1.1882], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.4644, 0.5112, 0.5326, 0.5542, 0.5758, 0.5919, 0.6055, 0.6049, 0.6137,
3:         0.6183, 0.5927, 0.5843, 0.5867, 0.5664, 0.5656, 0.6013, 0.6196, 0.6153,
3:         0.3961, 0.4359, 0.4635, 0.4871, 0.5137, 0.5431, 0.5666],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.5880, -0.5881, -0.5871, -0.5785, -0.5762, -0.5712, -0.5743, -0.5763,
1:         -0.5836, -0.5839, -0.5834, -0.5772, -0.5690, -0.5566, -0.5413, -0.5279,
1:         -0.5140, -0.5008, -0.5641, -0.5833, -0.5877, -0.5873, -0.5840, -0.5789,
1:         -0.5781], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9311, -0.9538, -0.9829, -1.0212, -1.0636, -1.1062, -1.1474, -1.1869,
0:         -1.2216, -1.2567, -1.2959, -1.3332, -1.3722, -1.4067, -1.4364, -1.4601,
0:         -1.4839, -1.5114, -0.9802, -1.0014, -1.0317, -1.0698, -1.1115, -1.1558,
0:         -1.1976], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.3984,  0.3624,  0.3282,  0.2938,  0.2565,  0.2168,  0.1763,  0.1384,
3:          0.1057,  0.0746,  0.0428,  0.0078, -0.0300, -0.0722, -0.1125, -0.1521,
3:         -0.1881, -0.2242, -0.2626, -0.3051, -0.3507, -0.3946, -0.4340, -0.4666,
3:         -0.4955], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.6326, -0.6305, -0.6222, -0.6094, -0.6034, -0.5979, -0.5980, -0.5944,
2:         -0.5919, -0.5871, -0.5794, -0.5722, -0.5695, -0.5688, -0.5671, -0.5676,
2:         -0.5653, -0.5610, -0.6054, -0.6210, -0.6198, -0.6153, -0.6063, -0.5972,
2:         -0.5945], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.1568, -0.1592, -0.1661, -0.1661, -0.1696, -0.1723, -0.1743, -0.1694,
3:         -0.1695, -0.1635, -0.1595, -0.1659, -0.1689, -0.1713, -0.1681, -0.1732,
3:         -0.1727, -0.1658, -0.1633, -0.1654, -0.1627, -0.1623, -0.1643, -0.1647,
3:         -0.1681], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 pred values: tensor([0.4452, 0.4987, 0.5352, 0.5429, 0.5584, 0.5822, 0.5907, 0.5975, 0.6118,
1:         0.6013, 0.5703, 0.5647, 0.5760, 0.5607, 0.5442, 0.5741, 0.6195, 0.6369,
1:         0.3859, 0.4386, 0.4860, 0.4993, 0.5262, 0.5669, 0.5756],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5720, -0.5784, -0.5820, -0.5802, -0.5803, -0.5789, -0.5753, -0.5714,
0:         -0.5685, -0.5617, -0.5523, -0.5412, -0.5285, -0.5186, -0.5113, -0.5107,
0:         -0.5130, -0.5166, -0.5682, -0.5902, -0.6000, -0.6045, -0.6034, -0.5963,
0:         -0.5859], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.4735, 0.5357, 0.5688, 0.5804, 0.5992, 0.6130, 0.6169, 0.6190, 0.6400,
2:         0.6438, 0.6108, 0.6058, 0.5967, 0.5543, 0.5421, 0.5777, 0.6331, 0.6727,
2:         0.4344, 0.4863, 0.5154, 0.5206, 0.5440, 0.5750, 0.5810],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
1:      first 25 pred values: tensor([ 0.3651,  0.3321,  0.3047,  0.2789,  0.2506,  0.2177,  0.1806,  0.1428,
1:          0.1058,  0.0689,  0.0310, -0.0091, -0.0501, -0.0924, -0.1318, -0.1680,
1:         -0.2015, -0.2344, -0.2697, -0.3093, -0.3511, -0.3935, -0.4316, -0.4632,
1:         -0.4935], device='cuda:1', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.4348, 0.4616, 0.4770, 0.4909, 0.5143, 0.5378, 0.5451, 0.5422, 0.5552,
0:         0.5545, 0.5348, 0.5410, 0.5499, 0.5381, 0.5334, 0.5508, 0.5820, 0.5945,
0:         0.4007, 0.4199, 0.4431, 0.4520, 0.4779, 0.5192, 0.5322],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.1569, -0.1591, -0.1645, -0.1636, -0.1651, -0.1679, -0.1648, -0.1605,
1:         -0.1589, -0.1627, -0.1596, -0.1652, -0.1677, -0.1693, -0.1624, -0.1663,
1:         -0.1639, -0.1561, -0.1640, -0.1642, -0.1619, -0.1615, -0.1630, -0.1608,
1:         -0.1628], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 pred values: tensor([ 0.3964,  0.3651,  0.3340,  0.3014,  0.2659,  0.2291,  0.1919,  0.1559,
2:          0.1210,  0.0851,  0.0487,  0.0120, -0.0246, -0.0622, -0.1005, -0.1410,
2:         -0.1811, -0.2204, -0.2592, -0.2981, -0.3379, -0.3783, -0.4187, -0.4579,
2:         -0.4970], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.3732,  0.3366,  0.3061,  0.2795,  0.2522,  0.2221,  0.1884,  0.1544,
0:          0.1197,  0.0839,  0.0455,  0.0049, -0.0368, -0.0785, -0.1187, -0.1559,
0:         -0.1910, -0.2241, -0.2569, -0.2913, -0.3286, -0.3681, -0.4079, -0.4468,
0:         -0.4860], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1486, -0.1504, -0.1547, -0.1577, -0.1586, -0.1629, -0.1646, -0.1622,
2:         -0.1625, -0.1573, -0.1530, -0.1595, -0.1611, -0.1644, -0.1616, -0.1651,
2:         -0.1635, -0.1576, -0.1612, -0.1616, -0.1577, -0.1579, -0.1578, -0.1582,
2:         -0.1589], device='cuda:2', grad_fn=<SliceBackward0>)
0:      first 25 pred values: tensor([-0.1458, -0.1498, -0.1525, -0.1524, -0.1544, -0.1588, -0.1565, -0.1541,
0:         -0.1552, -0.1547, -0.1512, -0.1565, -0.1602, -0.1594, -0.1580, -0.1630,
0:         -0.1595, -0.1551, -0.1578, -0.1596, -0.1580, -0.1592, -0.1605, -0.1589,
0:         -0.1630], device='cuda:0', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [1/5 (20%)]	Loss: nan : nan :: 0.16209 (1.53 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [2/5 (40%)]	Loss: nan : nan :: 0.13993 (10.38 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [3/5 (60%)]	Loss: nan : nan :: 0.16787 (10.40 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [4/5 (80%)]	Loss: nan : nan :: 0.14743 (10.39 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_embeds_token_info_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_u_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_v_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_specific_humidity_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_z_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_temperature_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_total_precip_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_u_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_v_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_specific_humidity_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_z_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_temperature_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_total_precip_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_u_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_v_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_specific_humidity_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_z_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_temperature_idqfvmwu56_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_total_precip_idqfvmwu56_epoch-1.mod
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch -1 : nan
0: validation loss for velocity_u : 0.035927772521972656
0: validation loss for velocity_v : 0.061805807054042816
0: validation loss for specific_humidity : 0.025523774325847626
0: validation loss for velocity_z : 0.4894365668296814
0: validation loss for temperature : 0.08267496526241302
0: validation loss for total_precip : nan
0: 0 : 20:54:14 :: batch_size = 96, lr = 1e-05
2: 0 : 20:54:14 :: batch_size = 96, lr = 1e-05
3: 0 : 20:54:14 :: batch_size = 96, lr = 1e-05
1: 0 : 20:54:14 :: batch_size = 96, lr = 1e-05
2: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 0, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-1.1139, -1.1024, -1.1093, -1.1132, -1.0970, -1.0731, -1.0552, -1.0443,
2:         -1.0396, -1.0524, -1.0832, -1.1023, -1.0871, -1.0443, -1.0059, -0.9964,
2:         -1.0045, -1.0173, -1.1518, -1.1452, -1.1405, -1.1270, -1.1026, -1.0804,
2:         -1.0659], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.0679, -0.0771, -0.0870, -0.1096, -0.1298, -0.1618, -0.2266, -0.2822,
2:         -0.2969, -0.2707, -0.2366, -0.2181, -0.2040, -0.1782, -0.1366, -0.0983,
2:         -0.0683, -0.0483, -0.0906, -0.1023, -0.1148, -0.1386, -0.1737, -0.2219,
2:         -0.2679], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2841, -0.1178,  0.0744,  0.2569,  0.4292,  0.5835,  0.7407,  0.8915,
2:          1.1441,  1.2918,  1.4146,  1.5659,  1.6920,  1.6440,  1.3929,  1.1727,
2:          1.0492,  1.0108, -0.2209, -0.0048,  0.1976,  0.3211,  0.4036,  0.5254,
2:          0.7236], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1698, -0.1655, -0.1613, -0.1129,  0.1038,  0.3395,  0.5141,  0.6614,
2:          0.5394,  0.3710,  0.4868,  0.5036,  0.4384,  0.4889,  0.6046,  0.6783,
2:          0.6362,  0.7309, -0.1613, -0.3423, -0.2139, -0.0372,  0.0870,  0.2048,
2:          0.2827], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([ 0.4712,  0.3727,  0.2028, -0.0361, -0.2029, -0.3358, -0.5679, -0.9258,
2:         -1.2944, -1.5201, -1.7225, -1.9302, -2.0749, -1.9415, -1.5039, -1.0952,
2:         -0.8630, -0.7680, -0.6460, -0.4466, -0.2554, -0.1560, -0.1840, -0.2558,
2:         -0.3335], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([ 2.7943,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,  0.0389,     nan,     nan,     nan, -0.2090,     nan,     nan,
2:             nan, -0.1612,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,  0.6370,  0.3586,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.1372,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.0851,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,  2.5268,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1329,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.0981,     nan,
2:         -0.1894,     nan,     nan,     nan, -0.2155,     nan,     nan,     nan,
2:         -0.1590,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1525,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,  0.1868,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,  0.1933,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1590,
2:             nan,     nan,     nan, -0.1546,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2090,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1025,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.0785,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.0720,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.0416,     nan,
2:             nan,  0.1824,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 0, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.3043, -1.3287, -1.3542, -1.3764, -1.3938, -1.4121, -1.4314, -1.4539,
2:         -1.4759, -1.4924, -1.5014, -1.5036, -1.4995, -1.4882, -1.4739, -1.4535,
2:         -1.4317, -1.4079, -1.2999, -1.3271, -1.3564, -1.3785, -1.3954, -1.4099,
2:         -1.4219], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0791, -0.0891, -0.1125, -0.1451, -0.1857, -0.2281, -0.2734, -0.3189,
2:         -0.3624, -0.4066, -0.4444, -0.4745, -0.4957, -0.5037, -0.4954, -0.4765,
2:         -0.4518, -0.4293, -0.0957, -0.1004, -0.1206, -0.1500, -0.1848, -0.2248,
2:         -0.2705], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.1765, 1.3329, 1.4937, 1.6390, 1.7498, 1.8399, 1.9040, 1.9664, 2.0211,
2:         2.0896, 2.1651, 2.2411, 2.2912, 2.3269, 2.3369, 2.3294, 2.3239, 2.3339,
2:         1.1591, 1.3339, 1.5144, 1.6696, 1.7917, 1.8753, 1.9397],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0037, -0.0263, -0.0386,  0.0105,  0.0612,  0.1033,  0.1658,  0.1939,
2:          0.2013,  0.1832,  0.1480,  0.1722,  0.2068,  0.2596,  0.3584,  0.4043,
2:          0.3649,  0.2672,  0.0491, -0.0088, -0.0346,  0.0388,  0.0946,  0.1052,
2:          0.1695], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.4727, -0.6754, -0.9983, -1.3995, -1.8261, -2.2274, -2.5718, -2.8429,
2:         -3.0375, -3.1528, -3.1999, -3.1872, -3.1275, -3.0334, -2.9181, -2.7884,
2:         -2.6566, -2.5220, -2.3884, -2.2610, -2.1423, -2.0229, -1.8870, -1.7156,
2:         -1.5039], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1720, -0.1735, -0.1769, -0.1750, -0.1748, -0.1747, -0.1789, -0.1754,
2:         -0.1777, -0.1766, -0.1718, -0.1736, -0.1744, -0.1749, -0.1733, -0.1777,
2:         -0.1777, -0.1738, -0.1728, -0.1711, -0.1682, -0.1687, -0.1651, -0.1663,
2:         -0.1712], device='cuda:2', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 0, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 0, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 0, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1139, -1.1024, -1.1093, -1.1132, -1.0970, -1.0731, -1.0552, -1.0443,
0:         -1.0396, -1.0524, -1.0832, -1.1023, -1.0871, -1.0443, -1.0059, -0.9964,
0:         -1.0045, -1.0173, -1.1518, -1.1452, -1.1405, -1.1270, -1.1026, -1.0804,
0:         -1.0659], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0679, -0.0771, -0.0870, -0.1096, -0.1298, -0.1618, -0.2266, -0.2822,
0:         -0.2969, -0.2707, -0.2366, -0.2181, -0.2040, -0.1782, -0.1366, -0.0983,
0:         -0.0683, -0.0483, -0.0906, -0.1023, -0.1148, -0.1386, -0.1737, -0.2219,
0:         -0.2679], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2841, -0.1178,  0.0744,  0.2569,  0.4292,  0.5835,  0.7407,  0.8915,
0:          1.1441,  1.2918,  1.4146,  1.5659,  1.6920,  1.6440,  1.3929,  1.1727,
0:          1.0492,  1.0108, -0.2209, -0.0048,  0.1976,  0.3211,  0.4036,  0.5254,
0:          0.7236], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1698, -0.1655, -0.1613, -0.1129,  0.1038,  0.3395,  0.5141,  0.6614,
0:          0.5394,  0.3710,  0.4868,  0.5036,  0.4384,  0.4889,  0.6046,  0.6783,
0:          0.6362,  0.7309, -0.1613, -0.3423, -0.2139, -0.0372,  0.0870,  0.2048,
0:          0.2827], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.4712,  0.3727,  0.2028, -0.0361, -0.2029, -0.3358, -0.5679, -0.9258,
0:         -1.2944, -1.5201, -1.7225, -1.9302, -2.0749, -1.9415, -1.5039, -1.0952,
0:         -0.8630, -0.7680, -0.6460, -0.4466, -0.2554, -0.1560, -0.1840, -0.2558,
0:         -0.3335], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.1677,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.3586,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2155,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2047,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2177,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1894,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1242,
0:         -0.1590, -0.1655,     nan,  0.3695,     nan,     nan,     nan, -0.2003,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2155,  0.0607,     nan, -0.2047,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0981,     nan,
0:             nan, -0.1960,     nan,     nan, -0.2155,     nan,     nan, -0.1394,
0:         -0.1590,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1894,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1111,     nan,     nan,     nan,     nan,     nan,
0:         -0.1481,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1938,     nan, -0.1590, -0.1394,     nan,     nan,     nan, -0.1590,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2068,     nan,     nan, -0.1742,     nan,     nan,     nan,  0.1324,
0:         -0.0611,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1981, -0.2090,     nan,     nan,     nan,
0:             nan, -0.0546,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1198,
0:             nan,     nan,  0.0694,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0959])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 0, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.3521, -1.3659, -1.3806, -1.3942, -1.4065, -1.4222, -1.4409, -1.4631,
0:         -1.4846, -1.4992, -1.5063, -1.5089, -1.5065, -1.4950, -1.4815, -1.4625,
0:         -1.4414, -1.4187, -1.3294, -1.3489, -1.3709, -1.3866, -1.4016, -1.4173,
0:         -1.4308], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0841, -0.1057, -0.1395, -0.1792, -0.2216, -0.2606, -0.3019, -0.3433,
0:         -0.3836, -0.4234, -0.4558, -0.4809, -0.4977, -0.5042, -0.4988, -0.4839,
0:         -0.4611, -0.4362, -0.0988, -0.1189, -0.1505, -0.1835, -0.2180, -0.2547,
0:         -0.2966], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0230, 1.1686, 1.3116, 1.4422, 1.5434, 1.6291, 1.7081, 1.7847, 1.8689,
0:         1.9559, 2.0440, 2.1284, 2.1937, 2.2366, 2.2544, 2.2632, 2.2663, 2.2727,
0:         0.9880, 1.1635, 1.3370, 1.4774, 1.5968, 1.6869, 1.7644],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0264, -0.0284, -0.0455,  0.0118,  0.0838,  0.1398,  0.2324,  0.2649,
0:          0.2573,  0.2424,  0.2107,  0.2551,  0.2891,  0.3071,  0.3878,  0.4282,
0:          0.4039,  0.3429,  0.0010, -0.0610, -0.0965, -0.0095,  0.0722,  0.0985,
0:          0.2049], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4740, -0.7281, -1.0974, -1.5289, -1.9653, -2.3575, -2.6800, -2.9342,
0:         -3.1279, -3.2625, -3.3374, -3.3476, -3.2936, -3.1894, -3.0498, -2.8966,
0:         -2.7364, -2.5803, -2.4315, -2.2981, -2.1850, -2.0727, -1.9403, -1.7659,
0:         -1.5394], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1665, -0.1705, -0.1758, -0.1763, -0.1763, -0.1797, -0.1815, -0.1767,
0:         -0.1757, -0.1701, -0.1670, -0.1745, -0.1759, -0.1783, -0.1767, -0.1813,
0:         -0.1791, -0.1726, -0.1666, -0.1689, -0.1678, -0.1704, -0.1728, -0.1712,
0:         -0.1744], device='cuda:0', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([-1.1139, -1.1024, -1.1093, -1.1132, -1.0970, -1.0731, -1.0552, -1.0443,
3:         -1.0396, -1.0524, -1.0832, -1.1023, -1.0871, -1.0443, -1.0059, -0.9964,
3:         -1.0045, -1.0173, -1.1518, -1.1452, -1.1405, -1.1270, -1.1026, -1.0804,
3:         -1.0659], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.0679, -0.0771, -0.0870, -0.1096, -0.1298, -0.1618, -0.2266, -0.2822,
3:         -0.2969, -0.2707, -0.2366, -0.2181, -0.2040, -0.1782, -0.1366, -0.0983,
3:         -0.0683, -0.0483, -0.0906, -0.1023, -0.1148, -0.1386, -0.1737, -0.2219,
3:         -0.2679], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2841, -0.1178,  0.0744,  0.2569,  0.4292,  0.5835,  0.7407,  0.8915,
3:          1.1441,  1.2918,  1.4146,  1.5659,  1.6920,  1.6440,  1.3929,  1.1727,
3:          1.0492,  1.0108, -0.2209, -0.0048,  0.1976,  0.3211,  0.4036,  0.5254,
3:          0.7236], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1698, -0.1655, -0.1613, -0.1129,  0.1038,  0.3395,  0.5141,  0.6614,
3:          0.5394,  0.3710,  0.4868,  0.5036,  0.4384,  0.4889,  0.6046,  0.6783,
3:          0.6362,  0.7309, -0.1613, -0.3423, -0.2139, -0.0372,  0.0870,  0.2048,
3:          0.2827], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([ 0.4712,  0.3727,  0.2028, -0.0361, -0.2029, -0.3358, -0.5679, -0.9258,
3:         -1.2944, -1.5201, -1.7225, -1.9302, -2.0749, -1.9415, -1.5039, -1.0952,
3:         -0.8630, -0.7680, -0.6460, -0.4466, -0.2554, -0.1560, -0.1840, -0.2558,
3:         -0.3335], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,  4.1949,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,  0.1998,     nan,     nan,     nan, -0.2003,     nan,
3:             nan, -0.1612,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1829,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.1503,     nan,     nan, -0.1503,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,  0.0106,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2351,     nan,     nan,
3:             nan, -0.1677,     nan, -0.1786,     nan,     nan,     nan,     nan,
3:         -0.1459,     nan, -0.1394,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  0.1933,     nan,     nan,     nan,
3:             nan, -0.1786,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2068,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1324,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2090,     nan,     nan,     nan,
3:             nan, -0.0546,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.0416,     nan,
3:             nan,     nan, -0.0959])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 0, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.3430, -1.3597, -1.3786, -1.3963, -1.4114, -1.4290, -1.4445, -1.4654,
3:         -1.4850, -1.4999, -1.5071, -1.5071, -1.5007, -1.4867, -1.4701, -1.4476,
3:         -1.4205, -1.3897, -1.3297, -1.3469, -1.3718, -1.3897, -1.4065, -1.4218,
3:         -1.4346], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.0882, -0.1121, -0.1461, -0.1850, -0.2248, -0.2647, -0.3080, -0.3522,
3:         -0.3951, -0.4350, -0.4652, -0.4869, -0.4998, -0.5047, -0.5008, -0.4863,
3:         -0.4650, -0.4426, -0.0899, -0.1080, -0.1389, -0.1728, -0.2055, -0.2420,
3:         -0.2847], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.1009, 1.2751, 1.4429, 1.5843, 1.6934, 1.7743, 1.8429, 1.9029, 1.9600,
3:         2.0341, 2.1145, 2.1987, 2.2769, 2.3437, 2.3924, 2.4180, 2.4354, 2.4512,
3:         1.0829, 1.2889, 1.4769, 1.6323, 1.7475, 1.8289, 1.8892],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0412,  0.0602,  0.0356,  0.0460,  0.0673,  0.0837,  0.1511,  0.1835,
3:          0.2169,  0.2397,  0.2018,  0.2364,  0.2845,  0.3178,  0.3879,  0.4168,
3:          0.3978,  0.3233,  0.0325, -0.0159, -0.0427,  0.0318,  0.0847,  0.0706,
3:          0.1468], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.3593, -0.6203, -0.9936, -1.4299, -1.8727, -2.2772, -2.6165, -2.8836,
3:         -3.0796, -3.2077, -3.2762, -3.2856, -3.2425, -3.1575, -3.0385, -2.8996,
3:         -2.7550, -2.6112, -2.4746, -2.3501, -2.2355, -2.1152, -1.9699, -1.7817,
3:         -1.5470], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1639, -0.1696, -0.1767, -0.1756, -0.1772, -0.1781, -0.1799, -0.1746,
3:         -0.1725, -0.1693, -0.1671, -0.1750, -0.1793, -0.1808, -0.1768, -0.1831,
3:         -0.1802, -0.1736, -0.1674, -0.1722, -0.1705, -0.1717, -0.1750, -0.1768,
3:         -0.1774], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-1.1139, -1.1024, -1.1093, -1.1132, -1.0970, -1.0731, -1.0552, -1.0443,
1:         -1.0396, -1.0524, -1.0832, -1.1023, -1.0871, -1.0443, -1.0059, -0.9964,
1:         -1.0045, -1.0173, -1.1518, -1.1452, -1.1405, -1.1270, -1.1026, -1.0804,
1:         -1.0659], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0679, -0.0771, -0.0870, -0.1096, -0.1298, -0.1618, -0.2266, -0.2822,
1:         -0.2969, -0.2707, -0.2366, -0.2181, -0.2040, -0.1782, -0.1366, -0.0983,
1:         -0.0683, -0.0483, -0.0906, -0.1023, -0.1148, -0.1386, -0.1737, -0.2219,
1:         -0.2679], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2841, -0.1178,  0.0744,  0.2569,  0.4292,  0.5835,  0.7407,  0.8915,
1:          1.1441,  1.2918,  1.4146,  1.5659,  1.6920,  1.6440,  1.3929,  1.1727,
1:          1.0492,  1.0108, -0.2209, -0.0048,  0.1976,  0.3211,  0.4036,  0.5254,
1:          0.7236], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1698, -0.1655, -0.1613, -0.1129,  0.1038,  0.3395,  0.5141,  0.6614,
1:          0.5394,  0.3710,  0.4868,  0.5036,  0.4384,  0.4889,  0.6046,  0.6783,
1:          0.6362,  0.7309, -0.1613, -0.3423, -0.2139, -0.0372,  0.0870,  0.2048,
1:          0.2827], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([ 0.4712,  0.3727,  0.2028, -0.0361, -0.2029, -0.3358, -0.5679, -0.9258,
1:         -1.2944, -1.5201, -1.7225, -1.9302, -2.0749, -1.9415, -1.5039, -1.0952,
1:         -0.8630, -0.7680, -0.6460, -0.4466, -0.2554, -0.1560, -0.1840, -0.2558,
1:         -0.3335], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,  2.9553,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,  0.2172,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2003,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,  0.2912,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.1742,     nan,     nan,     nan,     nan,
1:         -0.1503,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,  0.1194,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1894, -0.1786,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,  0.1150, -0.0307,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,  0.2129,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2068,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1025,     nan,     nan,     nan,     nan,     nan,     nan,
1:          0.1542,     nan,     nan,     nan,     nan, -0.2155,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.0959, -0.0329,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.0416,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 0, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.3809, -1.3985, -1.4182, -1.4369, -1.4530, -1.4700, -1.4824, -1.4994,
1:         -1.5154, -1.5218, -1.5221, -1.5190, -1.5100, -1.4963, -1.4817, -1.4582,
1:         -1.4303, -1.3981, -1.3646, -1.3879, -1.4085, -1.4268, -1.4451, -1.4608,
1:         -1.4707], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.0996, -0.1160, -0.1434, -0.1737, -0.2103, -0.2496, -0.2950, -0.3459,
1:         -0.3962, -0.4422, -0.4777, -0.5038, -0.5190, -0.5224, -0.5163, -0.4967,
1:         -0.4717, -0.4443, -0.1116, -0.1255, -0.1485, -0.1737, -0.2004, -0.2365,
1:         -0.2829], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.1923, 1.3007, 1.4169, 1.5297, 1.6225, 1.7079, 1.7873, 1.8732, 1.9573,
1:         2.0525, 2.1505, 2.2420, 2.3174, 2.3739, 2.4009, 2.4065, 2.4027, 2.3963,
1:         1.1665, 1.3057, 1.4580, 1.5910, 1.7008, 1.7874, 1.8624],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.0574,  0.0676,  0.0228,  0.0516,  0.0903,  0.1289,  0.2292,  0.2511,
1:          0.2499,  0.2492,  0.1961,  0.2274,  0.2707,  0.3117,  0.4045,  0.4516,
1:          0.4580,  0.3948,  0.0574,  0.0266, -0.0280,  0.0461,  0.0936,  0.0774,
1:          0.1857], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.4620, -0.7236, -1.1015, -1.5362, -1.9710, -2.3520, -2.6583, -2.8885,
1:         -3.0520, -3.1574, -3.2110, -3.2107, -3.1600, -3.0721, -2.9545, -2.8291,
1:         -2.7023, -2.5777, -2.4535, -2.3304, -2.2074, -2.0705, -1.9100, -1.7121,
1:         -1.4691], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1655, -0.1688, -0.1731, -0.1740, -0.1745, -0.1767, -0.1796, -0.1749,
1:         -0.1751, -0.1718, -0.1700, -0.1711, -0.1758, -0.1756, -0.1745, -0.1790,
1:         -0.1768, -0.1705, -0.1666, -0.1704, -0.1695, -0.1704, -0.1714, -0.1693,
1:         -0.1717], device='cuda:1', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [1/5 (20%)]	Loss: nan : nan :: 0.14308 (1.60 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [2/5 (40%)]	Loss: nan : nan :: 0.13900 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [3/5 (60%)]	Loss: nan : nan :: 0.13427 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [4/5 (80%)]	Loss: nan : nan :: 0.13718 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_embeds_token_info_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_u_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_v_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_specific_humidity_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_z_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_temperature_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_total_precip_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_u_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_v_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_specific_humidity_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_z_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_temperature_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_total_precip_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_u_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_v_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_specific_humidity_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_z_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_temperature_idqfvmwu56_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_total_precip_idqfvmwu56_epoch0.mod
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 0 : nan
0: validation loss for velocity_u : 0.029855452477931976
0: validation loss for velocity_v : 0.056524842977523804
0: validation loss for specific_humidity : 0.026788605377078056
0: validation loss for velocity_z : 0.4325673282146454
0: validation loss for temperature : 0.07820216566324234
0: validation loss for total_precip : nan
3: 1 : 21:01:03 :: batch_size = 96, lr = 1.5000000000000002e-05
1: 1 : 21:01:03 :: batch_size = 96, lr = 1.5000000000000002e-05
0: 1 : 21:01:03 :: batch_size = 96, lr = 1.5000000000000002e-05
2: 1 : 21:01:03 :: batch_size = 96, lr = 1.5000000000000002e-05
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 1, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.6799, 1.6983, 1.7191, 1.7428, 1.7684, 1.7937, 1.8174, 1.8382, 1.8553,
3:         1.8685, 1.8781, 1.8856, 1.8929, 1.9027, 1.9162, 1.9341, 1.9563, 1.9813,
3:         1.5977, 1.6137, 1.6320, 1.6515, 1.6718, 1.6929, 1.7139],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-1.2191, -1.2385, -1.2558, -1.2702, -1.2821, -1.2915, -1.2984, -1.3012,
3:         -1.2984, -1.2894, -1.2747, -1.2547, -1.2297, -1.2012, -1.1717, -1.1431,
3:         -1.1172, -1.0965, -1.1465, -1.1719, -1.1972, -1.2209, -1.2426, -1.2616,
3:         -1.2777], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4574, -0.4638, -0.4713, -0.4786, -0.4861, -0.4956, -0.5047, -0.5149,
3:         -0.5246, -0.5345, -0.5435, -0.5519, -0.5587, -0.5641, -0.5681, -0.5695,
3:         -0.5693, -0.5638, -0.4780, -0.4821, -0.4868, -0.4918, -0.4974, -0.5036,
3:         -0.5107], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.2434,  0.1869,  0.1779,  0.2548,  0.4051,  0.5791,  0.7294,  0.8266,
3:          0.8549,  0.8187,  0.7430,  0.6379,  0.4797,  0.2807,  0.0954, -0.0561,
3:         -0.1770, -0.2414,  0.0841,  0.1779,  0.3497,  0.5769,  0.8266,  1.0459,
3:          1.1996], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-1.0244, -1.0525, -1.0812, -1.1115, -1.1429, -1.1719, -1.1975, -1.2240,
3:         -1.2521, -1.2811, -1.3121, -1.3453, -1.3797, -1.4148, -1.4514, -1.4879,
3:         -1.5229, -1.5567, -1.5891, -1.6198, -1.6487, -1.6754, -1.6988, -1.7188,
3:         -1.7378], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2421,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2233,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2433,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.1420,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  0.0853,     nan, -0.0313,     nan, -0.0584,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2315,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2374,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.0737, -0.0242,     nan,     nan,     nan, -0.2480,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2398,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2445, -0.2445,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2080,     nan,     nan,     nan, -0.2186,     nan,     nan,     nan,
3:             nan,     nan, -0.2256,     nan,     nan,     nan,     nan,  0.1359,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 1, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.7653, 0.7980, 0.8363, 0.8785, 0.9261, 0.9763, 1.0321, 1.0863, 1.1435,
3:         1.1994, 1.2521, 1.3060, 1.3566, 1.4095, 1.4579, 1.4980, 1.5266, 1.5380,
3:         0.7062, 0.7466, 0.7861, 0.8277, 0.8711, 0.9183, 0.9676],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.9476, -0.9555, -0.9684, -0.9840, -1.0044, -1.0171, -1.0265, -1.0247,
3:         -1.0142, -0.9962, -0.9764, -0.9562, -0.9346, -0.9079, -0.8774, -0.8518,
3:         -0.8342, -0.8407, -0.9154, -0.9225, -0.9309, -0.9423, -0.9582, -0.9717,
3:         -0.9799], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.0598, -0.0752, -0.1137, -0.1694, -0.2438, -0.3208, -0.3997, -0.4734,
3:         -0.5334, -0.5821, -0.6174, -0.6407, -0.6576, -0.6676, -0.6729, -0.6740,
3:         -0.6698, -0.6617, -0.0994, -0.1200, -0.1621, -0.2232, -0.2972, -0.3782,
3:         -0.4565], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.1205, -0.1089, -0.1058, -0.1001, -0.0843, -0.0815, -0.1104, -0.1506,
3:         -0.1826, -0.2024, -0.1948, -0.1354, -0.0745, -0.0317,  0.0361,  0.1153,
3:          0.2014,  0.2839, -0.0917, -0.0855, -0.0588, -0.0287, -0.0211, -0.0367,
3:         -0.0798], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([0.9535, 0.9537, 0.9597, 0.9609, 0.9561, 0.9399, 0.9150, 0.8851, 0.8526,
3:         0.8201, 0.7873, 0.7492, 0.7088, 0.6638, 0.6136, 0.5635, 0.5124, 0.4619,
3:         0.4147, 0.3707, 0.3305, 0.2955, 0.2676, 0.2470, 0.2298],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1587, -0.1607, -0.1637, -0.1663, -0.1672, -0.1722, -0.1764, -0.1726,
3:         -0.1741, -0.1660, -0.1619, -0.1659, -0.1690, -0.1707, -0.1705, -0.1773,
3:         -0.1760, -0.1730, -0.1688, -0.1688, -0.1668, -0.1645, -0.1675, -0.1672,
3:         -0.1713], device='cuda:3', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 1, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 1, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.6799, 1.6983, 1.7191, 1.7428, 1.7684, 1.7937, 1.8174, 1.8382, 1.8553,
1:         1.8685, 1.8781, 1.8856, 1.8929, 1.9027, 1.9162, 1.9341, 1.9563, 1.9813,
1:         1.5977, 1.6137, 1.6320, 1.6515, 1.6718, 1.6929, 1.7139],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-1.2191, -1.2385, -1.2558, -1.2702, -1.2821, -1.2915, -1.2984, -1.3012,
1:         -1.2984, -1.2894, -1.2747, -1.2547, -1.2297, -1.2012, -1.1717, -1.1431,
1:         -1.1172, -1.0965, -1.1465, -1.1719, -1.1972, -1.2209, -1.2426, -1.2616,
1:         -1.2777], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4574, -0.4638, -0.4713, -0.4786, -0.4861, -0.4956, -0.5047, -0.5149,
1:         -0.5246, -0.5345, -0.5435, -0.5519, -0.5587, -0.5641, -0.5681, -0.5695,
1:         -0.5693, -0.5638, -0.4780, -0.4821, -0.4868, -0.4918, -0.4974, -0.5036,
1:         -0.5107], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.2434,  0.1869,  0.1779,  0.2548,  0.4051,  0.5791,  0.7294,  0.8266,
1:          0.8549,  0.8187,  0.7430,  0.6379,  0.4797,  0.2807,  0.0954, -0.0561,
1:         -0.1770, -0.2414,  0.0841,  0.1779,  0.3497,  0.5769,  0.8266,  1.0459,
1:          1.1996], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-1.0244, -1.0525, -1.0812, -1.1115, -1.1429, -1.1719, -1.1975, -1.2240,
1:         -1.2521, -1.2811, -1.3121, -1.3453, -1.3797, -1.4148, -1.4514, -1.4879,
1:         -1.5229, -1.5567, -1.5891, -1.6198, -1.6487, -1.6754, -1.6988, -1.7188,
1:         -1.7378], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
2:      first 25 values: tensor([1.6799, 1.6983, 1.7191, 1.7428, 1.7684, 1.7937, 1.8174, 1.8382, 1.8553,
2:         1.8685, 1.8781, 1.8856, 1.8929, 1.9027, 1.9162, 1.9341, 1.9563, 1.9813,
2:         1.5977, 1.6137, 1.6320, 1.6515, 1.6718, 1.6929, 1.7139],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] TARGET BATCH
1: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2:      first 25 values: tensor([-1.2191, -1.2385, -1.2558, -1.2702, -1.2821, -1.2915, -1.2984, -1.3012,
2:         -1.2984, -1.2894, -1.2747, -1.2547, -1.2297, -1.2012, -1.1717, -1.1431,
2:         -1.1172, -1.0965, -1.1465, -1.1719, -1.1972, -1.2209, -1.2426, -1.2616,
2:         -1.2777], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] First 243 batch values:
1: tensor([-0.2480, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2280,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2433,     nan,     nan,
1:         -0.2351,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1420,     nan, -0.1868,     nan,     nan,     nan,     nan,     nan,
1:          0.0818,     nan,     nan, -0.0313,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2445,     nan,
1:         -0.2445,     nan,     nan,     nan, -0.2457,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2374,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2256,     nan, -0.1856,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.1856,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2480, -0.2480,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2480, -0.2480,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2480,
1:             nan,     nan, -0.2445,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2327,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.1726,     nan,     nan,     nan,     nan,
1:          0.1501,     nan,  0.3456])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 1, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([-0.4574, -0.4638, -0.4713, -0.4786, -0.4861, -0.4956, -0.5047, -0.5149,
2:         -0.5246, -0.5345, -0.5435, -0.5519, -0.5587, -0.5641, -0.5681, -0.5695,
2:         -0.5693, -0.5638, -0.4780, -0.4821, -0.4868, -0.4918, -0.4974, -0.5036,
2:         -0.5107], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 pred values: tensor([0.7405, 0.7767, 0.8179, 0.8642, 0.9151, 0.9697, 1.0324, 1.0900, 1.1525,
1:         1.2076, 1.2612, 1.3103, 1.3589, 1.4095, 1.4563, 1.4933, 1.5205, 1.5315,
1:         0.6779, 0.7232, 0.7655, 0.8120, 0.8600, 0.9129, 0.9707],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([ 0.2434,  0.1869,  0.1779,  0.2548,  0.4051,  0.5791,  0.7294,  0.8266,
2:          0.8549,  0.8187,  0.7430,  0.6379,  0.4797,  0.2807,  0.0954, -0.0561,
2:         -0.1770, -0.2414,  0.0841,  0.1779,  0.3497,  0.5769,  0.8266,  1.0459,
2:          1.1996], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-1.0244, -1.0525, -1.0812, -1.1115, -1.1429, -1.1719, -1.1975, -1.2240,
2:         -1.2521, -1.2811, -1.3121, -1.3453, -1.3797, -1.4148, -1.4514, -1.4879,
2:         -1.5229, -1.5567, -1.5891, -1.6198, -1.6487, -1.6754, -1.6988, -1.7188,
2:         -1.7378], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 pred values: tensor([-0.9833, -1.0004, -1.0178, -1.0371, -1.0565, -1.0663, -1.0745, -1.0722,
1:         -1.0607, -1.0437, -1.0243, -1.0017, -0.9802, -0.9550, -0.9291, -0.9040,
1:         -0.8891, -0.8931, -0.9465, -0.9578, -0.9714, -0.9827, -0.9940, -1.0053,
1:         -1.0126], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
1:      first 25 pred values: tensor([-0.0588, -0.0817, -0.1278, -0.1904, -0.2664, -0.3455, -0.4249, -0.4982,
1:         -0.5608, -0.6127, -0.6512, -0.6768, -0.6892, -0.6911, -0.6870, -0.6812,
1:         -0.6738, -0.6676, -0.0893, -0.1183, -0.1692, -0.2352, -0.3112, -0.3927,
1:         -0.4686], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2: [DEBUG] TARGET BATCH
2: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1860, -0.2215, -0.2430, -0.2333, -0.1752, -0.1480, -0.1599, -0.1465,
1:         -0.1250, -0.1119, -0.0937, -0.0347,  0.0139,  0.0459,  0.1024,  0.1865,
1:          0.3273,  0.4473, -0.1488, -0.1854, -0.1837, -0.1821, -0.1455, -0.1361,
1:         -0.1481], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2: [DEBUG] First 243 batch values:
2: tensor([    nan, -0.2480,     nan,     nan,     nan,     nan, -0.2480,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2421,
2:             nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2268,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2303,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2268,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1679,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,
2:         -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,
2:         -0.2445,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2292, -0.2303, -0.2351,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2398,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2044,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,  0.0217,     nan,     nan,
2:         -0.0737, -0.0242,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2433,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1750,
2:             nan,     nan,     nan,     nan,     nan, -0.0042,     nan,     nan,
2:             nan,  0.2961,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 1, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.9188, 0.9171, 0.9189, 0.9180, 0.9159, 0.9085, 0.8931, 0.8700, 0.8403,
1:         0.8089, 0.7774, 0.7429, 0.7052, 0.6591, 0.6052, 0.5494, 0.4935, 0.4398,
1:         0.3888, 0.3378, 0.2871, 0.2396, 0.1994, 0.1697, 0.1501],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([0.7275, 0.7604, 0.8002, 0.8454, 0.8970, 0.9536, 1.0168, 1.0777, 1.1366,
2:         1.1953, 1.2525, 1.3109, 1.3662, 1.4173, 1.4635, 1.4950, 1.5148, 1.5190,
2:         0.6729, 0.7100, 0.7497, 0.7927, 0.8419, 0.8968, 0.9572],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1483, -0.1537, -0.1581, -0.1557, -0.1579, -0.1585, -0.1596, -0.1473,
1:         -0.1523, -0.1510, -0.1498, -0.1542, -0.1565, -0.1591, -0.1558, -0.1553,
1:         -0.1520, -0.1439, -0.1493, -0.1482, -0.1480, -0.1471, -0.1485, -0.1467,
1:         -0.1504], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 pred values: tensor([-1.0184, -1.0428, -1.0620, -1.0805, -1.0949, -1.1031, -1.1079, -1.1004,
2:         -1.0821, -1.0558, -1.0250, -0.9921, -0.9629, -0.9320, -0.9017, -0.8734,
2:         -0.8504, -0.8465, -0.9647, -0.9796, -0.9929, -1.0010, -1.0087, -1.0174,
2:         -1.0248], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0917, -0.1164, -0.1584, -0.2119, -0.2776, -0.3448, -0.4132, -0.4809,
2:         -0.5388, -0.5897, -0.6278, -0.6534, -0.6705, -0.6781, -0.6815, -0.6828,
2:         -0.6790, -0.6727, -0.1256, -0.1529, -0.1967, -0.2513, -0.3165, -0.3871,
2:         -0.4557], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1044, -0.0925, -0.0835, -0.0756, -0.0548, -0.0453, -0.0597, -0.0709,
2:         -0.0882, -0.0966, -0.0912, -0.0573, -0.0412, -0.0289,  0.0363,  0.1336,
2:          0.2479,  0.3146, -0.0510, -0.0438, -0.0183, -0.0147, -0.0098, -0.0306,
2:         -0.0602], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([0.9356, 0.9230, 0.9197, 0.9206, 0.9235, 0.9210, 0.9091, 0.8872, 0.8561,
2:         0.8192, 0.7791, 0.7361, 0.6927, 0.6479, 0.6032, 0.5587, 0.5121, 0.4627,
2:         0.4118, 0.3583, 0.3058, 0.2585, 0.2214, 0.1954, 0.1761],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1587, -0.1623, -0.1689, -0.1684, -0.1728, -0.1771, -0.1787, -0.1749,
2:         -0.1754, -0.1606, -0.1584, -0.1646, -0.1680, -0.1731, -0.1732, -0.1780,
2:         -0.1756, -0.1704, -0.1581, -0.1587, -0.1604, -0.1627, -0.1649, -0.1673,
2:         -0.1693], device='cuda:2', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.6799, 1.6983, 1.7191, 1.7428, 1.7684, 1.7937, 1.8174, 1.8382, 1.8553,
0:         1.8685, 1.8781, 1.8856, 1.8929, 1.9027, 1.9162, 1.9341, 1.9563, 1.9813,
0:         1.5977, 1.6137, 1.6320, 1.6515, 1.6718, 1.6929, 1.7139],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2191, -1.2385, -1.2558, -1.2702, -1.2821, -1.2915, -1.2984, -1.3012,
0:         -1.2984, -1.2894, -1.2747, -1.2547, -1.2297, -1.2012, -1.1717, -1.1431,
0:         -1.1172, -1.0965, -1.1465, -1.1719, -1.1972, -1.2209, -1.2426, -1.2616,
0:         -1.2777], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4574, -0.4638, -0.4713, -0.4786, -0.4861, -0.4956, -0.5047, -0.5149,
0:         -0.5246, -0.5345, -0.5435, -0.5519, -0.5587, -0.5641, -0.5681, -0.5695,
0:         -0.5693, -0.5638, -0.4780, -0.4821, -0.4868, -0.4918, -0.4974, -0.5036,
0:         -0.5107], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2434,  0.1869,  0.1779,  0.2548,  0.4051,  0.5791,  0.7294,  0.8266,
0:          0.8549,  0.8187,  0.7430,  0.6379,  0.4797,  0.2807,  0.0954, -0.0561,
0:         -0.1770, -0.2414,  0.0841,  0.1779,  0.3497,  0.5769,  0.8266,  1.0459,
0:          1.1996], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.0244, -1.0525, -1.0812, -1.1115, -1.1429, -1.1719, -1.1975, -1.2240,
0:         -1.2521, -1.2811, -1.3121, -1.3453, -1.3797, -1.4148, -1.4514, -1.4879,
0:         -1.5229, -1.5567, -1.5891, -1.6198, -1.6487, -1.6754, -1.6988, -1.7188,
0:         -1.7378], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2480,     nan,     nan,
0:             nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2280,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2245,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2327,     nan,     nan,     nan, -0.2327,
0:             nan,     nan, -0.2268,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1514,     nan,     nan,     nan, -0.1679,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2480, -0.2480,     nan,
0:         -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2468,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2280,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2480,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2480,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2398,     nan, -0.2398,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2433,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1962,     nan,
0:             nan,     nan,     nan, -0.1726,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.3456])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7597, 0.7925, 0.8278, 0.8655, 0.9102, 0.9592, 1.0138, 1.0667, 1.1243,
0:         1.1795, 1.2345, 1.2910, 1.3438, 1.3975, 1.4479, 1.4854, 1.5116, 1.5213,
0:         0.7159, 0.7563, 0.7921, 0.8276, 0.8685, 0.9152, 0.9672],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0390, -1.0524, -1.0626, -1.0715, -1.0804, -1.0836, -1.0813, -1.0675,
0:         -1.0438, -1.0123, -0.9825, -0.9530, -0.9289, -0.9061, -0.8836, -0.8641,
0:         -0.8493, -0.8493, -0.9920, -1.0033, -1.0144, -1.0184, -1.0253, -1.0297,
0:         -1.0306], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0500, -0.0735, -0.1184, -0.1805, -0.2582, -0.3369, -0.4165, -0.4888,
0:         -0.5493, -0.5988, -0.6354, -0.6583, -0.6700, -0.6725, -0.6703, -0.6647,
0:         -0.6575, -0.6490, -0.0943, -0.1241, -0.1748, -0.2404, -0.3159, -0.3939,
0:         -0.4685], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1757, -0.1431, -0.1370, -0.1537, -0.1436, -0.1006, -0.0837, -0.0778,
0:         -0.0995, -0.1428, -0.1501, -0.1133, -0.0866, -0.0786, -0.0293,  0.0688,
0:          0.1785,  0.2770, -0.0869, -0.0754, -0.0462, -0.0333, -0.0160, -0.0008,
0:         -0.0181], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.9694, 0.9542, 0.9463, 0.9406, 0.9313, 0.9151, 0.8887, 0.8557, 0.8207,
0:         0.7868, 0.7544, 0.7224, 0.6898, 0.6520, 0.6090, 0.5620, 0.5122, 0.4618,
0:         0.4135, 0.3649, 0.3195, 0.2769, 0.2419, 0.2185, 0.1998],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1697, -0.1733, -0.1797, -0.1793, -0.1825, -0.1841, -0.1856, -0.1815,
0:         -0.1809, -0.1737, -0.1727, -0.1780, -0.1798, -0.1819, -0.1799, -0.1869,
0:         -0.1839, -0.1805, -0.1725, -0.1736, -0.1724, -0.1766, -0.1776, -0.1766,
0:         -0.1802], device='cuda:0', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [1/5 (20%)]	Loss: nan : nan :: 0.14560 (1.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [2/5 (40%)]	Loss: nan : nan :: 0.14682 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [3/5 (60%)]	Loss: nan : nan :: 0.13573 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [4/5 (80%)]	Loss: nan : nan :: 0.14701 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_embeds_token_info_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_u_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_v_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_specific_humidity_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_velocity_z_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_temperature_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_encoder_total_precip_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_u_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_v_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_specific_humidity_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_velocity_z_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_temperature_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_decoder_total_precip_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_u_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_v_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_specific_humidity_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_velocity_z_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_temperature_idqfvmwu56_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqfvmwu56/AtmoRep_tail_total_precip_idqfvmwu56_epoch1.mod
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
