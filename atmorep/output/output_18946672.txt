0: Wandb run: atmorep-8qppxgks-18946672
0: l50075:2962787:2962787 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.156<0>
0: l50075:2962787:2962787 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50075:2962787:2962787 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50075:2962787:2962787 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50075:2962787:2962787 [0] NCCL INFO cudaDriverVersion 12060
0: NCCL version 2.21.5+cuda12.4
1: l50075:2962788:2962788 [1] NCCL INFO cudaDriverVersion 12060
1: l50075:2962788:2962788 [1] NCCL INFO Bootstrap : Using ib0:10.128.11.156<0>
1: l50075:2962788:2962788 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
1: l50075:2962788:2962788 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
1: l50075:2962788:2962788 [1] NCCL INFO NET/Plugin: Using internal network plugin.
3: l50075:2962790:2962790 [3] NCCL INFO cudaDriverVersion 12060
2: l50075:2962789:2962789 [2] NCCL INFO cudaDriverVersion 12060
2: l50075:2962789:2962789 [2] NCCL INFO Bootstrap : Using ib0:10.128.11.156<0>
3: l50075:2962790:2962790 [3] NCCL INFO Bootstrap : Using ib0:10.128.11.156<0>
2: l50075:2962789:2962789 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
2: l50075:2962789:2962789 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
2: l50075:2962789:2962789 [2] NCCL INFO NET/Plugin: Using internal network plugin.
3: l50075:2962790:2962790 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
3: l50075:2962790:2962790 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
3: l50075:2962790:2962790 [3] NCCL INFO NET/Plugin: Using internal network plugin.
1: l50075:2962788:2963314 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.156<0>
1: l50075:2962788:2963314 [1] NCCL INFO Using non-device net plugin version 0
1: l50075:2962788:2963314 [1] NCCL INFO Using network IB
0: l50075:2962787:2963298 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.156<0>
0: l50075:2962787:2963298 [0] NCCL INFO Using non-device net plugin version 0
0: l50075:2962787:2963298 [0] NCCL INFO Using network IB
2: l50075:2962789:2963318 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.156<0>
2: l50075:2962789:2963318 [2] NCCL INFO Using non-device net plugin version 0
2: l50075:2962789:2963318 [2] NCCL INFO Using network IB
3: l50075:2962790:2963319 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.156<0>
3: l50075:2962790:2963319 [3] NCCL INFO Using non-device net plugin version 0
3: l50075:2962790:2963319 [3] NCCL INFO Using network IB
1: l50075:2962788:2963314 [1] NCCL INFO DMA-BUF is available on GPU device 1
0: l50075:2962787:2963298 [0] NCCL INFO DMA-BUF is available on GPU device 0
2: l50075:2962789:2963318 [2] NCCL INFO DMA-BUF is available on GPU device 2
3: l50075:2962790:2963319 [3] NCCL INFO DMA-BUF is available on GPU device 3
1: l50075:2962788:2963314 [1] NCCL INFO ncclCommInitRank comm 0x55555ec94dc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 44000 commId 0xa0dcd083c419c37f - Init START
3: l50075:2962790:2963319 [3] NCCL INFO ncclCommInitRank comm 0x55555ec94ca0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c4000 commId 0xa0dcd083c419c37f - Init START
2: l50075:2962789:2963318 [2] NCCL INFO ncclCommInitRank comm 0x55555ec95030 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 84000 commId 0xa0dcd083c419c37f - Init START
0: l50075:2962787:2963298 [0] NCCL INFO ncclCommInitRank comm 0x55555f0d9e70 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3000 commId 0xa0dcd083c419c37f - Init START
2: l50075:2962789:2963318 [2] NCCL INFO NVLS multicast support is not available on dev 2
3: l50075:2962790:2963319 [3] NCCL INFO NVLS multicast support is not available on dev 3
1: l50075:2962788:2963314 [1] NCCL INFO NVLS multicast support is not available on dev 1
0: l50075:2962787:2963298 [0] NCCL INFO Setting affinity for GPU 0 to 010000,00000000,00000000,00000000,00010000,00000000
0: l50075:2962787:2963298 [0] NCCL INFO NVLS multicast support is not available on dev 0
3: l50075:2962790:2963319 [3] NCCL INFO comm 0x55555ec94ca0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
2: l50075:2962789:2963318 [2] NCCL INFO comm 0x55555ec95030 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
1: l50075:2962788:2963314 [1] NCCL INFO comm 0x55555ec94dc0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
3: l50075:2962790:2963319 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->1 [5] -1/-1/-1->3->1 [6] -1/-1/-1->3->1 [7] -1/-1/-1->3->1 [8] 2/-1/-1->3->0 [9] 2/-1/-1->3->0 [10] 2/-1/-1->3->0 [11] 2/-1/-1->3->0 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->1 [17] -1/-1/-1->3->1 [18] -1/-1/-1->3->1 [19] -1/-1/-1->3->1 [20] 2/-1/-1->3->0 [21] 2/-1/-1->3->0 [22] 2/-1/-1->3->0 [23] 2/-1/-1->3->0
3: l50075:2962790:2963319 [3] NCCL INFO P2P Chunksize set to 524288
0: l50075:2962787:2963298 [0] NCCL INFO comm 0x55555f0d9e70 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
0: l50075:2962787:2963298 [0] NCCL INFO Channel 00/24 :    0   1   2   3
2: l50075:2962789:2963318 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->0 [5] 1/-1/-1->2->0 [6] 1/-1/-1->2->0 [7] 1/-1/-1->2->0 [8] -1/-1/-1->2->3 [9] -1/-1/-1->2->3 [10] -1/-1/-1->2->3 [11] -1/-1/-1->2->3 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 1/-1/-1->2->0 [17] 1/-1/-1->2->0 [18] 1/-1/-1->2->0 [19] 1/-1/-1->2->0 [20] -1/-1/-1->2->3 [21] -1/-1/-1->2->3 [22] -1/-1/-1->2->3 [23] -1/-1/-1->2->3
2: l50075:2962789:2963318 [2] NCCL INFO P2P Chunksize set to 524288
0: l50075:2962787:2963298 [0] NCCL INFO Channel 01/24 :    0   1   3   2
0: l50075:2962787:2963298 [0] NCCL INFO Channel 02/24 :    0   2   3   1
0: l50075:2962787:2963298 [0] NCCL INFO Channel 03/24 :    0   2   1   3
0: l50075:2962787:2963298 [0] NCCL INFO Channel 04/24 :    0   3   1   2
0: l50075:2962787:2963298 [0] NCCL INFO Channel 05/24 :    0   3   2   1
0: l50075:2962787:2963298 [0] NCCL INFO Channel 06/24 :    0   1   2   3
0: l50075:2962787:2963298 [0] NCCL INFO Channel 07/24 :    0   1   3   2
0: l50075:2962787:2963298 [0] NCCL INFO Channel 08/24 :    0   2   3   1
0: l50075:2962787:2963298 [0] NCCL INFO Channel 09/24 :    0   2   1   3
0: l50075:2962787:2963298 [0] NCCL INFO Channel 10/24 :    0   3   1   2
1: l50075:2962788:2963314 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 3/-1/-1->1->2 [5] 3/-1/-1->1->2 [6] 3/-1/-1->1->2 [7] 3/-1/-1->1->2 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 3/-1/-1->1->2 [17] 3/-1/-1->1->2 [18] 3/-1/-1->1->2 [19] 3/-1/-1->1->2 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1
1: l50075:2962788:2963314 [1] NCCL INFO P2P Chunksize set to 524288
0: l50075:2962787:2963298 [0] NCCL INFO Channel 11/24 :    0   3   2   1
0: l50075:2962787:2963298 [0] NCCL INFO Channel 12/24 :    0   1   2   3
0: l50075:2962787:2963298 [0] NCCL INFO Channel 13/24 :    0   1   3   2
0: l50075:2962787:2963298 [0] NCCL INFO Channel 14/24 :    0   2   3   1
0: l50075:2962787:2963298 [0] NCCL INFO Channel 15/24 :    0   2   1   3
0: l50075:2962787:2963298 [0] NCCL INFO Channel 16/24 :    0   3   1   2
0: l50075:2962787:2963298 [0] NCCL INFO Channel 17/24 :    0   3   2   1
0: l50075:2962787:2963298 [0] NCCL INFO Channel 18/24 :    0   1   2   3
0: l50075:2962787:2963298 [0] NCCL INFO Channel 19/24 :    0   1   3   2
0: l50075:2962787:2963298 [0] NCCL INFO Channel 20/24 :    0   2   3   1
0: l50075:2962787:2963298 [0] NCCL INFO Channel 21/24 :    0   2   1   3
0: l50075:2962787:2963298 [0] NCCL INFO Channel 22/24 :    0   3   1   2
0: l50075:2962787:2963298 [0] NCCL INFO Channel 23/24 :    0   3   2   1
0: l50075:2962787:2963298 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 2/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 2/-1/-1->0->-1 [7] 2/-1/-1->0->-1 [8] 3/-1/-1->0->1 [9] 3/-1/-1->0->1 [10] 3/-1/-1->0->1 [11] 3/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 2/-1/-1->0->-1 [17] 2/-1/-1->0->-1 [18] 2/-1/-1->0->-1 [19] 2/-1/-1->0->-1 [20] 3/-1/-1->0->1 [21] 3/-1/-1->0->1 [22] 3/-1/-1->0->1 [23] 3/-1/-1->0->1
0: l50075:2962787:2963298 [0] NCCL INFO P2P Chunksize set to 524288
2: l50075:2962789:2963318 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 04/0 : 3[3] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 08/0 : 3[3] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 13/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 10/0 : 3[3] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 16/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 13/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 09/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 14/0 : 3[3] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 15/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 14/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 16/0 : 3[3] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 19/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 20/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 15/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 21/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 22/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 20/0 : 0[0] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 21/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 04/0 : 0[0] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 16/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 17/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 22/0 : 0[0] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 23/0 : 0[0] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Connected all rings
1: l50075:2962788:2963314 [1] NCCL INFO Connected all rings
3: l50075:2962790:2963319 [3] NCCL INFO Connected all rings
2: l50075:2962789:2963318 [2] NCCL INFO Connected all rings
0: l50075:2962787:2963298 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 04/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 05/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 06/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 04/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 07/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 07/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 05/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 17/0 : 3[3] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 16/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 18/0 : 3[3] -> 1[1] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 17/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 19/0 : 3[3] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 16/0 : 1[1] -> 3[3] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 17/0 : 2[2] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 18/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 17/0 : 1[1] -> 3[3] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 18/0 : 2[2] -> 0[0] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 19/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 18/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 08/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 20/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50075:2962787:2963298 [0] NCCL INFO Channel 21/0 : 0[0] -> 3[3] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
3: l50075:2962790:2963319 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50075:2962789:2963318 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read
1: l50075:2962788:2963314 [1] NCCL INFO Connected all trees
0: l50075:2962787:2963298 [0] NCCL INFO Connected all trees
1: l50075:2962788:2963314 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
1: l50075:2962788:2963314 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
2: l50075:2962789:2963318 [2] NCCL INFO Connected all trees
3: l50075:2962790:2963319 [3] NCCL INFO Connected all trees
2: l50075:2962789:2963318 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
2: l50075:2962789:2963318 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
3: l50075:2962790:2963319 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
3: l50075:2962790:2963319 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
0: l50075:2962787:2963298 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
0: l50075:2962787:2963298 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
2: l50075:2962789:2963318 [2] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
2: l50075:2962789:2963318 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
2: l50075:2962789:2963318 [2] NCCL INFO ncclCommInitRank comm 0x55555ec95030 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 84000 commId 0xa0dcd083c419c37f - Init COMPLETE
0: l50075:2962787:2963298 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50075:2962787:2963298 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50075:2962787:2963298 [0] NCCL INFO ncclCommInitRank comm 0x55555f0d9e70 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3000 commId 0xa0dcd083c419c37f - Init COMPLETE
1: l50075:2962788:2963314 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
1: l50075:2962788:2963314 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
1: l50075:2962788:2963314 [1] NCCL INFO ncclCommInitRank comm 0x55555ec94dc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 44000 commId 0xa0dcd083c419c37f - Init COMPLETE
3: l50075:2962790:2963319 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
3: l50075:2962790:2963319 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
3: l50075:2962790:2963319 [3] NCCL INFO ncclCommInitRank comm 0x55555ec94ca0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c4000 commId 0xa0dcd083c419c37f - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 1
0: par_rank : 0
0: par_size : 4
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 0, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 0, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.5, 0.9, 0.2, 0.05], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [0.25, 0.9, 0.1, 0.05]], ['t2m', [1, 1024, ['velocity_u', '
0: velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 2, 4], [3, 27, 27], [0.5, 0.9, 0.2, 0.05], 'Local']]
0: fields_prediction : [['velocity_u', 0.125], ['velocity_v', 0.125], ['specific_humidity', 0.05], ['velocity_z', 0.01], ['temperature', 0.1], ['total_precip', 0.01], ['t2m', 0.58]]
0: fields_targets : []
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[0.0, 360.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 128
0: num_samples_per_epoch : 480
0: num_samples_validate : 128
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble', 'stats']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : BERT
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 18946672
0: wandb_id : 8qppxgks
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25_with_t2m.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : qw047nnt
0: years_test : [2021]
2: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
3: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
3: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
3: self.lats : (721,)
3: self.lons : (1440,)
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.lats : (721,)
2: self.lons : (1440,)
0: self.lats : (721,)
0: self.lons : (1440,)
2: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
3: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
2: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.lats : (721,)
2: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
3: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
3: self.lats : (721,)
3: self.lons : (1440,)
0: self.lats : (721,)
0: self.lons : (1440,)
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
3: ['cuda:3'] 0
0: ['cuda:0'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
3: ['cuda:3'] 0
0: ['cuda:0'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
3: ['cuda:3'] 0
0: ['cuda:0'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
3: ['cuda:3'] 0
0: ['cuda:0'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
1: ['cuda:1'] 0
1: Loaded model id = qw047nnt.
1: Loaded run 'qw047nnt' at epoch -2.
0: Loaded model id = qw047nnt.
0: Loaded run 'qw047nnt' at epoch -2.
3: Loaded model id = qw047nnt.
3: Loaded run 'qw047nnt' at epoch -2.
2: Loaded model id = qw047nnt.
2: Loaded run 'qw047nnt' at epoch -2.
0: Number of trainable parameters: 886,234,640
0: -1 : 17:28:01 :: batch_size = 96, lr = 1e-05
3: -1 : 17:28:01 :: batch_size = 96, lr = 1e-05
2: -1 : 17:28:01 :: batch_size = 96, lr = 1e-05
1: -1 : 17:28:01 :: batch_size = 96, lr = 1e-05
0: [DEBUG] TRAIN INPUT BATCH
0: Epoch -1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] TRAIN INPUT BATCH
1: Epoch -1, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] TRAIN INPUT BATCH
3: Epoch -1, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] TRAIN INPUT BATCH
2: Epoch -1, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7703, -0.7949, -0.8180, -0.8378, -0.8539, -0.8664, -0.8773, -0.8879,
1:         -0.8993, -0.9123, -0.9270, -0.9434, -0.9609, -0.9793, -0.9980, -1.0158,
1:         -1.0324, -1.0469, -0.7848, -0.8167, -0.8449, -0.8674, -0.8836, -0.8950,
3:      first 25 values: tensor([-0.7703, -0.7949, -0.8180, -0.8378, -0.8539, -0.8664, -0.8773, -0.8879,
3:         -0.8993, -0.9123, -0.9270, -0.9434, -0.9609, -0.9793, -0.9980, -1.0158,
3:         -1.0324, -1.0469, -0.7848, -0.8167, -0.8449, -0.8674, -0.8836, -0.8950,
0:      first 25 values: tensor([-0.7703, -0.7949, -0.8180, -0.8378, -0.8539, -0.8664, -0.8773, -0.8879,
0:         -0.8993, -0.9123, -0.9270, -0.9434, -0.9609, -0.9793, -0.9980, -1.0158,
0:         -1.0324, -1.0469, -0.7848, -0.8167, -0.8449, -0.8674, -0.8836, -0.8950,
2:      first 25 values: tensor([-0.7703, -0.7949, -0.8180, -0.8378, -0.8539, -0.8664, -0.8773, -0.8879,
2:         -0.8993, -0.9123, -0.9270, -0.9434, -0.9609, -0.9793, -0.9980, -1.0158,
2:         -1.0324, -1.0469, -0.7848, -0.8167, -0.8449, -0.8674, -0.8836, -0.8950,
3:         -0.9032], device='cuda:3')
0:         -0.9032], device='cuda:0')
1:         -0.9032], device='cuda:1')
2:         -0.9032], device='cuda:2')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-2.4410, -2.5170, -2.5945, -2.6619, -2.7069, -2.7198, -2.6953, -2.6342,
3:         -2.5435, -2.4343, -2.3197, -2.2118, -2.1184, -2.0433, -1.9852, -1.9397,
3:         -1.9022, -1.8697, -2.4630, -2.5468, -2.6298, -2.6980, -2.7388, -2.7428,
3:         -2.7067], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.4410, -2.5170, -2.5945, -2.6619, -2.7069, -2.7198, -2.6953, -2.6342,
0:         -2.5435, -2.4343, -2.3197, -2.2118, -2.1184, -2.0433, -1.9852, -1.9397,
0:         -1.9022, -1.8697, -2.4630, -2.5468, -2.6298, -2.6980, -2.7388, -2.7428,
0:         -2.7067], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.2898,  0.2532,  0.2180,  0.1834,  0.0793, -0.0240, -0.1251, -0.1725,
2:         -0.2287, -0.2879, -0.2509, -0.2119, -0.1670, -0.1500, -0.1205, -0.0861,
2:         -0.1367, -0.1909,  0.2694,  0.2324,  0.1954,  0.1571,  0.0619, -0.0404,
2:         -0.1427], device='cuda:2')
1:      first 25 values: tensor([ 0.2898,  0.2532,  0.2180,  0.1834,  0.0793, -0.0240, -0.1251, -0.1725,
1:         -0.2287, -0.2879, -0.2509, -0.2119, -0.1670, -0.1500, -0.1205, -0.0861,
1:         -0.1367, -0.1909,  0.2694,  0.2324,  0.1954,  0.1571,  0.0619, -0.0404,
1:         -0.1427], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.2898,  0.2532,  0.2180,  0.1834,  0.0793, -0.0240, -0.1251, -0.1725,
3:         -0.2287, -0.2879, -0.2509, -0.2119, -0.1670, -0.1500, -0.1205, -0.0861,
3:         -0.1367, -0.1909,  0.2694,  0.2324,  0.1954,  0.1571,  0.0619, -0.0404,
3:         -0.1427], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2898,  0.2532,  0.2180,  0.1834,  0.0793, -0.0240, -0.1251, -0.1725,
0:         -0.2287, -0.2879, -0.2509, -0.2119, -0.1670, -0.1500, -0.1205, -0.0861,
0:         -0.1367, -0.1909,  0.2694,  0.2324,  0.1954,  0.1571,  0.0619, -0.0404,
0:         -0.1427], device='cuda:0')
1:      first 25 values: tensor([-0.3058, -0.0942,  0.1902,  0.4811,  0.7081,  0.8250,  0.8228,  0.7390,
1:          0.6200,  0.5142,  0.4414,  0.4062,  0.3951,  0.3996,  0.4128,  0.4282,
1:          0.4414,  0.4392, -0.1096,  0.0094,  0.2166,  0.4525,  0.6420,  0.7390,
1:          0.7236], device='cuda:1')
2:      first 25 values: tensor([-0.3058, -0.0942,  0.1902,  0.4811,  0.7081,  0.8250,  0.8228,  0.7390,
2:          0.6200,  0.5142,  0.4414,  0.4062,  0.3951,  0.3996,  0.4128,  0.4282,
2:          0.4414,  0.4392, -0.1096,  0.0094,  0.2166,  0.4525,  0.6420,  0.7390,
2:          0.7236], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.3058, -0.0942,  0.1902,  0.4811,  0.7081,  0.8250,  0.8228,  0.7390,
3:          0.6200,  0.5142,  0.4414,  0.4062,  0.3951,  0.3996,  0.4128,  0.4282,
3:          0.4414,  0.4392, -0.1096,  0.0094,  0.2166,  0.4525,  0.6420,  0.7390,
3:          0.7236], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.6555, -0.7017, -0.7558, -0.8130, -0.8691, -0.9215, -0.9689, -1.0131,
1:         -1.0555, -1.0988, -1.1432, -1.1897, -1.2371, -1.2845, -1.3308, -1.3739,
1:         -1.4139, -1.4495, -1.4814, -1.5092, -1.5338, -1.5545, -1.5723, -1.5864,
1:         -1.5980], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([-0.6555, -0.7017, -0.7558, -0.8130, -0.8691, -0.9215, -0.9689, -1.0131,
2:         -1.0555, -1.0988, -1.1432, -1.1897, -1.2371, -1.2845, -1.3308, -1.3739,
2:         -1.4139, -1.4495, -1.4814, -1.5092, -1.5338, -1.5545, -1.5723, -1.5864,
2:         -1.5980], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.3058, -0.0942,  0.1902,  0.4811,  0.7081,  0.8250,  0.8228,  0.7390,
0:          0.6200,  0.5142,  0.4414,  0.4062,  0.3951,  0.3996,  0.4128,  0.4282,
0:          0.4414,  0.4392, -0.1096,  0.0094,  0.2166,  0.4525,  0.6420,  0.7390,
0:          0.7236], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.6555, -0.7017, -0.7558, -0.8130, -0.8691, -0.9215, -0.9689, -1.0131,
3:         -1.0555, -1.0988, -1.1432, -1.1897, -1.2371, -1.2845, -1.3308, -1.3739,
3:         -1.4139, -1.4495, -1.4814, -1.5092, -1.5338, -1.5545, -1.5723, -1.5864,
3:         -1.5980], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([-0.2267, -0.2177, -0.2064, -0.1952, -0.1794, -0.1726, -0.1929, -0.1771,
1:         -0.1816, -0.2222, -0.2132, -0.2064, -0.2042, -0.2064, -0.2267, -0.2470,
1:         -0.2312, -0.2155, -0.2470, -0.2470, -0.2470, -0.2448, -0.2448, -0.2448,
1:         -0.2448], device='cuda:1')
1:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.2267, -0.2177, -0.2064, -0.1952, -0.1794, -0.1726, -0.1929, -0.1771,
2:         -0.1816, -0.2222, -0.2132, -0.2064, -0.2042, -0.2064, -0.2267, -0.2470,
2:         -0.2312, -0.2155, -0.2470, -0.2470, -0.2470, -0.2448, -0.2448, -0.2448,
2:         -0.2448], device='cuda:2')
2:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.6555, -0.7017, -0.7558, -0.8130, -0.8691, -0.9215, -0.9689, -1.0131,
0:         -1.0555, -1.0988, -1.1432, -1.1897, -1.2371, -1.2845, -1.3308, -1.3739,
0:         -1.4139, -1.4495, -1.4814, -1.5092, -1.5338, -1.5545, -1.5723, -1.5864,
3:      first 25 values: tensor([-0.2267, -0.2177, -0.2064, -0.1952, -0.1794, -0.1726, -0.1929, -0.1771,
3:         -0.1816, -0.2222, -0.2132, -0.2064, -0.2042, -0.2064, -0.2267, -0.2470,
3:         -0.2312, -0.2155, -0.2470, -0.2470, -0.2470, -0.2448, -0.2448, -0.2448,
3:         -0.2448], device='cuda:3')
0:         -1.5980], device='cuda:0')
3:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([-0.1218, -0.2644, -0.4212, -0.5859, -0.6946, -0.7505, -0.6623, -0.8894,
1:         -0.9831, -1.0285, -0.9449, -0.8756, -0.8544, -0.8030, -0.7557, -0.7072,
1:         -0.6633, -0.6090, -0.5286, -0.5157, -0.4976, -0.4763, -0.4256, -0.3856,
1:         -0.3806], device='cuda:1')
2:      first 25 values: tensor([-0.1218, -0.2644, -0.4212, -0.5859, -0.6946, -0.7505, -0.6623, -0.8894,
2:         -0.9831, -1.0285, -0.9449, -0.8756, -0.8544, -0.8030, -0.7557, -0.7072,
2:         -0.6633, -0.6090, -0.5286, -0.5157, -0.4976, -0.4763, -0.4256, -0.3856,
2:         -0.3806], device='cuda:2')
3:      first 25 values: tensor([-0.1218, -0.2644, -0.4212, -0.5859, -0.6946, -0.7505, -0.6623, -0.8894,
3:         -0.9831, -1.0285, -0.9449, -0.8756, -0.8544, -0.8030, -0.7557, -0.7072,
3:         -0.6633, -0.6090, -0.5286, -0.5157, -0.4976, -0.4763, -0.4256, -0.3856,
3:         -0.3806], device='cuda:3')
0:      first 25 values: tensor([-0.2267, -0.2177, -0.2064, -0.1952, -0.1794, -0.1726, -0.1929, -0.1771,
0:         -0.1816, -0.2222, -0.2132, -0.2064, -0.2042, -0.2064, -0.2267, -0.2470,
0:         -0.2312, -0.2155, -0.2470, -0.2470, -0.2470, -0.2448, -0.2448, -0.2448,
0:         -0.2448], device='cuda:0')
0:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1218, -0.2644, -0.4212, -0.5859, -0.6946, -0.7505, -0.6623, -0.8894,
0:         -0.9831, -1.0285, -0.9449, -0.8756, -0.8544, -0.8030, -0.7557, -0.7072,
0:         -0.6633, -0.6090, -0.5286, -0.5157, -0.4976, -0.4763, -0.4256, -0.3856,
0:         -0.3806], device='cuda:0')
1: [DEBUG] TRAIN TARGET BATCH
1: Epoch -1, batch 0 - Sparse-masked 't2m' shape target data: torch.Size([2355, 2187])
3: [DEBUG] TRAIN TARGET BATCH
3: Epoch -1, batch 0 - Sparse-masked 't2m' shape target data: torch.Size([2403, 2187])
2: [DEBUG] TRAIN TARGET BATCH
2: Epoch -1, batch 0 - Sparse-masked 't2m' shape target data: torch.Size([2483, 2187])
0: [DEBUG] TRAIN TARGET BATCH
0: Epoch -1, batch 0 - Sparse-masked 't2m' shape target data: torch.Size([2192, 2187])
0:     First 25 batch values:
0: tensor([0.5290, 0.5290, 0.5228, 0.4998, 0.4728, 0.4424, 0.4107, 0.3969, 0.3772,
0:         0.3419, 0.3204, 0.3008, 0.2819, 0.2694, 0.2587, 0.2538, 0.2589, 0.2505,
3:     First 25 batch values:
3: tensor([ 0.8252,  0.7403,  0.6275,  0.4920,  0.4478,  0.4448,  0.5365,  0.2493,
3:          0.0805, -0.0369, -0.0671, -0.1185, -0.2295, -0.2340, -0.2643, -0.3051,
3:         -0.2983, -0.2894, -0.2764, -0.2672, -0.2423, -0.2115, -0.1809, -0.1623,
3:         -0.1811])
3: [DEBUG] TRAIN PREDICTIONS BATCH
3: Epoch -1, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([25895, 972])
1:     First 25 batch values:
1: tensor([0.9629, 0.9554, 0.9715, 0.9866, 1.0050, 1.0344, 1.0752, 0.9625, 0.9082,
1:         0.8864, 0.9097, 0.9266, 0.9238, 0.9067, 0.9034, 0.9051, 0.8986, 0.8911,
1:         0.8821, 0.8920, 0.8879, 0.8767, 0.8835, 0.8831, 0.8642])
1: [DEBUG] TRAIN PREDICTIONS BATCH
1: Epoch -1, first predictions sample:
2:     First 25 batch values:
2: tensor([0.1015, 0.0892, 0.0775, 0.0709, 0.0664, 0.0707, 0.0745, 0.0800, 0.0895,
2:         0.0847, 0.1032, 0.0620, 0.1100, 0.1894, 0.2167, 0.2553, 0.2725, 0.2720,
2:         0.2696, 0.2664, 0.2571, 0.2472, 0.2263, 0.2189, 0.2281])
2: [DEBUG] TRAIN PREDICTIONS BATCH
2: Epoch -1, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([27007, 972])
1:   └─ Predictions for 'velocity_u' shape: torch.Size([25156, 972])
0:         0.2385, 0.2435, 0.2517, 0.2675, 0.2507, 0.2532, 0.2616])
0: [DEBUG] TRAIN PREDICTIONS BATCH
0: Epoch -1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([25006, 972])
0:      first 25 pred values: tensor([-1.4227, -1.4353, -1.4470, -1.4604, -1.4716, -1.4810, -1.4874, -1.4928,
0:         -1.4967, -1.4990, -1.4999, -1.5006, -1.5036, -1.5058, -1.5091, -1.5105,
0:         -1.5076, -1.5005, -1.4370, -1.4437, -1.4528, -1.4632, -1.4734, -1.4833,
3:      first 25 pred values: tensor([-0.3507, -0.3610, -0.3674, -0.3735, -0.3716, -0.3636, -0.3505, -0.3349,
3:         -0.3141, -0.2940, -0.2715, -0.2479, -0.2240, -0.2000, -0.1769, -0.1539,
3:         -0.1268, -0.1016, -0.4308, -0.4392, -0.4412, -0.4375, -0.4242, -0.4048,
3:         -0.3804], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 pred values: tensor([-0.6102, -0.6085, -0.6062, -0.6075, -0.6031, -0.5948, -0.5852, -0.5761,
1:         -0.5696, -0.5644, -0.5588, -0.5572, -0.5513, -0.5518, -0.5573, -0.5637,
1:         -0.5697, -0.5716, -0.6902, -0.6904, -0.6909, -0.6942, -0.6938, -0.6878,
1:         -0.6797], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 pred values: tensor([-1.4396, -1.4531, -1.4678, -1.4831, -1.4964, -1.5069, -1.5155, -1.5210,
2:         -1.5254, -1.5252, -1.5248, -1.5221, -1.5226, -1.5227, -1.5228, -1.5236,
2:         -1.5208, -1.5153, -1.4455, -1.4564, -1.4699, -1.4814, -1.4923, -1.5017,
2:         -1.5107], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([25743, 972])
3:   └─ Predictions for 'velocity_v' shape: torch.Size([27162, 972])
1:   └─ Predictions for 'velocity_v' shape: torch.Size([26222, 972])
0:         -1.4918], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([26318, 972])
2:      first 25 pred values: tensor([-1.9279, -1.9124, -1.8916, -1.8645, -1.8354, -1.8037, -1.7705, -1.7325,
2:         -1.6908, -1.6436, -1.5969, -1.5498, -1.5032, -1.4576, -1.4138, -1.3684,
2:         -1.3220, -1.2745, -1.9593, -1.9406, -1.9125, -1.8783, -1.8416, -1.8023,
2:         -1.7632], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([25940, 972])
3:      first 25 pred values: tensor([-0.1488, -0.0788, -0.0020,  0.0766,  0.1501,  0.2102,  0.2627,  0.3064,
3:          0.3458,  0.3780,  0.4069,  0.4177,  0.4142,  0.3914,  0.3585,  0.3152,
3:          0.2665,  0.2248,  0.0007,  0.0722,  0.1412,  0.2122,  0.2702,  0.3159,
3:          0.3486], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 pred values: tensor([-2.0682, -2.0786, -2.0765, -2.0647, -2.0410, -2.0162, -1.9902, -1.9656,
1:         -1.9374, -1.9123, -1.8852, -1.8592, -1.8314, -1.8066, -1.7825, -1.7595,
1:         -1.7314, -1.7033, -2.0957, -2.1049, -2.1111, -2.1018, -2.0844, -2.0571,
1:         -2.0299], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([25332, 972])
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([25108, 972])
0:      first 25 pred values: tensor([-0.5201, -0.4945, -0.4736, -0.4486, -0.4178, -0.3832, -0.3408, -0.2963,
0:         -0.2500, -0.2024, -0.1526, -0.1059, -0.0602, -0.0185,  0.0209,  0.0553,
0:          0.0830,  0.1040, -0.4864, -0.4595, -0.4375, -0.4122, -0.3793, -0.3427,
0:         -0.3014], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([24596, 972])
2:      first 25 pred values: tensor([ 0.2282,  0.1559,  0.0884,  0.0248, -0.0364, -0.0907, -0.1389, -0.1734,
2:         -0.1983, -0.2149, -0.2245, -0.2287, -0.2346, -0.2413, -0.2430, -0.2445,
2:         -0.2360, -0.2275,  0.1406,  0.0559, -0.0177, -0.0856, -0.1445, -0.1933,
2:         -0.2281], device='cuda:2', grad_fn=<SliceBackward0>)
3:      first 25 pred values: tensor([-0.6384, -0.6279, -0.6177, -0.6093, -0.5990, -0.5897, -0.5768, -0.5622,
3:         -0.5504, -0.5352, -0.5178, -0.5010, -0.4823, -0.4579, -0.4270, -0.3905,
3:         -0.3453, -0.2935, -0.6132, -0.6006, -0.5878, -0.5711, -0.5531, -0.5308,
3:         -0.5048], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([24706, 972])
1:      first 25 pred values: tensor([-0.5342, -0.5282, -0.5211, -0.5133, -0.5016, -0.4864, -0.4714, -0.4523,
1:         -0.4363, -0.4227, -0.4112, -0.4088, -0.4089, -0.4110, -0.4172, -0.4220,
1:         -0.4296, -0.4337, -0.5315, -0.5252, -0.5182, -0.5100, -0.4975, -0.4836,
1:         -0.4690], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([26688, 972])
2:   └─ Predictions for 'velocity_z' shape: torch.Size([25925, 972])
0:      first 25 pred values: tensor([-0.4841, -0.4872, -0.4865, -0.4850, -0.4841, -0.4831, -0.4829, -0.4866,
0:         -0.4919, -0.4963, -0.5002, -0.5024, -0.5034, -0.4997, -0.4944, -0.4881,
0:         -0.4847, -0.4842, -0.4506, -0.4532, -0.4556, -0.4568, -0.4589, -0.4598,
0:         -0.4648], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([27209, 972])
1:      first 25 pred values: tensor([0.4360, 0.4492, 0.4607, 0.4580, 0.4461, 0.4318, 0.4189, 0.4089, 0.3715,
1:         0.3168, 0.2379, 0.1626, 0.1259, 0.1017, 0.0711, 0.0479, 0.0357, 0.0139,
1:         0.2666, 0.2844, 0.2821, 0.2954, 0.3135, 0.3310, 0.3419],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 pred values: tensor([-0.1960, -0.3025, -0.4499, -0.5608, -0.6431, -0.7098, -0.7390, -0.7588,
2:         -0.8161, -0.8191, -0.7436, -0.6839, -0.6343, -0.5663, -0.5343, -0.5207,
2:         -0.5001, -0.4735, -0.1799, -0.2467, -0.3498, -0.4460, -0.5261, -0.6215,
2:         -0.6840], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([11777, 2187])
3:      first 25 pred values: tensor([ 0.2056,  0.2007,  0.2114,  0.1818,  0.1801,  0.2347,  0.2853,  0.3059,
3:          0.2942,  0.2445,  0.2344,  0.2744,  0.2380,  0.1199,  0.0322, -0.0581,
3:         -0.2194, -0.3370,  0.0970,  0.1285,  0.1698,  0.1523,  0.1616,  0.2129,
3:          0.2372], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([11524, 2187])
1:   └─ Predictions for 'temperature' shape: torch.Size([11808, 2187])
0:      first 25 pred values: tensor([-0.3709, -0.4355, -0.4968, -0.5161, -0.5166, -0.5495, -0.5737, -0.5393,
0:         -0.4962, -0.4479, -0.3902, -0.3675, -0.3480, -0.3352, -0.3154, -0.2974,
0:         -0.3110, -0.2584, -0.0721, -0.1211, -0.1802, -0.2031, -0.1973, -0.2168,
0:         -0.2575], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([12038, 2187])
2:      first 25 pred values: tensor([-1.0710, -1.1138, -1.1569, -1.2003, -1.2444, -1.2898, -1.3367, -1.3843,
2:         -1.4303, -1.4727, -1.5104, -1.5446, -1.5761, -1.6044, -1.6307, -1.6507,
2:         -1.6644, -1.6715, -1.6748, -1.6774, -1.6802, -1.6806, -1.6746, -1.6589,
2:         -1.6326], device='cuda:2', grad_fn=<SliceBackward0>)
3:      first 25 pred values: tensor([-0.9369, -0.9479, -0.9571, -0.9670, -0.9788, -0.9915, -1.0052, -1.0186,
3:         -1.0314, -1.0401, -1.0453, -1.0486, -1.0529, -1.0573, -1.0632, -1.0665,
3:         -1.0671, -1.0640, -1.0596, -1.0568, -1.0533, -1.0470, -1.0360, -1.0206,
3:         -0.9996], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([9957, 243])
1:      first 25 pred values: tensor([-2.0074, -2.0332, -2.0602, -2.0814, -2.0982, -2.1081, -2.1138, -2.1196,
1:         -2.1268, -2.1376, -2.1529, -2.1702, -2.1886, -2.2046, -2.2179, -2.2301,
1:         -2.2429, -2.2570, -2.2716, -2.2843, -2.2925, -2.2961, -2.2999, -2.3080,
1:         -2.3196], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([9157, 243])
2:   └─ Predictions for 'total_precip' shape: torch.Size([11054, 243])
0:      first 25 pred values: tensor([-1.8392, -1.8782, -1.9113, -1.9379, -1.9609, -1.9821, -2.0058, -2.0353,
0:         -2.0721, -2.1126, -2.1524, -2.1871, -2.2158, -2.2380, -2.2570, -2.2721,
0:         -2.2844, -2.2933, -2.2989, -2.3017, -2.3015, -2.2963, -2.2876, -2.2785,
0:         -2.2703], device='cuda:0', grad_fn=<SliceBackward0>)
3:      first 25 pred values: tensor([-0.1291, -0.1509, -0.1549, -0.0847,  0.0288,  0.1612,  0.3022,  0.4339,
3:          0.5240, -0.0693, -0.0811, -0.0703, -0.0077,  0.0771,  0.2035,  0.3349,
3:          0.4456,  0.5264, -0.0181, -0.0135,  0.0147,  0.0475,  0.1069,  0.1860,
3:          0.2886], device='cuda:3', grad_fn=<SliceBackward0>)
2:      first 25 pred values: tensor([-0.0475, -0.0786, -0.1025, -0.1275, -0.1573, -0.1867, -0.2031, -0.2145,
2:         -0.2185, -0.0820, -0.1042, -0.1245, -0.1475, -0.1676, -0.1898, -0.1976,
2:         -0.1979, -0.2027, -0.1052, -0.1228, -0.1465, -0.1591, -0.1696, -0.1754,
2:         -0.1803], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 't2m' shape: torch.Size([2483, 2187])
3:   └─ Predictions for 't2m' shape: torch.Size([2403, 2187])
0:   └─ Predictions for 'total_precip' shape: torch.Size([9611, 243])
1:      first 25 pred values: tensor([-0.1912, -0.1880, -0.2401, -0.2105, -0.1724, -0.1982, -0.2626, -0.2399,
1:         -0.1987, -0.0938, -0.1012, -0.1006, -0.0944, -0.0827, -0.0592, -0.0661,
1:         -0.0559, -0.0199,  0.0401,  0.0824,  0.1662,  0.2366,  0.2998,  0.3859,
1:          0.4391], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 't2m' shape: torch.Size([2355, 2187])
3:      first 25 pred values: tensor([0.4364, 0.4325, 0.4208, 0.4558, 0.4107, 0.4511, 0.4222, 0.3793, 0.3715,
3:         0.3310, 0.3000, 0.2790, 0.2478, 0.2428, 0.2676, 0.1888, 0.2105, 0.1749,
3:         0.1664, 0.1070, 0.1160, 0.1544, 0.1237, 0.1073, 0.0487],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
2:      first 25 pred values: tensor([ 0.1184,  0.1084,  0.1500,  0.0760,  0.1028,  0.1594,  0.1291,  0.1158,
2:          0.0800,  0.0585,  0.0363,  0.0610,  0.0435,  0.0360,  0.0717,  0.0032,
2:          0.0137,  0.0226,  0.0427, -0.0016,  0.0870,  0.0204, -0.0083, -0.0218,
2:         -0.0652], device='cuda:2', grad_fn=<SliceBackward0>)
1:      first 25 pred values: tensor([1.0158, 0.9782, 0.9797, 0.9708, 0.9327, 0.9470, 0.9283, 0.8986, 0.8445,
1:         0.8725, 0.8609, 0.7850, 0.7674, 0.7929, 0.7389, 0.7392, 0.6962, 0.6690,
1:         0.7163, 0.6852, 0.6976, 0.6481, 0.6417, 0.6142, 0.6117],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
0:      first 25 pred values: tensor([0.2452, 0.2891, 0.3215, 0.3366, 0.3133, 0.2692, 0.1820, 0.1200, 0.0575,
0:         0.2899, 0.3164, 0.3348, 0.3307, 0.2909, 0.2327, 0.1508, 0.0570, 0.0170,
0:         0.2577, 0.2764, 0.2764, 0.2721, 0.2315, 0.1561, 0.0754],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 't2m' shape: torch.Size([2192, 2187])
0:      first 25 pred values: tensor([0.3935, 0.3907, 0.3674, 0.3926, 0.3260, 0.2716, 0.2752, 0.2770, 0.2604,
0:         0.2654, 0.1888, 0.1503, 0.1938, 0.1546, 0.1263, 0.1092, 0.0987, 0.1412,
0:         0.1220, 0.1210, 0.0615, 0.0752, 0.0374, 0.0802, 0.0384],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0: > /work/ab1412/atmorep/atmorep/core/trainer.py(579)loss()
0: -> stats_loss = torch.mean( diff * diff) + torch.mean( torch.sqrt( torch.abs( pred[1])) )
0: (Pdb) 
0: [1;34mwandb[0m:
0: [1;34mwandb[0m: You can sync this run to the cloud by running:
0: [1;34mwandb[0m: [1mwandb sync /work/ab1412/atmorep/wandb/offline-run-20250806_172634-8qppxgks[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250806_172634-8qppxgks/logs[0m
0: l50075:2962787:2963332 [0] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50075:2962787:2965222 [0] NCCL INFO comm 0x55555f0d9e70 rank 0 nranks 4 cudaDev 0 busId 3000 - Abort COMPLETE
