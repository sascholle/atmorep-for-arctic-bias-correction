0: Wandb run: atmorep-vhn9lny4-18524426
0: l50021:820521:820521 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.120<0>
0: l50021:820521:820521 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50021:820521:820521 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50021:820521:820521 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50021:820521:820521 [0] NCCL INFO cudaDriverVersion 12060
0: NCCL version 2.21.5+cuda12.4
1: l50045:1947457:1947457 [0] NCCL INFO cudaDriverVersion 12060
1: l50045:1947457:1947457 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.136<0>
1: l50045:1947457:1947457 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
1: l50045:1947457:1947457 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
1: l50045:1947457:1947457 [0] NCCL INFO NET/Plugin: Using internal network plugin.
1: l50045:1947457:1947613 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.136<0>
1: l50045:1947457:1947613 [0] NCCL INFO Using non-device net plugin version 0
1: l50045:1947457:1947613 [0] NCCL INFO Using network IB
0: l50021:820521:820918 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.120<0>
0: l50021:820521:820918 [0] NCCL INFO Using non-device net plugin version 0
0: l50021:820521:820918 [0] NCCL INFO Using network IB
1: l50045:1947457:1947613 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: l50021:820521:820918 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: l50021:820521:820918 [0] NCCL INFO ncclCommInitRank comm 0x55555f0ea810 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x79dc707795f156fd - Init START
1: l50045:1947457:1947613 [0] NCCL INFO ncclCommInitRank comm 0x55555ec9eda0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x79dc707795f156fd - Init START
1: l50045:1947457:1947613 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
0: l50021:820521:820918 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
1: l50045:1947457:1947613 [0] NCCL INFO comm 0x55555ec9eda0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
1: l50045:1947457:1947613 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
1: l50045:1947457:1947613 [0] NCCL INFO P2P Chunksize set to 131072
0: l50021:820521:820918 [0] NCCL INFO comm 0x55555f0ea810 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50021:820521:820918 [0] NCCL INFO Channel 00/04 :    0   1
0: l50021:820521:820918 [0] NCCL INFO Channel 01/04 :    0   1
0: l50021:820521:820918 [0] NCCL INFO Channel 02/04 :    0   1
0: l50021:820521:820918 [0] NCCL INFO Channel 03/04 :    0   1
0: l50021:820521:820918 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
0: l50021:820521:820918 [0] NCCL INFO P2P Chunksize set to 131072
0: l50021:820521:820918 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/IB/0
0: l50021:820521:820918 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IB/1
0: l50021:820521:820918 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/0
1: l50045:1947457:1947613 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/0
0: l50021:820521:820918 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/1
0: l50021:820521:820918 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/0
1: l50045:1947457:1947613 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/1
0: l50021:820521:820918 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/1
1: l50045:1947457:1947613 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/0
0: l50021:820521:820918 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/0
0: l50021:820521:820918 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/1
1: l50045:1947457:1947613 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/1
1: l50045:1947457:1947613 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/IB/0
1: l50045:1947457:1947613 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/IB/1
1: l50045:1947457:1947613 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/0
1: l50045:1947457:1947613 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/1
1: l50045:1947457:1947616 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 250.
1: l50045:1947457:1947616 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 50.
0: l50021:820521:820921 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 250.
0: l50021:820521:820921 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 50.
0: l50021:820521:820918 [0] NCCL INFO Connected all rings
0: l50021:820521:820918 [0] NCCL INFO Connected all trees
1: l50045:1947457:1947613 [0] NCCL INFO Connected all rings
1: l50045:1947457:1947613 [0] NCCL INFO Connected all trees
1: l50045:1947457:1947613 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
1: l50045:1947457:1947613 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50021:820521:820918 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
0: l50021:820521:820918 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50021:820521:820918 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50021:820521:820918 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50021:820521:820918 [0] NCCL INFO ncclCommInitRank comm 0x55555f0ea810 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x79dc707795f156fd - Init COMPLETE
1: l50045:1947457:1947613 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
1: l50045:1947457:1947613 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
1: l50045:1947457:1947613 [0] NCCL INFO ncclCommInitRank comm 0x55555ec9eda0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x79dc707795f156fd - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 1
0: par_rank : 0
0: par_size : 2
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 0, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 0, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.5, 0.9, 0.2, 0.05], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [0.25, 0.9, 0.1, 0.05]], ['t2m', [1, 1024, ['velocity_u', '
0: velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 2, 4], [3, 27, 27], [0.5, 0.9, 0.2, 0.05], 'Local']]
0: fields_prediction : [['velocity_u', 0.225], ['velocity_v', 0.225], ['specific_humidity', 0.1125], ['velocity_z', 0.01875], ['temperature', 0.15], ['total_precip', 0.01875], ['t2m', 0.25]]
0: fields_targets : []
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[0.0, 360.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 128
0: num_samples_per_epoch : 480
0: num_samples_validate : 128
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : BERT
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 18524426
0: wandb_id : vhn9lny4
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25_with_t2m.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : t2m
0: sparse_target_sparsity : 0.9
0: mask_input_field : t2m
0: mask_input_value : 0
0: years_test : [2021]
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded AtmoRep: ignoring 208 elements: ['embeds_token_info.6.weight', 'embeds_token_info.6.bias', 'embeds.6.weight', 'embeds.6.bias', 'encoders.6.embed.weight', 'encoders.6.embed.bias', 'encoders.6.heads.0.proj_out.weight', 'encoders.6.heads.0.proj_heads.weight', 'encoders.6.heads.0.proj_heads_other.0.weight', 'encoders.6.heads.0.proj_heads_other.1.weight', 'encoders.6.heads.0.proj_heads_other.2.weight', 'encoders.6.heads.0.proj_heads_other.3.weight', 'encoders.6.heads.0.proj_heads_other.4.weight', 'encoders.6.heads.1.proj_out.weight', 'encoders.6.heads.1.proj_heads.weight', 'encoders.6.heads.1.proj_heads_other.0.weight', 'encoders.6.heads.1.proj_heads_other.1.weight', 'encoders.6.heads.1.proj_heads_other.2.weight', 'encoders.6.heads.1.proj_heads_other.3.weight', 'encoders.6.heads.1.proj_heads_other.4.weight', 'encoders.6.heads.2.proj_out.weight', 'encoders.6.heads.2.proj_heads.weight', 'encoders.6.heads.2.proj_heads_other.0.weight', 'encoders.6.heads.2.proj_heads_other.1.weight', 'encoders.6.heads.2.proj_hea
0: ds_other.2.weight', 'encoders.6.heads.2.proj_heads_other.3.weight', 'encoders.6.heads.2.proj_heads_other.4.weight', 'encoders.6.heads.3.proj_out.weight', 'encoders.6.heads.3.proj_heads.weight', 'encoders.6.heads.3.proj_heads_other.0.weight', 'encoders.6.heads.3.proj_heads_other.1.weight', 'encoders.6.heads.3.proj_heads_other.2.weight', 'encoders.6.heads.3.proj_heads_other.3.weight', 'encoders.6.heads.3.proj_heads_other.4.weight', 'encoders.6.heads.4.proj_out.weight', 'encoders.6.heads.4.proj_heads.weight', 'encoders.6.heads.4.proj_heads_other.0.weight', 'encoders.6.heads.4.proj_heads_other.1.weight', 'encoders.6.heads.4.proj_heads_other.2.weight', 'encoders.6.heads.4.proj_heads_other.3.weight', 'encoders.6.heads.4.proj_heads_other.4.weight', 'encoders.6.heads.5.proj_out.weight', 'encoders.6.heads.5.proj_heads.weight', 'encoders.6.heads.5.proj_heads_other.0.weight', 'encoders.6.heads.5.proj_heads_other.1.weight', 'encoders.6.heads.5.proj_heads_other.2.weight', 'encoders.6.heads.5.proj_heads_other.3.weight', 'e
0: ncoders.6.heads.5.proj_heads_other.4.weight', 'encoders.6.mlps.0.blocks.0.weight', 'encoders.6.mlps.0.blocks.0.bias', 'encoders.6.mlps.0.blocks.3.weight', 'encoders.6.mlps.0.blocks.3.bias', 'encoders.6.mlps.1.blocks.0.weight', 'encoders.6.mlps.1.blocks.0.bias', 'encoders.6.mlps.1.blocks.3.weight', 'encoders.6.mlps.1.blocks.3.bias', 'encoders.6.mlps.2.blocks.0.weight', 'encoders.6.mlps.2.blocks.0.bias', 'encoders.6.mlps.2.blocks.3.weight', 'encoders.6.mlps.2.blocks.3.bias', 'encoders.6.mlps.3.blocks.0.weight', 'encoders.6.mlps.3.blocks.0.bias', 'encoders.6.mlps.3.blocks.3.weight', 'encoders.6.mlps.3.blocks.3.bias', 'encoders.6.mlps.4.blocks.0.weight', 'encoders.6.mlps.4.blocks.0.bias', 'encoders.6.mlps.4.blocks.3.weight', 'encoders.6.mlps.4.blocks.3.bias', 'encoders.6.mlps.5.blocks.0.weight', 'encoders.6.mlps.5.blocks.0.bias', 'encoders.6.mlps.5.blocks.3.weight', 'encoders.6.mlps.5.blocks.3.bias', 'decoders.6.blocks.0.proj_heads.weight', 'decoders.6.blocks.0.proj_heads_o_q.weight', 'decoders.6.blocks.0.proj_he
0: ads_o_kv.weight', 'decoders.6.blocks.0.ln_q.weight', 'decoders.6.blocks.0.ln_q.bias', 'decoders.6.blocks.0.ln_k.weight', 'decoders.6.blocks.0.ln_k.bias', 'decoders.6.blocks.0.proj_out.weight', 'decoders.6.blocks.1.blocks.0.weight', 'decoders.6.blocks.1.blocks.0.bias', 'decoders.6.blocks.1.blocks.3.weight', 'decoders.6.blocks.1.blocks.3.bias', 'decoders.6.blocks.2.proj_heads.weight', 'decoders.6.blocks.2.proj_heads_o_q.weight', 'decoders.6.blocks.2.proj_heads_o_kv.weight', 'decoders.6.blocks.2.ln_q.weight', 'decoders.6.blocks.2.ln_q.bias', 'decoders.6.blocks.2.ln_k.weight', 'decoders.6.blocks.2.ln_k.bias', 'decoders.6.blocks.2.proj_out.weight', 'decoders.6.blocks.3.blocks.0.weight', 'decoders.6.blocks.3.blocks.0.bias', 'decoders.6.blocks.3.blocks.3.weight', 'decoders.6.blocks.3.blocks.3.bias', 'decoders.6.blocks.4.proj_heads.weight', 'decoders.6.blocks.4.proj_heads_o_q.weight', 'decoders.6.blocks.4.proj_heads_o_kv.weight', 'decoders.6.blocks.4.ln_q.weight', 'decoders.6.blocks.4.ln_q.bias', 'decoders.6.blocks.4
0: .ln_k.weight', 'decoders.6.blocks.4.ln_k.bias', 'decoders.6.blocks.4.proj_out.weight', 'decoders.6.blocks.5.blocks.0.weight', 'decoders.6.blocks.5.blocks.0.bias', 'decoders.6.blocks.5.blocks.3.weight', 'decoders.6.blocks.5.blocks.3.bias', 'decoders.6.blocks.6.proj_heads.weight', 'decoders.6.blocks.6.proj_heads_o_q.weight', 'decoders.6.blocks.6.proj_heads_o_kv.weight', 'decoders.6.blocks.6.ln_q.weight', 'decoders.6.blocks.6.ln_q.bias', 'decoders.6.blocks.6.ln_k.weight', 'decoders.6.blocks.6.ln_k.bias', 'decoders.6.blocks.6.proj_out.weight', 'decoders.6.blocks.7.blocks.0.weight', 'decoders.6.blocks.7.blocks.0.bias', 'decoders.6.blocks.7.blocks.3.weight', 'decoders.6.blocks.7.blocks.3.bias', 'decoders.6.blocks.8.proj_heads.weight', 'decoders.6.blocks.8.proj_heads_o_q.weight', 'decoders.6.blocks.8.proj_heads_o_kv.weight', 'decoders.6.blocks.8.ln_q.weight', 'decoders.6.blocks.8.ln_q.bias', 'decoders.6.blocks.8.ln_k.weight', 'decoders.6.blocks.8.ln_k.bias', 'decoders.6.blocks.8.proj_out.weight', 'decoders.6.blocks.
0: 9.blocks.0.weight', 'decoders.6.blocks.9.blocks.0.bias', 'decoders.6.blocks.9.blocks.3.weight', 'decoders.6.blocks.9.blocks.3.bias', 'decoders.6.blocks.10.proj_heads.weight', 'decoders.6.blocks.10.proj_heads_o_q.weight', 'decoders.6.blocks.10.proj_heads_o_kv.weight', 'decoders.6.blocks.10.ln_q.weight', 'decoders.6.blocks.10.ln_q.bias', 'decoders.6.blocks.10.ln_k.weight', 'decoders.6.blocks.10.ln_k.bias', 'decoders.6.blocks.10.proj_out.weight', 'decoders.6.blocks.11.blocks.0.weight', 'decoders.6.blocks.11.blocks.0.bias', 'decoders.6.blocks.11.blocks.3.weight', 'decoders.6.blocks.11.blocks.3.bias', 'tails.6.tail_nets.0.0.weight', 'tails.6.tail_nets.0.0.bias', 'tails.6.tail_nets.0.1.weight', 'tails.6.tail_nets.0.1.bias', 'tails.6.tail_nets.1.0.weight', 'tails.6.tail_nets.1.0.bias', 'tails.6.tail_nets.1.1.weight', 'tails.6.tail_nets.1.1.bias', 'tails.6.tail_nets.2.0.weight', 'tails.6.tail_nets.2.0.bias', 'tails.6.tail_nets.2.1.weight', 'tails.6.tail_nets.2.1.bias', 'tails.6.tail_nets.3.0.weight', 'tails.6.tail_ne
0: ts.3.0.bias', 'tails.6.tail_nets.3.1.weight', 'tails.6.tail_nets.3.1.bias', 'tails.6.tail_nets.4.0.weight', 'tails.6.tail_nets.4.0.bias', 'tails.6.tail_nets.4.1.weight', 'tails.6.tail_nets.4.1.bias', 'tails.6.tail_nets.5.0.weight', 'tails.6.tail_nets.5.0.bias', 'tails.6.tail_nets.5.1.weight', 'tails.6.tail_nets.5.1.bias', 'tails.6.tail_nets.6.0.weight', 'tails.6.tail_nets.6.0.bias', 'tails.6.tail_nets.6.1.weight', 'tails.6.tail_nets.6.1.bias', 'tails.6.tail_nets.7.0.weight', 'tails.6.tail_nets.7.0.bias', 'tails.6.tail_nets.7.1.weight', 'tails.6.tail_nets.7.1.bias', 'tails.6.tail_nets.8.0.weight', 'tails.6.tail_nets.8.0.bias', 'tails.6.tail_nets.8.1.weight', 'tails.6.tail_nets.8.1.bias', 'tails.6.tail_nets.9.0.weight', 'tails.6.tail_nets.9.0.bias', 'tails.6.tail_nets.9.1.weight', 'tails.6.tail_nets.9.1.bias', 'tails.6.tail_nets.10.0.weight', 'tails.6.tail_nets.10.0.bias', 'tails.6.tail_nets.10.1.weight', 'tails.6.tail_nets.10.1.bias', 'tails.6.tail_nets.11.0.weight', 'tails.6.tail_nets.11.0.bias', 'tails.6.tai
0: l_nets.11.1.weight', 'tails.6.tail_nets.11.1.bias', 'tails.6.tail_nets.12.0.weight', 'tails.6.tail_nets.12.0.bias', 'tails.6.tail_nets.12.1.weight', 'tails.6.tail_nets.12.1.bias', 'tails.6.tail_nets.13.0.weight', 'tails.6.tail_nets.13.0.bias', 'tails.6.tail_nets.13.1.weight', 'tails.6.tail_nets.13.1.bias', 'tails.6.tail_nets.14.0.weight', 'tails.6.tail_nets.14.0.bias', 'tails.6.tail_nets.14.1.weight', 'tails.6.tail_nets.14.1.bias', 'tails.6.tail_nets.15.0.weight', 'tails.6.tail_nets.15.0.bias', 'tails.6.tail_nets.15.1.weight', 'tails.6.tail_nets.15.1.bias']
1: Loaded AtmoRep: ignoring 208 elements: ['embeds_token_info.6.weight', 'embeds_token_info.6.bias', 'embeds.6.weight', 'embeds.6.bias', 'encoders.6.embed.weight', 'encoders.6.embed.bias', 'encoders.6.heads.0.proj_out.weight', 'encoders.6.heads.0.proj_heads.weight', 'encoders.6.heads.0.proj_heads_other.0.weight', 'encoders.6.heads.0.proj_heads_other.1.weight', 'encoders.6.heads.0.proj_heads_other.2.weight', 'encoders.6.heads.0.proj_heads_other.3.weight', 'encoders.6.heads.0.proj_heads_other.4.weight', 'encoders.6.heads.1.proj_out.weight', 'encoders.6.heads.1.proj_heads.weight', 'encoders.6.heads.1.proj_heads_other.0.weight', 'encoders.6.heads.1.proj_heads_other.1.weight', 'encoders.6.heads.1.proj_heads_other.2.weight', 'encoders.6.heads.1.proj_heads_other.3.weight', 'encoders.6.heads.1.proj_heads_other.4.weight', 'encoders.6.heads.2.proj_out.weight', 'encoders.6.heads.2.proj_heads.weight', 'encoders.6.heads.2.proj_heads_other.0.weight', 'encoders.6.heads.2.proj_heads_other.1.weight', 'encoders.6.heads.2.proj_hea
1: ds_other.2.weight', 'encoders.6.heads.2.proj_heads_other.3.weight', 'encoders.6.heads.2.proj_heads_other.4.weight', 'encoders.6.heads.3.proj_out.weight', 'encoders.6.heads.3.proj_heads.weight', 'encoders.6.heads.3.proj_heads_other.0.weight', 'encoders.6.heads.3.proj_heads_other.1.weight', 'encoders.6.heads.3.proj_heads_other.2.weight', 'encoders.6.heads.3.proj_heads_other.3.weight', 'encoders.6.heads.3.proj_heads_other.4.weight', 'encoders.6.heads.4.proj_out.weight', 'encoders.6.heads.4.proj_heads.weight', 'encoders.6.heads.4.proj_heads_other.0.weight', 'encoders.6.heads.4.proj_heads_other.1.weight', 'encoders.6.heads.4.proj_heads_other.2.weight', 'encoders.6.heads.4.proj_heads_other.3.weight', 'encoders.6.heads.4.proj_heads_other.4.weight', 'encoders.6.heads.5.proj_out.weight', 'encoders.6.heads.5.proj_heads.weight', 'encoders.6.heads.5.proj_heads_other.0.weight', 'encoders.6.heads.5.proj_heads_other.1.weight', 'encoders.6.heads.5.proj_heads_other.2.weight', 'encoders.6.heads.5.proj_heads_other.3.weight', 'e
1: ncoders.6.heads.5.proj_heads_other.4.weight', 'encoders.6.mlps.0.blocks.0.weight', 'encoders.6.mlps.0.blocks.0.bias', 'encoders.6.mlps.0.blocks.3.weight', 'encoders.6.mlps.0.blocks.3.bias', 'encoders.6.mlps.1.blocks.0.weight', 'encoders.6.mlps.1.blocks.0.bias', 'encoders.6.mlps.1.blocks.3.weight', 'encoders.6.mlps.1.blocks.3.bias', 'encoders.6.mlps.2.blocks.0.weight', 'encoders.6.mlps.2.blocks.0.bias', 'encoders.6.mlps.2.blocks.3.weight', 'encoders.6.mlps.2.blocks.3.bias', 'encoders.6.mlps.3.blocks.0.weight', 'encoders.6.mlps.3.blocks.0.bias', 'encoders.6.mlps.3.blocks.3.weight', 'encoders.6.mlps.3.blocks.3.bias', 'encoders.6.mlps.4.blocks.0.weight', 'encoders.6.mlps.4.blocks.0.bias', 'encoders.6.mlps.4.blocks.3.weight', 'encoders.6.mlps.4.blocks.3.bias', 'encoders.6.mlps.5.blocks.0.weight', 'encoders.6.mlps.5.blocks.0.bias', 'encoders.6.mlps.5.blocks.3.weight', 'encoders.6.mlps.5.blocks.3.bias', 'decoders.6.blocks.0.proj_heads.weight', 'decoders.6.blocks.0.proj_heads_o_q.weight', 'decoders.6.blocks.0.proj_he
1: ads_o_kv.weight', 'decoders.6.blocks.0.ln_q.weight', 'decoders.6.blocks.0.ln_q.bias', 'decoders.6.blocks.0.ln_k.weight', 'decoders.6.blocks.0.ln_k.bias', 'decoders.6.blocks.0.proj_out.weight', 'decoders.6.blocks.1.blocks.0.weight', 'decoders.6.blocks.1.blocks.0.bias', 'decoders.6.blocks.1.blocks.3.weight', 'decoders.6.blocks.1.blocks.3.bias', 'decoders.6.blocks.2.proj_heads.weight', 'decoders.6.blocks.2.proj_heads_o_q.weight', 'decoders.6.blocks.2.proj_heads_o_kv.weight', 'decoders.6.blocks.2.ln_q.weight', 'decoders.6.blocks.2.ln_q.bias', 'decoders.6.blocks.2.ln_k.weight', 'decoders.6.blocks.2.ln_k.bias', 'decoders.6.blocks.2.proj_out.weight', 'decoders.6.blocks.3.blocks.0.weight', 'decoders.6.blocks.3.blocks.0.bias', 'decoders.6.blocks.3.blocks.3.weight', 'decoders.6.blocks.3.blocks.3.bias', 'decoders.6.blocks.4.proj_heads.weight', 'decoders.6.blocks.4.proj_heads_o_q.weight', 'decoders.6.blocks.4.proj_heads_o_kv.weight', 'decoders.6.blocks.4.ln_q.weight', 'decoders.6.blocks.4.ln_q.bias', 'decoders.6.blocks.4
1: .ln_k.weight', 'decoders.6.blocks.4.ln_k.bias', 'decoders.6.blocks.4.proj_out.weight', 'decoders.6.blocks.5.blocks.0.weight', 'decoders.6.blocks.5.blocks.0.bias', 'decoders.6.blocks.5.blocks.3.weight', 'decoders.6.blocks.5.blocks.3.bias', 'decoders.6.blocks.6.proj_heads.weight', 'decoders.6.blocks.6.proj_heads_o_q.weight', 'decoders.6.blocks.6.proj_heads_o_kv.weight', 'decoders.6.blocks.6.ln_q.weight', 'decoders.6.blocks.6.ln_q.bias', 'decoders.6.blocks.6.ln_k.weight', 'decoders.6.blocks.6.ln_k.bias', 'decoders.6.blocks.6.proj_out.weight', 'decoders.6.blocks.7.blocks.0.weight', 'decoders.6.blocks.7.blocks.0.bias', 'decoders.6.blocks.7.blocks.3.weight', 'decoders.6.blocks.7.blocks.3.bias', 'decoders.6.blocks.8.proj_heads.weight', 'decoders.6.blocks.8.proj_heads_o_q.weight', 'decoders.6.blocks.8.proj_heads_o_kv.weight', 'decoders.6.blocks.8.ln_q.weight', 'decoders.6.blocks.8.ln_q.bias', 'decoders.6.blocks.8.ln_k.weight', 'decoders.6.blocks.8.ln_k.bias', 'decoders.6.blocks.8.proj_out.weight', 'decoders.6.blocks.
1: 9.blocks.0.weight', 'decoders.6.blocks.9.blocks.0.bias', 'decoders.6.blocks.9.blocks.3.weight', 'decoders.6.blocks.9.blocks.3.bias', 'decoders.6.blocks.10.proj_heads.weight', 'decoders.6.blocks.10.proj_heads_o_q.weight', 'decoders.6.blocks.10.proj_heads_o_kv.weight', 'decoders.6.blocks.10.ln_q.weight', 'decoders.6.blocks.10.ln_q.bias', 'decoders.6.blocks.10.ln_k.weight', 'decoders.6.blocks.10.ln_k.bias', 'decoders.6.blocks.10.proj_out.weight', 'decoders.6.blocks.11.blocks.0.weight', 'decoders.6.blocks.11.blocks.0.bias', 'decoders.6.blocks.11.blocks.3.weight', 'decoders.6.blocks.11.blocks.3.bias', 'tails.6.tail_nets.0.0.weight', 'tails.6.tail_nets.0.0.bias', 'tails.6.tail_nets.0.1.weight', 'tails.6.tail_nets.0.1.bias', 'tails.6.tail_nets.1.0.weight', 'tails.6.tail_nets.1.0.bias', 'tails.6.tail_nets.1.1.weight', 'tails.6.tail_nets.1.1.bias', 'tails.6.tail_nets.2.0.weight', 'tails.6.tail_nets.2.0.bias', 'tails.6.tail_nets.2.1.weight', 'tails.6.tail_nets.2.1.bias', 'tails.6.tail_nets.3.0.weight', 'tails.6.tail_ne
1: ts.3.0.bias', 'tails.6.tail_nets.3.1.weight', 'tails.6.tail_nets.3.1.bias', 'tails.6.tail_nets.4.0.weight', 'tails.6.tail_nets.4.0.bias', 'tails.6.tail_nets.4.1.weight', 'tails.6.tail_nets.4.1.bias', 'tails.6.tail_nets.5.0.weight', 'tails.6.tail_nets.5.0.bias', 'tails.6.tail_nets.5.1.weight', 'tails.6.tail_nets.5.1.bias', 'tails.6.tail_nets.6.0.weight', 'tails.6.tail_nets.6.0.bias', 'tails.6.tail_nets.6.1.weight', 'tails.6.tail_nets.6.1.bias', 'tails.6.tail_nets.7.0.weight', 'tails.6.tail_nets.7.0.bias', 'tails.6.tail_nets.7.1.weight', 'tails.6.tail_nets.7.1.bias', 'tails.6.tail_nets.8.0.weight', 'tails.6.tail_nets.8.0.bias', 'tails.6.tail_nets.8.1.weight', 'tails.6.tail_nets.8.1.bias', 'tails.6.tail_nets.9.0.weight', 'tails.6.tail_nets.9.0.bias', 'tails.6.tail_nets.9.1.weight', 'tails.6.tail_nets.9.1.bias', 'tails.6.tail_nets.10.0.weight', 'tails.6.tail_nets.10.0.bias', 'tails.6.tail_nets.10.1.weight', 'tails.6.tail_nets.10.1.bias', 'tails.6.tail_nets.11.0.weight', 'tails.6.tail_nets.11.0.bias', 'tails.6.tai
1: l_nets.11.1.weight', 'tails.6.tail_nets.11.1.bias', 'tails.6.tail_nets.12.0.weight', 'tails.6.tail_nets.12.0.bias', 'tails.6.tail_nets.12.1.weight', 'tails.6.tail_nets.12.1.bias', 'tails.6.tail_nets.13.0.weight', 'tails.6.tail_nets.13.0.bias', 'tails.6.tail_nets.13.1.weight', 'tails.6.tail_nets.13.1.bias', 'tails.6.tail_nets.14.0.weight', 'tails.6.tail_nets.14.0.bias', 'tails.6.tail_nets.14.1.weight', 'tails.6.tail_nets.14.1.bias', 'tails.6.tail_nets.15.0.weight', 'tails.6.tail_nets.15.0.bias', 'tails.6.tail_nets.15.1.weight', 'tails.6.tail_nets.15.1.bias']
0: Loaded model id = wc5e2i3t.
1: Loaded model id = wc5e2i3t.
1: Loaded run 'wc5e2i3t' at epoch -2.
0: Loaded run 'wc5e2i3t' at epoch -2.
1: -1 : 23:50:01 :: batch_size = 96, lr = 1e-05
0: Number of trainable parameters: 886,234,640
0: -1 : 23:50:01 :: batch_size = 96, lr = 1e-05
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: [DEBUG] TRAIN INPUT BATCH
1: Epoch -1, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] TRAIN INPUT BATCH
0: Epoch -1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9524, 0.9779, 1.0034, 1.0291, 1.0550, 1.0811, 1.1072, 1.1337, 1.1600, 1.1857, 1.2100, 1.2324, 1.2521, 1.2694,
0:         1.2838, 1.2963, 1.3068, 1.3159, 1.0687, 1.1008, 1.1327, 1.1652, 1.1978, 1.2303, 1.2628], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.2594, 0.2920, 0.3253, 0.3591, 0.3940, 0.4305, 0.4687, 0.5081, 0.5484, 0.5899, 0.6320, 0.6745, 0.7176, 0.7620,
1:         0.8077, 0.8543, 0.9008, 0.9469, 0.1860, 0.2104, 0.2360, 0.2627, 0.2914, 0.3222, 0.3555], device='cuda:0')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.0616,  0.0218, -0.2074,  0.2352, -0.0336, -0.0867,  0.0874,  0.1093,  0.0448, -0.0362, -0.1534, -0.2188,
1:          0.1330, -0.0501, -0.0385,  0.0114,  0.0823, -0.0082, -0.0763,  0.0615,  0.0147,  0.0645,  0.0395, -0.1165,
1:         -0.0768], device='cuda:0')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4651, -0.4604, -0.4554, -0.4498, -0.4448, -0.4392, -0.4334, -0.4280, -0.4225, -0.4159, -0.4095, -0.4031,
1:         -0.3962, -0.3867, -0.3780, -0.3667, -0.3549, -0.3422, -0.5089, -0.5047, -0.5017, -0.4980, -0.4937, -0.4885,
1:         -0.4843], device='cuda:0')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1229, -0.0822, -0.0436, -0.0093,  0.0250,  0.0678,  0.1343,  0.2157,  0.2886,  0.3422,  0.3744,  0.3829,
1:          0.3594,  0.3036,  0.2286,  0.1600,  0.1193,  0.1086, -0.1529, -0.0993, -0.0436,  0.0143,  0.0786,  0.1493,
1:          0.2179], device='cuda:0')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([ 0.0411, -0.1491, -0.2041,  0.1194,  0.0307, -0.0542,  0.0787, -0.0998, -0.2140,  0.1342,  0.0958,  0.1371,
1:          0.0646, -0.1198, -0.1263, -0.0369, -0.0180, -0.1074,  0.0691,  0.0166,  0.0481,  0.1509,  0.0842,  0.1547,
1:         -0.1766], device='cuda:0')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([-0.2461, -0.2461, -0.2461, -0.2461, -0.2461, -0.2438, -0.2438, -0.2392, -0.2091, -0.2450, -0.2450, -0.2461,
1:         -0.2380, -0.2369, -0.2334, -0.2241, -0.1582, -0.0945, -0.2392, -0.2403, -0.2415, -0.2102, -0.1790, -0.1466,
1:         -0.1130], device='cuda:0')
1:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
1:        device='cuda:0')
1: [DEBUG] TRAIN TARGET BATCH
1: Epoch -1, batch 0 - Sparse-masked 't2m' shape target data: torch.Size([2388, 2187])
1:     First 25 batch values:
1: tensor([   nan,    nan,    nan,    nan,    nan, 0.1678,    nan,    nan,    nan,    nan,    nan,    nan, 0.0071,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan])
1: [DEBUG] TRAIN PREDICTIONS BATCH
1: Epoch -1, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([26119, 972])
1:      first 25 pred values: tensor([0.3279, 0.2998, 0.2716, 0.2469, 0.2252, 0.2105, 0.2054, 0.2085, 0.2203, 0.2387, 0.2603, 0.2755, 0.2867, 0.2883,
1:         0.2838, 0.2751, 0.2605, 0.2428, 0.3649, 0.3359, 0.3075, 0.2843, 0.2650, 0.2521, 0.2489], device='cuda:0',
1:        grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([26919, 972])
1:      first 25 pred values: tensor([1.7613, 1.7976, 1.8276, 1.8544, 1.8762, 1.8984, 1.9237, 1.9555, 1.9982, 2.0499, 2.1109, 2.1770, 2.2468, 2.3165,
1:         2.3876, 2.4510, 2.5026, 2.5410, 1.7444, 1.7804, 1.8124, 1.8399, 1.8644, 1.8894, 1.9179], device='cuda:0',
1:        grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([26542, 972])
0:      first 25 values: tensor([-0.4651, -0.4604, -0.4554, -0.4498, -0.4448, -0.4392, -0.4334, -0.4280, -0.4225, -0.4159, -0.4095, -0.4031,
0:         -0.3962, -0.3867, -0.3780, -0.3667, -0.3549, -0.3422, -0.5089, -0.5047, -0.5017, -0.4980, -0.4937, -0.4885,
0:         -0.4843], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 pred values: tensor([-0.7181, -0.7212, -0.7233, -0.7249, -0.7246, -0.7234, -0.7220, -0.7213, -0.7215, -0.7232, -0.7238, -0.7265,
1:         -0.7275, -0.7293, -0.7316, -0.7311, -0.7310, -0.7299, -0.7240, -0.7266, -0.7286, -0.7313, -0.7310, -0.7290,
1:         -0.7289], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([25417, 972])
1:      first 25 pred values: tensor([ 0.7148,  0.7080,  0.5904,  0.4146,  0.2162,  0.0729,  0.0267,  0.0101, -0.0007, -0.0286, -0.0863, -0.1569,
1:         -0.2541, -0.3279, -0.3781, -0.4491, -0.4779, -0.4538,  0.4387,  0.4564,  0.4121,  0.3232,  0.1806,  0.0593,
1:          0.0106], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([11804, 2187])
0:      first 25 values: tensor([-0.1229, -0.0822, -0.0436, -0.0093,  0.0250,  0.0678,  0.1343,  0.2157,  0.2886,  0.3422,  0.3744,  0.3829,
0:          0.3594,  0.3036,  0.2286,  0.1600,  0.1193,  0.1086, -0.1529, -0.0993, -0.0436,  0.0143,  0.0786,  0.1493,
0:          0.2179], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 pred values: tensor([ 0.3936,  0.3657,  0.3435,  0.3243,  0.3059,  0.2865,  0.2676,  0.2537,  0.2443,  0.2372,  0.2304,  0.2183,
1:          0.2007,  0.1742,  0.1424,  0.1008,  0.0521, -0.0032, -0.0631, -0.1232, -0.1853, -0.2484, -0.3149, -0.3815,
1:         -0.4458], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([10202, 243])
0:      first 25 values: tensor([ 0.1386, -0.0285,  0.0278,  0.1167, -0.2123, -0.1737,  0.1961, -0.1397, -0.0380,  0.0042, -0.1406, -0.0383,
0:         -0.1309, -0.0741,  0.0105,  0.0203,  0.1587, -0.0452, -0.0013, -0.1136,  0.1163,  0.0023,  0.0466, -0.0127,
0:         -0.0193], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 pred values: tensor([-0.2270, -0.2287, -0.2287, -0.2323, -0.2341, -0.2373, -0.2402, -0.2408, -0.2415, -0.2298, -0.2324, -0.2351,
1:         -0.2370, -0.2370, -0.2378, -0.2380, -0.2387, -0.2444, -0.2299, -0.2366, -0.2381, -0.2353, -0.2382, -0.2396,
1:         -0.2380], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 't2m' shape: torch.Size([2388, 2187])
1:      first 25 pred values: tensor([ 0.0566, -0.1114, -0.0112, -0.1043,  0.1666, -0.1638,  0.0980, -0.0345,  0.0103,  0.0795, -0.0778, -0.0534,
1:          0.0058, -0.1271, -0.0769,  0.1071, -0.0990,  0.3993, -0.0421, -0.2292, -0.0816,  0.0628,  0.0206, -0.1923,
1:         -0.3679], device='cuda:0', grad_fn=<SliceBackward0>)
0:      first 25 values: tensor([-0.2461, -0.2461, -0.2461, -0.2461, -0.2461, -0.2438, -0.2438, -0.2392, -0.2091, -0.2450, -0.2450, -0.2461,
0:         -0.2380, -0.2369, -0.2334, -0.2241, -0.1582, -0.0945, -0.2392, -0.2403, -0.2415, -0.2102, -0.1790, -0.1466,
0:         -0.1130], device='cuda:0')
0:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TRAIN TARGET BATCH
0: Epoch -1, batch 0 - Sparse-masked 't2m' shape target data: torch.Size([2546, 2187])
0:     First 25 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.6004,     nan,
0:             nan])
0: [DEBUG] TRAIN PREDICTIONS BATCH
0: Epoch -1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([26101, 972])
0:      first 25 pred values: tensor([ 0.1492,  0.0644, -0.0341, -0.1321, -0.2234, -0.3075, -0.3850, -0.4569, -0.5281, -0.5964, -0.6651, -0.7342,
0:         -0.8068, -0.8819, -0.9571, -1.0206, -1.0640, -1.0880,  0.0194, -0.0784, -0.1917, -0.3016, -0.4034, -0.4937,
0:         -0.5751], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([26021, 972])
0:      first 25 pred values: tensor([1.0099, 0.9930, 0.9761, 0.9692, 0.9672, 0.9759, 0.9915, 1.0113, 1.0304, 1.0540, 1.0867, 1.1294, 1.1790, 1.2306,
0:         1.2765, 1.3185, 1.3531, 1.3770, 1.0210, 1.0124, 1.0071, 1.0073, 1.0211, 1.0432, 1.0798], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([25996, 972])
0:      first 25 pred values: tensor([-0.5723, -0.5705, -0.5659, -0.5631, -0.5610, -0.5568, -0.5546, -0.5507, -0.5475, -0.5454, -0.5435, -0.5415,
0:         -0.5412, -0.5428, -0.5483, -0.5578, -0.5697, -0.5854, -0.5684, -0.5660, -0.5627, -0.5607, -0.5583, -0.5554,
0:         -0.5518], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([26822, 972])
0:      first 25 pred values: tensor([ 0.1250,  0.1278,  0.1028,  0.0585,  0.0170,  0.0029,  0.0291,  0.0660,  0.0968,  0.1111,  0.0891,  0.0505,
0:          0.0163,  0.0050,  0.0055, -0.0157, -0.0614, -0.0989,  0.1067,  0.1068,  0.0915,  0.0709,  0.0499,  0.0295,
0:          0.0334], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([11971, 2187])
0:      first 25 pred values: tensor([-0.4654, -0.4751, -0.4854, -0.4964, -0.5034, -0.5083, -0.5132, -0.5184, -0.5229, -0.5264, -0.5272, -0.5265,
0:         -0.5242, -0.5188, -0.5099, -0.4980, -0.4864, -0.4789, -0.4762, -0.4785, -0.4825, -0.4892, -0.4959, -0.5017,
0:         -0.5072], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([9765, 243])
0:      first 25 pred values: tensor([-0.2299, -0.2314, -0.2324, -0.2321, -0.2334, -0.2317, -0.2310, -0.2280, -0.2249, -0.2327, -0.2334, -0.2325,
0:         -0.2347, -0.2325, -0.2329, -0.2317, -0.2299, -0.2284, -0.2304, -0.2330, -0.2320, -0.2339, -0.2336, -0.2322,
0:         -0.2317], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 't2m' shape: torch.Size([2546, 2187])
0:      first 25 pred values: tensor([ 0.0396, -0.2120,  0.1190, -0.0878,  0.1326, -0.1409,  0.1993, -0.0234, -0.0978,  0.1001,  0.1062,  0.1258,
0:         -0.0437, -0.0361, -0.0666,  0.2168,  0.1305,  0.1925,  0.0144, -0.1782, -0.0236, -0.0219, -0.0688, -0.3027,
0:         -0.3251], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [1/5 (20%)]	Loss: nan : nan :: 0.06671 (1.88 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [2/5 (40%)]	Loss: nan : nan :: 0.06998 (10.18 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [3/5 (60%)]	Loss: nan : nan :: 0.07534 (10.21 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [4/5 (80%)]	Loss: nan : nan :: 0.07269 (10.21 s/sec)
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: validation loss for strategy=BERT at epoch -1 : nan
0: validation loss for velocity_u : 0.0036525712348520756
0: validation loss for velocity_v : 0.004846558906137943
0: validation loss for specific_humidity : 0.005997907370328903
0: validation loss for velocity_z : 0.10981369018554688
0: validation loss for temperature : 0.01824904978275299
0: validation loss for total_precip : 0.328907310962677
0: validation loss for t2m : nan
0: 0 : 23:56:00 :: batch_size = 96, lr = 1e-05
1: 0 : 23:56:00 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for t2m with 10.0% data retained
0: [DEBUG] TRAIN INPUT BATCH
0: Epoch 0, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3820, -0.4175, -0.4530, -0.4858, -0.5147, -0.5407, -0.5630, -0.5794, -0.5921, -0.6045, -0.6157, -0.6245,
0:         -0.6332, -0.6453, -0.6627, -0.6849, -0.7104, -0.7389, -0.3099, -0.3360, -0.3635, -0.3898, -0.4137, -0.4354,
0:         -0.4551], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4546, 0.4692, 0.4811, 0.4912, 0.4987, 0.5034, 0.5048, 0.4979, 0.4890, 0.4794, 0.4693, 0.4561, 0.4426, 0.4292,
0:         0.4226, 0.4146, 0.4068, 0.3938, 0.3587, 0.3700, 0.3829, 0.3924, 0.3968, 0.3951, 0.3963], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3996, -0.4949, -0.6457, -0.7200, -0.7067, -0.7067, -0.7189, -0.7134, -0.6989, -0.6247, -0.4761, -0.3663,
0:         -0.3120, -0.2211, -0.1435, -0.1667, -0.1911, -0.1468, -0.4029, -0.5038, -0.6391, -0.6635, -0.5892, -0.5382,
0:         -0.5116], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0672,  0.2644,  0.1588, -0.0766, -0.0675, -0.0929,  0.0804,  0.1094, -0.1287,  0.1057,  0.0736, -0.1709,
0:         -0.0756, -0.1150, -0.0391,  0.0795,  0.0627,  0.0841, -0.1180,  0.0784,  0.0167, -0.0098, -0.0052, -0.1233,
0:         -0.1108], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451,
0:         -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451,
0:         -0.2451], device='cuda:0')
0:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TRAIN TARGET BATCH
0: Epoch 0, batch 0 - Sparse-masked 't2m' shape target data: torch.Size([2546, 2187])
0:     First 25 batch values:
0: tensor([    nan,     nan, -0.2881,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.8510,     nan,     nan,
0:         -0.7669])
0: [DEBUG] TRAIN PREDICTIONS BATCH
0: Epoch 0, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([26101, 972])
0:      first 25 pred values: tensor([-0.3015, -0.2160, -0.1089,  0.0037,  0.1137,  0.2044,  0.2592,  0.2704,  0.2462,  0.2085,  0.1830,  0.1908,
0:          0.2361,  0.3227,  0.4203,  0.5120,  0.5798,  0.6163, -0.2654, -0.1705, -0.0612,  0.0452,  0.1351,  0.1988,
0:          0.2341], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([26021, 972])
0:      first 25 pred values: tensor([ 0.1881,  0.1769,  0.1642,  0.1532,  0.1334,  0.1015,  0.0629,  0.0208, -0.0192, -0.0492, -0.0741, -0.1002,
0:         -0.1224, -0.1394, -0.1481, -0.1417, -0.1273, -0.1056,  0.2743,  0.2488,  0.2281,  0.2072,  0.1759,  0.1305,
0:          0.0758], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([25996, 972])
0:      first 25 pred values: tensor([-0.4127, -0.4079, -0.4057, -0.4053, -0.4071, -0.4075, -0.4070, -0.4085, -0.4109, -0.4169, -0.4257, -0.4402,
0:         -0.4606, -0.4834, -0.5075, -0.5292, -0.5465, -0.5528, -0.4384, -0.4350, -0.4337, -0.4329, -0.4326, -0.4309,
0:         -0.4288], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([26822, 972])
0:      first 25 pred values: tensor([ 0.9155,  0.9135, -0.3359, -0.0470,  0.2338, -0.4706,  0.1218,  0.6711,  0.1576,  0.0410,  0.1535,  0.1429,
0:          0.1459,  0.1694,  0.4909,  0.6888,  0.3544, -0.1084,  0.9238,  0.5162, -0.4118,  0.4826,  0.7033, -0.3984,
0:          0.3426], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([11971, 2187])
0:      first 25 pred values: tensor([ 0.6072,  0.5843,  0.5584,  0.5221,  0.4752,  0.4239,  0.3777,  0.3454,  0.3228,  0.2983,  0.2621,  0.2111,
0:          0.1518,  0.0963,  0.0518,  0.0189, -0.0071, -0.0307, -0.0516, -0.0719, -0.0914, -0.1126, -0.1349, -0.1575,
0:         -0.1790], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([9765, 243])
0:      first 25 pred values: tensor([-0.2400, -0.2398, -0.2401, -0.2435, -0.2438, -0.2447, -0.2471, -0.2489, -0.2496, -0.2403, -0.2397, -0.2414,
0:         -0.2432, -0.2434, -0.2455, -0.2455, -0.2481, -0.2473, -0.2398, -0.2413, -0.2424, -0.2439, -0.2448, -0.2458,
0:         -0.2456], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 't2m' shape: torch.Size([2546, 2187])
0:      first 25 pred values: tensor([-0.0375, -0.1586,  0.1092, -0.1173,  0.1694, -0.1560,  0.2138, -0.0332, -0.1139,  0.1088,  0.0399,  0.1141,
0:          0.0067,  0.0521, -0.0935,  0.2192,  0.1130,  0.2739, -0.0610, -0.1539, -0.0491, -0.0314, -0.0497, -0.2822,
0:         -0.2881], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for t2m with 10.0% data retained
1: [DEBUG] TRAIN INPUT BATCH
1: Epoch 0, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.7378, 0.7576, 0.7779, 0.7967, 0.8128, 0.8278, 0.8443, 0.8614, 0.8764, 0.8883, 0.8974, 0.9044, 0.9101, 0.9174,
1:         0.9288, 0.9449, 0.9641, 0.9851, 0.6691, 0.6873, 0.7082, 0.7285, 0.7458, 0.7619, 0.7785], device='cuda:0')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.1376,  0.1296,  0.0352, -0.0033, -0.1184, -0.0130,  0.0177,  0.0401, -0.0416, -0.1067, -0.0170,  0.1763,
1:          0.0216, -0.0267, -0.0472,  0.0171, -0.0451, -0.0765, -0.0925, -0.1695,  0.0047,  0.0194,  0.0788,  0.0129,
1:         -0.0739], device='cuda:0')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.4546, 0.4692, 0.4811, 0.4912, 0.4987, 0.5034, 0.5048, 0.4979, 0.4890, 0.4794, 0.4693, 0.4561, 0.4426, 0.4292,
1:         0.4226, 0.4146, 0.4068, 0.3938, 0.3587, 0.3700, 0.3829, 0.3924, 0.3968, 0.3951, 0.3963], device='cuda:0')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.3996, -0.4949, -0.6457, -0.7200, -0.7067, -0.7067, -0.7189, -0.7134, -0.6989, -0.6247, -0.4761, -0.3663,
1:         -0.3120, -0.2211, -0.1435, -0.1667, -0.1911, -0.1468, -0.4029, -0.5038, -0.6391, -0.6635, -0.5892, -0.5382,
1:         -0.5116], device='cuda:0')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([ 0.1405, -0.0007,  0.0218, -0.0238, -0.0637,  0.1975, -0.0428, -0.0772,  0.0322,  0.0620,  0.0640, -0.2729,
1:          0.1707, -0.0626, -0.1336,  0.1260,  0.0613, -0.1467, -0.0244, -0.2098,  0.0753, -0.0585, -0.0481,  0.1923,
1:         -0.2391], device='cuda:0')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([-0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451,
1:         -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451,
1:         -0.2451], device='cuda:0')
1:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
1:        device='cuda:0')
1: [DEBUG] TRAIN TARGET BATCH
1: Epoch 0, batch 0 - Sparse-masked 't2m' shape target data: torch.Size([2388, 2187])
1:     First 25 batch values:
1: tensor([    nan, -1.2028,     nan,     nan,     nan, -0.7813,     nan,     nan,     nan,     nan,  0.0803,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.3431,     nan,     nan,     nan,     nan,     nan,     nan,
1:          0.3379])
1: [DEBUG] TRAIN PREDICTIONS BATCH
1: Epoch 0, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([26119, 972])
1:      first 25 pred values: tensor([-0.3097, -0.2608, -0.2169, -0.1810, -0.1461, -0.1095, -0.0714, -0.0314,  0.0106,  0.0541,  0.1019,  0.1539,
1:          0.2083,  0.2649,  0.3187,  0.3706,  0.4214,  0.4693, -0.4444, -0.3897, -0.3371, -0.2952, -0.2561, -0.2157,
1:         -0.1697], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([26919, 972])
1:      first 25 pred values: tensor([-0.6486, -0.6801, -0.7133, -0.7427, -0.7727, -0.8044, -0.8308, -0.8453, -0.8471, -0.8334, -0.8093, -0.7841,
1:         -0.7621, -0.7483, -0.7430, -0.7430, -0.7437, -0.7416, -0.6207, -0.6491, -0.6719, -0.6867, -0.7017, -0.7188,
1:         -0.7395], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([26542, 972])
1:      first 25 pred values: tensor([-0.4749, -0.4824, -0.4896, -0.4963, -0.5036, -0.5117, -0.5209, -0.5317, -0.5435, -0.5533, -0.5600, -0.5654,
1:         -0.5614, -0.5497, -0.5292, -0.5005, -0.4669, -0.4336, -0.4983, -0.5091, -0.5166, -0.5233, -0.5286, -0.5343,
1:         -0.5407], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([25417, 972])
1:      first 25 pred values: tensor([-0.5547, -0.4785, -0.5964, -0.9285, -1.3392, -1.3236, -0.9088, -0.6168, -0.2689, -0.1348, -0.3832, -0.0376,
1:          0.3370, -0.0038,  0.1423,  0.5084,  0.2153,  0.0054, -0.4127, -0.0887, -0.0927, -0.2788, -0.4452, -0.4395,
1:         -0.3274], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([11804, 2187])
1:      first 25 pred values: tensor([0.8953, 0.9067, 0.9150, 0.9200, 0.9220, 0.9228, 0.9233, 0.9245, 0.9238, 0.9204, 0.9153, 0.9076, 0.8973, 0.8859,
1:         0.8741, 0.8616, 0.8478, 0.8319, 0.8131, 0.7937, 0.7735, 0.7547, 0.7366, 0.7187, 0.6992], device='cuda:0',
1:        grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([10202, 243])
1:      first 25 pred values: tensor([-0.2430, -0.2438, -0.2423, -0.2432, -0.2416, -0.2424, -0.2426, -0.2431, -0.2430, -0.2434, -0.2422, -0.2419,
1:         -0.2409, -0.2403, -0.2396, -0.2427, -0.2415, -0.2431, -0.2404, -0.2417, -0.2416, -0.2397, -0.2396, -0.2383,
1:         -0.2404], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 't2m' shape: torch.Size([2388, 2187])
1:      first 25 pred values: tensor([ 0.0300, -0.1101,  0.0182, -0.1288,  0.1742, -0.1865,  0.0410, -0.1026,  0.0067,  0.0167, -0.0611, -0.0174,
1:          0.0167, -0.1017, -0.0511,  0.1378, -0.0930,  0.3924, -0.1154, -0.1439, -0.0819,  0.0284,  0.0180, -0.2059,
1:         -0.3750], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: 0 [1/5 (20%)]	Loss: nan : nan :: 0.06601 (1.84 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: 0 [2/5 (40%)]	Loss: nan : nan :: 0.06891 (10.20 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: 0 [3/5 (60%)]	Loss: nan : nan :: 0.06963 (10.21 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
0: epoch: 0 [4/5 (80%)]	Loss: nan : nan :: 0.07200 (10.20 s/sec)
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
