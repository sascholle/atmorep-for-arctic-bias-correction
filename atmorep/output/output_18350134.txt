0: Wandb run: atmorep-o95h2aic-18350134
0: l50012:3042126:3042126 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.114<0>
0: l50012:3042126:3042126 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50012:3042126:3042126 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50012:3042126:3042126 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50012:3042126:3042126 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
1: l50015:2450358:2450358 [0] NCCL INFO cudaDriverVersion 12050
1: l50015:2450358:2450358 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.116<0>
1: l50015:2450358:2450358 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
1: l50015:2450358:2450358 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
1: l50015:2450358:2450358 [0] NCCL INFO NET/Plugin: Using internal network plugin.
1: l50015:2450358:2450510 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.116<0>
1: l50015:2450358:2450510 [0] NCCL INFO Using non-device net plugin version 0
1: l50015:2450358:2450510 [0] NCCL INFO Using network IB
0: l50012:3042126:3042525 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.114<0>
0: l50012:3042126:3042525 [0] NCCL INFO Using non-device net plugin version 0
0: l50012:3042126:3042525 [0] NCCL INFO Using network IB
0: l50012:3042126:3042525 [0] NCCL INFO ncclCommInitRank comm 0x55555f293050 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x427f87e2e1c4c300 - Init START
1: l50015:2450358:2450510 [0] NCCL INFO ncclCommInitRank comm 0x55555ee4d140 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x427f87e2e1c4c300 - Init START
0: l50012:3042126:3042525 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
1: l50015:2450358:2450510 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
1: l50015:2450358:2450510 [0] NCCL INFO comm 0x55555ee4d140 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
1: l50015:2450358:2450510 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
1: l50015:2450358:2450510 [0] NCCL INFO P2P Chunksize set to 131072
0: l50012:3042126:3042525 [0] NCCL INFO comm 0x55555f293050 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50012:3042126:3042525 [0] NCCL INFO Channel 00/04 :    0   1
0: l50012:3042126:3042525 [0] NCCL INFO Channel 01/04 :    0   1
0: l50012:3042126:3042525 [0] NCCL INFO Channel 02/04 :    0   1
0: l50012:3042126:3042525 [0] NCCL INFO Channel 03/04 :    0   1
0: l50012:3042126:3042525 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
0: l50012:3042126:3042525 [0] NCCL INFO P2P Chunksize set to 131072
1: l50015:2450358:2450510 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/0
0: l50012:3042126:3042525 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/IB/0
1: l50015:2450358:2450510 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/1
0: l50012:3042126:3042525 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IB/1
1: l50015:2450358:2450510 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/0
1: l50015:2450358:2450510 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/1
0: l50012:3042126:3042525 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/0
1: l50015:2450358:2450510 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/IB/0
0: l50012:3042126:3042525 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/1
1: l50015:2450358:2450510 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/IB/1
0: l50012:3042126:3042525 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/0
1: l50015:2450358:2450510 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/0
0: l50012:3042126:3042525 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/1
0: l50012:3042126:3042525 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/0
1: l50015:2450358:2450510 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/1
0: l50012:3042126:3042525 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/1
1: l50015:2450358:2450513 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 250.
1: l50015:2450358:2450513 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 50.
0: l50012:3042126:3042528 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 250.
0: l50012:3042126:3042528 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 50.
0: l50012:3042126:3042525 [0] NCCL INFO Connected all rings
0: l50012:3042126:3042525 [0] NCCL INFO Connected all trees
1: l50015:2450358:2450510 [0] NCCL INFO Connected all rings
1: l50015:2450358:2450510 [0] NCCL INFO Connected all trees
1: l50015:2450358:2450510 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
1: l50015:2450358:2450510 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50012:3042126:3042525 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
0: l50012:3042126:3042525 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50012:3042126:3042525 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50012:3042126:3042525 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50012:3042126:3042525 [0] NCCL INFO ncclCommInitRank comm 0x55555f293050 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x427f87e2e1c4c300 - Init COMPLETE
1: l50015:2450358:2450510 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
1: l50015:2450358:2450510 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
1: l50015:2450358:2450510 [0] NCCL INFO ncclCommInitRank comm 0x55555ee4d140 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x427f87e2e1c4c300 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 1
0: par_rank : 0
0: par_size : 2
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 0, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 0, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.5, 0.9, 0.2, 0.05], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [0.25, 0.9, 0.1, 0.05]], ['t2m', [1, 1024, ['velocity_u', '
0: velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 2, 4], [3, 27, 27], [0.5, 0.9, 0.2, 0.05], 'Local']]
0: fields_prediction : [['velocity_u', 0.25], ['velocity_v', 0.25], ['specific_humidity', 0.15], ['velocity_z', 0.025], ['temperature', 0.2], ['total_precip', 0.025], ['t2m', 0.1]]
0: fields_targets : []
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[0.0, 360.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 128
0: num_samples_per_epoch : 480
0: num_samples_validate : 128
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : BERT
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 18350134
0: wandb_id : o95h2aic
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25_with_t2m.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : t2m
0: sparse_target_sparsity : 0.9
0: years_test : [2021]
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded AtmoRep: ignoring 208 elements: ['embeds_token_info.6.weight', 'embeds_token_info.6.bias', 'embeds.6.weight', 'embeds.6.bias', 'encoders.6.embed.weight', 'encoders.6.embed.bias', 'encoders.6.heads.0.proj_out.weight', 'encoders.6.heads.0.proj_heads.weight', 'encoders.6.heads.0.proj_heads_other.0.weight', 'encoders.6.heads.0.proj_heads_other.1.weight', 'encoders.6.heads.0.proj_heads_other.2.weight', 'encoders.6.heads.0.proj_heads_other.3.weight', 'encoders.6.heads.0.proj_heads_other.4.weight', 'encoders.6.heads.1.proj_out.weight', 'encoders.6.heads.1.proj_heads.weight', 'encoders.6.heads.1.proj_heads_other.0.weight', 'encoders.6.heads.1.proj_heads_other.1.weight', 'encoders.6.heads.1.proj_heads_other.2.weight', 'encoders.6.heads.1.proj_heads_other.3.weight', 'encoders.6.heads.1.proj_heads_other.4.weight', 'encoders.6.heads.2.proj_out.weight', 'encoders.6.heads.2.proj_heads.weight', 'encoders.6.heads.2.proj_heads_other.0.weight', 'encoders.6.heads.2.proj_heads_other.1.weight', 'encoders.6.heads.2.proj_hea
0: ds_other.2.weight', 'encoders.6.heads.2.proj_heads_other.3.weight', 'encoders.6.heads.2.proj_heads_other.4.weight', 'encoders.6.heads.3.proj_out.weight', 'encoders.6.heads.3.proj_heads.weight', 'encoders.6.heads.3.proj_heads_other.0.weight', 'encoders.6.heads.3.proj_heads_other.1.weight', 'encoders.6.heads.3.proj_heads_other.2.weight', 'encoders.6.heads.3.proj_heads_other.3.weight', 'encoders.6.heads.3.proj_heads_other.4.weight', 'encoders.6.heads.4.proj_out.weight', 'encoders.6.heads.4.proj_heads.weight', 'encoders.6.heads.4.proj_heads_other.0.weight', 'encoders.6.heads.4.proj_heads_other.1.weight', 'encoders.6.heads.4.proj_heads_other.2.weight', 'encoders.6.heads.4.proj_heads_other.3.weight', 'encoders.6.heads.4.proj_heads_other.4.weight', 'encoders.6.heads.5.proj_out.weight', 'encoders.6.heads.5.proj_heads.weight', 'encoders.6.heads.5.proj_heads_other.0.weight', 'encoders.6.heads.5.proj_heads_other.1.weight', 'encoders.6.heads.5.proj_heads_other.2.weight', 'encoders.6.heads.5.proj_heads_other.3.weight', 'e
0: ncoders.6.heads.5.proj_heads_other.4.weight', 'encoders.6.mlps.0.blocks.0.weight', 'encoders.6.mlps.0.blocks.0.bias', 'encoders.6.mlps.0.blocks.3.weight', 'encoders.6.mlps.0.blocks.3.bias', 'encoders.6.mlps.1.blocks.0.weight', 'encoders.6.mlps.1.blocks.0.bias', 'encoders.6.mlps.1.blocks.3.weight', 'encoders.6.mlps.1.blocks.3.bias', 'encoders.6.mlps.2.blocks.0.weight', 'encoders.6.mlps.2.blocks.0.bias', 'encoders.6.mlps.2.blocks.3.weight', 'encoders.6.mlps.2.blocks.3.bias', 'encoders.6.mlps.3.blocks.0.weight', 'encoders.6.mlps.3.blocks.0.bias', 'encoders.6.mlps.3.blocks.3.weight', 'encoders.6.mlps.3.blocks.3.bias', 'encoders.6.mlps.4.blocks.0.weight', 'encoders.6.mlps.4.blocks.0.bias', 'encoders.6.mlps.4.blocks.3.weight', 'encoders.6.mlps.4.blocks.3.bias', 'encoders.6.mlps.5.blocks.0.weight', 'encoders.6.mlps.5.blocks.0.bias', 'encoders.6.mlps.5.blocks.3.weight', 'encoders.6.mlps.5.blocks.3.bias', 'decoders.6.blocks.0.proj_heads.weight', 'decoders.6.blocks.0.proj_heads_o_q.weight', 'decoders.6.blocks.0.proj_he
0: ads_o_kv.weight', 'decoders.6.blocks.0.ln_q.weight', 'decoders.6.blocks.0.ln_q.bias', 'decoders.6.blocks.0.ln_k.weight', 'decoders.6.blocks.0.ln_k.bias', 'decoders.6.blocks.0.proj_out.weight', 'decoders.6.blocks.1.blocks.0.weight', 'decoders.6.blocks.1.blocks.0.bias', 'decoders.6.blocks.1.blocks.3.weight', 'decoders.6.blocks.1.blocks.3.bias', 'decoders.6.blocks.2.proj_heads.weight', 'decoders.6.blocks.2.proj_heads_o_q.weight', 'decoders.6.blocks.2.proj_heads_o_kv.weight', 'decoders.6.blocks.2.ln_q.weight', 'decoders.6.blocks.2.ln_q.bias', 'decoders.6.blocks.2.ln_k.weight', 'decoders.6.blocks.2.ln_k.bias', 'decoders.6.blocks.2.proj_out.weight', 'decoders.6.blocks.3.blocks.0.weight', 'decoders.6.blocks.3.blocks.0.bias', 'decoders.6.blocks.3.blocks.3.weight', 'decoders.6.blocks.3.blocks.3.bias', 'decoders.6.blocks.4.proj_heads.weight', 'decoders.6.blocks.4.proj_heads_o_q.weight', 'decoders.6.blocks.4.proj_heads_o_kv.weight', 'decoders.6.blocks.4.ln_q.weight', 'decoders.6.blocks.4.ln_q.bias', 'decoders.6.blocks.4
0: .ln_k.weight', 'decoders.6.blocks.4.ln_k.bias', 'decoders.6.blocks.4.proj_out.weight', 'decoders.6.blocks.5.blocks.0.weight', 'decoders.6.blocks.5.blocks.0.bias', 'decoders.6.blocks.5.blocks.3.weight', 'decoders.6.blocks.5.blocks.3.bias', 'decoders.6.blocks.6.proj_heads.weight', 'decoders.6.blocks.6.proj_heads_o_q.weight', 'decoders.6.blocks.6.proj_heads_o_kv.weight', 'decoders.6.blocks.6.ln_q.weight', 'decoders.6.blocks.6.ln_q.bias', 'decoders.6.blocks.6.ln_k.weight', 'decoders.6.blocks.6.ln_k.bias', 'decoders.6.blocks.6.proj_out.weight', 'decoders.6.blocks.7.blocks.0.weight', 'decoders.6.blocks.7.blocks.0.bias', 'decoders.6.blocks.7.blocks.3.weight', 'decoders.6.blocks.7.blocks.3.bias', 'decoders.6.blocks.8.proj_heads.weight', 'decoders.6.blocks.8.proj_heads_o_q.weight', 'decoders.6.blocks.8.proj_heads_o_kv.weight', 'decoders.6.blocks.8.ln_q.weight', 'decoders.6.blocks.8.ln_q.bias', 'decoders.6.blocks.8.ln_k.weight', 'decoders.6.blocks.8.ln_k.bias', 'decoders.6.blocks.8.proj_out.weight', 'decoders.6.blocks.
0: 9.blocks.0.weight', 'decoders.6.blocks.9.blocks.0.bias', 'decoders.6.blocks.9.blocks.3.weight', 'decoders.6.blocks.9.blocks.3.bias', 'decoders.6.blocks.10.proj_heads.weight', 'decoders.6.blocks.10.proj_heads_o_q.weight', 'decoders.6.blocks.10.proj_heads_o_kv.weight', 'decoders.6.blocks.10.ln_q.weight', 'decoders.6.blocks.10.ln_q.bias', 'decoders.6.blocks.10.ln_k.weight', 'decoders.6.blocks.10.ln_k.bias', 'decoders.6.blocks.10.proj_out.weight', 'decoders.6.blocks.11.blocks.0.weight', 'decoders.6.blocks.11.blocks.0.bias', 'decoders.6.blocks.11.blocks.3.weight', 'decoders.6.blocks.11.blocks.3.bias', 'tails.6.tail_nets.0.0.weight', 'tails.6.tail_nets.0.0.bias', 'tails.6.tail_nets.0.1.weight', 'tails.6.tail_nets.0.1.bias', 'tails.6.tail_nets.1.0.weight', 'tails.6.tail_nets.1.0.bias', 'tails.6.tail_nets.1.1.weight', 'tails.6.tail_nets.1.1.bias', 'tails.6.tail_nets.2.0.weight', 'tails.6.tail_nets.2.0.bias', 'tails.6.tail_nets.2.1.weight', 'tails.6.tail_nets.2.1.bias', 'tails.6.tail_nets.3.0.weight', 'tails.6.tail_ne
0: ts.3.0.bias', 'tails.6.tail_nets.3.1.weight', 'tails.6.tail_nets.3.1.bias', 'tails.6.tail_nets.4.0.weight', 'tails.6.tail_nets.4.0.bias', 'tails.6.tail_nets.4.1.weight', 'tails.6.tail_nets.4.1.bias', 'tails.6.tail_nets.5.0.weight', 'tails.6.tail_nets.5.0.bias', 'tails.6.tail_nets.5.1.weight', 'tails.6.tail_nets.5.1.bias', 'tails.6.tail_nets.6.0.weight', 'tails.6.tail_nets.6.0.bias', 'tails.6.tail_nets.6.1.weight', 'tails.6.tail_nets.6.1.bias', 'tails.6.tail_nets.7.0.weight', 'tails.6.tail_nets.7.0.bias', 'tails.6.tail_nets.7.1.weight', 'tails.6.tail_nets.7.1.bias', 'tails.6.tail_nets.8.0.weight', 'tails.6.tail_nets.8.0.bias', 'tails.6.tail_nets.8.1.weight', 'tails.6.tail_nets.8.1.bias', 'tails.6.tail_nets.9.0.weight', 'tails.6.tail_nets.9.0.bias', 'tails.6.tail_nets.9.1.weight', 'tails.6.tail_nets.9.1.bias', 'tails.6.tail_nets.10.0.weight', 'tails.6.tail_nets.10.0.bias', 'tails.6.tail_nets.10.1.weight', 'tails.6.tail_nets.10.1.bias', 'tails.6.tail_nets.11.0.weight', 'tails.6.tail_nets.11.0.bias', 'tails.6.tai
0: l_nets.11.1.weight', 'tails.6.tail_nets.11.1.bias', 'tails.6.tail_nets.12.0.weight', 'tails.6.tail_nets.12.0.bias', 'tails.6.tail_nets.12.1.weight', 'tails.6.tail_nets.12.1.bias', 'tails.6.tail_nets.13.0.weight', 'tails.6.tail_nets.13.0.bias', 'tails.6.tail_nets.13.1.weight', 'tails.6.tail_nets.13.1.bias', 'tails.6.tail_nets.14.0.weight', 'tails.6.tail_nets.14.0.bias', 'tails.6.tail_nets.14.1.weight', 'tails.6.tail_nets.14.1.bias', 'tails.6.tail_nets.15.0.weight', 'tails.6.tail_nets.15.0.bias', 'tails.6.tail_nets.15.1.weight', 'tails.6.tail_nets.15.1.bias']
1: Loaded AtmoRep: ignoring 208 elements: ['embeds_token_info.6.weight', 'embeds_token_info.6.bias', 'embeds.6.weight', 'embeds.6.bias', 'encoders.6.embed.weight', 'encoders.6.embed.bias', 'encoders.6.heads.0.proj_out.weight', 'encoders.6.heads.0.proj_heads.weight', 'encoders.6.heads.0.proj_heads_other.0.weight', 'encoders.6.heads.0.proj_heads_other.1.weight', 'encoders.6.heads.0.proj_heads_other.2.weight', 'encoders.6.heads.0.proj_heads_other.3.weight', 'encoders.6.heads.0.proj_heads_other.4.weight', 'encoders.6.heads.1.proj_out.weight', 'encoders.6.heads.1.proj_heads.weight', 'encoders.6.heads.1.proj_heads_other.0.weight', 'encoders.6.heads.1.proj_heads_other.1.weight', 'encoders.6.heads.1.proj_heads_other.2.weight', 'encoders.6.heads.1.proj_heads_other.3.weight', 'encoders.6.heads.1.proj_heads_other.4.weight', 'encoders.6.heads.2.proj_out.weight', 'encoders.6.heads.2.proj_heads.weight', 'encoders.6.heads.2.proj_heads_other.0.weight', 'encoders.6.heads.2.proj_heads_other.1.weight', 'encoders.6.heads.2.proj_hea
1: ds_other.2.weight', 'encoders.6.heads.2.proj_heads_other.3.weight', 'encoders.6.heads.2.proj_heads_other.4.weight', 'encoders.6.heads.3.proj_out.weight', 'encoders.6.heads.3.proj_heads.weight', 'encoders.6.heads.3.proj_heads_other.0.weight', 'encoders.6.heads.3.proj_heads_other.1.weight', 'encoders.6.heads.3.proj_heads_other.2.weight', 'encoders.6.heads.3.proj_heads_other.3.weight', 'encoders.6.heads.3.proj_heads_other.4.weight', 'encoders.6.heads.4.proj_out.weight', 'encoders.6.heads.4.proj_heads.weight', 'encoders.6.heads.4.proj_heads_other.0.weight', 'encoders.6.heads.4.proj_heads_other.1.weight', 'encoders.6.heads.4.proj_heads_other.2.weight', 'encoders.6.heads.4.proj_heads_other.3.weight', 'encoders.6.heads.4.proj_heads_other.4.weight', 'encoders.6.heads.5.proj_out.weight', 'encoders.6.heads.5.proj_heads.weight', 'encoders.6.heads.5.proj_heads_other.0.weight', 'encoders.6.heads.5.proj_heads_other.1.weight', 'encoders.6.heads.5.proj_heads_other.2.weight', 'encoders.6.heads.5.proj_heads_other.3.weight', 'e
1: ncoders.6.heads.5.proj_heads_other.4.weight', 'encoders.6.mlps.0.blocks.0.weight', 'encoders.6.mlps.0.blocks.0.bias', 'encoders.6.mlps.0.blocks.3.weight', 'encoders.6.mlps.0.blocks.3.bias', 'encoders.6.mlps.1.blocks.0.weight', 'encoders.6.mlps.1.blocks.0.bias', 'encoders.6.mlps.1.blocks.3.weight', 'encoders.6.mlps.1.blocks.3.bias', 'encoders.6.mlps.2.blocks.0.weight', 'encoders.6.mlps.2.blocks.0.bias', 'encoders.6.mlps.2.blocks.3.weight', 'encoders.6.mlps.2.blocks.3.bias', 'encoders.6.mlps.3.blocks.0.weight', 'encoders.6.mlps.3.blocks.0.bias', 'encoders.6.mlps.3.blocks.3.weight', 'encoders.6.mlps.3.blocks.3.bias', 'encoders.6.mlps.4.blocks.0.weight', 'encoders.6.mlps.4.blocks.0.bias', 'encoders.6.mlps.4.blocks.3.weight', 'encoders.6.mlps.4.blocks.3.bias', 'encoders.6.mlps.5.blocks.0.weight', 'encoders.6.mlps.5.blocks.0.bias', 'encoders.6.mlps.5.blocks.3.weight', 'encoders.6.mlps.5.blocks.3.bias', 'decoders.6.blocks.0.proj_heads.weight', 'decoders.6.blocks.0.proj_heads_o_q.weight', 'decoders.6.blocks.0.proj_he
1: ads_o_kv.weight', 'decoders.6.blocks.0.ln_q.weight', 'decoders.6.blocks.0.ln_q.bias', 'decoders.6.blocks.0.ln_k.weight', 'decoders.6.blocks.0.ln_k.bias', 'decoders.6.blocks.0.proj_out.weight', 'decoders.6.blocks.1.blocks.0.weight', 'decoders.6.blocks.1.blocks.0.bias', 'decoders.6.blocks.1.blocks.3.weight', 'decoders.6.blocks.1.blocks.3.bias', 'decoders.6.blocks.2.proj_heads.weight', 'decoders.6.blocks.2.proj_heads_o_q.weight', 'decoders.6.blocks.2.proj_heads_o_kv.weight', 'decoders.6.blocks.2.ln_q.weight', 'decoders.6.blocks.2.ln_q.bias', 'decoders.6.blocks.2.ln_k.weight', 'decoders.6.blocks.2.ln_k.bias', 'decoders.6.blocks.2.proj_out.weight', 'decoders.6.blocks.3.blocks.0.weight', 'decoders.6.blocks.3.blocks.0.bias', 'decoders.6.blocks.3.blocks.3.weight', 'decoders.6.blocks.3.blocks.3.bias', 'decoders.6.blocks.4.proj_heads.weight', 'decoders.6.blocks.4.proj_heads_o_q.weight', 'decoders.6.blocks.4.proj_heads_o_kv.weight', 'decoders.6.blocks.4.ln_q.weight', 'decoders.6.blocks.4.ln_q.bias', 'decoders.6.blocks.4
1: .ln_k.weight', 'decoders.6.blocks.4.ln_k.bias', 'decoders.6.blocks.4.proj_out.weight', 'decoders.6.blocks.5.blocks.0.weight', 'decoders.6.blocks.5.blocks.0.bias', 'decoders.6.blocks.5.blocks.3.weight', 'decoders.6.blocks.5.blocks.3.bias', 'decoders.6.blocks.6.proj_heads.weight', 'decoders.6.blocks.6.proj_heads_o_q.weight', 'decoders.6.blocks.6.proj_heads_o_kv.weight', 'decoders.6.blocks.6.ln_q.weight', 'decoders.6.blocks.6.ln_q.bias', 'decoders.6.blocks.6.ln_k.weight', 'decoders.6.blocks.6.ln_k.bias', 'decoders.6.blocks.6.proj_out.weight', 'decoders.6.blocks.7.blocks.0.weight', 'decoders.6.blocks.7.blocks.0.bias', 'decoders.6.blocks.7.blocks.3.weight', 'decoders.6.blocks.7.blocks.3.bias', 'decoders.6.blocks.8.proj_heads.weight', 'decoders.6.blocks.8.proj_heads_o_q.weight', 'decoders.6.blocks.8.proj_heads_o_kv.weight', 'decoders.6.blocks.8.ln_q.weight', 'decoders.6.blocks.8.ln_q.bias', 'decoders.6.blocks.8.ln_k.weight', 'decoders.6.blocks.8.ln_k.bias', 'decoders.6.blocks.8.proj_out.weight', 'decoders.6.blocks.
1: 9.blocks.0.weight', 'decoders.6.blocks.9.blocks.0.bias', 'decoders.6.blocks.9.blocks.3.weight', 'decoders.6.blocks.9.blocks.3.bias', 'decoders.6.blocks.10.proj_heads.weight', 'decoders.6.blocks.10.proj_heads_o_q.weight', 'decoders.6.blocks.10.proj_heads_o_kv.weight', 'decoders.6.blocks.10.ln_q.weight', 'decoders.6.blocks.10.ln_q.bias', 'decoders.6.blocks.10.ln_k.weight', 'decoders.6.blocks.10.ln_k.bias', 'decoders.6.blocks.10.proj_out.weight', 'decoders.6.blocks.11.blocks.0.weight', 'decoders.6.blocks.11.blocks.0.bias', 'decoders.6.blocks.11.blocks.3.weight', 'decoders.6.blocks.11.blocks.3.bias', 'tails.6.tail_nets.0.0.weight', 'tails.6.tail_nets.0.0.bias', 'tails.6.tail_nets.0.1.weight', 'tails.6.tail_nets.0.1.bias', 'tails.6.tail_nets.1.0.weight', 'tails.6.tail_nets.1.0.bias', 'tails.6.tail_nets.1.1.weight', 'tails.6.tail_nets.1.1.bias', 'tails.6.tail_nets.2.0.weight', 'tails.6.tail_nets.2.0.bias', 'tails.6.tail_nets.2.1.weight', 'tails.6.tail_nets.2.1.bias', 'tails.6.tail_nets.3.0.weight', 'tails.6.tail_ne
1: ts.3.0.bias', 'tails.6.tail_nets.3.1.weight', 'tails.6.tail_nets.3.1.bias', 'tails.6.tail_nets.4.0.weight', 'tails.6.tail_nets.4.0.bias', 'tails.6.tail_nets.4.1.weight', 'tails.6.tail_nets.4.1.bias', 'tails.6.tail_nets.5.0.weight', 'tails.6.tail_nets.5.0.bias', 'tails.6.tail_nets.5.1.weight', 'tails.6.tail_nets.5.1.bias', 'tails.6.tail_nets.6.0.weight', 'tails.6.tail_nets.6.0.bias', 'tails.6.tail_nets.6.1.weight', 'tails.6.tail_nets.6.1.bias', 'tails.6.tail_nets.7.0.weight', 'tails.6.tail_nets.7.0.bias', 'tails.6.tail_nets.7.1.weight', 'tails.6.tail_nets.7.1.bias', 'tails.6.tail_nets.8.0.weight', 'tails.6.tail_nets.8.0.bias', 'tails.6.tail_nets.8.1.weight', 'tails.6.tail_nets.8.1.bias', 'tails.6.tail_nets.9.0.weight', 'tails.6.tail_nets.9.0.bias', 'tails.6.tail_nets.9.1.weight', 'tails.6.tail_nets.9.1.bias', 'tails.6.tail_nets.10.0.weight', 'tails.6.tail_nets.10.0.bias', 'tails.6.tail_nets.10.1.weight', 'tails.6.tail_nets.10.1.bias', 'tails.6.tail_nets.11.0.weight', 'tails.6.tail_nets.11.0.bias', 'tails.6.tai
1: l_nets.11.1.weight', 'tails.6.tail_nets.11.1.bias', 'tails.6.tail_nets.12.0.weight', 'tails.6.tail_nets.12.0.bias', 'tails.6.tail_nets.12.1.weight', 'tails.6.tail_nets.12.1.bias', 'tails.6.tail_nets.13.0.weight', 'tails.6.tail_nets.13.0.bias', 'tails.6.tail_nets.13.1.weight', 'tails.6.tail_nets.13.1.bias', 'tails.6.tail_nets.14.0.weight', 'tails.6.tail_nets.14.0.bias', 'tails.6.tail_nets.14.1.weight', 'tails.6.tail_nets.14.1.bias', 'tails.6.tail_nets.15.0.weight', 'tails.6.tail_nets.15.0.bias', 'tails.6.tail_nets.15.1.weight', 'tails.6.tail_nets.15.1.bias']
1: Loaded model id = wc5e2i3t.
1: Loaded run 'wc5e2i3t' at epoch -2.
0: Loaded model id = wc5e2i3t.
0: Loaded run 'wc5e2i3t' at epoch -2.
1: -1 : 14:45:15 :: batch_size = 96, lr = 1e-05
0: Number of trainable parameters: 886,234,640
0: -1 : 14:45:15 :: batch_size = 96, lr = 1e-05
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch -1, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7007, -0.7030, -0.7031, -0.7005, -0.6949, -0.6859, -0.6745, -0.6621, -0.6501, -0.6380, -0.6256, -0.6129,
1:         -0.5998, -0.5853, -0.5684, -0.5501, -0.5303, -0.5083, -0.7175, -0.7193, -0.7198, -0.7180, -0.7139, -0.7071,
1:         -0.6977], device='cuda:0')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-1.4603, -1.4850, -1.5082, -1.5303, -1.5526, -1.5741, -1.5936, -1.6111, -1.6272, -1.6413, -1.6523, -1.6610,
1:         -1.6666, -1.6681, -1.6651, -1.6590, -1.6508, -1.6400, -1.4059, -1.4288, -1.4518, -1.4750, -1.4986, -1.5214,
1:         -1.5418], device='cuda:0')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.3862, -0.4172, -0.4461, -0.4736, -0.4985, -0.5184, -0.5373, -0.5527, -0.5674, -0.5799, -0.5914, -0.6024,
1:         -0.6109, -0.6187, -0.6234, -0.6287, -0.6282, -0.6272, -0.3862, -0.4145, -0.4418, -0.4648, -0.4895, -0.5106,
1:         -0.5306], device='cuda:0')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.6047, 0.6402, 0.6735, 0.6690, 0.6136, 0.5403, 0.4849, 0.4361, 0.3784, 0.3274, 0.3140, 0.3185, 0.3118, 0.3030,
1:         0.3207, 0.3473, 0.3451, 0.3140, 0.4494, 0.5426, 0.6202, 0.6668, 0.6979, 0.7156, 0.6934], device='cuda:0')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.9869, 0.9862, 0.7789, 0.9768, 0.8807, 0.7809, 0.6941, 0.9047, 0.9572, 0.8163, 0.7765, 0.7982, 0.8328, 0.7979,
1:         0.7039, 0.8718, 0.6100, 0.8425, 0.7158, 0.9516, 0.7186, 0.7559, 0.7158, 0.4272, 0.5267], device='cuda:0')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],
1:        device='cuda:0')
1:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([ 0.0561,  0.0522,  0.0737, -0.0117, -0.0885, -0.0019, -0.0460,  0.0063, -0.0517,  0.0043,  0.1308,  0.0354,
1:         -0.0617,  0.0197, -0.0543,  0.0758,  0.1490,  0.0680, -0.1060, -0.1529,  0.0203, -0.0700, -0.0715, -0.0265,
1:         -0.0558], device='cuda:0')
1: [DEBUG] TARGET BATCH
1: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([10814, 243])
1: [DEBUG] First 243 batch values:
1: tensor([-0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2457, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2231, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.1983, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
1:         -0.2547, -0.2547, -0.2547])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch -1, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([25704, 972])
1:      first 25 pred values: tensor([-1.5033, -1.4853, -1.4574, -1.4200, -1.3717, -1.3092, -1.2433, -1.1681, -1.0953, -1.0285, -0.9709, -0.9120,
1:         -0.8534, -0.7887, -0.7168, -0.6457, -0.5891, -0.5514, -1.3974, -1.3893, -1.3613, -1.3234, -1.2750, -1.2132,
1:         -1.1404], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([26011, 972])
1:      first 25 pred values: tensor([ 0.2075,  0.1932,  0.1856,  0.1789,  0.1722,  0.1569,  0.1402,  0.1220,  0.1069,  0.0863,  0.0597,  0.0180,
1:         -0.0319, -0.0883, -0.1405, -0.1874, -0.2227, -0.2531,  0.2430,  0.1979,  0.1608,  0.1356,  0.1182,  0.1025,
1:          0.0919], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([26718, 972])
1:      first 25 pred values: tensor([-0.6479, -0.6586, -0.6474, -0.6277, -0.6029, -0.5791, -0.5638, -0.5545, -0.5499, -0.5430, -0.5292, -0.5153,
1:         -0.4976, -0.4855, -0.4815, -0.4905, -0.4991, -0.5083, -0.6461, -0.6538, -0.6406, -0.6220, -0.5999, -0.5822,
1:         -0.5626], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([25717, 972])
1:      first 25 pred values: tensor([-0.2089, -0.1132, -0.0185,  0.0414,  0.0589,  0.0630,  0.0547,  0.0741,  0.0894,  0.0589,  0.0563,  0.0618,
1:          0.0018, -0.0713, -0.1011, -0.1221, -0.1436, -0.1213, -0.1359, -0.0368,  0.0461,  0.0875,  0.0785,  0.0577,
1:          0.0260], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([11591, 2187])
1:      first 25 pred values: tensor([-0.1116, -0.1139, -0.1291, -0.1572, -0.1921, -0.2263, -0.2530, -0.2755, -0.2941, -0.3129, -0.3328, -0.3480,
1:         -0.3527, -0.3494, -0.3433, -0.3495, -0.3818, -0.4470, -0.5402, -0.6539, -0.7749, -0.8944, -1.0048, -1.1035,
1:         -1.1946], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([10814, 243])
1:      first 25 pred values: tensor([-0.2471, -0.2485, -0.2496, -0.2520, -0.2521, -0.2519, -0.2525, -0.2518, -0.2496, -0.2472, -0.2493, -0.2511,
1:         -0.2524, -0.2523, -0.2531, -0.2515, -0.2512, -0.2489, -0.2483, -0.2485, -0.2508, -0.2533, -0.2541, -0.2525,
1:         -0.2526], device='cuda:0', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 't2m' shape: torch.Size([2494, 2187])
1:      first 25 pred values: tensor([ 0.1744,  0.0240, -0.0159, -0.1136,  0.0412, -0.0545, -0.0184,  0.1212, -0.0762,  0.0696, -0.1549,  0.0941,
1:          0.0883, -0.2476,  0.0522,  0.1613, -0.1994, -0.0226, -0.0048, -0.2317, -0.1064,  0.0835, -0.0107,  0.0402,
1:         -0.0829], device='cuda:0', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch -1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7007, -0.7030, -0.7031, -0.7005, -0.6949, -0.6859, -0.6745, -0.6621, -0.6501, -0.6380, -0.6256, -0.6129,
0:         -0.5998, -0.5853, -0.5684, -0.5501, -0.5303, -0.5083, -0.7175, -0.7193, -0.7198, -0.7180, -0.7139, -0.7071,
0:         -0.6977], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3862, -0.4172, -0.4461, -0.4736, -0.4985, -0.5184, -0.5373, -0.5527, -0.5674, -0.5799, -0.5914, -0.6024,
0:         -0.6109, -0.6187, -0.6234, -0.6287, -0.6282, -0.6272, -0.3862, -0.4145, -0.4418, -0.4648, -0.4895, -0.5106,
0:         -0.5306], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6047, 0.6402, 0.6735, 0.6690, 0.6136, 0.5403, 0.4849, 0.4361, 0.3784, 0.3274, 0.3140, 0.3185, 0.3118, 0.3030,
0:         0.3207, 0.3473, 0.3451, 0.3140, 0.4494, 0.5426, 0.6202, 0.6668, 0.6979, 0.7156, 0.6934], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2525, -0.2525, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547], device='cuda:0')
0:   └─ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([9355, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547, -0.2547,
0:         -0.2547, -0.2547, -0.2547])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch -1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([26810, 972])
0:      first 25 pred values: tensor([0.1330, 0.1341, 0.1387, 0.1486, 0.1649, 0.1891, 0.2152, 0.2382, 0.2590, 0.2730, 0.2833, 0.2870, 0.2858, 0.2835,
0:         0.2714, 0.2614, 0.2588, 0.2626, 0.1269, 0.1352, 0.1509, 0.1672, 0.1887, 0.2116, 0.2368], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([25440, 972])
0:      first 25 pred values: tensor([-0.3029, -0.3800, -0.4684, -0.5619, -0.6544, -0.7397, -0.8124, -0.8768, -0.9300, -0.9758, -1.0127, -1.0440,
0:         -1.0679, -1.0893, -1.1076, -1.1200, -1.1264, -1.1212, -0.1664, -0.2344, -0.3143, -0.3995, -0.4811, -0.5585,
0:         -0.6273], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([26425, 972])
0:      first 25 pred values: tensor([-0.6757, -0.6838, -0.6878, -0.6851, -0.6808, -0.6737, -0.6623, -0.6462, -0.6265, -0.6047, -0.5793, -0.5520,
0:         -0.5241, -0.4998, -0.4773, -0.4579, -0.4432, -0.4282, -0.6618, -0.6632, -0.6582, -0.6492, -0.6350, -0.6187,
0:         -0.5987], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([25661, 972])
0:      first 25 pred values: tensor([0.4105, 0.5913, 0.6303, 0.5926, 0.5538, 0.5468, 0.6055, 0.6941, 0.7569, 0.8041, 0.8491, 0.8895, 0.9637, 1.0309,
0:         1.0123, 0.9041, 0.7936, 0.6852, 0.3743, 0.4964, 0.5194, 0.5509, 0.6269, 0.7074, 0.7649], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([12474, 2187])
0:      first 25 pred values: tensor([-0.0495, -0.0320, -0.0068,  0.0268,  0.0606,  0.0938,  0.1214,  0.1390,  0.1444,  0.1415,  0.1350,  0.1300,
0:          0.1283,  0.1261,  0.1193,  0.1077,  0.0990,  0.1014,  0.1191,  0.1440,  0.1617,  0.1614,  0.1442,  0.1208,
0:          0.0992], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([9355, 243])
0:      first 25 pred values: tensor([-0.2561, -0.2566, -0.2545, -0.2528, -0.2528, -0.2523, -0.2541, -0.2546, -0.2537, -0.2571, -0.2553, -0.2528,
0:         -0.2528, -0.2517, -0.2504, -0.2537, -0.2533, -0.2535, -0.2559, -0.2572, -0.2530, -0.2529, -0.2507, -0.2526,
0:         -0.2528], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 't2m' shape: torch.Size([2248, 2187])
0:      first 25 pred values: tensor([ 0.0975, -0.0993, -0.2187, -0.3076, -0.0236, -0.1687,  0.1158, -0.0798, -0.1302,  0.1607, -0.0017, -0.0118,
0:          0.0785, -0.2951,  0.0248,  0.2352, -0.0120, -0.0448, -0.2058, -0.3197, -0.0430, -0.0903,  0.0932, -0.0927,
0:         -0.1123], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [1/5 (20%)]	Loss: nan : nan :: 0.07057 (1.42 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [2/5 (40%)]	Loss: nan : nan :: 0.07153 (10.20 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [3/5 (60%)]	Loss: nan : nan :: 0.06763 (10.22 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [4/5 (80%)]	Loss: nan : nan :: 0.07039 (10.22 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_embeds_token_info_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_encoder_velocity_u_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_encoder_velocity_v_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_encoder_specific_humidity_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_encoder_velocity_z_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_encoder_temperature_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_encoder_total_precip_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_encoder_t2m_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_decoder_velocity_u_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_decoder_velocity_v_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_decoder_specific_humidity_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_decoder_velocity_z_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_decoder_temperature_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_decoder_total_precip_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_decoder_t2m_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_tail_velocity_u_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_tail_velocity_v_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_tail_specific_humidity_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_tail_velocity_z_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_tail_temperature_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_tail_total_precip_ido95h2aic_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/ido95h2aic/AtmoRep_tail_t2m_ido95h2aic_epoch-1.mod
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
