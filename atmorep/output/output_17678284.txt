0: Wandb run: atmorep-pjppryaq-17678284
0: l50012:1350250:1350250 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.114<0>
0: l50012:1350250:1350250 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50012:1350250:1350250 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50012:1350250:1350250 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50012:1350250:1350250 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
1: l50012:1350251:1350251 [1] NCCL INFO cudaDriverVersion 12050
2: l50012:1350252:1350252 [2] NCCL INFO cudaDriverVersion 12050
3: l50012:1350253:1350253 [3] NCCL INFO cudaDriverVersion 12050
1: l50012:1350251:1350251 [1] NCCL INFO Bootstrap : Using ib0:10.128.11.114<0>
1: l50012:1350251:1350251 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
1: l50012:1350251:1350251 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
1: l50012:1350251:1350251 [1] NCCL INFO NET/Plugin: Using internal network plugin.
2: l50012:1350252:1350252 [2] NCCL INFO Bootstrap : Using ib0:10.128.11.114<0>
3: l50012:1350253:1350253 [3] NCCL INFO Bootstrap : Using ib0:10.128.11.114<0>
2: l50012:1350252:1350252 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
3: l50012:1350253:1350253 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
2: l50012:1350252:1350252 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
2: l50012:1350252:1350252 [2] NCCL INFO NET/Plugin: Using internal network plugin.
3: l50012:1350253:1350253 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
3: l50012:1350253:1350253 [3] NCCL INFO NET/Plugin: Using internal network plugin.
2: l50012:1350252:1350727 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.114<0>
3: l50012:1350253:1350726 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.114<0>
2: l50012:1350252:1350727 [2] NCCL INFO Using non-device net plugin version 0
2: l50012:1350252:1350727 [2] NCCL INFO Using network IB
3: l50012:1350253:1350726 [3] NCCL INFO Using non-device net plugin version 0
3: l50012:1350253:1350726 [3] NCCL INFO Using network IB
0: l50012:1350250:1350715 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.114<0>
0: l50012:1350250:1350715 [0] NCCL INFO Using non-device net plugin version 0
0: l50012:1350250:1350715 [0] NCCL INFO Using network IB
1: l50012:1350251:1350725 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.114<0>
1: l50012:1350251:1350725 [1] NCCL INFO Using non-device net plugin version 0
1: l50012:1350251:1350725 [1] NCCL INFO Using network IB
3: l50012:1350253:1350726 [3] NCCL INFO ncclCommInitRank comm 0x55555ee28100 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c4000 commId 0xc27b17ba9a5adf35 - Init START
0: l50012:1350250:1350715 [0] NCCL INFO ncclCommInitRank comm 0x55555f26d2a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3000 commId 0xc27b17ba9a5adf35 - Init START
1: l50012:1350251:1350725 [1] NCCL INFO ncclCommInitRank comm 0x55555ee28440 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 44000 commId 0xc27b17ba9a5adf35 - Init START
2: l50012:1350252:1350727 [2] NCCL INFO ncclCommInitRank comm 0x55555ee282d0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 84000 commId 0xc27b17ba9a5adf35 - Init START
1: l50012:1350251:1350725 [1] NCCL INFO NVLS multicast support is not available on dev 1
2: l50012:1350252:1350727 [2] NCCL INFO NVLS multicast support is not available on dev 2
3: l50012:1350253:1350726 [3] NCCL INFO NVLS multicast support is not available on dev 3
0: l50012:1350250:1350715 [0] NCCL INFO NVLS multicast support is not available on dev 0
1: l50012:1350251:1350725 [1] NCCL INFO comm 0x55555ee28440 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
3: l50012:1350253:1350726 [3] NCCL INFO comm 0x55555ee28100 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
0: l50012:1350250:1350715 [0] NCCL INFO comm 0x55555f26d2a0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
0: l50012:1350250:1350715 [0] NCCL INFO Channel 00/24 :    0   1   2   3
0: l50012:1350250:1350715 [0] NCCL INFO Channel 01/24 :    0   1   3   2
0: l50012:1350250:1350715 [0] NCCL INFO Channel 02/24 :    0   2   3   1
0: l50012:1350250:1350715 [0] NCCL INFO Channel 03/24 :    0   2   1   3
0: l50012:1350250:1350715 [0] NCCL INFO Channel 04/24 :    0   3   1   2
0: l50012:1350250:1350715 [0] NCCL INFO Channel 05/24 :    0   3   2   1
2: l50012:1350252:1350727 [2] NCCL INFO comm 0x55555ee282d0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
3: l50012:1350253:1350726 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->1 [5] -1/-1/-1->3->1 [6] -1/-1/-1->3->1 [7] -1/-1/-1->3->1 [8] 2/-1/-1->3->0 [9] 2/-1/-1->3->0 [10] 2/-1/-1->3->0 [11] 2/-1/-1->3->0 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->1 [17] -1/-1/-1->3->1 [18] -1/-1/-1->3->1 [19] -1/-1/-1->3->1 [20] 2/-1/-1->3->0 [21] 2/-1/-1->3->0 [22] 2/-1/-1->3->0 [23] 2/-1/-1->3->0
3: l50012:1350253:1350726 [3] NCCL INFO P2P Chunksize set to 524288
0: l50012:1350250:1350715 [0] NCCL INFO Channel 06/24 :    0   1   2   3
0: l50012:1350250:1350715 [0] NCCL INFO Channel 07/24 :    0   1   3   2
0: l50012:1350250:1350715 [0] NCCL INFO Channel 08/24 :    0   2   3   1
0: l50012:1350250:1350715 [0] NCCL INFO Channel 09/24 :    0   2   1   3
0: l50012:1350250:1350715 [0] NCCL INFO Channel 10/24 :    0   3   1   2
0: l50012:1350250:1350715 [0] NCCL INFO Channel 11/24 :    0   3   2   1
0: l50012:1350250:1350715 [0] NCCL INFO Channel 12/24 :    0   1   2   3
0: l50012:1350250:1350715 [0] NCCL INFO Channel 13/24 :    0   1   3   2
0: l50012:1350250:1350715 [0] NCCL INFO Channel 14/24 :    0   2   3   1
0: l50012:1350250:1350715 [0] NCCL INFO Channel 15/24 :    0   2   1   3
0: l50012:1350250:1350715 [0] NCCL INFO Channel 16/24 :    0   3   1   2
0: l50012:1350250:1350715 [0] NCCL INFO Channel 17/24 :    0   3   2   1
0: l50012:1350250:1350715 [0] NCCL INFO Channel 18/24 :    0   1   2   3
0: l50012:1350250:1350715 [0] NCCL INFO Channel 19/24 :    0   1   3   2
1: l50012:1350251:1350725 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 3/-1/-1->1->2 [5] 3/-1/-1->1->2 [6] 3/-1/-1->1->2 [7] 3/-1/-1->1->2 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 3/-1/-1->1->2 [17] 3/-1/-1->1->2 [18] 3/-1/-1->1->2 [19] 3/-1/-1->1->2 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1
1: l50012:1350251:1350725 [1] NCCL INFO P2P Chunksize set to 524288
0: l50012:1350250:1350715 [0] NCCL INFO Channel 20/24 :    0   2   3   1
0: l50012:1350250:1350715 [0] NCCL INFO Channel 21/24 :    0   2   1   3
0: l50012:1350250:1350715 [0] NCCL INFO Channel 22/24 :    0   3   1   2
0: l50012:1350250:1350715 [0] NCCL INFO Channel 23/24 :    0   3   2   1
0: l50012:1350250:1350715 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 2/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 2/-1/-1->0->-1 [7] 2/-1/-1->0->-1 [8] 3/-1/-1->0->1 [9] 3/-1/-1->0->1 [10] 3/-1/-1->0->1 [11] 3/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 2/-1/-1->0->-1 [17] 2/-1/-1->0->-1 [18] 2/-1/-1->0->-1 [19] 2/-1/-1->0->-1 [20] 3/-1/-1->0->1 [21] 3/-1/-1->0->1 [22] 3/-1/-1->0->1 [23] 3/-1/-1->0->1
2: l50012:1350252:1350727 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->0 [5] 1/-1/-1->2->0 [6] 1/-1/-1->2->0 [7] 1/-1/-1->2->0 [8] -1/-1/-1->2->3 [9] -1/-1/-1->2->3 [10] -1/-1/-1->2->3 [11] -1/-1/-1->2->3 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 1/-1/-1->2->0 [17] 1/-1/-1->2->0 [18] 1/-1/-1->2->0 [19] 1/-1/-1->2->0 [20] -1/-1/-1->2->3 [21] -1/-1/-1->2->3 [22] -1/-1/-1->2->3 [23] -1/-1/-1->2->3
2: l50012:1350252:1350727 [2] NCCL INFO P2P Chunksize set to 524288
0: l50012:1350250:1350715 [0] NCCL INFO P2P Chunksize set to 524288
3: l50012:1350253:1350726 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 04/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 08/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 10/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 09/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 13/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 14/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 14/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 13/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 15/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 16/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 15/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 16/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 19/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 20/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 20/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 21/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 22/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 21/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 04/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 16/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 17/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 22/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 23/0 : 0[0] -> 3[3] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Connected all rings
2: l50012:1350252:1350727 [2] NCCL INFO Connected all rings
3: l50012:1350253:1350726 [3] NCCL INFO Connected all rings
0: l50012:1350250:1350715 [0] NCCL INFO Connected all rings
0: l50012:1350250:1350715 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 04/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 05/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 06/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 04/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 07/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 07/0 : 0[0] -> 2[2] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 05/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 17/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 16/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 18/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 17/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 16/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 19/0 : 3[3] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 18/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 17/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 17/0 : 1[1] -> 3[3] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 19/0 : 0[0] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 18/0 : 2[2] -> 0[0] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 18/0 : 1[1] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 08/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 20/0 : 0[0] -> 3[3] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Channel 21/0 : 0[0] -> 3[3] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
3: l50012:1350253:1350726 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
1: l50012:1350251:1350725 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read
2: l50012:1350252:1350727 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read
0: l50012:1350250:1350715 [0] NCCL INFO Connected all trees
2: l50012:1350252:1350727 [2] NCCL INFO Connected all trees
3: l50012:1350253:1350726 [3] NCCL INFO Connected all trees
3: l50012:1350253:1350726 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
3: l50012:1350253:1350726 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
1: l50012:1350251:1350725 [1] NCCL INFO Connected all trees
1: l50012:1350251:1350725 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
1: l50012:1350251:1350725 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
2: l50012:1350252:1350727 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
2: l50012:1350252:1350727 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
0: l50012:1350250:1350715 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
0: l50012:1350250:1350715 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
1: l50012:1350251:1350725 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
1: l50012:1350251:1350725 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
1: l50012:1350251:1350725 [1] NCCL INFO ncclCommInitRank comm 0x55555ee28440 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 44000 commId 0xc27b17ba9a5adf35 - Init COMPLETE
2: l50012:1350252:1350727 [2] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
2: l50012:1350252:1350727 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
2: l50012:1350252:1350727 [2] NCCL INFO ncclCommInitRank comm 0x55555ee282d0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 84000 commId 0xc27b17ba9a5adf35 - Init COMPLETE
3: l50012:1350253:1350726 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
3: l50012:1350253:1350726 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
3: l50012:1350253:1350726 [3] NCCL INFO ncclCommInitRank comm 0x55555ee28100 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c4000 commId 0xc27b17ba9a5adf35 - Init COMPLETE
0: l50012:1350250:1350715 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50012:1350250:1350715 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50012:1350250:1350715 [0] NCCL INFO ncclCommInitRank comm 0x55555f26d2a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 3000 commId 0xc27b17ba9a5adf35 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 1
0: par_rank : 0
0: par_size : 4
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 0, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 0, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17678284
0: wandb_id : pjppryaq
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: mask_input_field : total_precip
0: mask_input_value : 0
0: years_test : [2021]
2: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
3: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
3: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.lats : (721,)
2: self.lons : (1440,)
3: self.lats : (721,)
3: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.lats : (721,)
0: self.lons : (1440,)
3: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
2: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
3: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
3: self.lats : (721,)
3: self.lons : (1440,)
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
2: self.lats : (721,)
2: self.lons : (1440,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: ['cuda:0'] 0
2: ['cuda:2'] 0
3: ['cuda:3'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
2: ['cuda:2'] 0
0: ['cuda:0'] 0
3: ['cuda:3'] 0
1: ['cuda:1'] 0
1: ['cuda:1'] 0
2: Loaded model id = wc5e2i3t.
2: Loaded run 'wc5e2i3t' at epoch -2.
3: Loaded model id = wc5e2i3t.
3: Loaded run 'wc5e2i3t' at epoch -2.
1: Loaded model id = wc5e2i3t.
1: Loaded run 'wc5e2i3t' at epoch -2.
0: Loaded model id = wc5e2i3t.
0: Loaded run 'wc5e2i3t' at epoch -2.
3: -1 : 22:08:36 :: batch_size = 96, lr = 1e-05
1: -1 : 22:08:36 :: batch_size = 96, lr = 1e-05
2: -1 : 22:08:36 :: batch_size = 96, lr = 1e-05
0: Number of trainable parameters: 741,136,272
0: -1 : 22:08:36 :: batch_size = 96, lr = 1e-05
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch -1, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch -1, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] INPUT BATCH
0: Epoch -1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch -1, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.2536, 0.2671, 0.2806, 0.2941, 0.3076, 0.3210, 0.3345, 0.3478, 0.3611,
3:         0.3744, 0.3876, 0.4007, 0.4137, 0.4267, 0.4395, 0.4523, 0.4648, 0.4772,
3:         0.2961, 0.3106, 0.3249, 0.3392, 0.3532, 0.3672, 0.3809],
0:      first 25 values: tensor([0.2536, 0.2671, 0.2806, 0.2941, 0.3076, 0.3210, 0.3345, 0.3478, 0.3611,
0:         0.3744, 0.3876, 0.4007, 0.4137, 0.4267, 0.4395, 0.4523, 0.4648, 0.4772,
0:         0.2961, 0.3106, 0.3249, 0.3392, 0.3532, 0.3672, 0.3809],
1:      first 25 values: tensor([0.2536, 0.2671, 0.2806, 0.2941, 0.3076, 0.3210, 0.3345, 0.3478, 0.3611,
1:         0.3744, 0.3876, 0.4007, 0.4137, 0.4267, 0.4395, 0.4523, 0.4648, 0.4772,
1:         0.2961, 0.3106, 0.3249, 0.3392, 0.3532, 0.3672, 0.3809],
2:      first 25 values: tensor([0.2536, 0.2671, 0.2806, 0.2941, 0.3076, 0.3210, 0.3345, 0.3478, 0.3611,
2:         0.3744, 0.3876, 0.4007, 0.4137, 0.4267, 0.4395, 0.4523, 0.4648, 0.4772,
2:         0.2961, 0.3106, 0.3249, 0.3392, 0.3532, 0.3672, 0.3809],
0:        device='cuda:0')
3:        device='cuda:3')
1:        device='cuda:1')
2:        device='cuda:2')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.2839, 1.2839, 1.2835, 1.2831, 1.2825, 1.2819, 1.2809, 1.2799, 1.2787,
1:         1.2775, 1.2760, 1.2744, 1.2728, 1.2712, 1.2694, 1.2674, 1.2654, 1.2631,
1:         1.4051, 1.4067, 1.4077, 1.4083, 1.4087, 1.4085, 1.4083],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.2839, 1.2839, 1.2835, 1.2831, 1.2825, 1.2819, 1.2809, 1.2799, 1.2787,
3:         1.2775, 1.2760, 1.2744, 1.2728, 1.2712, 1.2694, 1.2674, 1.2654, 1.2631,
3:         1.4051, 1.4067, 1.4077, 1.4083, 1.4087, 1.4085, 1.4083],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2839, 1.2839, 1.2835, 1.2831, 1.2825, 1.2819, 1.2809, 1.2799, 1.2787,
0:         1.2775, 1.2760, 1.2744, 1.2728, 1.2712, 1.2694, 1.2674, 1.2654, 1.2631,
0:         1.4051, 1.4067, 1.4077, 1.4083, 1.4087, 1.4085, 1.4083],
2:      first 25 values: tensor([1.2839, 1.2839, 1.2835, 1.2831, 1.2825, 1.2819, 1.2809, 1.2799, 1.2787,
2:         1.2775, 1.2760, 1.2744, 1.2728, 1.2712, 1.2694, 1.2674, 1.2654, 1.2631,
2:         1.4051, 1.4067, 1.4077, 1.4083, 1.4087, 1.4085, 1.4083],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.2607, 0.2604, 0.2601, 0.2613, 0.2626, 0.2639, 0.2652, 0.2665, 0.2678,
1:         0.2690, 0.2688, 0.2678, 0.2668, 0.2657, 0.2646, 0.2635, 0.2623, 0.2617,
1:         0.2994, 0.3012, 0.3029, 0.3047, 0.3064, 0.3081, 0.3095],
1:        device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.2607, 0.2604, 0.2601, 0.2613, 0.2626, 0.2639, 0.2652, 0.2665, 0.2678,
3:         0.2690, 0.2688, 0.2678, 0.2668, 0.2657, 0.2646, 0.2635, 0.2623, 0.2617,
3:         0.2994, 0.3012, 0.3029, 0.3047, 0.3064, 0.3081, 0.3095],
3:        device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.2607, 0.2604, 0.2601, 0.2613, 0.2626, 0.2639, 0.2652, 0.2665, 0.2678,
2:         0.2690, 0.2688, 0.2678, 0.2668, 0.2657, 0.2646, 0.2635, 0.2623, 0.2617,
2:         0.2994, 0.3012, 0.3029, 0.3047, 0.3064, 0.3081, 0.3095],
2:        device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2607, 0.2604, 0.2601, 0.2613, 0.2626, 0.2639, 0.2652, 0.2665, 0.2678,
0:         0.2690, 0.2688, 0.2678, 0.2668, 0.2657, 0.2646, 0.2635, 0.2623, 0.2617,
0:         0.2994, 0.3012, 0.3029, 0.3047, 0.3064, 0.3081, 0.3095],
0:        device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.8523,  0.8250,  0.7976,  0.7681,  0.7387,  0.7092,  0.6787,  0.6482,
1:          0.6187,  0.5882,  0.5588,  0.5293,  0.4998,  0.4714,  0.4430,  0.4146,
1:          0.3862,  0.3568, -0.0346, -0.0672, -0.0988, -0.1293, -0.1598, -0.1882,
1:         -0.2167], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([ 0.8523,  0.8250,  0.7976,  0.7681,  0.7387,  0.7092,  0.6787,  0.6482,
3:          0.6187,  0.5882,  0.5588,  0.5293,  0.4998,  0.4714,  0.4430,  0.4146,
3:          0.3862,  0.3568, -0.0346, -0.0672, -0.0988, -0.1293, -0.1598, -0.1882,
3:         -0.2167], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([ 0.8523,  0.8250,  0.7976,  0.7681,  0.7387,  0.7092,  0.6787,  0.6482,
2:          0.6187,  0.5882,  0.5588,  0.5293,  0.4998,  0.4714,  0.4430,  0.4146,
2:          0.3862,  0.3568, -0.0346, -0.0672, -0.0988, -0.1293, -0.1598, -0.1882,
2:         -0.2167], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.8523,  0.8250,  0.7976,  0.7681,  0.7387,  0.7092,  0.6787,  0.6482,
0:          0.6187,  0.5882,  0.5588,  0.5293,  0.4998,  0.4714,  0.4430,  0.4146,
0:          0.3862,  0.3568, -0.0346, -0.0672, -0.0988, -0.1293, -0.1598, -0.1882,
0:         -0.2167], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.8325, 0.8365, 0.8402, 0.8440, 0.8478, 0.8515, 0.8550, 0.8585, 0.8620,
1:         0.8655, 0.8687, 0.8720, 0.8752, 0.8784, 0.8816, 0.8849, 0.8878, 0.8908,
1:         0.8938, 0.8967, 0.8997, 0.9026, 0.9051, 0.9078, 0.9103],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0.8325, 0.8365, 0.8402, 0.8440, 0.8478, 0.8515, 0.8550, 0.8585, 0.8620,
3:         0.8655, 0.8687, 0.8720, 0.8752, 0.8784, 0.8816, 0.8849, 0.8878, 0.8908,
3:         0.8938, 0.8967, 0.8997, 0.9026, 0.9051, 0.9078, 0.9103],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0.8325, 0.8365, 0.8402, 0.8440, 0.8478, 0.8515, 0.8550, 0.8585, 0.8620,
2:         0.8655, 0.8687, 0.8720, 0.8752, 0.8784, 0.8816, 0.8849, 0.8878, 0.8908,
2:         0.8938, 0.8967, 0.8997, 0.9026, 0.9051, 0.9078, 0.9103],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
0:      first 25 values: tensor([0.8325, 0.8365, 0.8402, 0.8440, 0.8478, 0.8515, 0.8550, 0.8585, 0.8620,
0:         0.8655, 0.8687, 0.8720, 0.8752, 0.8784, 0.8816, 0.8849, 0.8878, 0.8908,
0:         0.8938, 0.8967, 0.8997, 0.9026, 0.9051, 0.9078, 0.9103],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
1: [DEBUG] TARGET BATCH
1: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] TARGET BATCH
2: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] TARGET BATCH
3: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] TARGET BATCH
0: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2221,     nan,     nan,     nan,     nan, -0.2221,     nan,
2:         -0.2221,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2351, -0.2351,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2373, -0.2373,
2:             nan,     nan,     nan,     nan,     nan, -0.2329,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2177,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2177,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2264,     nan,     nan, -0.2264,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2351,
2:             nan, -0.2351,     nan,     nan,     nan,     nan,     nan, -0.2329,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2308,     nan, -0.2329,     nan,
2:         -0.2286,     nan,     nan,     nan,     nan,     nan,     nan, -0.2286,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2112,     nan,     nan,     nan,
2:             nan,     nan, -0.2090, -0.2112, -0.2112,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2308,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2286,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2199,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch -1, first predictions sample:
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan, -0.2199,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2221,     nan,     nan,     nan,     nan, -0.2221,     nan,
3:         -0.2221,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2373,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2286,     nan,     nan, -0.2221,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2177,     nan, -0.2177,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2264,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2351,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2351,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2329,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2286, -0.2286,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2090,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2090,
3:             nan,     nan,     nan,     nan, -0.2199,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2308,     nan,     nan,     nan,     nan, -0.2112,
3:             nan, -0.2134,     nan, -0.2155,     nan,     nan,     nan, -0.2177,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch -1, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2221,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2373,     nan,     nan,     nan,     nan,     nan, -0.2329,     nan,
0:             nan,     nan,     nan,     nan, -0.2329,     nan, -0.2286,     nan,
0:         -0.2286,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2221,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2177,
0:             nan,     nan,     nan,     nan,     nan, -0.2264,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2308,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2351,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2351,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2286,     nan, -0.2264,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2090,     nan,     nan,     nan,     nan,     nan, -0.2090,
0:             nan,     nan, -0.2090,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2242,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2286,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2286,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan, -0.2199,     nan, -0.2177,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2308,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2351, -0.2351,
1:             nan,     nan,     nan, -0.2351,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2286,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2221,
1:         -0.2221, -0.2221,     nan,     nan,     nan,     nan,     nan, -0.2199,
1:         -0.2242,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2199,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2177,     nan, -0.2177,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2264,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2351, -0.2351,
1:             nan,     nan,     nan,     nan, -0.2351,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2308,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2286, -0.2286,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2090,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2177,     nan,     nan, -0.2155,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2221,     nan,     nan,     nan,
1:             nan, -0.2308,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch -1, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch -1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1487, -0.1468, -0.1478, -0.1480, -0.1487, -0.1490, -0.1431, -0.1409,
0:         -0.1367, -0.1348, -0.1356, -0.1344, -0.1287, -0.1201, -0.1079, -0.0969,
0:         -0.0918, -0.0914, -0.1621, -0.1590, -0.1608, -0.1597, -0.1606, -0.1581,
3:      first 25 pred values: tensor([-0.2305, -0.2197, -0.2086, -0.1991, -0.1921, -0.1883, -0.1825, -0.1784,
3:         -0.1708, -0.1633, -0.1593, -0.1540, -0.1505, -0.1443, -0.1390, -0.1354,
3:         -0.1301, -0.1279, -0.2447, -0.2341, -0.2225, -0.2103, -0.2030, -0.1979,
3:         -0.1927], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.2008, -0.1909, -0.1827, -0.1752, -0.1709, -0.1702, -0.1672, -0.1672,
1:         -0.1616, -0.1554, -0.1501, -0.1437, -0.1375, -0.1291, -0.1218, -0.1131,
1:         -0.1055, -0.0975, -0.2113, -0.1989, -0.1864, -0.1758, -0.1711, -0.1702,
1:         -0.1694], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 pred values: tensor([-0.2058, -0.1959, -0.1857, -0.1734, -0.1657, -0.1607, -0.1550, -0.1524,
2:         -0.1475, -0.1438, -0.1428, -0.1395, -0.1354, -0.1316, -0.1251, -0.1180,
2:         -0.1094, -0.1027, -0.2212, -0.2117, -0.1999, -0.1875, -0.1795, -0.1735,
2:         -0.1688], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:         -0.1539], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.3255, 0.3241, 0.3244, 0.3238, 0.3323, 0.3474, 0.3652, 0.3812, 0.3988,
3:         0.4107, 0.4236, 0.4377, 0.4550, 0.4713, 0.4899, 0.5133, 0.5364, 0.5626,
3:         0.3138, 0.3170, 0.3223, 0.3284, 0.3416, 0.3596, 0.3799],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.3527, 0.3556, 0.3570, 0.3535, 0.3584, 0.3714, 0.3869, 0.4011, 0.4154,
2:         0.4223, 0.4285, 0.4369, 0.4442, 0.4551, 0.4658, 0.4820, 0.4968, 0.5176,
2:         0.3311, 0.3394, 0.3463, 0.3503, 0.3643, 0.3819, 0.4027],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.3675, 0.3588, 0.3564, 0.3540, 0.3592, 0.3720, 0.3866, 0.4046, 0.4265,
1:         0.4435, 0.4567, 0.4681, 0.4779, 0.4908, 0.5073, 0.5295, 0.5501, 0.5752,
1:         0.3431, 0.3391, 0.3406, 0.3459, 0.3559, 0.3695, 0.3878],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3417, 0.3426, 0.3423, 0.3390, 0.3419, 0.3519, 0.3673, 0.3867, 0.4056,
0:         0.4209, 0.4306, 0.4373, 0.4455, 0.4544, 0.4678, 0.4917, 0.5180, 0.5478,
0:         0.3356, 0.3409, 0.3416, 0.3437, 0.3519, 0.3644, 0.3819],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6395, -0.6371, -0.6323, -0.6235, -0.6165, -0.6093, -0.6045, -0.5935,
3:         -0.5814, -0.5657, -0.5479, -0.5324, -0.5223, -0.5135, -0.5072, -0.5015,
3:         -0.4896, -0.4762, -0.5998, -0.5970, -0.5880, -0.5751, -0.5631, -0.5515,
3:         -0.5384], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6277, -0.6313, -0.6327, -0.6254, -0.6182, -0.6100, -0.6025, -0.5884,
2:         -0.5740, -0.5541, -0.5339, -0.5132, -0.4977, -0.4853, -0.4790, -0.4749,
2:         -0.4714, -0.4637, -0.5872, -0.5872, -0.5812, -0.5687, -0.5582, -0.5452,
2:         -0.5313], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6144, -0.6175, -0.6194, -0.6150, -0.6107, -0.6045, -0.5970, -0.5842,
1:         -0.5716, -0.5550, -0.5349, -0.5169, -0.5006, -0.4880, -0.4813, -0.4742,
1:         -0.4672, -0.4594, -0.5772, -0.5770, -0.5710, -0.5618, -0.5519, -0.5394,
1:         -0.5238], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6206, -0.6226, -0.6241, -0.6170, -0.6089, -0.6032, -0.5959, -0.5842,
0:         -0.5729, -0.5587, -0.5422, -0.5269, -0.5139, -0.5020, -0.4942, -0.4859,
0:         -0.4799, -0.4738, -0.5867, -0.5877, -0.5836, -0.5722, -0.5609, -0.5471,
0:         -0.5333], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.0295, 0.0223, 0.0369, 0.0463, 0.0603, 0.0617, 0.0771, 0.0718, 0.0515,
3:         0.0407, 0.0362, 0.0484, 0.0390, 0.0219, 0.0207, 0.0119, 0.0249, 0.0278,
3:         0.0151, 0.0040, 0.0147, 0.0222, 0.0296, 0.0235, 0.0233],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([0.1392, 0.1159, 0.1237, 0.1217, 0.1338, 0.1330, 0.1124, 0.0870, 0.0746,
2:         0.0618, 0.0467, 0.0430, 0.0399, 0.0332, 0.0301, 0.0323, 0.0449, 0.0921,
2:         0.1173, 0.0936, 0.0984, 0.1025, 0.1133, 0.1093, 0.0846],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([ 0.1591,  0.1207,  0.1209,  0.1109,  0.0908,  0.0824,  0.0556,  0.0218,
1:          0.0217,  0.0203,  0.0092,  0.0078,  0.0027,  0.0042, -0.0020, -0.0073,
1:          0.0244,  0.0480,  0.1476,  0.1069,  0.1000,  0.0911,  0.0728,  0.0733,
1:          0.0549], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.2547, 0.2416, 0.2255, 0.2172, 0.2302, 0.2295, 0.2189, 0.1918, 0.1773,
0:         0.1495, 0.1055, 0.1051, 0.1389, 0.1544, 0.1550, 0.1598, 0.1495, 0.1295,
0:         0.2144, 0.2103, 0.1919, 0.1798, 0.1825, 0.1836, 0.1767],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.0505, -0.0454, -0.0402, -0.0412, -0.0437, -0.0470, -0.0496, -0.0471,
3:         -0.0438, -0.0406, -0.0414, -0.0454, -0.0504, -0.0528, -0.0540, -0.0551,
3:         -0.0594, -0.0658, -0.0740, -0.0784, -0.0821, -0.0843, -0.0862, -0.0869,
3:         -0.0854], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([ 0.0019, -0.0037, -0.0083, -0.0150, -0.0195, -0.0223, -0.0230, -0.0216,
2:         -0.0193, -0.0184, -0.0200, -0.0224, -0.0251, -0.0273, -0.0307, -0.0364,
2:         -0.0434, -0.0503, -0.0548, -0.0560, -0.0573, -0.0594, -0.0630, -0.0664,
2:         -0.0684], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.0174, -0.0177, -0.0159, -0.0170, -0.0192, -0.0226, -0.0265, -0.0296,
1:         -0.0319, -0.0337, -0.0381, -0.0412, -0.0432, -0.0426, -0.0424, -0.0437,
1:         -0.0490, -0.0548, -0.0594, -0.0612, -0.0630, -0.0661, -0.0706, -0.0746,
1:         -0.0751], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0503, -0.0435, -0.0356, -0.0335, -0.0332, -0.0349, -0.0357, -0.0335,
0:         -0.0318, -0.0304, -0.0317, -0.0350, -0.0397, -0.0408, -0.0418, -0.0396,
0:         -0.0378, -0.0365, -0.0364, -0.0366, -0.0398, -0.0445, -0.0502, -0.0565,
3:      first 25 pred values: tensor([-0.1612, -0.1609, -0.1645, -0.1636, -0.1649, -0.1673, -0.1740, -0.1712,
3:         -0.1720, -0.1650, -0.1584, -0.1646, -0.1650, -0.1655, -0.1641, -0.1723,
3:         -0.1730, -0.1701, -0.1627, -0.1595, -0.1585, -0.1570, -0.1588, -0.1596,
3:         -0.1634], device='cuda:3', grad_fn=<SliceBackward0>)
0:         -0.0646], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1681, -0.1674, -0.1760, -0.1750, -0.1791, -0.1788, -0.1815, -0.1757,
2:         -0.1747, -0.1728, -0.1674, -0.1743, -0.1776, -0.1805, -0.1773, -0.1840,
2:         -0.1790, -0.1747, -0.1710, -0.1745, -0.1721, -0.1732, -0.1744, -0.1760,
2:         -0.1779], device='cuda:2', grad_fn=<SliceBackward0>)
1:      first 25 pred values: tensor([-0.1528, -0.1582, -0.1651, -0.1680, -0.1697, -0.1727, -0.1723, -0.1686,
1:         -0.1709, -0.1597, -0.1561, -0.1601, -0.1656, -0.1688, -0.1690, -0.1731,
1:         -0.1713, -0.1638, -0.1556, -0.1567, -0.1612, -0.1603, -0.1643, -0.1654,
1:         -0.1665], device='cuda:1', grad_fn=<SliceBackward0>)
0:      first 25 pred values: tensor([-0.1737, -0.1753, -0.1790, -0.1787, -0.1797, -0.1782, -0.1795, -0.1745,
0:         -0.1715, -0.1751, -0.1713, -0.1736, -0.1757, -0.1792, -0.1723, -0.1787,
0:         -0.1750, -0.1688, -0.1692, -0.1697, -0.1689, -0.1673, -0.1682, -0.1677,
0:         -0.1680], device='cuda:0', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [1/5 (20%)]	Loss: nan : nan :: 0.13139 (1.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [2/5 (40%)]	Loss: nan : nan :: 0.13482 (10.40 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [3/5 (60%)]	Loss: nan : nan :: 0.14785 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [4/5 (80%)]	Loss: nan : nan :: 0.15644 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch-1.mod
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch -1 : nan
0: validation loss for velocity_u : 0.03691354766488075
0: validation loss for velocity_v : 0.06917095184326172
0: validation loss for specific_humidity : 0.024015769362449646
0: validation loss for velocity_z : 0.5627489686012268
0: validation loss for temperature : 0.06486202031373978
0: validation loss for total_precip : nan
2: 0 : 22:15:34 :: batch_size = 96, lr = 1e-05
1: 0 : 22:15:34 :: batch_size = 96, lr = 1e-05
3: 0 : 22:15:34 :: batch_size = 96, lr = 1e-05
0: 0 : 22:15:34 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 0, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2125, -1.2176, -1.2042, -1.1791, -1.1593, -1.1573, -1.1761, -1.2077,
0:         -1.2323, -1.2433, -1.2512, -1.2721, -1.3182, -1.3784, -1.4330, -1.4673,
0:         -1.4767, -1.4619, -1.2469, -1.2492, -1.2284, -1.1988, -1.1737, -1.1667,
0:         -1.1784], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3923, -0.3438, -0.3055, -0.2738, -0.2531, -0.2455, -0.2486, -0.2439,
0:         -0.2164, -0.1702, -0.1187, -0.0688, -0.0234,  0.0135,  0.0429,  0.0604,
0:          0.0624,  0.0612, -0.4028, -0.3592, -0.3262, -0.2993, -0.2816, -0.2775,
0:         -0.2816], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3469, 0.4088, 0.4380, 0.4070, 0.3059, 0.2103, 0.2315, 0.3840, 0.7069,
0:         1.0023, 1.1110, 1.0774, 1.0656, 1.1884, 1.4511, 1.7864, 2.0786, 2.2258,
0:         0.5836, 0.6046, 0.5870, 0.5053, 0.4010, 0.4186, 0.6443],
0:        device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3619,  0.4922,  0.4554,  0.2773,  0.1837,  0.0679, -0.0056, -0.1025,
0:         -0.2528, -0.3241, -0.2372,  0.0345,  0.3274,  0.6281,  0.5969,  0.1815,
0:          0.0824,  0.1960,  0.5089,  0.5390,  0.4432,  0.2951,  0.2060,  0.1514,
0:          0.0523], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.1405, 0.9372, 0.7361, 0.5943, 0.5744, 0.6270, 0.6797, 0.7346, 0.8038,
0:         0.9416, 1.1092, 1.2199, 1.1666, 0.9749, 0.7860, 0.6466, 0.5824, 0.5664,
0:         0.5828, 0.6318, 0.6686, 0.6754, 0.6315, 0.5406, 0.4639],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
0:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0996,     nan,     nan,     nan,
0:             nan,     nan,  0.0732,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0614,     nan,     nan,     nan,     nan,     nan, -0.2497,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2497, -0.2497,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2497, -0.2497,     nan,     nan,
0:          0.2150,  0.2150,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2497, -0.2497,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 0, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.3566, -1.3621, -1.3715, -1.3817, -1.3945, -1.4159, -1.4401, -1.4719,
0:         -1.4985, -1.5214, -1.5353, -1.5414, -1.5453, -1.5429, -1.5438, -1.5405,
0:         -1.5340, -1.5207, -1.4015, -1.4074, -1.4162, -1.4276, -1.4429, -1.4633,
0:         -1.4867], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0368, -0.0228, -0.0051,  0.0127,  0.0317,  0.0526,  0.0664,  0.0754,
0:          0.0805,  0.0770,  0.0683,  0.0557,  0.0396,  0.0166, -0.0109, -0.0401,
0:         -0.0689, -0.0909, -0.0558, -0.0369, -0.0165,  0.0077,  0.0327,  0.0542,
0:          0.0705], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6175, 0.6599, 0.7023, 0.7415, 0.7699, 0.7904, 0.8047, 0.8225, 0.8309,
0:         0.8324, 0.8188, 0.7966, 0.7598, 0.7149, 0.6635, 0.6213, 0.5815, 0.5532,
0:         0.5950, 0.6292, 0.6775, 0.7179, 0.7529, 0.7752, 0.8037],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1017, -0.0529,  0.0206,  0.0521,  0.1387,  0.2498,  0.2600,  0.2744,
0:          0.3164,  0.3248,  0.3385,  0.3302,  0.2936,  0.3104,  0.3669,  0.3465,
0:          0.3127,  0.3442,  0.0821,  0.0890,  0.1236,  0.1694,  0.2417,  0.2869,
0:          0.2668], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.0496, 1.0900, 1.1033, 1.0894, 1.0612, 1.0273, 0.9901, 0.9451, 0.8877,
0:         0.8119, 0.7215, 0.6182, 0.5163, 0.4189, 0.3338, 0.2617, 0.2053, 0.1687,
0:         0.1691, 0.2064, 0.2762, 0.3608, 0.4404, 0.5011, 0.5393],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1598, -0.1636, -0.1681, -0.1663, -0.1638, -0.1671, -0.1695, -0.1655,
0:         -0.1659, -0.1680, -0.1619, -0.1642, -0.1650, -0.1650, -0.1633, -0.1678,
0:         -0.1665, -0.1651, -0.1650, -0.1637, -0.1600, -0.1572, -0.1596, -0.1580,
0:         -0.1630], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 0, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-1.2125, -1.2176, -1.2042, -1.1791, -1.1593, -1.1573, -1.1761, -1.2077,
2:         -1.2323, -1.2433, -1.2512, -1.2721, -1.3182, -1.3784, -1.4330, -1.4673,
2:         -1.4767, -1.4619, -1.2469, -1.2492, -1.2284, -1.1988, -1.1737, -1.1667,
2:         -1.1784], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.3923, -0.3438, -0.3055, -0.2738, -0.2531, -0.2455, -0.2486, -0.2439,
2:         -0.2164, -0.1702, -0.1187, -0.0688, -0.0234,  0.0135,  0.0429,  0.0604,
2:          0.0624,  0.0612, -0.4028, -0.3592, -0.3262, -0.2993, -0.2816, -0.2775,
2:         -0.2816], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.3469, 0.4088, 0.4380, 0.4070, 0.3059, 0.2103, 0.2315, 0.3840, 0.7069,
2:         1.0023, 1.1110, 1.0774, 1.0656, 1.1884, 1.4511, 1.7864, 2.0786, 2.2258,
2:         0.5836, 0.6046, 0.5870, 0.5053, 0.4010, 0.4186, 0.6443],
2:        device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.3619,  0.4922,  0.4554,  0.2773,  0.1837,  0.0679, -0.0056, -0.1025,
2:         -0.2528, -0.3241, -0.2372,  0.0345,  0.3274,  0.6281,  0.5969,  0.1815,
2:          0.0824,  0.1960,  0.5089,  0.5390,  0.4432,  0.2951,  0.2060,  0.1514,
2:          0.0523], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([1.1405, 0.9372, 0.7361, 0.5943, 0.5744, 0.6270, 0.6797, 0.7346, 0.8038,
2:         0.9416, 1.1092, 1.2199, 1.1666, 0.9749, 0.7860, 0.6466, 0.5824, 0.5664,
2:         0.5828, 0.6318, 0.6686, 0.6754, 0.6315, 0.5406, 0.4639],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan, -0.2497, -0.2497,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2497, -0.2497,     nan,     nan,     nan,     nan,
2:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2497,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2497,     nan,     nan, -0.2497,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2497, -0.2497, -0.2497,
2:             nan,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,  0.0637,     nan,     nan,     nan,
2:             nan,     nan,     nan,  0.4127,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
2:             nan, -0.2497,     nan,     nan,     nan,     nan, -0.2497,     nan,
2:             nan,     nan,     nan, -0.2497,     nan,     nan, -0.2497,     nan,
2:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 0, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.3508, -1.3575, -1.3714, -1.3871, -1.4058, -1.4277, -1.4491, -1.4741,
2:         -1.4952, -1.5125, -1.5260, -1.5348, -1.5415, -1.5444, -1.5456, -1.5428,
2:         -1.5326, -1.5168, -1.3922, -1.3994, -1.4158, -1.4337, -1.4545, -1.4749,
2:         -1.4944], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0573, -0.0514, -0.0365, -0.0164,  0.0068,  0.0348,  0.0535,  0.0679,
2:          0.0745,  0.0725,  0.0607,  0.0478,  0.0287,  0.0088, -0.0132, -0.0356,
2:         -0.0559, -0.0720, -0.0721, -0.0581, -0.0373, -0.0116,  0.0170,  0.0413,
2:          0.0588], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.6400, 0.6769, 0.7045, 0.7352, 0.7530, 0.7656, 0.7827, 0.7986, 0.8066,
2:         0.8135, 0.8091, 0.8003, 0.7775, 0.7447, 0.6974, 0.6471, 0.5888, 0.5326,
2:         0.6164, 0.6397, 0.6695, 0.7000, 0.7261, 0.7505, 0.7862],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1867, -0.1432, -0.0538, -0.0304,  0.0237,  0.1134,  0.1086,  0.1201,
2:          0.1698,  0.1919,  0.2379,  0.2669,  0.2296,  0.2212,  0.2728,  0.2609,
2:          0.2476,  0.2944, -0.0327, -0.0419,  0.0150,  0.0641,  0.1225,  0.1736,
2:          0.1490], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([1.0901, 1.1370, 1.1488, 1.1275, 1.0846, 1.0396, 0.9962, 0.9525, 0.8982,
2:         0.8270, 0.7420, 0.6428, 0.5425, 0.4444, 0.3509, 0.2686, 0.1985, 0.1529,
2:         0.1445, 0.1777, 0.2486, 0.3408, 0.4312, 0.5025, 0.5430],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1802, -0.1793, -0.1842, -0.1831, -0.1849, -0.1845, -0.1877, -0.1848,
2:         -0.1856, -0.1833, -0.1806, -0.1827, -0.1828, -0.1827, -0.1796, -0.1855,
2:         -0.1864, -0.1819, -0.1815, -0.1823, -0.1798, -0.1774, -0.1768, -0.1758,
2:         -0.1766], device='cuda:2', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 0, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-1.2125, -1.2176, -1.2042, -1.1791, -1.1593, -1.1573, -1.1761, -1.2077,
1:         -1.2323, -1.2433, -1.2512, -1.2721, -1.3182, -1.3784, -1.4330, -1.4673,
1:         -1.4767, -1.4619, -1.2469, -1.2492, -1.2284, -1.1988, -1.1737, -1.1667,
1:         -1.1784], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.3923, -0.3438, -0.3055, -0.2738, -0.2531, -0.2455, -0.2486, -0.2439,
1:         -0.2164, -0.1702, -0.1187, -0.0688, -0.0234,  0.0135,  0.0429,  0.0604,
1:          0.0624,  0.0612, -0.4028, -0.3592, -0.3262, -0.2993, -0.2816, -0.2775,
1:         -0.2816], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.3469, 0.4088, 0.4380, 0.4070, 0.3059, 0.2103, 0.2315, 0.3840, 0.7069,
1:         1.0023, 1.1110, 1.0774, 1.0656, 1.1884, 1.4511, 1.7864, 2.0786, 2.2258,
1:         0.5836, 0.6046, 0.5870, 0.5053, 0.4010, 0.4186, 0.6443],
1:        device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.3619,  0.4922,  0.4554,  0.2773,  0.1837,  0.0679, -0.0056, -0.1025,
1:         -0.2528, -0.3241, -0.2372,  0.0345,  0.3274,  0.6281,  0.5969,  0.1815,
1:          0.0824,  0.1960,  0.5089,  0.5390,  0.4432,  0.2951,  0.2060,  0.1514,
1:          0.0523], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([1.1405, 0.9372, 0.7361, 0.5943, 0.5744, 0.6270, 0.6797, 0.7346, 0.8038,
1:         0.9416, 1.1092, 1.2199, 1.1666, 0.9749, 0.7860, 0.6466, 0.5824, 0.5664,
1:         0.5828, 0.6318, 0.6686, 0.6754, 0.6315, 0.5406, 0.4639],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2497,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan, -0.2497,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.1520,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan, -0.2497,
1:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
1:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan, -0.2497,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 0, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.3531, -1.3587, -1.3700, -1.3831, -1.3986, -1.4183, -1.4412, -1.4699,
1:         -1.4987, -1.5247, -1.5435, -1.5543, -1.5601, -1.5577, -1.5528, -1.5424,
1:         -1.5270, -1.5044, -1.4013, -1.4099, -1.4248, -1.4404, -1.4578, -1.4769,
1:         -1.4974], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.0683, -0.0511, -0.0271,  0.0018,  0.0269,  0.0488,  0.0602,  0.0639,
1:          0.0615,  0.0555,  0.0474,  0.0385,  0.0263,  0.0110, -0.0114, -0.0372,
1:         -0.0670, -0.0915, -0.0779, -0.0542, -0.0258,  0.0034,  0.0323,  0.0519,
1:          0.0637], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.6577, 0.6713, 0.6868, 0.7069, 0.7273, 0.7491, 0.7791, 0.8089, 0.8209,
1:         0.8214, 0.8094, 0.7882, 0.7551, 0.7161, 0.6654, 0.6220, 0.5723, 0.5256,
1:         0.6354, 0.6500, 0.6716, 0.6890, 0.7154, 0.7422, 0.7786],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.0801, -0.0270,  0.0283,  0.0421,  0.1069,  0.1963,  0.2194,  0.2728,
1:          0.3276,  0.3423,  0.3830,  0.3563,  0.2592,  0.2427,  0.2628,  0.2119,
1:          0.1996,  0.2593,  0.0576,  0.1082,  0.1553,  0.1783,  0.2060,  0.2267,
1:          0.2322], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([1.1148, 1.1351, 1.1371, 1.1273, 1.1066, 1.0817, 1.0436, 0.9909, 0.9201,
1:         0.8340, 0.7389, 0.6371, 0.5366, 0.4374, 0.3440, 0.2613, 0.1910, 0.1425,
1:         0.1324, 0.1595, 0.2237, 0.3104, 0.4000, 0.4734, 0.5226],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1700, -0.1715, -0.1753, -0.1727, -0.1711, -0.1713, -0.1737, -0.1693,
1:         -0.1706, -0.1754, -0.1706, -0.1724, -0.1735, -0.1717, -0.1686, -0.1729,
1:         -0.1722, -0.1679, -0.1724, -0.1715, -0.1688, -0.1674, -0.1674, -0.1644,
1:         -0.1702], device='cuda:1', grad_fn=<SliceBackward0>)
3: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 0, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-1.2125, -1.2176, -1.2042, -1.1791, -1.1593, -1.1573, -1.1761, -1.2077,
3:         -1.2323, -1.2433, -1.2512, -1.2721, -1.3182, -1.3784, -1.4330, -1.4673,
3:         -1.4767, -1.4619, -1.2469, -1.2492, -1.2284, -1.1988, -1.1737, -1.1667,
3:         -1.1784], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.3923, -0.3438, -0.3055, -0.2738, -0.2531, -0.2455, -0.2486, -0.2439,
3:         -0.2164, -0.1702, -0.1187, -0.0688, -0.0234,  0.0135,  0.0429,  0.0604,
3:          0.0624,  0.0612, -0.4028, -0.3592, -0.3262, -0.2993, -0.2816, -0.2775,
3:         -0.2816], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.3469, 0.4088, 0.4380, 0.4070, 0.3059, 0.2103, 0.2315, 0.3840, 0.7069,
3:         1.0023, 1.1110, 1.0774, 1.0656, 1.1884, 1.4511, 1.7864, 2.0786, 2.2258,
3:         0.5836, 0.6046, 0.5870, 0.5053, 0.4010, 0.4186, 0.6443],
3:        device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.3619,  0.4922,  0.4554,  0.2773,  0.1837,  0.0679, -0.0056, -0.1025,
3:         -0.2528, -0.3241, -0.2372,  0.0345,  0.3274,  0.6281,  0.5969,  0.1815,
3:          0.0824,  0.1960,  0.5089,  0.5390,  0.4432,  0.2951,  0.2060,  0.1514,
3:          0.0523], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([1.1405, 0.9372, 0.7361, 0.5943, 0.5744, 0.6270, 0.6797, 0.7346, 0.8038,
3:         0.9416, 1.1092, 1.2199, 1.1666, 0.9749, 0.7860, 0.6466, 0.5824, 0.5664,
3:         0.5828, 0.6318, 0.6686, 0.6754, 0.6315, 0.5406, 0.4639],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan, -0.2497, -0.2497,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2485,     nan, -0.2497,     nan,     nan, -0.2497,     nan,     nan,
3:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2461, -0.2497,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2497,     nan,     nan, -0.2497,     nan,
3:         -0.2497, -0.2497,     nan, -0.2497,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.1961,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,  0.0053, -0.0996,     nan,     nan, -0.2461,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
3:         -0.2497,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2497,     nan,     nan, -0.2497,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,  0.4711,     nan,     nan,     nan,     nan, -0.2497,
3:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2497,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 0, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.3529, -1.3604, -1.3710, -1.3794, -1.3892, -1.4030, -1.4184, -1.4404,
3:         -1.4618, -1.4803, -1.4965, -1.5099, -1.5198, -1.5236, -1.5255, -1.5206,
3:         -1.5119, -1.4992, -1.3938, -1.3993, -1.4118, -1.4210, -1.4332, -1.4463,
3:         -1.4614], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.0474, -0.0393, -0.0249, -0.0046,  0.0165,  0.0378,  0.0516,  0.0584,
3:          0.0591,  0.0546,  0.0479,  0.0389,  0.0233,  0.0036, -0.0211, -0.0473,
3:         -0.0716, -0.0897, -0.0599, -0.0463, -0.0291, -0.0048,  0.0220,  0.0425,
3:          0.0557], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.6488, 0.6743, 0.6984, 0.7256, 0.7448, 0.7636, 0.7871, 0.8112, 0.8207,
3:         0.8173, 0.8010, 0.7776, 0.7451, 0.7090, 0.6602, 0.6079, 0.5458, 0.4867,
3:         0.6458, 0.6604, 0.6796, 0.6973, 0.7201, 0.7467, 0.7856],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.0360, 0.0586, 0.0820, 0.0954, 0.1296, 0.1728, 0.1535, 0.1775, 0.2545,
3:         0.2791, 0.3128, 0.3010, 0.2284, 0.2465, 0.2756, 0.2130, 0.2002, 0.2446,
3:         0.1632, 0.1736, 0.1778, 0.2152, 0.2586, 0.2826, 0.2529],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([1.0971, 1.1452, 1.1700, 1.1650, 1.1354, 1.0915, 1.0337, 0.9650, 0.8857,
3:         0.7952, 0.7022, 0.6041, 0.5101, 0.4183, 0.3307, 0.2506, 0.1851, 0.1426,
3:         0.1360, 0.1675, 0.2350, 0.3232, 0.4108, 0.4807, 0.5173],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1786, -0.1817, -0.1841, -0.1842, -0.1860, -0.1828, -0.1865, -0.1822,
3:         -0.1848, -0.1844, -0.1826, -0.1864, -0.1861, -0.1887, -0.1828, -0.1879,
3:         -0.1862, -0.1816, -0.1812, -0.1852, -0.1833, -0.1824, -0.1829, -0.1801,
3:         -0.1833], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [1/5 (20%)]	Loss: nan : nan :: 0.14227 (1.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [2/5 (40%)]	Loss: nan : nan :: 0.14284 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [3/5 (60%)]	Loss: nan : nan :: 0.14674 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [4/5 (80%)]	Loss: nan : nan :: 0.14448 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch0.mod
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 0 : nan
0: validation loss for velocity_u : 0.03404822200536728
0: validation loss for velocity_v : 0.07063561677932739
0: validation loss for specific_humidity : 0.027677364647388458
0: validation loss for velocity_z : 0.4846714437007904
0: validation loss for temperature : 0.1037694439291954
0: validation loss for total_precip : nan
1: 1 : 22:22:25 :: batch_size = 96, lr = 1.5000000000000002e-05
2: 1 : 22:22:25 :: batch_size = 96, lr = 1.5000000000000002e-05
3: 1 : 22:22:25 :: batch_size = 96, lr = 1.5000000000000002e-05
0: 1 : 22:22:25 :: batch_size = 96, lr = 1.5000000000000002e-05
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8424, -0.8437, -0.8452, -0.8467, -0.8482, -0.8497, -0.8514, -0.8530,
0:         -0.8547, -0.8563, -0.8582, -0.8600, -0.8618, -0.8638, -0.8657, -0.8676,
0:         -0.8696, -0.8718, -0.8681, -0.8701, -0.8721, -0.8741, -0.8761, -0.8783,
0:         -0.8803], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0822, 0.0826, 0.0830, 0.0833, 0.0837, 0.0839, 0.0843, 0.0846, 0.0850,
0:         0.0852, 0.0856, 0.0861, 0.0863, 0.0867, 0.0874, 0.0878, 0.0885, 0.0891,
0:         0.0399, 0.0401, 0.0405, 0.0407, 0.0412, 0.0414, 0.0418],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411,
0:         -0.5411, -0.5411, -0.5412, -0.5412, -0.5413, -0.5413, -0.5413, -0.5410,
0:         -0.5406, -0.5403, -0.5426, -0.5424, -0.5422, -0.5421, -0.5421, -0.5421,
0:         -0.5421], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0003, 0.0048, 0.0094, 0.0162, 0.0208, 0.0254, 0.0322, 0.0367, 0.0413,
0:         0.0459, 0.0504, 0.0550, 0.0595, 0.0618, 0.0664, 0.0687, 0.0732, 0.0755,
0:         0.0755, 0.0823, 0.0869, 0.0915, 0.0960, 0.0983, 0.1029],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2148, -0.2107, -0.2068, -0.2027, -0.1990, -0.1948, -0.1911, -0.1870,
0:         -0.1830, -0.1794, -0.1754, -0.1719, -0.1680, -0.1644, -0.1605, -0.1571,
0:         -0.1532, -0.1498, -0.1464, -0.1426, -0.1393, -0.1358, -0.1327, -0.1293,
0:         -0.1259], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2510,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2486,     nan,     nan,     nan, -0.2486,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2437,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2474,     nan, -0.2486,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2522,
0:             nan,     nan,     nan, -0.2522,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2498,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2376,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2412,     nan,     nan,     nan,     nan,     nan,     nan, -0.2437,
0:             nan, -0.2425,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2461,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2510,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2510,
0:         -0.2510, -0.2498,     nan,     nan,     nan, -0.2474,     nan,     nan,
0:         -0.2474,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5139, -0.5106, -0.5068, -0.5038, -0.5003, -0.4992, -0.4974, -0.4985,
0:         -0.4979, -0.4984, -0.4999, -0.5040, -0.5083, -0.5107, -0.5148, -0.5140,
0:         -0.5107, -0.5041, -0.4952, -0.4911, -0.4861, -0.4818, -0.4808, -0.4804,
0:         -0.4796], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2127, -0.2160, -0.2223, -0.2272, -0.2326, -0.2353, -0.2371, -0.2362,
0:         -0.2337, -0.2290, -0.2276, -0.2268, -0.2281, -0.2303, -0.2321, -0.2323,
0:         -0.2333, -0.2356, -0.2548, -0.2553, -0.2579, -0.2583, -0.2575, -0.2565,
0:         -0.2572], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5894, -0.5862, -0.5855, -0.5825, -0.5829, -0.5824, -0.5824, -0.5830,
0:         -0.5851, -0.5856, -0.5845, -0.5840, -0.5834, -0.5828, -0.5851, -0.5871,
0:         -0.5908, -0.5933, -0.5882, -0.5867, -0.5855, -0.5844, -0.5850, -0.5853,
0:         -0.5850], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2687, 0.2672, 0.2566, 0.2444, 0.2493, 0.2386, 0.2241, 0.2147, 0.2264,
0:         0.2411, 0.2302, 0.2201, 0.2245, 0.2116, 0.1961, 0.2087, 0.2304, 0.2573,
0:         0.2994, 0.2923, 0.2759, 0.2575, 0.2548, 0.2459, 0.2383],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.7599, -0.7551, -0.7487, -0.7453, -0.7424, -0.7420, -0.7405, -0.7406,
0:         -0.7412, -0.7431, -0.7463, -0.7488, -0.7490, -0.7477, -0.7471, -0.7484,
0:         -0.7519, -0.7567, -0.7598, -0.7600, -0.7585, -0.7563, -0.7554, -0.7577,
0:         -0.7620], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1581, -0.1600, -0.1654, -0.1642, -0.1674, -0.1652, -0.1692, -0.1642,
0:         -0.1663, -0.1662, -0.1610, -0.1642, -0.1671, -0.1683, -0.1659, -0.1707,
0:         -0.1706, -0.1665, -0.1641, -0.1666, -0.1664, -0.1638, -0.1661, -0.1677,
0:         -0.1716], device='cuda:0', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 1, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 1, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: Created sparse mask for total_precip with 10.0% data retained
1:      first 25 values: tensor([-0.8424, -0.8437, -0.8452, -0.8467, -0.8482, -0.8497, -0.8514, -0.8530,
1:         -0.8547, -0.8563, -0.8582, -0.8600, -0.8618, -0.8638, -0.8657, -0.8676,
1:         -0.8696, -0.8718, -0.8681, -0.8701, -0.8721, -0.8741, -0.8761, -0.8783,
1:         -0.8803], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.0822, 0.0826, 0.0830, 0.0833, 0.0837, 0.0839, 0.0843, 0.0846, 0.0850,
1:         0.0852, 0.0856, 0.0861, 0.0863, 0.0867, 0.0874, 0.0878, 0.0885, 0.0891,
1:         0.0399, 0.0401, 0.0405, 0.0407, 0.0412, 0.0414, 0.0418],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411,
1:         -0.5411, -0.5411, -0.5412, -0.5412, -0.5413, -0.5413, -0.5413, -0.5410,
1:         -0.5406, -0.5403, -0.5426, -0.5424, -0.5422, -0.5421, -0.5421, -0.5421,
1:         -0.5421], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.0003, 0.0048, 0.0094, 0.0162, 0.0208, 0.0254, 0.0322, 0.0367, 0.0413,
1:         0.0459, 0.0504, 0.0550, 0.0595, 0.0618, 0.0664, 0.0687, 0.0732, 0.0755,
1:         0.0755, 0.0823, 0.0869, 0.0915, 0.0960, 0.0983, 0.1029],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.2148, -0.2107, -0.2068, -0.2027, -0.1990, -0.1948, -0.1911, -0.1870,
1:         -0.1830, -0.1794, -0.1754, -0.1719, -0.1680, -0.1644, -0.1605, -0.1571,
1:         -0.1532, -0.1498, -0.1464, -0.1426, -0.1393, -0.1358, -0.1327, -0.1293,
1:         -0.1259], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2412,
1:             nan,     nan,     nan,     nan, -0.2474,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2510,     nan,     nan,     nan,
1:             nan, -0.2510,     nan,     nan, -0.2510,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2486,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2486,     nan,
1:             nan,     nan, -0.2486,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2425,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2461,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2498, -0.2522,     nan, -0.2522,     nan,
1:             nan,     nan, -0.2522,     nan, -0.2522,     nan, -0.2535,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2510,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2498,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2474,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2364,     nan,     nan,     nan,     nan,     nan,     nan, -0.2412,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2412,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 1, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5366, -0.5313, -0.5282, -0.5257, -0.5234, -0.5226, -0.5218, -0.5252,
1:         -0.5251, -0.5259, -0.5241, -0.5244, -0.5240, -0.5228, -0.5253, -0.5251,
1:         -0.5230, -0.5156, -0.5145, -0.5130, -0.5113, -0.5100, -0.5105, -0.5105,
1:         -0.5084], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.2319, -0.2276, -0.2290, -0.2313, -0.2333, -0.2326, -0.2294, -0.2255,
1:         -0.2198, -0.2167, -0.2138, -0.2144, -0.2176, -0.2228, -0.2268, -0.2281,
1:         -0.2280, -0.2271, -0.2674, -0.2601, -0.2614, -0.2625, -0.2625, -0.2626,
1:         -0.2604], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5808, -0.5824, -0.5853, -0.5850, -0.5866, -0.5854, -0.5855, -0.5843,
1:         -0.5850, -0.5847, -0.5854, -0.5856, -0.5873, -0.5869, -0.5873, -0.5868,
1:         -0.5868, -0.5871, -0.5838, -0.5872, -0.5888, -0.5897, -0.5902, -0.5898,
1:         -0.5876], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.3031, 0.3196, 0.3367, 0.3473, 0.3540, 0.3361, 0.3128, 0.2944, 0.2867,
1:         0.2782, 0.2787, 0.2905, 0.2978, 0.3056, 0.2957, 0.2763, 0.2937, 0.3155,
1:         0.3041, 0.3167, 0.3196, 0.3207, 0.3252, 0.3089, 0.2879],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.7628, -0.7605, -0.7568, -0.7557, -0.7535, -0.7528, -0.7526, -0.7539,
1:         -0.7561, -0.7586, -0.7604, -0.7606, -0.7590, -0.7574, -0.7566, -0.7562,
1:         -0.7575, -0.7592, -0.7602, -0.7592, -0.7566, -0.7516, -0.7477, -0.7477,
1:         -0.7502], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1422, -0.1492, -0.1585, -0.1633, -0.1647, -0.1687, -0.1707, -0.1610,
1:         -0.1612, -0.1449, -0.1424, -0.1546, -0.1597, -0.1647, -0.1634, -0.1658,
1:         -0.1654, -0.1572, -0.1410, -0.1439, -0.1469, -0.1477, -0.1536, -0.1584,
1:         -0.1587], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([-0.8424, -0.8437, -0.8452, -0.8467, -0.8482, -0.8497, -0.8514, -0.8530,
2:         -0.8547, -0.8563, -0.8582, -0.8600, -0.8618, -0.8638, -0.8657, -0.8676,
2:         -0.8696, -0.8718, -0.8681, -0.8701, -0.8721, -0.8741, -0.8761, -0.8783,
2:         -0.8803], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.0822, 0.0826, 0.0830, 0.0833, 0.0837, 0.0839, 0.0843, 0.0846, 0.0850,
2:         0.0852, 0.0856, 0.0861, 0.0863, 0.0867, 0.0874, 0.0878, 0.0885, 0.0891,
2:         0.0399, 0.0401, 0.0405, 0.0407, 0.0412, 0.0414, 0.0418],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411,
2:         -0.5411, -0.5411, -0.5412, -0.5412, -0.5413, -0.5413, -0.5413, -0.5410,
2:         -0.5406, -0.5403, -0.5426, -0.5424, -0.5422, -0.5421, -0.5421, -0.5421,
2:         -0.5421], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.0003, 0.0048, 0.0094, 0.0162, 0.0208, 0.0254, 0.0322, 0.0367, 0.0413,
2:         0.0459, 0.0504, 0.0550, 0.0595, 0.0618, 0.0664, 0.0687, 0.0732, 0.0755,
2:         0.0755, 0.0823, 0.0869, 0.0915, 0.0960, 0.0983, 0.1029],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.2148, -0.2107, -0.2068, -0.2027, -0.1990, -0.1948, -0.1911, -0.1870,
2:         -0.1830, -0.1794, -0.1754, -0.1719, -0.1680, -0.1644, -0.1605, -0.1571,
2:         -0.1532, -0.1498, -0.1464, -0.1426, -0.1393, -0.1358, -0.1327, -0.1293,
2:         -0.1259], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan, -0.2425,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2510,     nan,     nan,     nan,     nan,
2:         -0.2510,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2486,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2486,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2498,     nan,     nan,     nan,     nan,     nan, -0.2522,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2510,     nan,
2:             nan,     nan,     nan,     nan, -0.2486,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2364,     nan,     nan, -0.2351,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2364,
2:         -0.2364, -0.2364,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2388,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2412,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2412,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2498,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2522,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2486,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 1, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.5269, -0.5237, -0.5205, -0.5186, -0.5169, -0.5175, -0.5173, -0.5199,
2:         -0.5211, -0.5202, -0.5178, -0.5181, -0.5181, -0.5194, -0.5263, -0.5309,
2:         -0.5323, -0.5296, -0.5064, -0.5031, -0.5010, -0.4987, -0.4988, -0.4985,
2:         -0.4968], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.2391, -0.2365, -0.2397, -0.2460, -0.2551, -0.2589, -0.2603, -0.2568,
2:         -0.2501, -0.2414, -0.2355, -0.2315, -0.2309, -0.2310, -0.2315, -0.2300,
2:         -0.2286, -0.2280, -0.2749, -0.2710, -0.2730, -0.2779, -0.2824, -0.2866,
2:         -0.2859], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.5958, -0.5932, -0.5917, -0.5884, -0.5897, -0.5891, -0.5898, -0.5892,
2:         -0.5895, -0.5878, -0.5860, -0.5845, -0.5849, -0.5858, -0.5885, -0.5919,
2:         -0.5951, -0.5986, -0.5959, -0.5963, -0.5947, -0.5936, -0.5935, -0.5938,
2:         -0.5923], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.2189, 0.2380, 0.2733, 0.3013, 0.3125, 0.2949, 0.2869, 0.2627, 0.2520,
2:         0.2648, 0.2545, 0.2670, 0.2869, 0.2697, 0.2517, 0.2423, 0.2623, 0.3027,
2:         0.2642, 0.2705, 0.2738, 0.2791, 0.2837, 0.2727, 0.2802],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.7394, -0.7356, -0.7325, -0.7327, -0.7336, -0.7360, -0.7368, -0.7398,
2:         -0.7413, -0.7431, -0.7450, -0.7449, -0.7429, -0.7416, -0.7409, -0.7426,
2:         -0.7457, -0.7502, -0.7543, -0.7571, -0.7580, -0.7566, -0.7550, -0.7561,
2:         -0.7580], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1397, -0.1437, -0.1546, -0.1581, -0.1621, -0.1676, -0.1683, -0.1622,
2:         -0.1605, -0.1474, -0.1453, -0.1528, -0.1609, -0.1672, -0.1664, -0.1704,
2:         -0.1683, -0.1617, -0.1480, -0.1489, -0.1517, -0.1561, -0.1590, -0.1641,
2:         -0.1693], device='cuda:2', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 1, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.8424, -0.8437, -0.8452, -0.8467, -0.8482, -0.8497, -0.8514, -0.8530,
3:         -0.8547, -0.8563, -0.8582, -0.8600, -0.8618, -0.8638, -0.8657, -0.8676,
3:         -0.8696, -0.8718, -0.8681, -0.8701, -0.8721, -0.8741, -0.8761, -0.8783,
3:         -0.8803], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.0822, 0.0826, 0.0830, 0.0833, 0.0837, 0.0839, 0.0843, 0.0846, 0.0850,
3:         0.0852, 0.0856, 0.0861, 0.0863, 0.0867, 0.0874, 0.0878, 0.0885, 0.0891,
3:         0.0399, 0.0401, 0.0405, 0.0407, 0.0412, 0.0414, 0.0418],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411, -0.5411,
3:         -0.5411, -0.5411, -0.5412, -0.5412, -0.5413, -0.5413, -0.5413, -0.5410,
3:         -0.5406, -0.5403, -0.5426, -0.5424, -0.5422, -0.5421, -0.5421, -0.5421,
3:         -0.5421], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.0003, 0.0048, 0.0094, 0.0162, 0.0208, 0.0254, 0.0322, 0.0367, 0.0413,
3:         0.0459, 0.0504, 0.0550, 0.0595, 0.0618, 0.0664, 0.0687, 0.0732, 0.0755,
3:         0.0755, 0.0823, 0.0869, 0.0915, 0.0960, 0.0983, 0.1029],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.2148, -0.2107, -0.2068, -0.2027, -0.1990, -0.1948, -0.1911, -0.1870,
3:         -0.1830, -0.1794, -0.1754, -0.1719, -0.1680, -0.1644, -0.1605, -0.1571,
3:         -0.1532, -0.1498, -0.1464, -0.1426, -0.1393, -0.1358, -0.1327, -0.1293,
3:         -0.1259], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2486,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2474,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2486,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2425,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2498,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2498,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2364,     nan,
3:             nan, -0.2364,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2437,     nan, -0.2425,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2486,     nan,     nan,     nan,     nan, -0.2510,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 1, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5259, -0.5204, -0.5152, -0.5126, -0.5100, -0.5113, -0.5131, -0.5183,
3:         -0.5214, -0.5227, -0.5211, -0.5212, -0.5218, -0.5225, -0.5270, -0.5281,
3:         -0.5279, -0.5224, -0.4984, -0.4963, -0.4939, -0.4909, -0.4899, -0.4908,
3:         -0.4902], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.2175, -0.2097, -0.2086, -0.2120, -0.2178, -0.2192, -0.2194, -0.2166,
3:         -0.2103, -0.2040, -0.2013, -0.2000, -0.1995, -0.1986, -0.1967, -0.1948,
3:         -0.1945, -0.1949, -0.2506, -0.2439, -0.2419, -0.2416, -0.2419, -0.2441,
3:         -0.2428], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5877, -0.5868, -0.5873, -0.5852, -0.5859, -0.5846, -0.5848, -0.5836,
3:         -0.5839, -0.5835, -0.5827, -0.5831, -0.5837, -0.5836, -0.5844, -0.5860,
3:         -0.5886, -0.5926, -0.5880, -0.5893, -0.5886, -0.5871, -0.5864, -0.5861,
3:         -0.5842], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.1769, 0.2138, 0.2449, 0.2606, 0.2865, 0.2870, 0.2594, 0.2343, 0.2307,
3:         0.2247, 0.2133, 0.2229, 0.2341, 0.2354, 0.2303, 0.2243, 0.2417, 0.2681,
3:         0.2126, 0.2387, 0.2526, 0.2463, 0.2580, 0.2621, 0.2439],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.7391, -0.7403, -0.7416, -0.7445, -0.7458, -0.7466, -0.7451, -0.7440,
3:         -0.7430, -0.7432, -0.7446, -0.7467, -0.7481, -0.7499, -0.7524, -0.7567,
3:         -0.7624, -0.7679, -0.7716, -0.7722, -0.7707, -0.7675, -0.7655, -0.7667,
3:         -0.7704], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1528, -0.1556, -0.1620, -0.1640, -0.1637, -0.1652, -0.1664, -0.1590,
3:         -0.1590, -0.1565, -0.1511, -0.1577, -0.1604, -0.1637, -0.1592, -0.1631,
3:         -0.1658, -0.1561, -0.1532, -0.1546, -0.1527, -0.1526, -0.1540, -0.1561,
3:         -0.1579], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [1/5 (20%)]	Loss: nan : nan :: 0.14158 (1.73 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [2/5 (40%)]	Loss: nan : nan :: 0.14508 (10.52 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [3/5 (60%)]	Loss: nan : nan :: 0.15841 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 1 [4/5 (80%)]	Loss: nan : nan :: 0.13301 (10.42 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch1.mod
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 1 : nan
0: validation loss for velocity_u : 0.03647060692310333
0: validation loss for velocity_v : 0.06910236179828644
0: validation loss for specific_humidity : 0.02444598264992237
0: validation loss for velocity_z : 0.44547736644744873
0: validation loss for temperature : 0.08723518252372742
0: validation loss for total_precip : nan
2: 2 : 22:29:00 :: batch_size = 96, lr = 2e-05
1: 2 : 22:29:00 :: batch_size = 96, lr = 2e-05
3: 2 : 22:29:00 :: batch_size = 96, lr = 2e-05
0: 2 : 22:29:00 :: batch_size = 96, lr = 2e-05
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 2, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4732, -0.4748, -0.4764, -0.4780, -0.4797, -0.4813, -0.4829, -0.4845,
1:         -0.4861, -0.4877, -0.4893, -0.4909, -0.4925, -0.4941, -0.4956, -0.4972,
1:         -0.4988, -0.5004, -0.4157, -0.4169, -0.4181, -0.4194, -0.4206, -0.4218,
1:         -0.4231], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7105, -0.7094, -0.7085, -0.7074, -0.7063, -0.7053, -0.7042, -0.7031,
1:         -0.7020, -0.7009, -0.6998, -0.6988, -0.6977, -0.6966, -0.6955, -0.6944,
1:         -0.6933, -0.6922, -0.6237, -0.6228, -0.6217, -0.6206, -0.6197, -0.6187,
1:         -0.6178], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7393, -0.7393, -0.7393, -0.7395, -0.7395, -0.7395, -0.7395, -0.7397,
1:         -0.7397, -0.7397, -0.7397, -0.7397, -0.7399, -0.7399, -0.7399, -0.7399,
1:         -0.7401, -0.7401, -0.7333, -0.7333, -0.7333, -0.7333, -0.7333, -0.7333,
1:         -0.7333], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.3164, 0.3164, 0.3186, 0.3186, 0.3186, 0.3186, 0.3208, 0.3208, 0.3208,
1:         0.3208, 0.3208, 0.3230, 0.3230, 0.3230, 0.3230, 0.3251, 0.3251, 0.3251,
1:         0.3620, 0.3620, 0.3642, 0.3642, 0.3642, 0.3642, 0.3664],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.8765, -0.8757, -0.8745, -0.8738, -0.8730, -0.8722, -0.8714, -0.8706,
1:         -0.8698, -0.8690, -0.8682, -0.8674, -0.8666, -0.8658, -0.8651, -0.8642,
1:         -0.8634, -0.8627, -0.8618, -0.8611, -0.8602, -0.8594, -0.8586, -0.8578,
1:         -0.8569], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan, -0.0672,     nan,     nan,
1:             nan, -0.1519,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2121,     nan,     nan,
1:             nan,     nan,     nan, -0.2289,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2333, -0.2333,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2333,     nan,     nan,     nan,
1:         -0.2333,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2311,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.0137,     nan,     nan,     nan,     nan,     nan,
1:         -0.0137,     nan,     nan,     nan,     nan,     nan, -0.0962,     nan,
1:             nan,     nan,     nan,     nan, -0.1820, -0.1820,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2211,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2367,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2356,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2311,     nan,     nan,     nan, -0.2311,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2266,     nan,     nan,
1:             nan,     nan,     nan,     nan,  0.0622,  0.0633,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,  0.0488,  0.0488,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2255,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2400,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2378,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2233,     nan,     nan,
1:             nan,     nan, -0.2244])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 2, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.2013, -1.2138, -1.2182, -1.2204, -1.2181, -1.2179, -1.2142, -1.2148,
1:         -1.2181, -1.2206, -1.2234, -1.2271, -1.2271, -1.2271, -1.2231, -1.2201,
1:         -1.2179, -1.2076, -1.1590, -1.1601, -1.1696, -1.1710, -1.1682, -1.1714,
1:         -1.1715], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.4155, -1.4242, -1.4311, -1.4323, -1.4231, -1.4027, -1.3802, -1.3602,
1:         -1.3423, -1.3301, -1.3237, -1.3212, -1.3221, -1.3182, -1.3096, -1.2934,
1:         -1.2727, -1.2496, -1.3633, -1.3543, -1.3460, -1.3394, -1.3249, -1.3090,
1:         -1.2930], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.2026, -0.2110, -0.2151, -0.2152, -0.2175, -0.2171, -0.2181, -0.2184,
1:         -0.2205, -0.2163, -0.2129, -0.2058, -0.2070, -0.2064, -0.2054, -0.1998,
1:         -0.1903, -0.1786, -0.2149, -0.2269, -0.2312, -0.2327, -0.2301, -0.2262,
1:         -0.2228], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.3267, -0.3303, -0.3059, -0.2999, -0.3151, -0.3862, -0.4633, -0.5179,
1:         -0.5275, -0.5234, -0.5439, -0.5315, -0.5299, -0.5830, -0.6136, -0.6220,
1:         -0.6099, -0.5258, -0.1145, -0.0860, -0.0594, -0.0355, -0.0139, -0.0511,
1:         -0.0994], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.1215, -0.1214, -0.1132, -0.1003, -0.0893, -0.0793, -0.0724, -0.0618,
1:         -0.0448, -0.0271, -0.0152, -0.0118, -0.0170, -0.0258, -0.0273, -0.0239,
1:         -0.0167, -0.0109, -0.0077, -0.0064, -0.0084, -0.0118, -0.0220, -0.0372,
1:         -0.0530], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1422, -0.1468, -0.1529, -0.1549, -0.1562, -0.1603, -0.1637, -0.1553,
1:         -0.1539, -0.1527, -0.1479, -0.1544, -0.1614, -0.1622, -0.1593, -0.1635,
1:         -0.1614, -0.1553, -0.1546, -0.1546, -0.1559, -0.1584, -0.1589, -0.1613,
1:         -0.1613], device='cuda:1', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 2, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.4732, -0.4748, -0.4764, -0.4780, -0.4797, -0.4813, -0.4829, -0.4845,
2:         -0.4861, -0.4877, -0.4893, -0.4909, -0.4925, -0.4941, -0.4956, -0.4972,
2:         -0.4988, -0.5004, -0.4157, -0.4169, -0.4181, -0.4194, -0.4206, -0.4218,
2:         -0.4231], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7105, -0.7094, -0.7085, -0.7074, -0.7063, -0.7053, -0.7042, -0.7031,
2:         -0.7020, -0.7009, -0.6998, -0.6988, -0.6977, -0.6966, -0.6955, -0.6944,
2:         -0.6933, -0.6922, -0.6237, -0.6228, -0.6217, -0.6206, -0.6197, -0.6187,
2:         -0.6178], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7393, -0.7393, -0.7393, -0.7395, -0.7395, -0.7395, -0.7395, -0.7397,
2:         -0.7397, -0.7397, -0.7397, -0.7397, -0.7399, -0.7399, -0.7399, -0.7399,
2:         -0.7401, -0.7401, -0.7333, -0.7333, -0.7333, -0.7333, -0.7333, -0.7333,
2:         -0.7333], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.3164, 0.3164, 0.3186, 0.3186, 0.3186, 0.3186, 0.3208, 0.3208, 0.3208,
2:         0.3208, 0.3208, 0.3230, 0.3230, 0.3230, 0.3230, 0.3251, 0.3251, 0.3251,
2:         0.3620, 0.3620, 0.3642, 0.3642, 0.3642, 0.3642, 0.3664],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.8765, -0.8757, -0.8745, -0.8738, -0.8730, -0.8722, -0.8714, -0.8706,
2:         -0.8698, -0.8690, -0.8682, -0.8674, -0.8666, -0.8658, -0.8651, -0.8642,
2:         -0.8634, -0.8627, -0.8618, -0.8611, -0.8602, -0.8594, -0.8586, -0.8578,
2:         -0.8569], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0683,
2:             nan,     nan,     nan,     nan, -0.1553,     nan, -0.1564,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2300,     nan,     nan,     nan,
2:             nan, -0.2311,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2333,     nan, -0.2344,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2333,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2311,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.0137,     nan, -0.0137, -0.0137,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1809,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2233,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2333,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2300,     nan, -0.2311,     nan,
2:             nan,     nan,     nan,     nan, -0.2400,     nan,     nan, -0.2400,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2322,     nan,     nan,     nan,     nan,
2:             nan, -0.2322,     nan,     nan, -0.2233,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 2, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.2390, -1.2703, -1.2904, -1.2999, -1.2996, -1.2959, -1.2828, -1.2790,
2:         -1.2761, -1.2749, -1.2775, -1.2836, -1.2923, -1.2983, -1.3030, -1.3022,
2:         -1.2960, -1.2795, -1.1771, -1.1877, -1.2057, -1.2118, -1.2139, -1.2163,
2:         -1.2112], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.3986, -1.4054, -1.4089, -1.4038, -1.3950, -1.3816, -1.3709, -1.3624,
2:         -1.3578, -1.3503, -1.3429, -1.3312, -1.3218, -1.3123, -1.3016, -1.2842,
2:         -1.2589, -1.2318, -1.3380, -1.3317, -1.3258, -1.3181, -1.3047, -1.2924,
2:         -1.2855], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1812, -0.1906, -0.2002, -0.2027, -0.2060, -0.2055, -0.2058, -0.2063,
2:         -0.2074, -0.2068, -0.2086, -0.2055, -0.2126, -0.2160, -0.2171, -0.2116,
2:         -0.2036, -0.1904, -0.1883, -0.2002, -0.2126, -0.2169, -0.2170, -0.2132,
2:         -0.2133], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.4229, -0.4291, -0.3970, -0.3623, -0.3284, -0.3772, -0.4364, -0.4569,
2:         -0.4449, -0.4327, -0.4527, -0.4506, -0.4301, -0.4549, -0.4607, -0.4362,
2:         -0.4320, -0.3858, -0.1696, -0.1312, -0.0971, -0.0831, -0.0552, -0.0865,
2:         -0.1269], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.1103, -0.1179, -0.1167, -0.1086, -0.1009, -0.0953, -0.0958, -0.0941,
2:         -0.0866, -0.0743, -0.0628, -0.0564, -0.0551, -0.0544, -0.0485, -0.0367,
2:         -0.0250, -0.0168, -0.0169, -0.0224, -0.0329, -0.0443, -0.0562, -0.0672,
2:         -0.0766], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1150, -0.1262, -0.1394, -0.1482, -0.1602, -0.1659, -0.1705, -0.1639,
2:         -0.1614, -0.1222, -0.1251, -0.1392, -0.1508, -0.1600, -0.1640, -0.1691,
2:         -0.1661, -0.1590, -0.1251, -0.1300, -0.1379, -0.1438, -0.1543, -0.1605,
2:         -0.1640], device='cuda:2', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 2, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] INPUT BATCH
0: Epoch 2, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4732, -0.4748, -0.4764, -0.4780, -0.4797, -0.4813, -0.4829, -0.4845,
3:         -0.4861, -0.4877, -0.4893, -0.4909, -0.4925, -0.4941, -0.4956, -0.4972,
3:         -0.4988, -0.5004, -0.4157, -0.4169, -0.4181, -0.4194, -0.4206, -0.4218,
3:         -0.4231], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7105, -0.7094, -0.7085, -0.7074, -0.7063, -0.7053, -0.7042, -0.7031,
3:         -0.7020, -0.7009, -0.6998, -0.6988, -0.6977, -0.6966, -0.6955, -0.6944,
3:         -0.6933, -0.6922, -0.6237, -0.6228, -0.6217, -0.6206, -0.6197, -0.6187,
3:         -0.6178], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7393, -0.7393, -0.7393, -0.7395, -0.7395, -0.7395, -0.7395, -0.7397,
3:         -0.7397, -0.7397, -0.7397, -0.7397, -0.7399, -0.7399, -0.7399, -0.7399,
3:         -0.7401, -0.7401, -0.7333, -0.7333, -0.7333, -0.7333, -0.7333, -0.7333,
3:         -0.7333], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.3164, 0.3164, 0.3186, 0.3186, 0.3186, 0.3186, 0.3208, 0.3208, 0.3208,
3:         0.3208, 0.3208, 0.3230, 0.3230, 0.3230, 0.3230, 0.3251, 0.3251, 0.3251,
3:         0.3620, 0.3620, 0.3642, 0.3642, 0.3642, 0.3642, 0.3664],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.8765, -0.8757, -0.8745, -0.8738, -0.8730, -0.8722, -0.8714, -0.8706,
3:         -0.8698, -0.8690, -0.8682, -0.8674, -0.8666, -0.8658, -0.8651, -0.8642,
3:         -0.8634, -0.8627, -0.8618, -0.8611, -0.8602, -0.8594, -0.8586, -0.8578,
3:         -0.8569], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
0:      first 25 values: tensor([-0.4732, -0.4748, -0.4764, -0.4780, -0.4797, -0.4813, -0.4829, -0.4845,
0:         -0.4861, -0.4877, -0.4893, -0.4909, -0.4925, -0.4941, -0.4956, -0.4972,
0:         -0.4988, -0.5004, -0.4157, -0.4169, -0.4181, -0.4194, -0.4206, -0.4218,
0:         -0.4231], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] TARGET BATCH
3: Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0:      first 25 values: tensor([-0.7105, -0.7094, -0.7085, -0.7074, -0.7063, -0.7053, -0.7042, -0.7031,
0:         -0.7020, -0.7009, -0.6998, -0.6988, -0.6977, -0.6966, -0.6955, -0.6944,
0:         -0.6933, -0.6922, -0.6237, -0.6228, -0.6217, -0.6206, -0.6197, -0.6187,
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan, -0.0661,     nan,     nan,     nan,     nan,     nan,
3:         -0.0694,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2121,     nan,     nan,     nan,
3:             nan, -0.2144,     nan,     nan, -0.2300,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2333,     nan,     nan,     nan,     nan,
3:         -0.2311,     nan,     nan, -0.2311,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2266,     nan,
3:         -0.2278,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.0973,     nan,     nan,     nan,     nan, -0.1820,     nan,     nan,
3:             nan, -0.1843,     nan, -0.1854,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2356,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2333,
3:             nan, -0.2311, -0.2311,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2255,     nan, -0.2266, -0.2266,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,  0.0499,  0.0499,  0.0499,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.0683,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2289,     nan,     nan,     nan,     nan,
3:         -0.2400,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2378,     nan, -0.2378,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2322,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.6178], device='cuda:0')
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 2, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 pred values: tensor([-1.1983, -1.2211, -1.2392, -1.2477, -1.2564, -1.2637, -1.2637, -1.2634,
3:         -1.2568, -1.2544, -1.2567, -1.2633, -1.2673, -1.2697, -1.2633, -1.2505,
3:         -1.2325, -1.2101, -1.1349, -1.1535, -1.1728, -1.1825, -1.1908, -1.2003,
3:         -1.1997], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 values: tensor([-0.7393, -0.7393, -0.7393, -0.7395, -0.7395, -0.7395, -0.7395, -0.7397,
0:         -0.7397, -0.7397, -0.7397, -0.7397, -0.7399, -0.7399, -0.7399, -0.7399,
0:         -0.7401, -0.7401, -0.7333, -0.7333, -0.7333, -0.7333, -0.7333, -0.7333,
0:         -0.7333], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 pred values: tensor([-1.3740, -1.3871, -1.3991, -1.4040, -1.4010, -1.3892, -1.3755, -1.3590,
3:         -1.3420, -1.3254, -1.3121, -1.3048, -1.3042, -1.3047, -1.2994, -1.2861,
3:         -1.2651, -1.2471, -1.3201, -1.3174, -1.3173, -1.3143, -1.3050, -1.2918,
3:         -1.2803], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 values: tensor([0.3164, 0.3164, 0.3186, 0.3186, 0.3186, 0.3186, 0.3208, 0.3208, 0.3208,
0:         0.3208, 0.3208, 0.3230, 0.3230, 0.3230, 0.3230, 0.3251, 0.3251, 0.3251,
0:         0.3620, 0.3620, 0.3642, 0.3642, 0.3642, 0.3642, 0.3664],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 pred values: tensor([-0.1670, -0.1792, -0.1872, -0.1915, -0.1995, -0.2021, -0.2062, -0.2062,
3:         -0.2063, -0.2007, -0.1957, -0.1860, -0.1875, -0.1889, -0.1927, -0.1927,
3:         -0.1890, -0.1793, -0.1942, -0.2090, -0.2185, -0.2254, -0.2278, -0.2285,
3:         -0.2295], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 values: tensor([-0.8765, -0.8757, -0.8745, -0.8738, -0.8730, -0.8722, -0.8714, -0.8706,
0:         -0.8698, -0.8690, -0.8682, -0.8674, -0.8666, -0.8658, -0.8651, -0.8642,
0:         -0.8634, -0.8627, -0.8618, -0.8611, -0.8602, -0.8594, -0.8586, -0.8578,
0:         -0.8569], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 pred values: tensor([-0.3179, -0.3346, -0.3267, -0.3321, -0.3581, -0.4235, -0.4841, -0.5163,
3:         -0.5092, -0.5097, -0.5184, -0.4902, -0.4917, -0.5498, -0.5886, -0.5832,
3:         -0.5727, -0.5427, -0.1263, -0.0803, -0.0474, -0.0311, -0.0319, -0.0733,
3:         -0.1226], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
3:      first 25 pred values: tensor([-0.1164, -0.1217, -0.1159, -0.1038, -0.0921, -0.0827, -0.0813, -0.0785,
3:         -0.0726, -0.0653, -0.0590, -0.0582, -0.0603, -0.0632, -0.0584, -0.0469,
3:         -0.0331, -0.0235, -0.0193, -0.0196, -0.0245, -0.0303, -0.0393, -0.0472,
3:         -0.0538], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1417, -0.1489, -0.1596, -0.1633, -0.1668, -0.1707, -0.1701, -0.1655,
3:         -0.1629, -0.1533, -0.1490, -0.1565, -0.1667, -0.1696, -0.1669, -0.1701,
3:         -0.1679, -0.1591, -0.1519, -0.1535, -0.1537, -0.1579, -0.1613, -0.1646,
3:         -0.1649], device='cuda:3', grad_fn=<SliceBackward0>)
0: [DEBUG] TARGET BATCH
0: Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.0650,     nan,     nan,     nan,     nan, -0.0683, -0.0683,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1575,
0:         -0.1586, -0.1597,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2144,     nan,     nan, -0.2289,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2311,     nan, -0.2311,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0137,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1809,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2233,     nan,     nan,     nan,     nan, -0.2367,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2333,     nan,
0:             nan,     nan, -0.2311,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2266,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0705, -0.0705,     nan, -0.0728,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1809,     nan, -0.2255,
0:             nan,     nan,     nan, -0.2289,     nan,     nan, -0.2311,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2378,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2322,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2233,     nan,     nan, -0.2233,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 2, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1622, -1.1970, -1.2268, -1.2423, -1.2534, -1.2567, -1.2517, -1.2465,
0:         -1.2417, -1.2354, -1.2339, -1.2381, -1.2425, -1.2496, -1.2525, -1.2512,
0:         -1.2417, -1.2183, -1.0788, -1.0979, -1.1223, -1.1385, -1.1483, -1.1619,
0:         -1.1682], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.4044, -1.4103, -1.4189, -1.4211, -1.4170, -1.4003, -1.3842, -1.3664,
0:         -1.3537, -1.3427, -1.3347, -1.3283, -1.3246, -1.3195, -1.3101, -1.2955,
0:         -1.2764, -1.2613, -1.3336, -1.3229, -1.3187, -1.3153, -1.3077, -1.2960,
0:         -1.2837], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1956, -0.2022, -0.2095, -0.2115, -0.2181, -0.2161, -0.2148, -0.2136,
0:         -0.2114, -0.2066, -0.2057, -0.2012, -0.2043, -0.2022, -0.1983, -0.1899,
0:         -0.1788, -0.1627, -0.1992, -0.2100, -0.2185, -0.2225, -0.2247, -0.2228,
0:         -0.2183], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4086, -0.4202, -0.3708, -0.3675, -0.3846, -0.4445, -0.5098, -0.5456,
0:         -0.5332, -0.5281, -0.5514, -0.5335, -0.5354, -0.5861, -0.5853, -0.5727,
0:         -0.5815, -0.5497, -0.1549, -0.1246, -0.0889, -0.0883, -0.0764, -0.0998,
0:         -0.1371], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.2741e-01, -1.2490e-01, -1.1455e-01, -1.0073e-01, -8.8976e-02,
0:         -8.1132e-02, -7.7616e-02, -7.1413e-02, -6.0081e-02, -4.7446e-02,
0:         -3.6828e-02, -3.2513e-02, -3.3562e-02, -3.6428e-02, -3.6416e-02,
0:         -3.3175e-02, -2.7686e-02, -2.2159e-02, -1.2615e-02, -2.9938e-05,
0:          9.8009e-03,  1.5885e-02,  1.2285e-02,  4.5065e-04, -1.3843e-02],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1307, -0.1326, -0.1432, -0.1490, -0.1533, -0.1584, -0.1636, -0.1581,
0:         -0.1552, -0.1395, -0.1358, -0.1452, -0.1524, -0.1584, -0.1595, -0.1632,
0:         -0.1623, -0.1538, -0.1426, -0.1439, -0.1466, -0.1492, -0.1531, -0.1563,
0:         -0.1594], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 2 [1/5 (20%)]	Loss: nan : nan :: 0.13974 (1.91 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 2 [2/5 (40%)]	Loss: nan : nan :: 0.16312 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 2 [3/5 (60%)]	Loss: nan : nan :: 0.13768 (10.42 s/sec)
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 2 [4/5 (80%)]	Loss: nan : nan :: 0.14083 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch2.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch2.mod
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 2 : nan
0: validation loss for velocity_u : 0.03277414292097092
0: validation loss for velocity_v : 0.0628073439002037
0: validation loss for specific_humidity : 0.02318628691136837
0: validation loss for velocity_z : 0.41340580582618713
0: validation loss for temperature : 0.07708601653575897
0: validation loss for total_precip : nan
2: 3 : 22:35:27 :: batch_size = 96, lr = 1.9512195121951222e-05
1: 3 : 22:35:27 :: batch_size = 96, lr = 1.9512195121951222e-05
0: 3 : 22:35:27 :: batch_size = 96, lr = 1.9512195121951222e-05
3: 3 : 22:35:27 :: batch_size = 96, lr = 1.9512195121951222e-05
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 3, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.8662, -0.9314, -0.9901, -1.0391, -1.0777, -1.1060, -1.1248, -1.1349,
3:         -1.1369, -1.1310, -1.1178, -1.0972, -1.0694, -1.0347, -0.9934, -0.9460,
3:         -0.8933, -0.8363, -0.6860, -0.7361, -0.7836, -0.8256, -0.8601, -0.8871,
3:         -0.9067], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.3477,  0.2771,  0.1987,  0.1167,  0.0330, -0.0513, -0.1375, -0.2268,
3:         -0.3188, -0.4117, -0.5024, -0.5897, -0.6721, -0.7482, -0.8167, -0.8764,
3:         -0.9266, -0.9678,  0.2025,  0.1445,  0.0798,  0.0106, -0.0598, -0.1299,
3:         -0.1989], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.3312, -0.3335, -0.3502, -0.3619, -0.3773, -0.3955, -0.4039, -0.4187,
3:         -0.4280, -0.4350, -0.4414, -0.4433, -0.4441, -0.4432, -0.4377, -0.4328,
3:         -0.4268, -0.4189, -0.3184, -0.3155, -0.3126, -0.3160, -0.3195, -0.3208,
3:         -0.3243], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 1.1403,  0.9838,  0.7155,  0.4160,  0.1533, -0.0300, -0.1273, -0.1463,
3:         -0.1027, -0.0088,  0.1175,  0.2539,  0.3791,  0.4730,  0.5233,  0.5210,
3:          0.4674,  0.3679,  0.8061,  0.8888,  0.7882,  0.5468,  0.2472, -0.0222,
3:         -0.2078], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.5652, 0.5490, 0.5401, 0.5376, 0.5396, 0.5441, 0.5496, 0.5561, 0.5631,
3:         0.5705, 0.5775, 0.5839, 0.5897, 0.5955, 0.6015, 0.6081, 0.6154, 0.6236,
3:         0.6326, 0.6420, 0.6517, 0.6615, 0.6719, 0.6828, 0.6938],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1809,     nan,     nan,     nan, -0.1891,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1045,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.1644,     nan,     nan, -0.1656,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.1632,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.0352,     nan,     nan,     nan,
3:             nan,     nan,  0.0939,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0540,
3:             nan, -0.0047, -0.1668,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.1621,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.1503,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1351,
3:             nan,     nan,     nan, -0.0658,     nan,     nan,     nan,     nan,
3:          0.2900,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,  0.3722,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  0.2536,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 3, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([4.1126, 4.0657, 4.0061, 3.9241, 3.8236, 3.7261, 3.6184, 3.5127, 3.4306,
3:         3.3418, 3.2540, 3.1517, 3.0479, 2.9517, 2.8671, 2.7989, 2.7572, 2.7104,
3:         4.2082, 4.2106, 4.1703, 4.1232, 4.0364, 3.9342, 3.8108],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.2493, -1.2910, -1.3100, -1.3244, -1.3326, -1.3195, -1.3075, -1.2696,
3:         -1.2243, -1.1698, -1.1232, -1.0695, -1.0260, -0.9620, -0.8934, -0.8299,
3:         -0.7692, -0.7228, -1.2965, -1.3434, -1.3644, -1.3879, -1.3994, -1.4121,
3:         -1.4212], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.7165, -0.7202, -0.7130, -0.7007, -0.6824, -0.6682, -0.6575, -0.6449,
3:         -0.6321, -0.6169, -0.5981, -0.5755, -0.5493, -0.5263, -0.5040, -0.4870,
3:         -0.4737, -0.4652, -0.7260, -0.7369, -0.7358, -0.7256, -0.7158, -0.7056,
3:         -0.6974], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.3114,  0.2042, -0.0129, -0.2143, -0.3094, -0.2892, -0.2557, -0.3157,
3:         -0.3771, -0.4406, -0.5499, -0.6836, -0.8434, -1.0170, -1.1399, -1.1984,
3:         -1.2637, -1.2585,  0.3370,  0.2756,  0.1549,  0.0218, -0.1047, -0.1682,
3:         -0.2145], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([1.5580, 1.6438, 1.7212, 1.7807, 1.8236, 1.8503, 1.8701, 1.8899, 1.9030,
3:         1.9125, 1.9193, 1.9219, 1.9236, 1.9243, 1.9188, 1.9091, 1.8968, 1.8903,
3:         1.8961, 1.9137, 1.9344, 1.9539, 1.9687, 1.9796, 1.9956],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1583, -0.1566, -0.1568, -0.1519, -0.1517, -0.1514, -0.1515, -0.1466,
3:         -0.1491, -0.1624, -0.1563, -0.1552, -0.1550, -0.1544, -0.1496, -0.1561,
3:         -0.1527, -0.1469, -0.1604, -0.1587, -0.1522, -0.1524, -0.1508, -0.1483,
3:         -0.1512], device='cuda:3', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 3, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 3, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 3, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8662, -0.9314, -0.9901, -1.0391, -1.0777, -1.1060, -1.1248, -1.1349,
0:         -1.1369, -1.1310, -1.1178, -1.0972, -1.0694, -1.0347, -0.9934, -0.9460,
0:         -0.8933, -0.8363, -0.6860, -0.7361, -0.7836, -0.8256, -0.8601, -0.8871,
0:         -0.9067], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3477,  0.2771,  0.1987,  0.1167,  0.0330, -0.0513, -0.1375, -0.2268,
0:         -0.3188, -0.4117, -0.5024, -0.5897, -0.6721, -0.7482, -0.8167, -0.8764,
0:         -0.9266, -0.9678,  0.2025,  0.1445,  0.0798,  0.0106, -0.0598, -0.1299,
0:         -0.1989], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3312, -0.3335, -0.3502, -0.3619, -0.3773, -0.3955, -0.4039, -0.4187,
0:         -0.4280, -0.4350, -0.4414, -0.4433, -0.4441, -0.4432, -0.4377, -0.4328,
0:         -0.4268, -0.4189, -0.3184, -0.3155, -0.3126, -0.3160, -0.3195, -0.3208,
0:         -0.3243], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.1403,  0.9838,  0.7155,  0.4160,  0.1533, -0.0300, -0.1273, -0.1463,
0:         -0.1027, -0.0088,  0.1175,  0.2539,  0.3791,  0.4730,  0.5233,  0.5210,
0:          0.4674,  0.3679,  0.8061,  0.8888,  0.7882,  0.5468,  0.2472, -0.0222,
0:         -0.2078], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.5652, 0.5490, 0.5401, 0.5376, 0.5396, 0.5441, 0.5496, 0.5561, 0.5631,
0:         0.5705, 0.5775, 0.5839, 0.5897, 0.5955, 0.6015, 0.6081, 0.6154, 0.6236,
0:         0.6326, 0.6420, 0.6517, 0.6615, 0.6719, 0.6828, 0.6938],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1703,     nan,
0:         -0.1351,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1351,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1445,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1515,     nan,     nan, -0.1045,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1069,     nan,
0:             nan, -0.0810,     nan,     nan,     nan,     nan, -0.0928,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0775,     nan,     nan,
0:         -0.0634,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1762,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1480,     nan,     nan,     nan,     nan, -0.0576,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0388, -0.0352,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.1820,     nan,  0.1468,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1891,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1644,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1973,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1820,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.0634,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1327,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1843,     nan,     nan,     nan,  0.1256,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 3, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([4.1912, 4.1653, 4.1200, 4.0524, 3.9537, 3.8574, 3.7560, 3.6562, 3.5730,
0:         3.4929, 3.4068, 3.3110, 3.2221, 3.1434, 3.0776, 3.0238, 2.9808, 2.9199,
0:         4.2589, 4.2712, 4.2404, 4.1966, 4.1236, 4.0396, 3.9381],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0930, -1.1353, -1.1528, -1.1716, -1.1902, -1.1965, -1.2083, -1.1952,
0:         -1.1615, -1.1133, -1.0705, -1.0148, -0.9621, -0.8861, -0.8041, -0.7424,
0:         -0.6972, -0.6749, -1.1724, -1.2354, -1.2591, -1.2767, -1.2894, -1.3144,
0:         -1.3394], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6963, -0.7005, -0.6960, -0.6845, -0.6707, -0.6606, -0.6522, -0.6386,
0:         -0.6256, -0.6080, -0.5853, -0.5618, -0.5370, -0.5149, -0.4940, -0.4781,
0:         -0.4623, -0.4522, -0.7170, -0.7229, -0.7231, -0.7139, -0.7058, -0.6988,
0:         -0.6937], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1091,  0.0206, -0.0183, -0.0906, -0.1000, -0.0665, -0.1082, -0.2665,
0:         -0.4180, -0.5449, -0.6935, -0.8078, -0.8963, -1.0013, -1.0685, -1.0594,
0:         -1.0063, -0.9080,  0.1234,  0.1218,  0.1213,  0.0384, -0.0196, -0.0319,
0:         -0.0769], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.5172, 1.5915, 1.6589, 1.7097, 1.7527, 1.7837, 1.8188, 1.8614, 1.9003,
0:         1.9308, 1.9496, 1.9517, 1.9433, 1.9342, 1.9198, 1.9078, 1.8978, 1.8942,
0:         1.8969, 1.9060, 1.9144, 1.9240, 1.9360, 1.9497, 1.9700],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1660, -0.1698, -0.1742, -0.1744, -0.1761, -0.1785, -0.1816, -0.1780,
0:         -0.1766, -0.1752, -0.1703, -0.1744, -0.1779, -0.1802, -0.1787, -0.1845,
0:         -0.1810, -0.1768, -0.1757, -0.1747, -0.1722, -0.1728, -0.1748, -0.1773,
0:         -0.1792], device='cuda:0', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-0.8662, -0.9314, -0.9901, -1.0391, -1.0777, -1.1060, -1.1248, -1.1349,
1:         -1.1369, -1.1310, -1.1178, -1.0972, -1.0694, -1.0347, -0.9934, -0.9460,
1:         -0.8933, -0.8363, -0.6860, -0.7361, -0.7836, -0.8256, -0.8601, -0.8871,
1:         -0.9067], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.3477,  0.2771,  0.1987,  0.1167,  0.0330, -0.0513, -0.1375, -0.2268,
1:         -0.3188, -0.4117, -0.5024, -0.5897, -0.6721, -0.7482, -0.8167, -0.8764,
1:         -0.9266, -0.9678,  0.2025,  0.1445,  0.0798,  0.0106, -0.0598, -0.1299,
1:         -0.1989], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.3312, -0.3335, -0.3502, -0.3619, -0.3773, -0.3955, -0.4039, -0.4187,
1:         -0.4280, -0.4350, -0.4414, -0.4433, -0.4441, -0.4432, -0.4377, -0.4328,
1:         -0.4268, -0.4189, -0.3184, -0.3155, -0.3126, -0.3160, -0.3195, -0.3208,
1:         -0.3243], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 1.1403,  0.9838,  0.7155,  0.4160,  0.1533, -0.0300, -0.1273, -0.1463,
1:         -0.1027, -0.0088,  0.1175,  0.2539,  0.3791,  0.4730,  0.5233,  0.5210,
1:          0.4674,  0.3679,  0.8061,  0.8888,  0.7882,  0.5468,  0.2472, -0.0222,
1:         -0.2078], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.5652, 0.5490, 0.5401, 0.5376, 0.5396, 0.5441, 0.5496, 0.5561, 0.5631,
1:         0.5705, 0.5775, 0.5839, 0.5897, 0.5955, 0.6015, 0.6081, 0.6154, 0.6236,
1:         0.6326, 0.6420, 0.6517, 0.6615, 0.6719, 0.6828, 0.6938],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
2:      first 25 values: tensor([-0.8662, -0.9314, -0.9901, -1.0391, -1.0777, -1.1060, -1.1248, -1.1349,
2:         -1.1369, -1.1310, -1.1178, -1.0972, -1.0694, -1.0347, -0.9934, -0.9460,
2:         -0.8933, -0.8363, -0.6860, -0.7361, -0.7836, -0.8256, -0.8601, -0.8871,
2:         -0.9067], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] TARGET BATCH
1: Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2:      first 25 values: tensor([ 0.3477,  0.2771,  0.1987,  0.1167,  0.0330, -0.0513, -0.1375, -0.2268,
2:         -0.3188, -0.4117, -0.5024, -0.5897, -0.6721, -0.7482, -0.8167, -0.8764,
2:         -0.9266, -0.9678,  0.2025,  0.1445,  0.0798,  0.0106, -0.0598, -0.1299,
2:         -0.1989], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan, -0.1656,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1351, -0.1128, -0.1809,     nan,     nan, -0.1903,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1691,     nan,     nan, -0.1362,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.0810,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.0141,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2032,     nan,     nan,     nan,     nan,     nan, -0.1644,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1679,     nan, -0.1762,     nan, -0.1421,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1010,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.1374,     nan,     nan,     nan,
1:             nan,     nan, -0.0223,     nan,     nan,     nan,     nan,     nan,
1:          0.2184,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2008,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1081,     nan,     nan,
1:          0.2900,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  0.4074,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:          0.0622,     nan,     nan,  0.1679,     nan,  0.1327,     nan,     nan,
1:          0.0399,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 3, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([-0.3312, -0.3335, -0.3502, -0.3619, -0.3773, -0.3955, -0.4039, -0.4187,
2:         -0.4280, -0.4350, -0.4414, -0.4433, -0.4441, -0.4432, -0.4377, -0.4328,
2:         -0.4268, -0.4189, -0.3184, -0.3155, -0.3126, -0.3160, -0.3195, -0.3208,
2:         -0.3243], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 pred values: tensor([4.1156, 4.1071, 4.0960, 4.0613, 3.9904, 3.8999, 3.7820, 3.6577, 3.5612,
1:         3.4626, 3.3795, 3.2909, 3.2054, 3.1311, 3.0610, 2.9998, 2.9538, 2.9057,
1:         4.2248, 4.2576, 4.2652, 4.2540, 4.1960, 4.1072, 3.9784],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([ 1.1403,  0.9838,  0.7155,  0.4160,  0.1533, -0.0300, -0.1273, -0.1463,
2:         -0.1027, -0.0088,  0.1175,  0.2539,  0.3791,  0.4730,  0.5233,  0.5210,
2:          0.4674,  0.3679,  0.8061,  0.8888,  0.7882,  0.5468,  0.2472, -0.0222,
2:         -0.2078], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 pred values: tensor([-1.1085, -1.1435, -1.1768, -1.2234, -1.2698, -1.2980, -1.3237, -1.3182,
1:         -1.2880, -1.2402, -1.1990, -1.1543, -1.1205, -1.0601, -0.9880, -0.9054,
1:         -0.8196, -0.7579, -1.0848, -1.1357, -1.1790, -1.2301, -1.2868, -1.3503,
1:         -1.4107], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([0.5652, 0.5490, 0.5401, 0.5376, 0.5396, 0.5441, 0.5496, 0.5561, 0.5631,
2:         0.5705, 0.5775, 0.5839, 0.5897, 0.5955, 0.6015, 0.6081, 0.6154, 0.6236,
2:         0.6326, 0.6420, 0.6517, 0.6615, 0.6719, 0.6828, 0.6938],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
1:      first 25 pred values: tensor([-0.7282, -0.7241, -0.7129, -0.6974, -0.6783, -0.6622, -0.6453, -0.6247,
1:         -0.6042, -0.5811, -0.5534, -0.5252, -0.4921, -0.4632, -0.4333, -0.4115,
1:         -0.3940, -0.3862, -0.7513, -0.7544, -0.7502, -0.7362, -0.7242, -0.7088,
1:         -0.6934], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.3407,  0.1912, -0.0292, -0.2715, -0.4565, -0.5065, -0.5216, -0.6494,
1:         -0.7723, -0.8558, -0.9741, -1.1065, -1.2196, -1.3160, -1.3347, -1.2915,
1:         -1.2700, -1.1924,  0.5335,  0.4061,  0.2380,  0.0385, -0.1823, -0.2945,
1:         -0.3714], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([1.5512, 1.6224, 1.6905, 1.7471, 1.7959, 1.8345, 1.8692, 1.9015, 1.9258,
1:         1.9409, 1.9474, 1.9471, 1.9448, 1.9423, 1.9331, 1.9201, 1.9078, 1.9069,
1:         1.9204, 1.9455, 1.9720, 1.9942, 2.0096, 2.0234, 2.0411],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1553, -0.1561, -0.1627, -0.1576, -0.1588, -0.1612, -0.1638, -0.1621,
1:         -0.1598, -0.1658, -0.1594, -0.1630, -0.1636, -0.1620, -0.1586, -0.1629,
1:         -0.1632, -0.1580, -0.1655, -0.1619, -0.1603, -0.1570, -0.1587, -0.1586,
1:         -0.1619], device='cuda:1', grad_fn=<SliceBackward0>)
2: [DEBUG] TARGET BATCH
2: Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan, -0.2102,     nan,     nan, -0.1820,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1762, -0.1809,     nan,
2:         -0.1785,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1515,     nan, -0.1351,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1022,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.0634,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1832,     nan,     nan,     nan,     nan,
2:             nan, -0.1315,     nan,     nan, -0.1679,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.0376,     nan,     nan,
2:             nan,     nan,  0.0939,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,  0.2524,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1668, -0.1891,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1820,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.0740,     nan,     nan,     nan,     nan,     nan,
2:          0.2900,     nan,     nan,     nan,  0.0704,     nan,     nan,  0.1749,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,  0.2806,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 3, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([4.1384, 4.1278, 4.1051, 4.0611, 3.9936, 3.9287, 3.8425, 3.7618, 3.7012,
2:         3.6298, 3.5607, 3.4755, 3.3815, 3.2884, 3.2043, 3.1241, 3.0648, 3.0075,
2:         4.3276, 4.3432, 4.3171, 4.2747, 4.2089, 4.1237, 4.0160],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.0773, -1.1209, -1.1397, -1.1527, -1.1588, -1.1572, -1.1594, -1.1437,
2:         -1.1219, -1.0892, -1.0631, -1.0297, -0.9977, -0.9421, -0.8809, -0.8279,
2:         -0.7857, -0.7670, -1.1119, -1.1635, -1.1791, -1.1908, -1.2050, -1.2328,
2:         -1.2660], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6767, -0.6851, -0.6836, -0.6739, -0.6567, -0.6448, -0.6346, -0.6257,
2:         -0.6189, -0.6098, -0.5961, -0.5750, -0.5472, -0.5220, -0.4969, -0.4765,
2:         -0.4603, -0.4484, -0.6943, -0.7089, -0.7126, -0.7043, -0.6937, -0.6815,
2:         -0.6744], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.5329,  0.4601,  0.3148,  0.1287, -0.1012, -0.3085, -0.4482, -0.5985,
2:         -0.7089, -0.7522, -0.7778, -0.8513, -0.9995, -1.1281, -1.1334, -1.0994,
2:         -1.0547, -0.9226,  0.6306,  0.5778,  0.4550,  0.2671,  0.0259, -0.2018,
2:         -0.3912], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([1.5437, 1.6339, 1.7122, 1.7687, 1.8084, 1.8345, 1.8606, 1.8907, 1.9205,
2:         1.9452, 1.9592, 1.9609, 1.9542, 1.9456, 1.9317, 1.9180, 1.9057, 1.9015,
2:         1.9086, 1.9242, 1.9410, 1.9565, 1.9696, 1.9844, 2.0065],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1733, -0.1728, -0.1774, -0.1728, -0.1707, -0.1722, -0.1740, -0.1694,
2:         -0.1703, -0.1791, -0.1739, -0.1743, -0.1751, -0.1743, -0.1705, -0.1725,
2:         -0.1707, -0.1671, -0.1775, -0.1748, -0.1697, -0.1677, -0.1668, -0.1638,
2:         -0.1677], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 3 [1/5 (20%)]	Loss: nan : nan :: 0.14584 (1.74 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 3 [2/5 (40%)]	Loss: nan : nan :: 0.14588 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 3 [3/5 (60%)]	Loss: nan : nan :: 0.14410 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 3 [4/5 (80%)]	Loss: nan : nan :: 0.13953 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch3.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch3.mod
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 3 : nan
0: validation loss for velocity_u : 0.03739774972200394
0: validation loss for velocity_v : 0.07081315666437149
0: validation loss for specific_humidity : 0.0265545304864645
0: validation loss for velocity_z : 0.5013574361801147
0: validation loss for temperature : 0.07317745685577393
0: validation loss for total_precip : nan
2: 4 : 22:42:06 :: batch_size = 96, lr = 1.903628792385485e-05
0: 4 : 22:42:06 :: batch_size = 96, lr = 1.903628792385485e-05
3: 4 : 22:42:06 :: batch_size = 96, lr = 1.903628792385485e-05
1: 4 : 22:42:06 :: batch_size = 96, lr = 1.903628792385485e-05
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 4, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 4, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.2840, 1.2636, 1.2429, 1.2217, 1.1993, 1.1756, 1.1508, 1.1248, 1.0982,
2:         1.0705, 1.0400, 1.0052, 0.9661, 0.9241, 0.8819, 0.8426, 0.8079, 0.7781,
2:         1.2788, 1.2616, 1.2438, 1.2241, 1.2021, 1.1772, 1.1503],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.8801, 1.9569, 2.0297, 2.0876, 2.1224, 2.1313, 2.1112, 2.0582, 1.9698,
2:         1.8508, 1.7119, 1.5657, 1.4226, 1.2912, 1.1804, 1.0972, 1.0453, 1.0221,
2:         1.8658, 1.9422, 2.0082, 2.0549, 2.0766, 2.0698, 2.0316],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.4163, -0.4271, -0.4456, -0.4712, -0.5098, -0.5508, -0.5983, -0.6466,
2:         -0.6838, -0.7231, -0.7410, -0.7546, -0.7557, -0.7573, -0.7555, -0.7533,
2:         -0.7518, -0.7507, -0.4238, -0.4375, -0.4578, -0.4880, -0.5264, -0.5722,
2:         -0.6201], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.5509, 0.7293, 0.8988, 1.0304, 1.1263, 1.2066, 1.2691, 1.2892, 1.2468,
2:         1.1620, 1.0750, 1.0081, 0.9635, 0.9523, 0.9836, 1.0438, 1.0884, 1.0884,
2:         0.6022, 0.7516, 0.8765, 0.9747, 1.0527, 1.1129, 1.1486],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.5429, -0.5785, -0.6096, -0.6390, -0.6713, -0.7101, -0.7600, -0.8240,
2:         -0.9048, -1.0027, -1.1134, -1.2298, -1.3432, -1.4441, -1.5247, -1.5813,
2:         -1.6153, -1.6301, -1.6308, -1.6231, -1.6100, -1.5929, -1.5725, -1.5509,
2:         -1.5320], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1447,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2249,     nan,     nan, -0.2272,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2446,
2:             nan,     nan, -0.2446,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2423,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2400,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2446,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2434,     nan,
2:             nan,     nan, -0.2434, -0.2434,     nan,     nan,     nan, -0.2446,
2:             nan,     nan,     nan,     nan, -0.2423,     nan,     nan, -0.2423,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2434,     nan, -0.2423,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2434,     nan,
2:             nan,     nan,     nan,     nan, -0.2411,     nan,     nan,     nan,
2:             nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2411,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,
2:             nan,     nan, -0.2458,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 4, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.2012, 0.2173, 0.2384, 0.2628, 0.2895, 0.3156, 0.3403, 0.3615, 0.3824,
2:         0.4024, 0.4249, 0.4467, 0.4707, 0.4954, 0.5224, 0.5519, 0.5879, 0.6358,
2:         0.2766, 0.3016, 0.3251, 0.3510, 0.3774, 0.4046, 0.4310],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.2457, 1.2029, 1.1544, 1.1067, 1.0585, 1.0126, 0.9652, 0.9226, 0.8865,
2:         0.8585, 0.8340, 0.8140, 0.7904, 0.7670, 0.7450, 0.7230, 0.7082, 0.6978,
2:         1.2742, 1.2262, 1.1759, 1.1283, 1.0860, 1.0460, 1.0020],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.5000, -0.4997, -0.4980, -0.4962, -0.4952, -0.4947, -0.4974, -0.4964,
2:         -0.4988, -0.4993, -0.4995, -0.4994, -0.5021, -0.5021, -0.5038, -0.5083,
2:         -0.5130, -0.5179, -0.5327, -0.5322, -0.5266, -0.5193, -0.5149, -0.5115,
2:         -0.5088], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.1372,  0.0864,  0.0564,  0.0461,  0.0433,  0.0336,  0.0101, -0.0137,
2:         -0.0259, -0.0321, -0.0164,  0.0063,  0.0122, -0.0066, -0.0314, -0.0233,
2:          0.0003, -0.0017,  0.1653,  0.1203,  0.0808,  0.0431,  0.0295,  0.0293,
2:          0.0008], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.7296, -0.7436, -0.7581, -0.7735, -0.7866, -0.8014, -0.8148, -0.8273,
2:         -0.8342, -0.8363, -0.8359, -0.8332, -0.8300, -0.8250, -0.8205, -0.8153,
2:         -0.8098, -0.8044, -0.7984, -0.7905, -0.7829, -0.7754, -0.7681, -0.7581,
2:         -0.7458], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1365, -0.1409, -0.1495, -0.1517, -0.1554, -0.1618, -0.1610, -0.1552,
2:         -0.1534, -0.1445, -0.1385, -0.1491, -0.1544, -0.1591, -0.1566, -0.1608,
2:         -0.1596, -0.1497, -0.1409, -0.1444, -0.1449, -0.1454, -0.1509, -0.1528,
2:         -0.1557], device='cuda:2', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([1.2840, 1.2636, 1.2429, 1.2217, 1.1993, 1.1756, 1.1508, 1.1248, 1.0982,
3:         1.0705, 1.0400, 1.0052, 0.9661, 0.9241, 0.8819, 0.8426, 0.8079, 0.7781,
3:         1.2788, 1.2616, 1.2438, 1.2241, 1.2021, 1.1772, 1.1503],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.8801, 1.9569, 2.0297, 2.0876, 2.1224, 2.1313, 2.1112, 2.0582, 1.9698,
3:         1.8508, 1.7119, 1.5657, 1.4226, 1.2912, 1.1804, 1.0972, 1.0453, 1.0221,
3:         1.8658, 1.9422, 2.0082, 2.0549, 2.0766, 2.0698, 2.0316],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4163, -0.4271, -0.4456, -0.4712, -0.5098, -0.5508, -0.5983, -0.6466,
3:         -0.6838, -0.7231, -0.7410, -0.7546, -0.7557, -0.7573, -0.7555, -0.7533,
3:         -0.7518, -0.7507, -0.4238, -0.4375, -0.4578, -0.4880, -0.5264, -0.5722,
3:         -0.6201], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.5509, 0.7293, 0.8988, 1.0304, 1.1263, 1.2066, 1.2691, 1.2892, 1.2468,
3:         1.1620, 1.0750, 1.0081, 0.9635, 0.9523, 0.9836, 1.0438, 1.0884, 1.0884,
3:         0.6022, 0.7516, 0.8765, 0.9747, 1.0527, 1.1129, 1.1486],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.5429, -0.5785, -0.6096, -0.6390, -0.6713, -0.7101, -0.7600, -0.8240,
3:         -0.9048, -1.0027, -1.1134, -1.2298, -1.3432, -1.4441, -1.5247, -1.5813,
3:         -1.6153, -1.6301, -1.6308, -1.6231, -1.6100, -1.5929, -1.5725, -1.5509,
3:         -1.5320], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2249,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2423,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2434,     nan,     nan, -0.2446,
3:             nan, -0.2446,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2411,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2214, -0.2249,     nan,     nan,     nan,
3:         -0.2214,     nan,     nan,     nan,     nan,     nan,     nan, -0.2388,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2446,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2423,     nan,     nan, -0.2446,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2446,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2446,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2411,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2411,     nan,     nan,     nan,     nan,
3:             nan, -0.2411,     nan, -0.2411,     nan,     nan,     nan,     nan,
3:         -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2411,     nan,     nan, -0.2458, -0.2446,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 4, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.1539, 0.1820, 0.2094, 0.2320, 0.2551, 0.2779, 0.2998, 0.3202, 0.3438,
3:         0.3625, 0.3850, 0.4080, 0.4333, 0.4641, 0.4965, 0.5287, 0.5650, 0.6028,
3:         0.2452, 0.2783, 0.3054, 0.3285, 0.3521, 0.3743, 0.4024],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.2499, 1.2011, 1.1485, 1.0999, 1.0534, 1.0077, 0.9600, 0.9130, 0.8707,
3:         0.8356, 0.8079, 0.7859, 0.7624, 0.7388, 0.7152, 0.6975, 0.6878, 0.6847,
3:         1.2773, 1.2192, 1.1589, 1.1076, 1.0632, 1.0230, 0.9814],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5095, -0.5062, -0.5017, -0.4937, -0.4867, -0.4803, -0.4780, -0.4756,
3:         -0.4775, -0.4799, -0.4834, -0.4863, -0.4919, -0.4971, -0.5037, -0.5133,
3:         -0.5238, -0.5344, -0.5450, -0.5430, -0.5366, -0.5262, -0.5172, -0.5095,
3:         -0.5026], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.2389,  0.1920,  0.1784,  0.1650,  0.1587,  0.1525,  0.1202,  0.0748,
3:          0.0405,  0.0242,  0.0117,  0.0169,  0.0442,  0.0473,  0.0335,  0.0352,
3:          0.0253, -0.0088,  0.2732,  0.2143,  0.1921,  0.1570,  0.1352,  0.1309,
3:          0.0986], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.7324, -0.7526, -0.7667, -0.7750, -0.7795, -0.7852, -0.7932, -0.8050,
3:         -0.8154, -0.8241, -0.8304, -0.8338, -0.8342, -0.8347, -0.8326, -0.8307,
3:         -0.8266, -0.8227, -0.8172, -0.8094, -0.7998, -0.7867, -0.7717, -0.7559,
3:         -0.7427], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1664, -0.1715, -0.1775, -0.1782, -0.1807, -0.1801, -0.1818, -0.1752,
3:         -0.1730, -0.1715, -0.1684, -0.1754, -0.1790, -0.1806, -0.1786, -0.1806,
3:         -0.1780, -0.1701, -0.1673, -0.1702, -0.1702, -0.1696, -0.1720, -0.1742,
3:         -0.1745], device='cuda:3', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 4, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.2840, 1.2636, 1.2429, 1.2217, 1.1993, 1.1756, 1.1508, 1.1248, 1.0982,
1:         1.0705, 1.0400, 1.0052, 0.9661, 0.9241, 0.8819, 0.8426, 0.8079, 0.7781,
1:         1.2788, 1.2616, 1.2438, 1.2241, 1.2021, 1.1772, 1.1503],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.8801, 1.9569, 2.0297, 2.0876, 2.1224, 2.1313, 2.1112, 2.0582, 1.9698,
1:         1.8508, 1.7119, 1.5657, 1.4226, 1.2912, 1.1804, 1.0972, 1.0453, 1.0221,
1:         1.8658, 1.9422, 2.0082, 2.0549, 2.0766, 2.0698, 2.0316],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4163, -0.4271, -0.4456, -0.4712, -0.5098, -0.5508, -0.5983, -0.6466,
1:         -0.6838, -0.7231, -0.7410, -0.7546, -0.7557, -0.7573, -0.7555, -0.7533,
1:         -0.7518, -0.7507, -0.4238, -0.4375, -0.4578, -0.4880, -0.5264, -0.5722,
1:         -0.6201], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.5509, 0.7293, 0.8988, 1.0304, 1.1263, 1.2066, 1.2691, 1.2892, 1.2468,
1:         1.1620, 1.0750, 1.0081, 0.9635, 0.9523, 0.9836, 1.0438, 1.0884, 1.0884,
1:         0.6022, 0.7516, 0.8765, 0.9747, 1.0527, 1.1129, 1.1486],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.5429, -0.5785, -0.6096, -0.6390, -0.6713, -0.7101, -0.7600, -0.8240,
1:         -0.9048, -1.0027, -1.1134, -1.2298, -1.3432, -1.4441, -1.5247, -1.5813,
1:         -1.6153, -1.6301, -1.6308, -1.6231, -1.6100, -1.5929, -1.5725, -1.5509,
1:         -1.5320], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1970,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2446,     nan,     nan,     nan,     nan,     nan,
1:         -0.2446,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2411,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2446, -0.2434,     nan, -0.2423,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2446,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2446,
1:             nan,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2446,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2423,     nan,     nan, -0.2434,     nan,
1:         -0.2434,     nan,     nan,     nan, -0.2411,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2434,
1:             nan,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2411,     nan,     nan, -0.2423,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2400,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2423])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 4, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.1343, 0.1691, 0.2036, 0.2366, 0.2662, 0.2929, 0.3140, 0.3321, 0.3517,
1:         0.3699, 0.3920, 0.4150, 0.4402, 0.4677, 0.4981, 0.5268, 0.5595, 0.6014,
1:         0.2251, 0.2631, 0.2970, 0.3307, 0.3616, 0.3898, 0.4164],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.2149, 1.1760, 1.1297, 1.0877, 1.0460, 1.0081, 0.9689, 0.9295, 0.8944,
1:         0.8675, 0.8424, 0.8225, 0.7990, 0.7722, 0.7466, 0.7268, 0.7173, 0.7148,
1:         1.2331, 1.1887, 1.1448, 1.1044, 1.0704, 1.0378, 1.0052],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5053, -0.5060, -0.5032, -0.4973, -0.4895, -0.4817, -0.4763, -0.4705,
1:         -0.4695, -0.4696, -0.4711, -0.4728, -0.4779, -0.4834, -0.4890, -0.4978,
1:         -0.5073, -0.5161, -0.5354, -0.5348, -0.5286, -0.5183, -0.5077, -0.4984,
1:         -0.4893], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.2309, 0.2067, 0.1826, 0.1634, 0.1507, 0.1142, 0.0738, 0.0308, 0.0036,
1:         0.0132, 0.0366, 0.0595, 0.0598, 0.0367, 0.0204, 0.0227, 0.0282, 0.0040,
1:         0.2273, 0.2082, 0.1714, 0.1398, 0.1270, 0.1065, 0.0807],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.7531, -0.7629, -0.7742, -0.7874, -0.7979, -0.8088, -0.8173, -0.8278,
1:         -0.8354, -0.8402, -0.8393, -0.8339, -0.8250, -0.8166, -0.8117, -0.8102,
1:         -0.8093, -0.8076, -0.8033, -0.7977, -0.7918, -0.7846, -0.7757, -0.7646,
1:         -0.7543], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1621, -0.1620, -0.1691, -0.1693, -0.1719, -0.1764, -0.1794, -0.1749,
1:         -0.1756, -0.1655, -0.1604, -0.1672, -0.1681, -0.1721, -0.1721, -0.1775,
1:         -0.1761, -0.1705, -0.1619, -0.1646, -0.1597, -0.1588, -0.1631, -0.1654,
1:         -0.1707], device='cuda:1', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 4, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2840, 1.2636, 1.2429, 1.2217, 1.1993, 1.1756, 1.1508, 1.1248, 1.0982,
0:         1.0705, 1.0400, 1.0052, 0.9661, 0.9241, 0.8819, 0.8426, 0.8079, 0.7781,
0:         1.2788, 1.2616, 1.2438, 1.2241, 1.2021, 1.1772, 1.1503],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.8801, 1.9569, 2.0297, 2.0876, 2.1224, 2.1313, 2.1112, 2.0582, 1.9698,
0:         1.8508, 1.7119, 1.5657, 1.4226, 1.2912, 1.1804, 1.0972, 1.0453, 1.0221,
0:         1.8658, 1.9422, 2.0082, 2.0549, 2.0766, 2.0698, 2.0316],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4163, -0.4271, -0.4456, -0.4712, -0.5098, -0.5508, -0.5983, -0.6466,
0:         -0.6838, -0.7231, -0.7410, -0.7546, -0.7557, -0.7573, -0.7555, -0.7533,
0:         -0.7518, -0.7507, -0.4238, -0.4375, -0.4578, -0.4880, -0.5264, -0.5722,
0:         -0.6201], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5509, 0.7293, 0.8988, 1.0304, 1.1263, 1.2066, 1.2691, 1.2892, 1.2468,
0:         1.1620, 1.0750, 1.0081, 0.9635, 0.9523, 0.9836, 1.0438, 1.0884, 1.0884,
0:         0.6022, 0.7516, 0.8765, 0.9747, 1.0527, 1.1129, 1.1486],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.5429, -0.5785, -0.6096, -0.6390, -0.6713, -0.7101, -0.7600, -0.8240,
0:         -0.9048, -1.0027, -1.1134, -1.2298, -1.3432, -1.4441, -1.5247, -1.5813,
0:         -1.6153, -1.6301, -1.6308, -1.6231, -1.6100, -1.5929, -1.5725, -1.5509,
0:         -1.5320], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1796,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2458,     nan,     nan, -0.2446,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2423,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2411,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2423,     nan, -0.2446,     nan,     nan,
0:         -0.2434,     nan,     nan,     nan,     nan,     nan,     nan, -0.2446,
0:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2423, -0.2423,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2411, -0.2411,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2411,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2446,     nan, -0.2434,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2400,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2446,     nan,     nan,
0:         -0.2423,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 4, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1856, 0.2035, 0.2231, 0.2424, 0.2660, 0.2921, 0.3160, 0.3352, 0.3579,
0:         0.3767, 0.4014, 0.4273, 0.4558, 0.4880, 0.5178, 0.5462, 0.5803, 0.6190,
0:         0.2687, 0.2957, 0.3219, 0.3459, 0.3739, 0.3992, 0.4264],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2070, 1.1543, 1.1021, 1.0567, 1.0160, 0.9791, 0.9415, 0.9022, 0.8674,
0:         0.8367, 0.8108, 0.7898, 0.7703, 0.7522, 0.7359, 0.7263, 0.7211, 0.7238,
0:         1.2281, 1.1700, 1.1178, 1.0759, 1.0438, 1.0151, 0.9858],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5176, -0.5153, -0.5131, -0.5061, -0.4994, -0.4919, -0.4861, -0.4817,
0:         -0.4797, -0.4794, -0.4802, -0.4827, -0.4883, -0.4937, -0.5006, -0.5081,
0:         -0.5164, -0.5243, -0.5400, -0.5392, -0.5345, -0.5262, -0.5184, -0.5104,
0:         -0.5025], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2221,  0.1665,  0.1586,  0.1471,  0.1248,  0.1076,  0.0644,  0.0089,
0:         -0.0169, -0.0201, -0.0199, -0.0093,  0.0026, -0.0022, -0.0007,  0.0293,
0:          0.0479,  0.0368,  0.2355,  0.1724,  0.1570,  0.1310,  0.1084,  0.1048,
0:          0.0679], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.7337, -0.7590, -0.7758, -0.7888, -0.7980, -0.8100, -0.8223, -0.8336,
0:         -0.8388, -0.8376, -0.8314, -0.8236, -0.8162, -0.8105, -0.8086, -0.8076,
0:         -0.8048, -0.8000, -0.7925, -0.7837, -0.7771, -0.7694, -0.7597, -0.7456,
0:         -0.7303], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1577, -0.1612, -0.1661, -0.1602, -0.1618, -0.1594, -0.1619, -0.1572,
0:         -0.1567, -0.1632, -0.1565, -0.1631, -0.1636, -0.1628, -0.1624, -0.1643,
0:         -0.1639, -0.1538, -0.1626, -0.1617, -0.1615, -0.1575, -0.1596, -0.1600,
0:         -0.1611], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 4 [1/5 (20%)]	Loss: nan : nan :: 0.13785 (1.85 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 4 [2/5 (40%)]	Loss: nan : nan :: 0.14094 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 4 [3/5 (60%)]	Loss: nan : nan :: 0.14258 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 4 [4/5 (80%)]	Loss: nan : nan :: 0.15046 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch4.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch4.mod
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 4 : nan
0: validation loss for velocity_u : 0.033694930374622345
0: validation loss for velocity_v : 0.06306701898574829
0: validation loss for specific_humidity : 0.025069033727049828
0: validation loss for velocity_z : 0.46760448813438416
0: validation loss for temperature : 0.07694627344608307
0: validation loss for total_precip : nan
0: 5 : 22:48:39 :: batch_size = 96, lr = 1.857198821839498e-05
2: 5 : 22:48:39 :: batch_size = 96, lr = 1.857198821839498e-05
3: 5 : 22:48:39 :: batch_size = 96, lr = 1.857198821839498e-05
1: 5 : 22:48:39 :: batch_size = 96, lr = 1.857198821839498e-05
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 5, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: Created sparse mask for total_precip with 10.0% data retained
3:      first 25 values: tensor([0.6119, 0.6359, 0.6595, 0.6812, 0.7008, 0.7206, 0.7416, 0.7624, 0.7831,
3:         0.8037, 0.8237, 0.8428, 0.8625, 0.8831, 0.9050, 0.9277, 0.9489, 0.9659,
3:         0.5972, 0.6228, 0.6477, 0.6708, 0.6909, 0.7095, 0.7288],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-1.6379, -1.7040, -1.7544, -1.7960, -1.8356, -1.8769, -1.9232, -1.9714,
3:         -2.0117, -2.0361, -2.0431, -2.0337, -2.0097, -1.9749, -1.9366, -1.9036,
3:         -1.8843, -1.8784, -1.5721, -1.6502, -1.7120, -1.7596, -1.7987, -1.8347,
3:         -1.8731], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4753, -0.5461, -0.6108, -0.6563, -0.6913, -0.7249, -0.7465, -0.7570,
3:         -0.7653, -0.7641, -0.7614, -0.7590, -0.7546, -0.7490, -0.7427, -0.7372,
3:         -0.7341, -0.7320, -0.3883, -0.4858, -0.5596, -0.6110, -0.6591, -0.6966,
3:         -0.7232], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.5161, 0.4726, 0.4095, 0.3400, 0.2791, 0.2617, 0.2878, 0.3465, 0.4443,
3:         0.5552, 0.6444, 0.7205, 0.7814, 0.7792, 0.7162, 0.6400, 0.5400, 0.4552,
3:         0.5204, 0.4922, 0.4378, 0.3791, 0.3182, 0.2812, 0.2856],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.5687, -0.6074, -0.6471, -0.6905, -0.7384, -0.7851, -0.8287, -0.8715,
3:         -0.9133, -0.9555, -1.0058, -1.0666, -1.1355, -1.2126, -1.2947, -1.3724,
3:         -1.4397, -1.4989, -1.5532, -1.6109, -1.6848, -1.7814, -1.8999, -2.0325,
3:         -2.1660], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,
3:             nan,     nan, -0.2430,     nan,     nan,     nan, -0.2430,     nan,
3:             nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.1881,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2430,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2430, -0.2430,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1446,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2418,     nan,     nan,     nan,     nan, -0.2430,     nan,
3:         -0.2430,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 5, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.5180,  0.4756,  0.4279,  0.3717,  0.3159,  0.2626,  0.2154,  0.1780,
3:          0.1448,  0.1201,  0.0964,  0.0686,  0.0395,  0.0070, -0.0281, -0.0614,
3:         -0.0926, -0.1172,  0.4826,  0.4414,  0.3898,  0.3331,  0.2748,  0.2231,
3:          0.1815], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.3021, -1.2822, -1.2707, -1.2693, -1.2789, -1.2914, -1.3157, -1.3444,
3:         -1.3745, -1.4034, -1.4319, -1.4586, -1.4822, -1.5013, -1.5239, -1.5486,
3:         -1.5786, -1.6149, -1.3338, -1.3105, -1.2981, -1.2902, -1.2937, -1.3086,
3:         -1.3347], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 1.0971,  1.1084,  1.0756,  0.9961,  0.8448,  0.6454,  0.4034,  0.1552,
3:         -0.0758, -0.2623, -0.3932, -0.4668, -0.5017, -0.5051, -0.4949, -0.4803,
3:         -0.4753, -0.4716,  1.0749,  1.0908,  1.0731,  0.9955,  0.8562,  0.6523,
3:          0.4136], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6325, -0.6039, -0.4882, -0.3047, -0.0766,  0.1228,  0.2653,  0.3403,
3:          0.3804,  0.4000,  0.3896,  0.3941,  0.4111,  0.3879,  0.3651,  0.3727,
3:          0.4029,  0.4256, -0.5464, -0.5244, -0.4172, -0.2416, -0.0175,  0.1752,
3:          0.3128], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([ 1.1175,  1.0623,  1.0073,  0.9573,  0.9132,  0.8770,  0.8486,  0.8314,
3:          0.8261,  0.8266,  0.8251,  0.8100,  0.7730,  0.7137,  0.6357,  0.5462,
3:          0.4533,  0.3619,  0.2756,  0.1964,  0.1242,  0.0607,  0.0035, -0.0459,
3:         -0.0868], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1676, -0.1701, -0.1749, -0.1731, -0.1733, -0.1753, -0.1771, -0.1725,
3:         -0.1712, -0.1687, -0.1659, -0.1714, -0.1738, -0.1720, -0.1701, -0.1755,
3:         -0.1738, -0.1666, -0.1644, -0.1674, -0.1632, -0.1652, -0.1652, -0.1661,
3:         -0.1693], device='cuda:3', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 5, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 5, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.6119, 0.6359, 0.6595, 0.6812, 0.7008, 0.7206, 0.7416, 0.7624, 0.7831,
1:         0.8037, 0.8237, 0.8428, 0.8625, 0.8831, 0.9050, 0.9277, 0.9489, 0.9659,
1:         0.5972, 0.6228, 0.6477, 0.6708, 0.6909, 0.7095, 0.7288],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-1.6379, -1.7040, -1.7544, -1.7960, -1.8356, -1.8769, -1.9232, -1.9714,
1:         -2.0117, -2.0361, -2.0431, -2.0337, -2.0097, -1.9749, -1.9366, -1.9036,
1:         -1.8843, -1.8784, -1.5721, -1.6502, -1.7120, -1.7596, -1.7987, -1.8347,
1:         -1.8731], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4753, -0.5461, -0.6108, -0.6563, -0.6913, -0.7249, -0.7465, -0.7570,
1:         -0.7653, -0.7641, -0.7614, -0.7590, -0.7546, -0.7490, -0.7427, -0.7372,
1:         -0.7341, -0.7320, -0.3883, -0.4858, -0.5596, -0.6110, -0.6591, -0.6966,
1:         -0.7232], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.5161, 0.4726, 0.4095, 0.3400, 0.2791, 0.2617, 0.2878, 0.3465, 0.4443,
1:         0.5552, 0.6444, 0.7205, 0.7814, 0.7792, 0.7162, 0.6400, 0.5400, 0.4552,
1:         0.5204, 0.4922, 0.4378, 0.3791, 0.3182, 0.2812, 0.2856],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.5687, -0.6074, -0.6471, -0.6905, -0.7384, -0.7851, -0.8287, -0.8715,
1:         -0.9133, -0.9555, -1.0058, -1.0666, -1.1355, -1.2126, -1.2947, -1.3724,
1:         -1.4397, -1.4989, -1.5532, -1.6109, -1.6848, -1.7814, -1.8999, -2.0325,
1:         -2.1660], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2430,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,
1:         -0.2430, -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2407,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2407,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
1:         -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2384,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
1:             nan, -0.2407,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2407,     nan,     nan,     nan, -0.0371,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2430,     nan,     nan,     nan, -0.2430,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2430,     nan, -0.2430,     nan,     nan,
1:             nan,     nan, -0.2395,     nan,     nan, -0.2430,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 5, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.4454,  0.4001,  0.3502,  0.2982,  0.2502,  0.2061,  0.1684,  0.1373,
1:          0.1131,  0.0936,  0.0771,  0.0566,  0.0316,  0.0028, -0.0291, -0.0616,
1:         -0.0926, -0.1176,  0.4229,  0.3771,  0.3240,  0.2704,  0.2212,  0.1807,
1:          0.1472], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.2370, -1.2313, -1.2289, -1.2325, -1.2504, -1.2739, -1.3094, -1.3474,
1:         -1.3833, -1.4129, -1.4424, -1.4674, -1.4938, -1.5136, -1.5318, -1.5484,
1:         -1.5638, -1.5795, -1.2615, -1.2587, -1.2565, -1.2617, -1.2758, -1.3019,
1:         -1.3400], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 1.1198,  1.1049,  1.0546,  0.9575,  0.8024,  0.5994,  0.3640,  0.1199,
1:         -0.1036, -0.2781, -0.3990, -0.4616, -0.4863, -0.4739, -0.4462, -0.4143,
1:         -0.3888, -0.3704,  1.0920,  1.0855,  1.0477,  0.9633,  0.8201,  0.6179,
1:          0.3824], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6612, -0.6305, -0.5010, -0.3323, -0.1350,  0.0632,  0.1622,  0.1959,
1:          0.2613,  0.3107,  0.2989,  0.2844,  0.2851,  0.2529,  0.2119,  0.2299,
1:          0.3041,  0.3546, -0.5904, -0.5789, -0.4569, -0.2869, -0.0974,  0.0968,
1:          0.2092], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([ 1.1114,  1.0679,  1.0197,  0.9693,  0.9225,  0.8855,  0.8613,  0.8482,
1:          0.8470,  0.8499,  0.8469,  0.8322,  0.8008,  0.7494,  0.6816,  0.6004,
1:          0.5116,  0.4180,  0.3244,  0.2351,  0.1541,  0.0847,  0.0266, -0.0214,
1:         -0.0639], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1530, -0.1548, -0.1561, -0.1572, -0.1595, -0.1639, -0.1659, -0.1639,
1:         -0.1627, -0.1555, -0.1509, -0.1562, -0.1590, -0.1639, -0.1610, -0.1671,
1:         -0.1674, -0.1621, -0.1536, -0.1524, -0.1520, -0.1520, -0.1526, -0.1587,
1:         -0.1626], device='cuda:1', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 5, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.6119, 0.6359, 0.6595, 0.6812, 0.7008, 0.7206, 0.7416, 0.7624, 0.7831,
2:         0.8037, 0.8237, 0.8428, 0.8625, 0.8831, 0.9050, 0.9277, 0.9489, 0.9659,
2:         0.5972, 0.6228, 0.6477, 0.6708, 0.6909, 0.7095, 0.7288],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-1.6379, -1.7040, -1.7544, -1.7960, -1.8356, -1.8769, -1.9232, -1.9714,
2:         -2.0117, -2.0361, -2.0431, -2.0337, -2.0097, -1.9749, -1.9366, -1.9036,
2:         -1.8843, -1.8784, -1.5721, -1.6502, -1.7120, -1.7596, -1.7987, -1.8347,
2:         -1.8731], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.4753, -0.5461, -0.6108, -0.6563, -0.6913, -0.7249, -0.7465, -0.7570,
2:         -0.7653, -0.7641, -0.7614, -0.7590, -0.7546, -0.7490, -0.7427, -0.7372,
2:         -0.7341, -0.7320, -0.3883, -0.4858, -0.5596, -0.6110, -0.6591, -0.6966,
2:         -0.7232], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.5161, 0.4726, 0.4095, 0.3400, 0.2791, 0.2617, 0.2878, 0.3465, 0.4443,
2:         0.5552, 0.6444, 0.7205, 0.7814, 0.7792, 0.7162, 0.6400, 0.5400, 0.4552,
2:         0.5204, 0.4922, 0.4378, 0.3791, 0.3182, 0.2812, 0.2856],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.5687, -0.6074, -0.6471, -0.6905, -0.7384, -0.7851, -0.8287, -0.8715,
2:         -0.9133, -0.9555, -1.0058, -1.0666, -1.1355, -1.2126, -1.2947, -1.3724,
2:         -1.4397, -1.4989, -1.5532, -1.6109, -1.6848, -1.7814, -1.8999, -2.0325,
2:         -2.1660], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([-0.2361,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,
2:             nan, -0.2407,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2430,     nan,     nan, -0.2430,     nan, -0.2430,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
2:             nan, -0.2430,     nan, -0.2430, -0.2430,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2361,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,
2:         -0.2430, -0.2430,     nan,     nan, -0.2430,     nan,     nan,     nan,
2:             nan,     nan, -0.2430,     nan,     nan,     nan,     nan, -0.2430,
2:             nan,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,
2:             nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2418,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2418,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2327,
2:             nan,     nan,     nan,     nan, -0.2430,     nan,     nan,     nan,
2:             nan, -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2430,     nan, -0.2395,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 5, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.4531,  0.4121,  0.3707,  0.3262,  0.2867,  0.2513,  0.2153,  0.1834,
2:          0.1524,  0.1243,  0.1005,  0.0743,  0.0482,  0.0219, -0.0072, -0.0326,
2:         -0.0556, -0.0713,  0.4164,  0.3742,  0.3291,  0.2838,  0.2436,  0.2109,
2:          0.1850], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.3094, -1.2796, -1.2578, -1.2496, -1.2635, -1.2856, -1.3228, -1.3615,
2:         -1.3992, -1.4325, -1.4627, -1.4858, -1.5079, -1.5211, -1.5348, -1.5485,
2:         -1.5644, -1.5866, -1.3426, -1.3161, -1.2974, -1.2865, -1.2906, -1.3127,
2:         -1.3454], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 1.1445,  1.1444,  1.1044,  1.0201,  0.8718,  0.6732,  0.4308,  0.1703,
2:         -0.0716, -0.2694, -0.4045, -0.4706, -0.4848, -0.4633, -0.4309, -0.3959,
2:         -0.3834, -0.3861,  1.1169,  1.1224,  1.0934,  1.0139,  0.8723,  0.6717,
2:          0.4274], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6703, -0.6463, -0.5362, -0.3574, -0.1300,  0.0650,  0.1921,  0.2588,
2:          0.3027,  0.3509,  0.3633,  0.3658,  0.3853,  0.3595,  0.3074,  0.2930,
2:          0.3390,  0.3866, -0.6129, -0.6062, -0.5050, -0.3102, -0.0614,  0.1399,
2:          0.2614], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([ 1.1089,  1.0564,  1.0028,  0.9517,  0.9085,  0.8752,  0.8550,  0.8446,
2:          0.8444,  0.8452,  0.8372,  0.8143,  0.7741,  0.7149,  0.6431,  0.5585,
2:          0.4684,  0.3751,  0.2836,  0.1973,  0.1168,  0.0449, -0.0206, -0.0776,
2:         -0.1242], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1753, -0.1754, -0.1765, -0.1760, -0.1779, -0.1784, -0.1838, -0.1809,
2:         -0.1804, -0.1797, -0.1754, -0.1766, -0.1777, -0.1779, -0.1754, -0.1843,
2:         -0.1832, -0.1803, -0.1761, -0.1753, -0.1711, -0.1721, -0.1706, -0.1733,
2:         -0.1773], device='cuda:2', grad_fn=<SliceBackward0>)
0:      first 25 values: tensor([0.6119, 0.6359, 0.6595, 0.6812, 0.7008, 0.7206, 0.7416, 0.7624, 0.7831,
0:         0.8037, 0.8237, 0.8428, 0.8625, 0.8831, 0.9050, 0.9277, 0.9489, 0.9659,
0:         0.5972, 0.6228, 0.6477, 0.6708, 0.6909, 0.7095, 0.7288],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.6379, -1.7040, -1.7544, -1.7960, -1.8356, -1.8769, -1.9232, -1.9714,
0:         -2.0117, -2.0361, -2.0431, -2.0337, -2.0097, -1.9749, -1.9366, -1.9036,
0:         -1.8843, -1.8784, -1.5721, -1.6502, -1.7120, -1.7596, -1.7987, -1.8347,
0:         -1.8731], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4753, -0.5461, -0.6108, -0.6563, -0.6913, -0.7249, -0.7465, -0.7570,
0:         -0.7653, -0.7641, -0.7614, -0.7590, -0.7546, -0.7490, -0.7427, -0.7372,
0:         -0.7341, -0.7320, -0.3883, -0.4858, -0.5596, -0.6110, -0.6591, -0.6966,
0:         -0.7232], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5161, 0.4726, 0.4095, 0.3400, 0.2791, 0.2617, 0.2878, 0.3465, 0.4443,
0:         0.5552, 0.6444, 0.7205, 0.7814, 0.7792, 0.7162, 0.6400, 0.5400, 0.4552,
0:         0.5204, 0.4922, 0.4378, 0.3791, 0.3182, 0.2812, 0.2856],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.5687, -0.6074, -0.6471, -0.6905, -0.7384, -0.7851, -0.8287, -0.8715,
0:         -0.9133, -0.9555, -1.0058, -1.0666, -1.1355, -1.2126, -1.2947, -1.3724,
0:         -1.4397, -1.4989, -1.5532, -1.6109, -1.6848, -1.7814, -1.8999, -2.0325,
0:         -2.1660], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2407, -0.2430,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1881,     nan,     nan,     nan,     nan,
0:         -0.2407,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2430,     nan, -0.2430,     nan,
0:             nan,     nan,  0.0829,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2418,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,
0:         -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2395,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 5, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.4557,  0.4218,  0.3842,  0.3429,  0.3038,  0.2646,  0.2299,  0.1958,
0:          0.1633,  0.1325,  0.1052,  0.0744,  0.0442,  0.0151, -0.0168, -0.0456,
0:         -0.0750, -0.0973,  0.4332,  0.3993,  0.3582,  0.3148,  0.2715,  0.2334,
0:          0.2013], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2540, -1.2475, -1.2432, -1.2424, -1.2556, -1.2738, -1.3057, -1.3405,
0:         -1.3770, -1.4087, -1.4361, -1.4620, -1.4843, -1.4975, -1.5074, -1.5142,
0:         -1.5239, -1.5464, -1.2780, -1.2701, -1.2646, -1.2655, -1.2730, -1.2935,
0:         -1.3288], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 1.2195,  1.2074,  1.1501,  1.0407,  0.8697,  0.6554,  0.4094,  0.1632,
0:         -0.0588, -0.2323, -0.3505, -0.4173, -0.4509, -0.4515, -0.4461, -0.4298,
0:         -0.4232, -0.4228,  1.1792,  1.1670,  1.1232,  1.0193,  0.8595,  0.6416,
0:          0.3970], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5272, -0.4727, -0.3550, -0.2216, -0.0537,  0.1087,  0.1948,  0.2425,
0:          0.3053,  0.3338,  0.3058,  0.3022,  0.3328,  0.3039,  0.2584,  0.2634,
0:          0.3162,  0.3814, -0.4556, -0.4350, -0.3203, -0.1673,  0.0058,  0.1681,
0:          0.2557], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 1.0913,  1.0429,  0.9991,  0.9598,  0.9282,  0.9015,  0.8803,  0.8619,
0:          0.8509,  0.8450,  0.8381,  0.8230,  0.7935,  0.7427,  0.6729,  0.5877,
0:          0.4954,  0.4009,  0.3090,  0.2224,  0.1422,  0.0727,  0.0141, -0.0318,
0:         -0.0677], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1609, -0.1651, -0.1680, -0.1681, -0.1675, -0.1712, -0.1731, -0.1668,
0:         -0.1675, -0.1707, -0.1642, -0.1667, -0.1684, -0.1707, -0.1692, -0.1728,
0:         -0.1742, -0.1670, -0.1623, -0.1658, -0.1619, -0.1619, -0.1620, -0.1652,
0:         -0.1689], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 5 [1/5 (20%)]	Loss: nan : nan :: 0.13722 (1.94 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 5 [2/5 (40%)]	Loss: nan : nan :: 0.13604 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 5 [3/5 (60%)]	Loss: nan : nan :: 0.13826 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 5 [4/5 (80%)]	Loss: nan : nan :: 0.14025 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch5.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch5.mod
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 5 : nan
0: validation loss for velocity_u : 0.03428489342331886
0: validation loss for velocity_v : 0.0593002624809742
0: validation loss for specific_humidity : 0.022890761494636536
0: validation loss for velocity_z : 0.46884626150131226
0: validation loss for temperature : 0.0762045755982399
0: validation loss for total_precip : nan
2: 6 : 22:55:05 :: batch_size = 96, lr = 1.8119012895995104e-05
1: 6 : 22:55:05 :: batch_size = 96, lr = 1.8119012895995104e-05
0: 6 : 22:55:05 :: batch_size = 96, lr = 1.8119012895995104e-05
3: 6 : 22:55:05 :: batch_size = 96, lr = 1.8119012895995104e-05
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 6, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2734, -0.2892, -0.3130, -0.3415, -0.3717, -0.3988, -0.4146, -0.4187,
3:         -0.4175, -0.4211, -0.4288, -0.4358, -0.4374, -0.4298, -0.4198, -0.4116,
3:         -0.4088, -0.4137, -0.2126, -0.2233, -0.2425, -0.2684, -0.2993, -0.3288,
3:         -0.3499], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.5107, -0.5055, -0.5055, -0.5112, -0.5203, -0.5281, -0.5277, -0.5189,
3:         -0.5055, -0.4944, -0.4877, -0.4796, -0.4669, -0.4468, -0.4167, -0.3766,
3:         -0.3346, -0.2965, -0.5086, -0.4951, -0.4880, -0.4890, -0.4972, -0.5070,
3:         -0.5112], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.1444,  0.0920,  0.0448,  0.0016, -0.0197, -0.0187, -0.0019,  0.0331,
3:          0.0522,  0.0723,  0.0734,  0.0718,  0.0574,  0.0350, -0.0123, -0.0743,
3:         -0.1736, -0.2588,  0.1651,  0.1135,  0.0678,  0.0212, -0.0092, -0.0262,
3:         -0.0139], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.6085,  0.5160,  0.3817,  0.2628,  0.0492, -0.0829, -0.1534, -0.2283,
3:         -0.1732, -0.2041, -0.2525, -0.2371, -0.2459, -0.1930, -0.1798, -0.1776,
3:         -0.1975, -0.2305,  0.6767,  0.6547,  0.5138,  0.4081,  0.2363,  0.0051,
3:         -0.0896], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([1.7366, 1.8013, 1.8544, 1.8797, 1.8711, 1.8229, 1.7356, 1.6449, 1.5706,
3:         1.5132, 1.4561, 1.3880, 1.3256, 1.2815, 1.2926, 1.3675, 1.4905, 1.6190,
3:         1.6999, 1.7451, 1.7662, 1.7785, 1.8066, 1.7964, 1.5787],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan, -0.2355,     nan,     nan,     nan,     nan, -0.2469,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2423,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2218,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2218,
3:             nan,     nan, -0.2173,     nan, -0.2423,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2401,     nan,     nan,
3:         -0.1991,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2105,     nan, -0.2218,     nan, -0.2241,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2423,     nan,     nan,     nan,     nan, -0.2332,     nan, -0.2423,
3:             nan,     nan,     nan, -0.2105,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2082,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2127,     nan,
3:             nan,     nan,     nan,     nan, -0.2241,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2264,     nan,     nan,     nan,     nan,
3:         -0.2196,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2469,     nan,     nan,     nan,     nan, -0.2264,     nan,
3:             nan,     nan,     nan, -0.1444,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2401,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2218,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 6, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.8276, -0.8421, -0.8549, -0.8643, -0.8714, -0.8812, -0.8873, -0.8950,
3:         -0.9048, -0.9130, -0.9201, -0.9298, -0.9344, -0.9371, -0.9385, -0.9327,
3:         -0.9218, -0.9035, -0.7770, -0.7929, -0.8091, -0.8252, -0.8413, -0.8535,
3:         -0.8652], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5730, -0.5575, -0.5323, -0.4991, -0.4647, -0.4266, -0.3891, -0.3567,
3:         -0.3276, -0.3036, -0.2802, -0.2553, -0.2322, -0.2160, -0.2172, -0.2370,
3:         -0.2761, -0.3161, -0.5975, -0.5809, -0.5548, -0.5200, -0.4801, -0.4431,
3:         -0.4096], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.4235,  0.3489,  0.2771,  0.2204,  0.1673,  0.1207,  0.0660,  0.0051,
3:         -0.0709, -0.1403, -0.2104, -0.2678, -0.3205, -0.3674, -0.4113, -0.4534,
3:         -0.4940, -0.5193,  0.4469,  0.3630,  0.2876,  0.2300,  0.1865,  0.1490,
3:          0.1089], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.2413, -0.1901, -0.1033, -0.0335,  0.0320,  0.0861,  0.1650,  0.1834,
3:          0.1208,  0.0837,  0.0779,  0.0969,  0.1612,  0.2203,  0.2441,  0.2649,
3:          0.3011,  0.3059, -0.2517, -0.1979, -0.1300, -0.0692,  0.0094,  0.0634,
3:          0.1191], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.6103, -0.6073, -0.5868, -0.5434, -0.4745, -0.3759, -0.2591, -0.1418,
3:         -0.0502, -0.0025,  0.0037, -0.0073, -0.0110,  0.0072,  0.0498,  0.1065,
3:          0.1647,  0.2191,  0.2710,  0.3292,  0.4139,  0.5457,  0.7411,  0.9987,
3:          1.2873], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1720, -0.1749, -0.1827, -0.1830, -0.1816, -0.1816, -0.1840, -0.1769,
3:         -0.1755, -0.1791, -0.1760, -0.1800, -0.1828, -0.1859, -0.1820, -0.1860,
3:         -0.1797, -0.1752, -0.1776, -0.1819, -0.1758, -0.1801, -0.1786, -0.1784,
3:         -0.1793], device='cuda:3', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 6, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 6, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2734, -0.2892, -0.3130, -0.3415, -0.3717, -0.3988, -0.4146, -0.4187,
0:         -0.4175, -0.4211, -0.4288, -0.4358, -0.4374, -0.4298, -0.4198, -0.4116,
0:         -0.4088, -0.4137, -0.2126, -0.2233, -0.2425, -0.2684, -0.2993, -0.3288,
0:         -0.3499], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5107, -0.5055, -0.5055, -0.5112, -0.5203, -0.5281, -0.5277, -0.5189,
0:         -0.5055, -0.4944, -0.4877, -0.4796, -0.4669, -0.4468, -0.4167, -0.3766,
0:         -0.3346, -0.2965, -0.5086, -0.4951, -0.4880, -0.4890, -0.4972, -0.5070,
0:         -0.5112], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1444,  0.0920,  0.0448,  0.0016, -0.0197, -0.0187, -0.0019,  0.0331,
0:          0.0522,  0.0723,  0.0734,  0.0718,  0.0574,  0.0350, -0.0123, -0.0743,
0:         -0.1736, -0.2588,  0.1651,  0.1135,  0.0678,  0.0212, -0.0092, -0.0262,
0:         -0.0139], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6085,  0.5160,  0.3817,  0.2628,  0.0492, -0.0829, -0.1534, -0.2283,
0:         -0.1732, -0.2041, -0.2525, -0.2371, -0.2459, -0.1930, -0.1798, -0.1776,
0:         -0.1975, -0.2305,  0.6767,  0.6547,  0.5138,  0.4081,  0.2363,  0.0051,
0:         -0.0896], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.7366, 1.8013, 1.8544, 1.8797, 1.8711, 1.8229, 1.7356, 1.6449, 1.5706,
0:         1.5132, 1.4561, 1.3880, 1.3256, 1.2815, 1.2926, 1.3675, 1.4905, 1.6190,
0:         1.6999, 1.7451, 1.7662, 1.7785, 1.8066, 1.7964, 1.5787],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2127,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2446,     nan,     nan,     nan,     nan, -0.2264,
0:             nan, -0.2401, -0.2446,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2401,     nan,     nan,
0:         -0.1991,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2105,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2355,     nan,     nan,     nan,     nan, -0.2127, -0.2082,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2196,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2401,     nan,
0:             nan,     nan,     nan, -0.2355,     nan,     nan,     nan,     nan,
0:         -0.2446,     nan,     nan,     nan, -0.2423,     nan,     nan,     nan,
0:         -0.2469,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2105,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1808,     nan,
0:             nan,     nan, -0.1922,     nan,     nan,     nan, -0.1991,     nan,
0:             nan,     nan,     nan, -0.1877,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2218,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 6, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8244, -0.8345, -0.8440, -0.8563, -0.8690, -0.8857, -0.9000, -0.9131,
0:         -0.9246, -0.9326, -0.9358, -0.9387, -0.9367, -0.9332, -0.9289, -0.9197,
0:         -0.9099, -0.8928, -0.7712, -0.7834, -0.7976, -0.8154, -0.8362, -0.8551,
0:         -0.8739], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5476, -0.5394, -0.5207, -0.4924, -0.4614, -0.4264, -0.3908, -0.3554,
0:         -0.3206, -0.2880, -0.2585, -0.2327, -0.2103, -0.1990, -0.2071, -0.2291,
0:         -0.2684, -0.3084, -0.5675, -0.5548, -0.5330, -0.5006, -0.4668, -0.4356,
0:         -0.4059], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.4889,  0.4238,  0.3517,  0.2774,  0.1970,  0.1225,  0.0506, -0.0213,
0:         -0.0981, -0.1657, -0.2343, -0.2884, -0.3378, -0.3825, -0.4235, -0.4631,
0:         -0.5081, -0.5421,  0.5030,  0.4352,  0.3605,  0.2832,  0.2103,  0.1468,
0:          0.0878], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1787, -0.1640, -0.1127, -0.0610,  0.0081,  0.0866,  0.1591,  0.1775,
0:          0.1523,  0.1253,  0.1243,  0.1831,  0.2497,  0.2628,  0.2751,  0.2742,
0:          0.2544,  0.2497, -0.1731, -0.1646, -0.1202, -0.0744, -0.0250,  0.0375,
0:          0.1040], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5918, -0.5770, -0.5544, -0.5198, -0.4662, -0.3866, -0.2844, -0.1721,
0:         -0.0727, -0.0090,  0.0124,  0.0063, -0.0036,  0.0029,  0.0321,  0.0813,
0:          0.1394,  0.2001,  0.2621,  0.3310,  0.4211,  0.5490,  0.7312,  0.9705,
0:          1.2400], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1760, -0.1781, -0.1783, -0.1799, -0.1761, -0.1772, -0.1800, -0.1759,
0:         -0.1796, -0.1813, -0.1774, -0.1785, -0.1777, -0.1759, -0.1724, -0.1800,
0:         -0.1777, -0.1736, -0.1786, -0.1762, -0.1730, -0.1736, -0.1691, -0.1708,
0:         -0.1720], device='cuda:0', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-0.2734, -0.2892, -0.3130, -0.3415, -0.3717, -0.3988, -0.4146, -0.4187,
1:         -0.4175, -0.4211, -0.4288, -0.4358, -0.4374, -0.4298, -0.4198, -0.4116,
1:         -0.4088, -0.4137, -0.2126, -0.2233, -0.2425, -0.2684, -0.2993, -0.3288,
1:         -0.3499], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.5107, -0.5055, -0.5055, -0.5112, -0.5203, -0.5281, -0.5277, -0.5189,
1:         -0.5055, -0.4944, -0.4877, -0.4796, -0.4669, -0.4468, -0.4167, -0.3766,
1:         -0.3346, -0.2965, -0.5086, -0.4951, -0.4880, -0.4890, -0.4972, -0.5070,
1:         -0.5112], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.1444,  0.0920,  0.0448,  0.0016, -0.0197, -0.0187, -0.0019,  0.0331,
1:          0.0522,  0.0723,  0.0734,  0.0718,  0.0574,  0.0350, -0.0123, -0.0743,
1:         -0.1736, -0.2588,  0.1651,  0.1135,  0.0678,  0.0212, -0.0092, -0.0262,
1:         -0.0139], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.6085,  0.5160,  0.3817,  0.2628,  0.0492, -0.0829, -0.1534, -0.2283,
1:         -0.1732, -0.2041, -0.2525, -0.2371, -0.2459, -0.1930, -0.1798, -0.1776,
1:         -0.1975, -0.2305,  0.6767,  0.6547,  0.5138,  0.4081,  0.2363,  0.0051,
1:         -0.0896], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([1.7366, 1.8013, 1.8544, 1.8797, 1.8711, 1.8229, 1.7356, 1.6449, 1.5706,
1:         1.5132, 1.4561, 1.3880, 1.3256, 1.2815, 1.2926, 1.3675, 1.4905, 1.6190,
1:         1.6999, 1.7451, 1.7662, 1.7785, 1.8066, 1.7964, 1.5787],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan, -0.2287,     nan,     nan,     nan,     nan,     nan, -0.2469,
1:             nan,     nan,     nan,     nan, -0.2287,     nan,     nan,     nan,
1:             nan, -0.2469,     nan,     nan,     nan,     nan, -0.2287,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2401,     nan,     nan,
1:             nan, -0.2218,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2218,     nan, -0.2241,     nan,     nan, -0.2423,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2105,     nan,     nan,     nan,     nan,
1:             nan, -0.2446,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2355,     nan,     nan, -0.1991,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2446,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2423, -0.2378,     nan, -0.1444, -0.1808,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2150,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2013,
1:             nan,     nan,     nan, -0.1877,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2218,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2423,     nan,     nan,     nan,
1:             nan,     nan, -0.2423, -0.2446,     nan,     nan,     nan, -0.2401,
1:             nan,     nan, -0.2423,     nan,     nan, -0.2446,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 6, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.8365, -0.8478, -0.8614, -0.8763, -0.8905, -0.9049, -0.9176, -0.9292,
1:         -0.9351, -0.9378, -0.9376, -0.9409, -0.9426, -0.9431, -0.9452, -0.9426,
1:         -0.9334, -0.9163, -0.7878, -0.8043, -0.8220, -0.8409, -0.8622, -0.8794,
1:         -0.8963], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5493, -0.5381, -0.5190, -0.4927, -0.4645, -0.4280, -0.3893, -0.3471,
1:         -0.3047, -0.2664, -0.2379, -0.2196, -0.2095, -0.2057, -0.2159, -0.2349,
1:         -0.2666, -0.2980, -0.5735, -0.5569, -0.5342, -0.5055, -0.4713, -0.4395,
1:         -0.4067], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.4826,  0.4203,  0.3473,  0.2776,  0.2044,  0.1309,  0.0551, -0.0196,
1:         -0.0998, -0.1658, -0.2278, -0.2786, -0.3259, -0.3772, -0.4283, -0.4798,
1:         -0.5345, -0.5731,  0.5000,  0.4305,  0.3581,  0.2876,  0.2218,  0.1593,
1:          0.0972], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1933, -0.1715, -0.0944, -0.0452,  0.0364,  0.1150,  0.1619,  0.1662,
1:          0.1240,  0.0844,  0.0979,  0.1671,  0.2705,  0.3485,  0.3668,  0.3492,
1:          0.3586,  0.3778, -0.2064, -0.1872, -0.1223, -0.0776,  0.0075,  0.0911,
1:          0.1383], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.5667, -0.5580, -0.5349, -0.4909, -0.4226, -0.3285, -0.2156, -0.1023,
1:         -0.0106,  0.0392,  0.0469,  0.0287,  0.0115,  0.0116,  0.0384,  0.0845,
1:          0.1425,  0.2053,  0.2690,  0.3386,  0.4287,  0.5549,  0.7322,  0.9608,
1:          1.2141], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1557, -0.1558, -0.1617, -0.1609, -0.1622, -0.1689, -0.1714, -0.1659,
1:         -0.1671, -0.1582, -0.1558, -0.1605, -0.1621, -0.1650, -0.1633, -0.1702,
1:         -0.1709, -0.1647, -0.1551, -0.1547, -0.1548, -0.1565, -0.1577, -0.1582,
1:         -0.1626], device='cuda:1', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 6, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2734, -0.2892, -0.3130, -0.3415, -0.3717, -0.3988, -0.4146, -0.4187,
2:         -0.4175, -0.4211, -0.4288, -0.4358, -0.4374, -0.4298, -0.4198, -0.4116,
2:         -0.4088, -0.4137, -0.2126, -0.2233, -0.2425, -0.2684, -0.2993, -0.3288,
2:         -0.3499], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.5107, -0.5055, -0.5055, -0.5112, -0.5203, -0.5281, -0.5277, -0.5189,
2:         -0.5055, -0.4944, -0.4877, -0.4796, -0.4669, -0.4468, -0.4167, -0.3766,
2:         -0.3346, -0.2965, -0.5086, -0.4951, -0.4880, -0.4890, -0.4972, -0.5070,
2:         -0.5112], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.1444,  0.0920,  0.0448,  0.0016, -0.0197, -0.0187, -0.0019,  0.0331,
2:          0.0522,  0.0723,  0.0734,  0.0718,  0.0574,  0.0350, -0.0123, -0.0743,
2:         -0.1736, -0.2588,  0.1651,  0.1135,  0.0678,  0.0212, -0.0092, -0.0262,
2:         -0.0139], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.6085,  0.5160,  0.3817,  0.2628,  0.0492, -0.0829, -0.1534, -0.2283,
2:         -0.1732, -0.2041, -0.2525, -0.2371, -0.2459, -0.1930, -0.1798, -0.1776,
2:         -0.1975, -0.2305,  0.6767,  0.6547,  0.5138,  0.4081,  0.2363,  0.0051,
2:         -0.0896], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([1.7366, 1.8013, 1.8544, 1.8797, 1.8711, 1.8229, 1.7356, 1.6449, 1.5706,
2:         1.5132, 1.4561, 1.3880, 1.3256, 1.2815, 1.2926, 1.3675, 1.4905, 1.6190,
2:         1.6999, 1.7451, 1.7662, 1.7785, 1.8066, 1.7964, 1.5787],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2196,     nan,     nan,     nan, -0.2378,
2:             nan,     nan,     nan,     nan,     nan, -0.2218,     nan, -0.2264,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1968,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2423,     nan,
2:             nan,     nan,     nan,     nan, -0.2241,     nan,     nan, -0.2423,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2378,
2:             nan,     nan,     nan,     nan,     nan, -0.2332,     nan, -0.2423,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2310,
2:             nan,     nan,     nan,     nan, -0.2082,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2082,
2:             nan, -0.2173,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2218,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2264,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.1808,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1968,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2059,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2264,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2401,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 6, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.8268, -0.8387, -0.8509, -0.8614, -0.8704, -0.8817, -0.8910, -0.9043,
2:         -0.9139, -0.9230, -0.9271, -0.9297, -0.9261, -0.9182, -0.9095, -0.8998,
2:         -0.8884, -0.8762, -0.7779, -0.7920, -0.8077, -0.8232, -0.8397, -0.8553,
2:         -0.8709], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.5440, -0.5369, -0.5236, -0.5014, -0.4728, -0.4382, -0.3999, -0.3622,
2:         -0.3250, -0.2927, -0.2658, -0.2444, -0.2281, -0.2200, -0.2286, -0.2517,
2:         -0.2905, -0.3281, -0.5708, -0.5614, -0.5446, -0.5174, -0.4837, -0.4494,
2:         -0.4158], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.4480,  0.3916,  0.3273,  0.2621,  0.1876,  0.1131,  0.0359, -0.0383,
2:         -0.1131, -0.1788, -0.2413, -0.2952, -0.3453, -0.3938, -0.4384, -0.4824,
2:         -0.5245, -0.5575,  0.4783,  0.4173,  0.3502,  0.2803,  0.2097,  0.1390,
2:          0.0726], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.4540e-01, -1.3478e-01, -1.0507e-01, -5.7276e-02,  6.4466e-02,
2:          1.5513e-01,  1.9700e-01,  2.1101e-01,  1.6511e-01,  1.0756e-01,
2:          9.4395e-02,  1.6348e-01,  2.7194e-01,  3.3541e-01,  4.0328e-01,
2:          4.4539e-01,  4.3892e-01,  4.2114e-01, -1.5237e-01, -1.2549e-01,
2:         -1.0254e-01, -8.5891e-02, -2.1399e-04,  6.9546e-02,  1.1289e-01],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.6022, -0.5726, -0.5340, -0.4887, -0.4313, -0.3534, -0.2546, -0.1460,
2:         -0.0514,  0.0093,  0.0337,  0.0363,  0.0358,  0.0451,  0.0705,  0.1063,
2:          0.1481,  0.1980,  0.2573,  0.3335,  0.4374,  0.5820,  0.7804,  1.0322,
2:          1.3124], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1610, -0.1642, -0.1692, -0.1685, -0.1685, -0.1700, -0.1714, -0.1693,
2:         -0.1681, -0.1655, -0.1619, -0.1667, -0.1676, -0.1683, -0.1677, -0.1719,
2:         -0.1701, -0.1652, -0.1623, -0.1610, -0.1602, -0.1615, -0.1602, -0.1621,
2:         -0.1641], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 6 [1/5 (20%)]	Loss: nan : nan :: 0.16098 (1.69 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 6 [2/5 (40%)]	Loss: nan : nan :: 0.13896 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 6 [3/5 (60%)]	Loss: nan : nan :: 0.14273 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 6 [4/5 (80%)]	Loss: nan : nan :: 0.14310 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch6.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch6.mod
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 6 : nan
0: validation loss for velocity_u : 0.0329323336482048
0: validation loss for velocity_v : 0.06336728483438492
0: validation loss for specific_humidity : 0.021755022928118706
0: validation loss for velocity_z : 0.4578665494918823
0: validation loss for temperature : 0.09467654675245285
0: validation loss for total_precip : nan
2: 7 : 23:01:36 :: batch_size = 96, lr = 1.7677085752190346e-05
0: 7 : 23:01:36 :: batch_size = 96, lr = 1.7677085752190346e-05
1: 7 : 23:01:36 :: batch_size = 96, lr = 1.7677085752190346e-05
3: 7 : 23:01:36 :: batch_size = 96, lr = 1.7677085752190346e-05
2: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 7, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.5098, 0.5111, 0.5101, 0.5087, 0.5099, 0.5165, 0.5308, 0.5538, 0.5841,
2:         0.6184, 0.6541, 0.6883, 0.7169, 0.7405, 0.7611, 0.7758, 0.7845, 0.7909,
2:         0.5842, 0.5844, 0.5822, 0.5827, 0.5899, 0.6030, 0.6246],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.3751, 1.4190, 1.4578, 1.4882, 1.5070, 1.5115, 1.4993, 1.4716, 1.4279,
2:         1.3670, 1.2964, 1.2210, 1.1368, 1.0483, 0.9603, 0.8729, 0.7923, 0.7237,
2:         1.4017, 1.4520, 1.4936, 1.5243, 1.5402, 1.5375, 1.5142],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.0333, -0.0674, -0.1137, -0.1637, -0.2145, -0.2587, -0.3084, -0.3503,
2:         -0.3814, -0.4131, -0.4156, -0.4163, -0.3754, -0.2672, -0.1079,  0.1076,
2:          0.3686,  0.6268, -0.0415, -0.0759, -0.1208, -0.1714, -0.2237, -0.2759,
2:         -0.3262], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-4.2713e-01, -3.3454e-01, -2.7318e-01, -1.8728e-01, -4.2252e-02,
2:          4.0302e-02,  7.4885e-02,  8.7157e-02,  2.5799e-02, -6.7911e-02,
2:         -1.6050e-01, -2.3302e-01, -2.7653e-01, -3.7693e-01, -4.9184e-01,
2:         -5.5208e-01, -6.2013e-01, -6.5248e-01, -5.0188e-01, -3.9143e-01,
2:         -2.9995e-01, -2.0959e-01, -8.0182e-02, -1.4362e-02,  1.4053e-04],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([ 0.4727,  0.4648,  0.4842,  0.5142,  0.5469,  0.5753,  0.5917,  0.5953,
2:          0.5830,  0.5535,  0.5084,  0.4509,  0.3884,  0.3195,  0.2387,  0.1466,
2:          0.0478, -0.0495, -0.1358, -0.2091, -0.2653, -0.3043, -0.3386, -0.3676,
2:         -0.3873], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2410,     nan,     nan,     nan,     nan, -0.2410,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2178,     nan,
2:         -0.2294,     nan, -0.2341,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2387,
2:             nan, -0.1819,     nan,     nan,     nan, -0.2074,     nan,     nan,
2:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2410,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan, -0.2341,
2:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1877,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1598,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2410,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2097,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.1610,     nan,     nan,     nan,     nan, -0.2027,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1830])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 7, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.0430, 1.0390, 1.0369, 1.0310, 1.0172, 0.9981, 0.9817, 0.9603, 0.9453,
2:         0.9344, 0.9157, 0.8990, 0.8870, 0.8780, 0.8721, 0.8633, 0.8545, 0.8410,
2:         1.0905, 1.0819, 1.0740, 1.0613, 1.0422, 1.0260, 1.0100],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.2968, -1.3004, -1.3037, -1.3035, -1.2989, -1.2814, -1.2574, -1.2240,
2:         -1.1891, -1.1583, -1.1267, -1.0981, -1.0722, -1.0446, -1.0206, -0.9998,
2:         -0.9768, -0.9564, -1.2812, -1.2840, -1.2909, -1.2971, -1.2956, -1.2843,
2:         -1.2653], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6440, -0.6464, -0.6593, -0.6745, -0.6884, -0.7028, -0.7112, -0.7016,
2:         -0.6771, -0.6415, -0.5865, -0.4999, -0.3975, -0.2649, -0.1028,  0.0417,
2:          0.1850,  0.3031, -0.6421, -0.6397, -0.6401, -0.6453, -0.6518, -0.6642,
2:         -0.6738], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.1221,  0.1354,  0.1158,  0.1111,  0.1055,  0.0744,  0.0384, -0.0070,
2:         -0.0280, -0.0448, -0.0629, -0.0553, -0.0237, -0.0163, -0.0452, -0.0537,
2:         -0.0590, -0.0722,  0.2093,  0.2145,  0.1942,  0.1992,  0.1913,  0.1477,
2:          0.1045], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([0.5172, 0.5332, 0.5459, 0.5513, 0.5535, 0.5509, 0.5474, 0.5437, 0.5378,
2:         0.5294, 0.5189, 0.5050, 0.4893, 0.4708, 0.4512, 0.4298, 0.4043, 0.3751,
2:         0.3442, 0.3175, 0.2999, 0.2960, 0.3050, 0.3216, 0.3440],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1586, -0.1594, -0.1636, -0.1617, -0.1607, -0.1652, -0.1673, -0.1633,
2:         -0.1660, -0.1655, -0.1577, -0.1610, -0.1615, -0.1640, -0.1635, -0.1698,
2:         -0.1679, -0.1629, -0.1622, -0.1606, -0.1563, -0.1555, -0.1583, -0.1561,
2:         -0.1620], device='cuda:2', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 7, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: Created sparse mask for total_precip with 10.0% data retained
0:      first 25 values: tensor([0.5098, 0.5111, 0.5101, 0.5087, 0.5099, 0.5165, 0.5308, 0.5538, 0.5841,
0:         0.6184, 0.6541, 0.6883, 0.7169, 0.7405, 0.7611, 0.7758, 0.7845, 0.7909,
0:         0.5842, 0.5844, 0.5822, 0.5827, 0.5899, 0.6030, 0.6246],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3751, 1.4190, 1.4578, 1.4882, 1.5070, 1.5115, 1.4993, 1.4716, 1.4279,
0:         1.3670, 1.2964, 1.2210, 1.1368, 1.0483, 0.9603, 0.8729, 0.7923, 0.7237,
0:         1.4017, 1.4520, 1.4936, 1.5243, 1.5402, 1.5375, 1.5142],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0333, -0.0674, -0.1137, -0.1637, -0.2145, -0.2587, -0.3084, -0.3503,
0:         -0.3814, -0.4131, -0.4156, -0.4163, -0.3754, -0.2672, -0.1079,  0.1076,
0:          0.3686,  0.6268, -0.0415, -0.0759, -0.1208, -0.1714, -0.2237, -0.2759,
0:         -0.3262], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-4.2713e-01, -3.3454e-01, -2.7318e-01, -1.8728e-01, -4.2252e-02,
0:          4.0302e-02,  7.4885e-02,  8.7157e-02,  2.5799e-02, -6.7911e-02,
0:         -1.6050e-01, -2.3302e-01, -2.7653e-01, -3.7693e-01, -4.9184e-01,
0:         -5.5208e-01, -6.2013e-01, -6.5248e-01, -5.0188e-01, -3.9143e-01,
0:         -2.9995e-01, -2.0959e-01, -8.0182e-02, -1.4362e-02,  1.4053e-04],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.4727,  0.4648,  0.4842,  0.5142,  0.5469,  0.5753,  0.5917,  0.5953,
0:          0.5830,  0.5535,  0.5084,  0.4509,  0.3884,  0.3195,  0.2387,  0.1466,
0:          0.0478, -0.0495, -0.1358, -0.2091, -0.2653, -0.3043, -0.3386, -0.3676,
0:         -0.3873], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2410,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2387,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2143,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2027,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2387,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2341,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2074,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2434,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2422,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2434,     nan,     nan,     nan,     nan, -0.2027,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2434,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2352,
0:         -0.2410,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2318,     nan,     nan,     nan, -0.2410,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2190,     nan,     nan,     nan,     nan,
0:         -0.1946,     nan,     nan,     nan,     nan, -0.1993,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2027,     nan,
0:             nan, -0.0728, -0.1111,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0786,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 7, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0814, 1.0722, 1.0634, 1.0556, 1.0395, 1.0172, 1.0022, 0.9817, 0.9666,
0:         0.9592, 0.9452, 0.9346, 0.9226, 0.9089, 0.9014, 0.8943, 0.8895, 0.8908,
0:         1.1142, 1.1150, 1.1114, 1.1049, 1.0861, 1.0716, 1.0539],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2980, -1.3012, -1.3017, -1.2982, -1.2885, -1.2699, -1.2392, -1.2043,
0:         -1.1671, -1.1329, -1.1015, -1.0761, -1.0565, -1.0390, -1.0244, -1.0129,
0:         -1.0011, -0.9880, -1.3020, -1.2983, -1.2973, -1.2994, -1.2921, -1.2802,
0:         -1.2596], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6706, -0.6656, -0.6637, -0.6671, -0.6687, -0.6824, -0.6931, -0.6927,
0:         -0.6813, -0.6631, -0.6194, -0.5450, -0.4420, -0.3017, -0.1345,  0.0301,
0:          0.1859,  0.3126, -0.6607, -0.6548, -0.6443, -0.6377, -0.6320, -0.6387,
0:         -0.6432], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1323,  0.1358,  0.0953,  0.0709,  0.0545,  0.0256,  0.0108, -0.0264,
0:         -0.0353, -0.0169, -0.0247, -0.0230, -0.0078, -0.0236, -0.0653, -0.0657,
0:         -0.0497, -0.0711,  0.2022,  0.2144,  0.1839,  0.1661,  0.1457,  0.1037,
0:          0.0809], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.5354, 0.5473, 0.5565, 0.5607, 0.5642, 0.5655, 0.5659, 0.5639, 0.5547,
0:         0.5407, 0.5232, 0.5037, 0.4842, 0.4651, 0.4447, 0.4220, 0.3953, 0.3656,
0:         0.3344, 0.3078, 0.2891, 0.2846, 0.2945, 0.3157, 0.3477],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1521, -0.1569, -0.1623, -0.1606, -0.1622, -0.1640, -0.1663, -0.1618,
0:         -0.1644, -0.1597, -0.1563, -0.1616, -0.1644, -0.1662, -0.1606, -0.1687,
0:         -0.1673, -0.1632, -0.1575, -0.1595, -0.1595, -0.1608, -0.1602, -0.1628,
0:         -0.1647], device='cuda:0', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 7, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.5098, 0.5111, 0.5101, 0.5087, 0.5099, 0.5165, 0.5308, 0.5538, 0.5841,
3:         0.6184, 0.6541, 0.6883, 0.7169, 0.7405, 0.7611, 0.7758, 0.7845, 0.7909,
3:         0.5842, 0.5844, 0.5822, 0.5827, 0.5899, 0.6030, 0.6246],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.3751, 1.4190, 1.4578, 1.4882, 1.5070, 1.5115, 1.4993, 1.4716, 1.4279,
3:         1.3670, 1.2964, 1.2210, 1.1368, 1.0483, 0.9603, 0.8729, 0.7923, 0.7237,
3:         1.4017, 1.4520, 1.4936, 1.5243, 1.5402, 1.5375, 1.5142],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.0333, -0.0674, -0.1137, -0.1637, -0.2145, -0.2587, -0.3084, -0.3503,
3:         -0.3814, -0.4131, -0.4156, -0.4163, -0.3754, -0.2672, -0.1079,  0.1076,
3:          0.3686,  0.6268, -0.0415, -0.0759, -0.1208, -0.1714, -0.2237, -0.2759,
3:         -0.3262], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-4.2713e-01, -3.3454e-01, -2.7318e-01, -1.8728e-01, -4.2252e-02,
3:          4.0302e-02,  7.4885e-02,  8.7157e-02,  2.5799e-02, -6.7911e-02,
3:         -1.6050e-01, -2.3302e-01, -2.7653e-01, -3.7693e-01, -4.9184e-01,
3:         -5.5208e-01, -6.2013e-01, -6.5248e-01, -5.0188e-01, -3.9143e-01,
3:         -2.9995e-01, -2.0959e-01, -8.0182e-02, -1.4362e-02,  1.4053e-04],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([ 0.4727,  0.4648,  0.4842,  0.5142,  0.5469,  0.5753,  0.5917,  0.5953,
3:          0.5830,  0.5535,  0.5084,  0.4509,  0.3884,  0.3195,  0.2387,  0.1466,
3:          0.0478, -0.0495, -0.1358, -0.2091, -0.2653, -0.3043, -0.3386, -0.3676,
3:         -0.3873], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan, -0.2434,     nan, -0.2434,     nan,     nan,     nan,
3:         -0.2410,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2399,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2329,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1981,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2039,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.1853,     nan,     nan,     nan,
3:             nan, -0.2422,     nan, -0.2434,     nan,     nan,     nan,     nan,
3:             nan, -0.2410,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2410,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2225,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2027, -0.2248,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2062,
3:             nan,     nan,     nan,     nan, -0.0066,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2434,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2213,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2178,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.1204,     nan,     nan, -0.0948,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 7, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.1320, 1.1060, 1.0855, 1.0687, 1.0500, 1.0304, 1.0145, 0.9960, 0.9787,
3:         0.9693, 0.9550, 0.9435, 0.9377, 0.9278, 0.9213, 0.9031, 0.8739, 0.8449,
3:         1.1620, 1.1435, 1.1237, 1.1074, 1.0842, 1.0707, 1.0500],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.3191, -1.3171, -1.3149, -1.3060, -1.2929, -1.2664, -1.2315, -1.1887,
3:         -1.1470, -1.1127, -1.0828, -1.0585, -1.0414, -1.0207, -1.0031, -0.9883,
3:         -0.9773, -0.9654, -1.3226, -1.3155, -1.3140, -1.3116, -1.3026, -1.2809,
3:         -1.2490], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5708, -0.5767, -0.5920, -0.6096, -0.6250, -0.6472, -0.6584, -0.6611,
3:         -0.6515, -0.6251, -0.5721, -0.4850, -0.3695, -0.2216, -0.0548,  0.1039,
3:          0.2511,  0.3657, -0.5834, -0.5807, -0.5803, -0.5855, -0.5955, -0.6151,
3:         -0.6305], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.2258,  0.2467,  0.2094,  0.1826,  0.1700,  0.1492,  0.1427,  0.1148,
3:          0.0974,  0.0837,  0.0438, -0.0082, -0.0302, -0.0539, -0.0940, -0.0825,
3:         -0.0578, -0.0450,  0.3008,  0.3185,  0.2698,  0.2355,  0.2089,  0.1751,
3:          0.1692], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([0.5455, 0.5546, 0.5643, 0.5708, 0.5768, 0.5789, 0.5789, 0.5761, 0.5691,
3:         0.5589, 0.5452, 0.5267, 0.5049, 0.4796, 0.4510, 0.4211, 0.3906, 0.3611,
3:         0.3349, 0.3162, 0.3061, 0.3064, 0.3155, 0.3301, 0.3533],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1608, -0.1590, -0.1670, -0.1614, -0.1593, -0.1604, -0.1580, -0.1514,
3:         -0.1506, -0.1648, -0.1617, -0.1635, -0.1633, -0.1629, -0.1560, -0.1588,
3:         -0.1568, -0.1507, -0.1650, -0.1664, -0.1610, -0.1595, -0.1588, -0.1574,
3:         -0.1592], device='cuda:3', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 7, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.5098, 0.5111, 0.5101, 0.5087, 0.5099, 0.5165, 0.5308, 0.5538, 0.5841,
1:         0.6184, 0.6541, 0.6883, 0.7169, 0.7405, 0.7611, 0.7758, 0.7845, 0.7909,
1:         0.5842, 0.5844, 0.5822, 0.5827, 0.5899, 0.6030, 0.6246],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.3751, 1.4190, 1.4578, 1.4882, 1.5070, 1.5115, 1.4993, 1.4716, 1.4279,
1:         1.3670, 1.2964, 1.2210, 1.1368, 1.0483, 0.9603, 0.8729, 0.7923, 0.7237,
1:         1.4017, 1.4520, 1.4936, 1.5243, 1.5402, 1.5375, 1.5142],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0333, -0.0674, -0.1137, -0.1637, -0.2145, -0.2587, -0.3084, -0.3503,
1:         -0.3814, -0.4131, -0.4156, -0.4163, -0.3754, -0.2672, -0.1079,  0.1076,
1:          0.3686,  0.6268, -0.0415, -0.0759, -0.1208, -0.1714, -0.2237, -0.2759,
1:         -0.3262], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-4.2713e-01, -3.3454e-01, -2.7318e-01, -1.8728e-01, -4.2252e-02,
1:          4.0302e-02,  7.4885e-02,  8.7157e-02,  2.5799e-02, -6.7911e-02,
1:         -1.6050e-01, -2.3302e-01, -2.7653e-01, -3.7693e-01, -4.9184e-01,
1:         -5.5208e-01, -6.2013e-01, -6.5248e-01, -5.0188e-01, -3.9143e-01,
1:         -2.9995e-01, -2.0959e-01, -8.0182e-02, -1.4362e-02,  1.4053e-04],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([ 0.4727,  0.4648,  0.4842,  0.5142,  0.5469,  0.5753,  0.5917,  0.5953,
1:          0.5830,  0.5535,  0.5084,  0.4509,  0.3884,  0.3195,  0.2387,  0.1466,
1:          0.0478, -0.0495, -0.1358, -0.2091, -0.2653, -0.3043, -0.3386, -0.3676,
1:         -0.3873], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2410,
1:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
1:         -0.2410,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2410,     nan,     nan, -0.2283,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2422,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2387,     nan,     nan,
1:             nan, -0.1958,     nan,     nan, -0.2236,     nan, -0.2387,     nan,
1:             nan,     nan,     nan, -0.1621,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2387,     nan,     nan,     nan,
1:         -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.0716,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1528,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2434, -0.2434,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2434,     nan, -0.2294,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1993,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1877,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1204,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 7, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.0386, 1.0242, 1.0103, 0.9971, 0.9818, 0.9614, 0.9484, 0.9323, 0.9125,
1:         0.8993, 0.8774, 0.8602, 0.8542, 0.8494, 0.8533, 0.8546, 0.8522, 0.8403,
1:         1.0885, 1.0838, 1.0707, 1.0553, 1.0324, 1.0174, 1.0009],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.3075, -1.3076, -1.3109, -1.3127, -1.3084, -1.2865, -1.2508, -1.2046,
1:         -1.1623, -1.1253, -1.0969, -1.0740, -1.0561, -1.0390, -1.0229, -1.0064,
1:         -0.9815, -0.9507, -1.3379, -1.3266, -1.3233, -1.3277, -1.3243, -1.3082,
1:         -1.2802], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6353, -0.6237, -0.6274, -0.6323, -0.6442, -0.6525, -0.6599, -0.6582,
1:         -0.6406, -0.6136, -0.5594, -0.4722, -0.3573, -0.2060, -0.0195,  0.1606,
1:          0.3343,  0.4719, -0.6368, -0.6258, -0.6144, -0.6078, -0.6088, -0.6199,
1:         -0.6287], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.1617,  0.1644,  0.1297,  0.1154,  0.1106,  0.0914,  0.0807,  0.0443,
1:          0.0304,  0.0155, -0.0264, -0.0567, -0.0686, -0.0874, -0.1126, -0.0972,
1:         -0.0662, -0.0371,  0.2378,  0.2347,  0.1920,  0.1775,  0.1613,  0.1213,
1:          0.1013], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([0.4937, 0.5102, 0.5251, 0.5352, 0.5421, 0.5456, 0.5473, 0.5477, 0.5434,
1:         0.5332, 0.5171, 0.4948, 0.4712, 0.4467, 0.4238, 0.3998, 0.3747, 0.3478,
1:         0.3213, 0.2995, 0.2852, 0.2821, 0.2899, 0.3070, 0.3346],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1444, -0.1500, -0.1540, -0.1547, -0.1581, -0.1631, -0.1630, -0.1587,
1:         -0.1584, -0.1562, -0.1548, -0.1587, -0.1621, -0.1654, -0.1618, -0.1659,
1:         -0.1642, -0.1571, -0.1587, -0.1600, -0.1572, -0.1564, -0.1589, -0.1613,
1:         -0.1629], device='cuda:1', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 7 [1/5 (20%)]	Loss: nan : nan :: 0.14527 (1.67 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 7 [2/5 (40%)]	Loss: nan : nan :: 0.14372 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 7 [3/5 (60%)]	Loss: nan : nan :: 0.14448 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 7 [4/5 (80%)]	Loss: nan : nan :: 0.14068 (10.42 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch7.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch7.mod
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 7 : nan
0: validation loss for velocity_u : 0.036987245082855225
0: validation loss for velocity_v : 0.06253376603126526
0: validation loss for specific_humidity : 0.028305094689130783
0: validation loss for velocity_z : 0.5232236385345459
0: validation loss for temperature : 0.07859339565038681
0: validation loss for total_precip : nan
2: 8 : 23:08:13 :: batch_size = 96, lr = 1.7245937319210094e-05
3: 8 : 23:08:13 :: batch_size = 96, lr = 1.7245937319210094e-05
0: 8 : 23:08:13 :: batch_size = 96, lr = 1.7245937319210094e-05
1: 8 : 23:08:13 :: batch_size = 96, lr = 1.7245937319210094e-05
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 8, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.1717, 0.1709, 0.1700, 0.1692, 0.1684, 0.1676, 0.1668, 0.1660, 0.1650,
2:         0.1642, 0.1633, 0.1625, 0.1615, 0.1607, 0.1599, 0.1589, 0.1581, 0.1571,
2:         0.0347, 0.0339, 0.0332, 0.0324, 0.0316, 0.0309, 0.0303],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.0583, -0.0607, -0.0630, -0.0654, -0.0677, -0.0699, -0.0724, -0.0747,
2:         -0.0769, -0.0794, -0.0816, -0.0839, -0.0861, -0.0886, -0.0908, -0.0931,
2:         -0.0954, -0.0976, -0.0472, -0.0486, -0.0499, -0.0513, -0.0525, -0.0540,
2:         -0.0552], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.6748, -0.6747, -0.6746, -0.6746, -0.6745, -0.6745, -0.6744, -0.6743,
2:         -0.6743, -0.6742, -0.6742, -0.6742, -0.6742, -0.6742, -0.6742, -0.6742,
2:         -0.6742, -0.6742, -0.6569, -0.6568, -0.6567, -0.6565, -0.6564, -0.6563,
2:         -0.6562], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.8275, -0.8275, -0.8275, -0.8275, -0.8275, -0.8275, -0.8275, -0.8275,
2:         -0.8264, -0.8264, -0.8264, -0.8253, -0.8253, -0.8242, -0.8242, -0.8231,
2:         -0.8231, -0.8220, -0.7528, -0.7506, -0.7495, -0.7473, -0.7451, -0.7440,
2:         -0.7418], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.5895, -0.5895, -0.5893, -0.5892, -0.5891, -0.5891, -0.5890, -0.5887,
2:         -0.5887, -0.5887, -0.5887, -0.5887, -0.5884, -0.5884, -0.5883, -0.5883,
2:         -0.5882, -0.5882, -0.5881, -0.5879, -0.5879, -0.5878, -0.5876, -0.5875,
2:         -0.5873], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan, -0.2425,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2365,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2281,     nan,     nan, -0.2270,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2305, -0.2425,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2401,     nan,     nan, -0.2329,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2317, -0.2317,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2389,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2365,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2365,     nan,     nan,
2:         -0.2305,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2246, -0.2234,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2258,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2413, -0.2401,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 8, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0835, -0.0815, -0.0769, -0.0774, -0.0804, -0.0857, -0.0881, -0.0891,
2:         -0.0856, -0.0784, -0.0715, -0.0683, -0.0658, -0.0680, -0.0709, -0.0695,
2:         -0.0647, -0.0553,  0.0410,  0.0458,  0.0482,  0.0464,  0.0424,  0.0393,
2:          0.0373], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0876, -0.0888, -0.0889, -0.0883, -0.0861, -0.0825, -0.0795, -0.0785,
2:         -0.0757, -0.0741, -0.0747, -0.0726, -0.0717, -0.0735, -0.0776, -0.0817,
2:         -0.0858, -0.0866, -0.0708, -0.0704, -0.0687, -0.0665, -0.0613, -0.0580,
2:         -0.0539], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.7152, -0.7108, -0.7083, -0.7055, -0.7055, -0.7057, -0.7058, -0.7043,
2:         -0.7026, -0.7004, -0.6996, -0.7002, -0.7028, -0.7066, -0.7108, -0.7156,
2:         -0.7212, -0.7266, -0.7078, -0.7044, -0.7005, -0.6982, -0.6979, -0.6992,
2:         -0.6982], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.1706, 0.1971, 0.2148, 0.2479, 0.2639, 0.2483, 0.2343, 0.2131, 0.1900,
2:         0.1791, 0.1842, 0.1853, 0.1784, 0.1723, 0.1632, 0.1700, 0.1915, 0.2171,
2:         0.1167, 0.1354, 0.1431, 0.1662, 0.1712, 0.1489, 0.1277],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-1.1582, -1.1573, -1.1568, -1.1578, -1.1590, -1.1598, -1.1583, -1.1567,
2:         -1.1552, -1.1553, -1.1573, -1.1600, -1.1619, -1.1616, -1.1606, -1.1596,
2:         -1.1599, -1.1632, -1.1693, -1.1771, -1.1829, -1.1842, -1.1819, -1.1800,
2:         -1.1798], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1396, -0.1403, -0.1481, -0.1527, -0.1583, -0.1621, -0.1658, -0.1586,
2:         -0.1594, -0.1453, -0.1418, -0.1487, -0.1540, -0.1585, -0.1580, -0.1633,
2:         -0.1632, -0.1540, -0.1436, -0.1447, -0.1480, -0.1442, -0.1491, -0.1514,
2:         -0.1556], device='cuda:2', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 8, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: Created sparse mask for total_precip with 10.0% data retained
1:      first 25 values: tensor([0.1717, 0.1709, 0.1700, 0.1692, 0.1684, 0.1676, 0.1668, 0.1660, 0.1650,
1:         0.1642, 0.1633, 0.1625, 0.1615, 0.1607, 0.1599, 0.1589, 0.1581, 0.1571,
1:         0.0347, 0.0339, 0.0332, 0.0324, 0.0316, 0.0309, 0.0303],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0583, -0.0607, -0.0630, -0.0654, -0.0677, -0.0699, -0.0724, -0.0747,
1:         -0.0769, -0.0794, -0.0816, -0.0839, -0.0861, -0.0886, -0.0908, -0.0931,
1:         -0.0954, -0.0976, -0.0472, -0.0486, -0.0499, -0.0513, -0.0525, -0.0540,
1:         -0.0552], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.6748, -0.6747, -0.6746, -0.6746, -0.6745, -0.6745, -0.6744, -0.6743,
1:         -0.6743, -0.6742, -0.6742, -0.6742, -0.6742, -0.6742, -0.6742, -0.6742,
1:         -0.6742, -0.6742, -0.6569, -0.6568, -0.6567, -0.6565, -0.6564, -0.6563,
1:         -0.6562], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.8275, -0.8275, -0.8275, -0.8275, -0.8275, -0.8275, -0.8275, -0.8275,
1:         -0.8264, -0.8264, -0.8264, -0.8253, -0.8253, -0.8242, -0.8242, -0.8231,
1:         -0.8231, -0.8220, -0.7528, -0.7506, -0.7495, -0.7473, -0.7451, -0.7440,
1:         -0.7418], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.5895, -0.5895, -0.5893, -0.5892, -0.5891, -0.5891, -0.5890, -0.5887,
1:         -0.5887, -0.5887, -0.5887, -0.5887, -0.5884, -0.5884, -0.5883, -0.5883,
1:         -0.5882, -0.5882, -0.5881, -0.5879, -0.5879, -0.5878, -0.5876, -0.5875,
1:         -0.5873], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan, -0.2425,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2341,
1:             nan,     nan,     nan,     nan, -0.2258,     nan,     nan,     nan,
1:         -0.2270,     nan,     nan,     nan,     nan,     nan,     nan, -0.2365,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2281,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2341,     nan,     nan,     nan,     nan,     nan,     nan, -0.2305,
1:             nan,     nan,     nan, -0.2413,     nan, -0.2413,     nan,     nan,
1:             nan, -0.2401, -0.2341,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2389,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2401,     nan,     nan,
1:             nan,     nan, -0.2389,     nan,     nan,     nan,     nan,     nan,
1:         -0.2293,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2234,     nan, -0.2222,     nan,     nan, -0.2329,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2389,
1:             nan, -0.2377,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2305,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2329,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 8, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1051, -0.1004, -0.0973, -0.0980, -0.0991, -0.1015, -0.1002, -0.0980,
1:         -0.0934, -0.0858, -0.0797, -0.0783, -0.0785, -0.0866, -0.0948, -0.0985,
1:         -0.0974, -0.0854,  0.0422,  0.0488,  0.0488,  0.0457,  0.0413,  0.0397,
1:          0.0398], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.0721, -0.0714, -0.0728, -0.0772, -0.0818, -0.0822, -0.0818, -0.0784,
1:         -0.0737, -0.0724, -0.0761, -0.0795, -0.0840, -0.0878, -0.0934, -0.0961,
1:         -0.1001, -0.1032, -0.0555, -0.0496, -0.0500, -0.0508, -0.0513, -0.0534,
1:         -0.0522], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.7103, -0.7114, -0.7140, -0.7141, -0.7135, -0.7114, -0.7095, -0.7054,
1:         -0.7026, -0.7006, -0.6991, -0.6980, -0.6979, -0.6991, -0.7015, -0.7038,
1:         -0.7073, -0.7104, -0.7076, -0.7094, -0.7098, -0.7109, -0.7081, -0.7063,
1:         -0.7029], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.1885, 0.2033, 0.2221, 0.2483, 0.2556, 0.2319, 0.2064, 0.1809, 0.1692,
1:         0.1686, 0.1642, 0.1699, 0.1703, 0.1565, 0.1356, 0.0966, 0.0897, 0.1323,
1:         0.1794, 0.1763, 0.1795, 0.1935, 0.1933, 0.1640, 0.1313],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-1.1825, -1.1795, -1.1726, -1.1652, -1.1582, -1.1555, -1.1548, -1.1564,
1:         -1.1578, -1.1596, -1.1620, -1.1638, -1.1659, -1.1680, -1.1690, -1.1697,
1:         -1.1707, -1.1732, -1.1767, -1.1796, -1.1797, -1.1757, -1.1682, -1.1614,
1:         -1.1587], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1450, -0.1463, -0.1529, -0.1490, -0.1518, -0.1533, -0.1551, -0.1487,
1:         -0.1528, -0.1551, -0.1487, -0.1522, -0.1557, -0.1573, -0.1537, -0.1568,
1:         -0.1564, -0.1480, -0.1542, -0.1541, -0.1513, -0.1484, -0.1496, -0.1526,
1:         -0.1545], device='cuda:1', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 8, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8524, -0.8551, -0.8532, -0.8612, -0.8790, -0.9030, -0.9250, -0.9300,
0:         -0.9267, -0.9216, -0.9125, -0.9045, -0.8996, -0.9038, -0.9119, -0.9143,
0:         -0.9003, -0.8681, -0.8265, -0.8333, -0.8397, -0.8510, -0.8680, -0.8906,
0:         -0.9091], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4257, -0.3651, -0.3208, -0.2833, -0.2492, -0.2473, -0.2365, -0.2290,
0:         -0.2264, -0.2214, -0.2337, -0.2449, -0.2572, -0.2647, -0.2658, -0.2658,
0:         -0.2637, -0.2613, -0.4332, -0.3929, -0.3526, -0.3169, -0.2925, -0.2869,
0:         -0.2727], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2071, 1.3121, 1.3605, 1.3687, 1.2924, 1.0949, 1.0086, 0.9424, 0.9425,
0:         0.9515, 0.9505, 0.9475, 0.9324, 0.9076, 0.8813, 0.8509, 0.8169, 0.7692,
0:         1.2423, 1.2924, 1.3008, 1.2573, 1.1747, 1.0875, 1.0378],
0:        device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5282,  0.3983,  0.3610,  0.2513,  0.2559,  0.3734,  0.2310,  0.0649,
0:         -0.1261, -0.0968, -0.1171, -0.0617, -0.0120, -0.0052,  0.0615,  0.0230,
0:          0.1259,  0.1338,  0.5045,  0.4130,  0.3191,  0.3723,  0.4118,  0.3644,
0:          0.2412], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1336, -0.1811, -0.2376, -0.3065, -0.3361, -0.2695, -0.2332, -0.1835,
0:         -0.0801,  0.0192,  0.1187,  0.1235,  0.1241,  0.1370,  0.1659,  0.2838,
0:          0.4170,  0.5593,  0.5981,  0.4989,  0.2888,  0.0478, -0.1711, -0.3555,
0:         -0.4827], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.8084,     nan,  0.4351,     nan,     nan,     nan,
0:             nan,  0.3479,     nan,     nan,     nan,     nan,  0.7236,  1.4856,
0:             nan,     nan,     nan,     nan,     nan,  0.3939,     nan,  0.7001,
0:             nan,     nan,     nan,     nan,  0.1677,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  2.4443,
0:             nan,     nan,  1.8790,     nan,     nan,     nan,     nan,  0.8320,
0:             nan,     nan,     nan,     nan,  0.0405,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0148,
0:             nan,     nan,     nan,  0.1312,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0182,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0947,
0:          1.2395,  1.8142,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.1053,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1936,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0820,     nan,
0:             nan,     nan,     nan,     nan,  1.2984,     nan,     nan,     nan,
0:          1.3643,  0.2761,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  1.0958,     nan,  1.4798,     nan,
0:          2.8554,  3.3701,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 8, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0958, -1.1006, -1.1152, -1.1371, -1.1599, -1.1805, -1.1925, -1.1978,
0:         -1.1974, -1.1923, -1.1834, -1.1732, -1.1603, -1.1438, -1.1294, -1.1151,
0:         -1.1041, -1.0974, -1.0879, -1.0939, -1.1122, -1.1368, -1.1632, -1.1831,
0:         -1.1917], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1813, 0.2051, 0.2269, 0.2413, 0.2520, 0.2587, 0.2606, 0.2595, 0.2592,
0:         0.2572, 0.2519, 0.2464, 0.2429, 0.2416, 0.2466, 0.2626, 0.2846, 0.3075,
0:         0.1893, 0.2119, 0.2333, 0.2484, 0.2619, 0.2673, 0.2704],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.8956, 1.9033, 1.9042, 1.9110, 1.9167, 1.9353, 1.9584, 1.9952, 2.0265,
0:         2.0571, 2.0949, 2.1327, 2.1591, 2.1691, 2.1625, 2.1382, 2.0932, 2.0482,
0:         1.9045, 1.9061, 1.9095, 1.9026, 1.9170, 1.9381, 1.9773],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2532, -0.2625, -0.2442, -0.1598, -0.0363,  0.0166,  0.0573,  0.1084,
0:          0.1432,  0.1709,  0.1591,  0.1344,  0.1312,  0.1484,  0.1727,  0.2089,
0:          0.2414,  0.2465, -0.3011, -0.2990, -0.2838, -0.2251, -0.1029, -0.0025,
0:          0.0934], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3461, -0.3407, -0.3261, -0.3110, -0.2950, -0.2989, -0.3374, -0.4304,
0:         -0.5818, -0.7813, -1.0047, -1.2133, -1.3585, -1.4137, -1.3757, -1.2535,
0:         -1.0825, -0.8875, -0.6802, -0.4648, -0.2340,  0.0137,  0.2771,  0.5477,
0:          0.8072], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1670, -0.1709, -0.1748, -0.1730, -0.1739, -0.1729, -0.1769, -0.1750,
0:         -0.1738, -0.1709, -0.1668, -0.1711, -0.1731, -0.1753, -0.1740, -0.1783,
0:         -0.1777, -0.1753, -0.1687, -0.1704, -0.1677, -0.1677, -0.1711, -0.1695,
0:         -0.1738], device='cuda:0', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 8, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.8524, -0.8551, -0.8532, -0.8612, -0.8790, -0.9030, -0.9250, -0.9300,
3:         -0.9267, -0.9216, -0.9125, -0.9045, -0.8996, -0.9038, -0.9119, -0.9143,
3:         -0.9003, -0.8681, -0.8265, -0.8333, -0.8397, -0.8510, -0.8680, -0.8906,
3:         -0.9091], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4257, -0.3651, -0.3208, -0.2833, -0.2492, -0.2473, -0.2365, -0.2290,
3:         -0.2264, -0.2214, -0.2337, -0.2449, -0.2572, -0.2647, -0.2658, -0.2658,
3:         -0.2637, -0.2613, -0.4332, -0.3929, -0.3526, -0.3169, -0.2925, -0.2869,
3:         -0.2727], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.2071, 1.3121, 1.3605, 1.3687, 1.2924, 1.0949, 1.0086, 0.9424, 0.9425,
3:         0.9515, 0.9505, 0.9475, 0.9324, 0.9076, 0.8813, 0.8509, 0.8169, 0.7692,
3:         1.2423, 1.2924, 1.3008, 1.2573, 1.1747, 1.0875, 1.0378],
3:        device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.5282,  0.3983,  0.3610,  0.2513,  0.2559,  0.3734,  0.2310,  0.0649,
3:         -0.1261, -0.0968, -0.1171, -0.0617, -0.0120, -0.0052,  0.0615,  0.0230,
3:          0.1259,  0.1338,  0.5045,  0.4130,  0.3191,  0.3723,  0.4118,  0.3644,
3:          0.2412], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.1336, -0.1811, -0.2376, -0.3065, -0.3361, -0.2695, -0.2332, -0.1835,
3:         -0.0801,  0.0192,  0.1187,  0.1235,  0.1241,  0.1370,  0.1659,  0.2838,
3:          0.4170,  0.5593,  0.5981,  0.4989,  0.2888,  0.0478, -0.1711, -0.3555,
3:         -0.4827], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  1.0228,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0558,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1373,     nan,     nan,
3:          0.0676,     nan,     nan,     nan,  0.1630,     nan,     nan,     nan,
3:          1.9026,     nan, -0.1043,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  0.0405,     nan,     nan, -0.1903,
3:             nan,     nan,  2.3266,     nan,     nan,  0.3738,     nan,     nan,
3:         -0.1620, -0.1161,  0.6494,     nan,     nan,     nan,     nan,  0.1159,
3:             nan, -0.0348,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  1.8142,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  0.6636,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  0.5305,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  2.6563,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 8, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.1180, -1.1157, -1.1230, -1.1385, -1.1606, -1.1840, -1.2030, -1.2132,
3:         -1.2161, -1.2074, -1.1905, -1.1720, -1.1539, -1.1365, -1.1269, -1.1177,
3:         -1.1104, -1.1052, -1.0994, -1.1014, -1.1146, -1.1348, -1.1587, -1.1825,
3:         -1.1959], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.1880, 0.2084, 0.2255, 0.2385, 0.2461, 0.2514, 0.2498, 0.2452, 0.2396,
3:         0.2304, 0.2227, 0.2182, 0.2143, 0.2116, 0.2119, 0.2156, 0.2265, 0.2434,
3:         0.1882, 0.2090, 0.2270, 0.2427, 0.2543, 0.2584, 0.2611],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.9021, 1.9190, 1.9138, 1.9018, 1.8871, 1.8940, 1.9114, 1.9576, 2.0065,
3:         2.0521, 2.1045, 2.1562, 2.1888, 2.2051, 2.1968, 2.1644, 2.1097, 2.0485,
3:         1.9286, 1.9427, 1.9378, 1.9132, 1.9077, 1.9136, 1.9487],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.4288, -0.4535, -0.4116, -0.2673, -0.0946,  0.0349,  0.1020,  0.1197,
3:          0.1216,  0.1409,  0.1584,  0.1255,  0.1071,  0.0932,  0.0501,  0.0508,
3:          0.0834,  0.1616, -0.3953, -0.3963, -0.3351, -0.2060, -0.0698,  0.0581,
3:          0.1559], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.3129, -0.3316, -0.3531, -0.3699, -0.3706, -0.3690, -0.3843, -0.4540,
3:         -0.5933, -0.7944, -1.0238, -1.2273, -1.3544, -1.3865, -1.3298, -1.2060,
3:         -1.0447, -0.8616, -0.6582, -0.4407, -0.2061,  0.0392,  0.2922,  0.5429,
3:          0.7800], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1502, -0.1568, -0.1606, -0.1626, -0.1626, -0.1674, -0.1656, -0.1590,
3:         -0.1602, -0.1599, -0.1568, -0.1605, -0.1625, -0.1663, -0.1640, -0.1681,
3:         -0.1621, -0.1547, -0.1577, -0.1594, -0.1577, -0.1581, -0.1575, -0.1585,
3:         -0.1600], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 8 [1/5 (20%)]	Loss: nan : nan :: 0.15543 (1.75 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 8 [2/5 (40%)]	Loss: nan : nan :: 0.15023 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 8 [3/5 (60%)]	Loss: nan : nan :: 0.14754 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 8 [4/5 (80%)]	Loss: nan : nan :: 0.14069 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch8.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch8.mod
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 8 : nan
0: validation loss for velocity_u : 0.030983885750174522
0: validation loss for velocity_v : 0.05490382760763168
0: validation loss for specific_humidity : 0.025644628331065178
0: validation loss for velocity_z : 0.4521248936653137
0: validation loss for temperature : 0.08149709552526474
0: validation loss for total_precip : nan
3: 9 : 23:15:05 :: batch_size = 96, lr = 1.6825304701668386e-05
1: 9 : 23:15:05 :: batch_size = 96, lr = 1.6825304701668386e-05
2: 9 : 23:15:05 :: batch_size = 96, lr = 1.6825304701668386e-05
0: 9 : 23:15:05 :: batch_size = 96, lr = 1.6825304701668386e-05
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 9, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4744, -0.4894, -0.4848, -0.4655, -0.4371, -0.4023, -0.3641, -0.3233,
0:         -0.2825, -0.2513, -0.2376, -0.2376, -0.2436, -0.2506, -0.2513, -0.2385,
0:         -0.2165, -0.1904, -0.3491, -0.3597, -0.3545, -0.3355, -0.3065, -0.2715,
0:         -0.2345], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.8394, 2.9011, 2.9441, 2.9689, 2.9799, 2.9779, 2.9607, 2.9322, 2.9027,
0:         2.8754, 2.8478, 2.8185, 2.7855, 2.7457, 2.7054, 2.6765, 2.6601, 2.6507,
0:         2.7951, 2.8589, 2.9041, 2.9316, 2.9437, 2.9425, 2.9275],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7020, -0.6992, -0.6953, -0.6920, -0.6883, -0.6867, -0.6861, -0.6872,
0:         -0.6892, -0.6920, -0.6952, -0.6983, -0.7010, -0.7026, -0.7045, -0.7062,
0:         -0.7081, -0.7096, -0.7018, -0.6985, -0.6940, -0.6891, -0.6844, -0.6821,
0:         -0.6803], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2043, -0.2541, -0.3194, -0.4953, -0.6846, -0.6846, -0.6215, -0.7145,
0:         -0.8252, -0.8229, -0.9192, -1.2092, -1.4461, -1.5567, -1.6696, -1.6242,
0:         -1.1992, -0.6492, -0.2828, -0.3183, -0.3603, -0.4776, -0.6115, -0.6558,
0:         -0.6946], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.4794, 0.5153, 0.5407, 0.5584, 0.5716, 0.5818, 0.5891, 0.5964, 0.6089,
0:         0.6294, 0.6558, 0.6874, 0.7261, 0.7722, 0.8230, 0.8817, 0.9530, 1.0321,
0:         1.1094, 1.1818, 1.2499, 1.3095, 1.3600, 1.4128, 1.4727],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.1752,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1752,     nan,     nan,     nan,     nan, -0.2133,     nan,     nan,
0:             nan,     nan,     nan, -0.2442,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1776,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.1672,     nan,
0:             nan,     nan,     nan,  0.1933,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2513,     nan,     nan, -0.2133,     nan, -0.2061,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1586,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1942,     nan,     nan,     nan,
0:             nan,     nan, -0.1800,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2156,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2370,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1824,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1966,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1752,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0730,     nan,     nan,
0:          0.0721,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  1.3204])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 9, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.3769, 2.4420, 2.4877, 2.5112, 2.5277, 2.5358, 2.5597, 2.5669, 2.5923,
0:         2.6044, 2.6049, 2.5923, 2.5723, 2.5512, 2.5205, 2.5011, 2.4893, 2.4890,
0:         2.2969, 2.3568, 2.3835, 2.3820, 2.3706, 2.3623, 2.3639],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.9391, 2.9999, 3.0699, 3.1357, 3.2048, 3.2688, 3.3177, 3.3618, 3.3950,
0:         3.4169, 3.4267, 3.4205, 3.3844, 3.3287, 3.2513, 3.1706, 3.0936, 3.0207,
0:         3.0347, 3.0950, 3.1682, 3.2389, 3.3174, 3.3856, 3.4375],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6571, -0.6451, -0.6349, -0.6220, -0.6110, -0.5991, -0.5901, -0.5797,
0:         -0.5694, -0.5592, -0.5457, -0.5329, -0.5212, -0.5163, -0.5137, -0.5147,
0:         -0.5153, -0.5183, -0.6248, -0.6131, -0.5991, -0.5831, -0.5692, -0.5561,
0:         -0.5448], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5398, -0.6173, -0.8602, -1.1897, -1.4697, -1.7481, -1.8884, -1.9428,
0:         -2.0793, -2.2459, -2.4236, -2.5042, -2.3395, -2.1564, -2.0315, -1.9058,
0:         -1.8255, -1.5976, -1.0197, -1.0815, -1.2691, -1.5749, -1.8387, -2.0556,
0:         -2.1369], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.2872, -1.2764, -1.2577, -1.2386, -1.2158, -1.1897, -1.1602, -1.1273,
0:         -1.0942, -1.0642, -1.0398, -1.0203, -1.0009, -0.9818, -0.9619, -0.9457,
0:         -0.9356, -0.9309, -0.9277, -0.9242, -0.9227, -0.9257, -0.9351, -0.9503,
0:         -0.9669], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1774, -0.1818, -0.1826, -0.1792, -0.1768, -0.1768, -0.1759, -0.1687,
0:         -0.1666, -0.1791, -0.1734, -0.1792, -0.1806, -0.1799, -0.1757, -0.1747,
0:         -0.1711, -0.1623, -0.1758, -0.1743, -0.1730, -0.1739, -0.1713, -0.1684,
0:         -0.1689], device='cuda:0', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 9, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4744, -0.4894, -0.4848, -0.4655, -0.4371, -0.4023, -0.3641, -0.3233,
3:         -0.2825, -0.2513, -0.2376, -0.2376, -0.2436, -0.2506, -0.2513, -0.2385,
3:         -0.2165, -0.1904, -0.3491, -0.3597, -0.3545, -0.3355, -0.3065, -0.2715,
3:         -0.2345], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([2.8394, 2.9011, 2.9441, 2.9689, 2.9799, 2.9779, 2.9607, 2.9322, 2.9027,
3:         2.8754, 2.8478, 2.8185, 2.7855, 2.7457, 2.7054, 2.6765, 2.6601, 2.6507,
3:         2.7951, 2.8589, 2.9041, 2.9316, 2.9437, 2.9425, 2.9275],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7020, -0.6992, -0.6953, -0.6920, -0.6883, -0.6867, -0.6861, -0.6872,
3:         -0.6892, -0.6920, -0.6952, -0.6983, -0.7010, -0.7026, -0.7045, -0.7062,
3:         -0.7081, -0.7096, -0.7018, -0.6985, -0.6940, -0.6891, -0.6844, -0.6821,
3:         -0.6803], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2043, -0.2541, -0.3194, -0.4953, -0.6846, -0.6846, -0.6215, -0.7145,
3:         -0.8252, -0.8229, -0.9192, -1.2092, -1.4461, -1.5567, -1.6696, -1.6242,
3:         -1.1992, -0.6492, -0.2828, -0.3183, -0.3603, -0.4776, -0.6115, -0.6558,
3:         -0.6946], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.4794, 0.5153, 0.5407, 0.5584, 0.5716, 0.5818, 0.5891, 0.5964, 0.6089,
3:         0.6294, 0.6558, 0.6874, 0.7261, 0.7722, 0.8230, 0.8817, 0.9530, 1.0321,
3:         1.1094, 1.1818, 1.2499, 1.3095, 1.3600, 1.4128, 1.4727],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan, -0.2038,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.1752, -0.1015,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2038,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,  0.3003,     nan,     nan,
3:             nan,     nan, -0.2513,  0.1933,     nan, -0.1182,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2347,     nan, -0.2109,     nan,
3:             nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,
3:             nan, -0.1562, -0.1253, -0.2513,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1419,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2489,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2466,     nan,     nan,     nan,     nan,     nan, -0.2489,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2513,     nan, -0.2489,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2204,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2299,     nan,
3:         -0.1966, -0.1847,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1610,     nan,     nan,
3:         -0.1752,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.0183,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 9, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([2.3314, 2.4009, 2.4593, 2.5045, 2.5483, 2.5763, 2.6167, 2.6301, 2.6534,
3:         2.6713, 2.6809, 2.6849, 2.6814, 2.6709, 2.6396, 2.6069, 2.5727, 2.5409,
3:         2.2720, 2.3460, 2.3866, 2.4066, 2.4149, 2.4180, 2.4188],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([2.9835, 3.0434, 3.1202, 3.1914, 3.2608, 3.3226, 3.3705, 3.4072, 3.4339,
3:         3.4502, 3.4469, 3.4319, 3.4008, 3.3579, 3.2966, 3.2331, 3.1596, 3.0702,
3:         3.1215, 3.1750, 3.2424, 3.3167, 3.3884, 3.4446, 3.4883],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6511, -0.6417, -0.6393, -0.6339, -0.6267, -0.6141, -0.6043, -0.5912,
3:         -0.5811, -0.5700, -0.5547, -0.5382, -0.5218, -0.5109, -0.5065, -0.5067,
3:         -0.5122, -0.5189, -0.6226, -0.6141, -0.6058, -0.5983, -0.5860, -0.5715,
3:         -0.5564], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.7023, -0.8083, -1.0640, -1.3473, -1.4979, -1.5628, -1.5755, -1.6347,
3:         -1.8098, -2.0179, -2.2539, -2.4000, -2.3168, -2.1433, -1.9156, -1.5765,
3:         -1.2261, -0.8896, -0.9043, -1.1115, -1.3738, -1.6892, -1.8803, -1.8994,
3:         -1.8205], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-1.3227, -1.2984, -1.2671, -1.2345, -1.1999, -1.1643, -1.1284, -1.0933,
3:         -1.0596, -1.0289, -1.0035, -0.9798, -0.9559, -0.9343, -0.9128, -0.8974,
3:         -0.8887, -0.8865, -0.8868, -0.8881, -0.8934, -0.9033, -0.9189, -0.9369,
3:         -0.9538], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1744, -0.1758, -0.1809, -0.1812, -0.1838, -0.1869, -0.1896, -0.1879,
3:         -0.1877, -0.1807, -0.1763, -0.1810, -0.1836, -0.1861, -0.1830, -0.1881,
3:         -0.1851, -0.1824, -0.1805, -0.1796, -0.1788, -0.1816, -0.1796, -0.1803,
3:         -0.1808], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 9, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 9, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4744, -0.4894, -0.4848, -0.4655, -0.4371, -0.4023, -0.3641, -0.3233,
1:         -0.2825, -0.2513, -0.2376, -0.2376, -0.2436, -0.2506, -0.2513, -0.2385,
1:         -0.2165, -0.1904, -0.3491, -0.3597, -0.3545, -0.3355, -0.3065, -0.2715,
1:         -0.2345], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([2.8394, 2.9011, 2.9441, 2.9689, 2.9799, 2.9779, 2.9607, 2.9322, 2.9027,
1:         2.8754, 2.8478, 2.8185, 2.7855, 2.7457, 2.7054, 2.6765, 2.6601, 2.6507,
1:         2.7951, 2.8589, 2.9041, 2.9316, 2.9437, 2.9425, 2.9275],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7020, -0.6992, -0.6953, -0.6920, -0.6883, -0.6867, -0.6861, -0.6872,
1:         -0.6892, -0.6920, -0.6952, -0.6983, -0.7010, -0.7026, -0.7045, -0.7062,
1:         -0.7081, -0.7096, -0.7018, -0.6985, -0.6940, -0.6891, -0.6844, -0.6821,
1:         -0.6803], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2043, -0.2541, -0.3194, -0.4953, -0.6846, -0.6846, -0.6215, -0.7145,
1:         -0.8252, -0.8229, -0.9192, -1.2092, -1.4461, -1.5567, -1.6696, -1.6242,
1:         -1.1992, -0.6492, -0.2828, -0.3183, -0.3603, -0.4776, -0.6115, -0.6558,
1:         -0.6946], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.4794, 0.5153, 0.5407, 0.5584, 0.5716, 0.5818, 0.5891, 0.5964, 0.6089,
1:         0.6294, 0.6558, 0.6874, 0.7261, 0.7722, 0.8230, 0.8817, 0.9530, 1.0321,
1:         1.1094, 1.1818, 1.2499, 1.3095, 1.3600, 1.4128, 1.4727],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1110,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2299,     nan,     nan,     nan,
1:             nan,     nan, -0.1063,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,
1:             nan, -0.1633, -0.1538,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,  0.1053,     nan,     nan, -0.0516,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2109,     nan,
1:         -0.1110,     nan,     nan, -0.2513,     nan,     nan, -0.2204,     nan,
1:             nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1800,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,
1:         -0.2466, -0.2156, -0.1800, -0.1514,     nan, -0.2489,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2394,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2489,     nan,     nan, -0.2442,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2014,
1:             nan,     nan,     nan,     nan, -0.2489, -0.2489, -0.2442,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1990,
1:             nan,     nan,     nan,     nan, -0.1610,     nan,     nan,     nan,
1:             nan,     nan, -0.1633,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0079,
1:             nan,     nan,     nan,     nan,     nan,     nan,  0.2551,  0.3526,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 9, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([2.3248, 2.4007, 2.4598, 2.4994, 2.5282, 2.5387, 2.5569, 2.5542, 2.5629,
1:         2.5739, 2.5792, 2.5835, 2.5757, 2.5656, 2.5375, 2.5162, 2.5044, 2.5013,
1:         2.2609, 2.3391, 2.3809, 2.3917, 2.3890, 2.3747, 2.3648],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([2.9241, 2.9806, 3.0491, 3.1174, 3.1839, 3.2319, 3.2713, 3.2997, 3.3235,
1:         3.3408, 3.3453, 3.3333, 3.3080, 3.2664, 3.2054, 3.1432, 3.0711, 2.9882,
1:         3.0431, 3.0925, 3.1566, 3.2276, 3.2992, 3.3534, 3.3885],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6500, -0.6375, -0.6272, -0.6125, -0.5998, -0.5848, -0.5758, -0.5657,
1:         -0.5564, -0.5484, -0.5359, -0.5242, -0.5115, -0.5052, -0.5021, -0.5051,
1:         -0.5105, -0.5178, -0.6261, -0.6149, -0.5989, -0.5814, -0.5652, -0.5505,
1:         -0.5350], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.8263, -1.0000, -1.1339, -1.2312, -1.3391, -1.4622, -1.4881, -1.5000,
1:         -1.6642, -1.8504, -2.0291, -2.1530, -1.9607, -1.7006, -1.5577, -1.4566,
1:         -1.4643, -1.4296, -0.8788, -1.1555, -1.3761, -1.5916, -1.7912, -1.8356,
1:         -1.7390], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-1.3004, -1.2883, -1.2633, -1.2334, -1.1976, -1.1591, -1.1229, -1.0919,
1:         -1.0650, -1.0422, -1.0221, -0.9997, -0.9724, -0.9471, -0.9234, -0.9086,
1:         -0.9017, -0.9020, -0.9044, -0.9078, -0.9144, -0.9257, -0.9426, -0.9613,
1:         -0.9796], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1767, -0.1794, -0.1847, -0.1820, -0.1819, -0.1830, -0.1814, -0.1827,
1:         -0.1814, -0.1813, -0.1778, -0.1796, -0.1812, -0.1833, -0.1807, -0.1838,
1:         -0.1823, -0.1783, -0.1743, -0.1755, -0.1743, -0.1753, -0.1730, -0.1732,
1:         -0.1754], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([-0.4744, -0.4894, -0.4848, -0.4655, -0.4371, -0.4023, -0.3641, -0.3233,
2:         -0.2825, -0.2513, -0.2376, -0.2376, -0.2436, -0.2506, -0.2513, -0.2385,
2:         -0.2165, -0.1904, -0.3491, -0.3597, -0.3545, -0.3355, -0.3065, -0.2715,
2:         -0.2345], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([2.8394, 2.9011, 2.9441, 2.9689, 2.9799, 2.9779, 2.9607, 2.9322, 2.9027,
2:         2.8754, 2.8478, 2.8185, 2.7855, 2.7457, 2.7054, 2.6765, 2.6601, 2.6507,
2:         2.7951, 2.8589, 2.9041, 2.9316, 2.9437, 2.9425, 2.9275],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7020, -0.6992, -0.6953, -0.6920, -0.6883, -0.6867, -0.6861, -0.6872,
2:         -0.6892, -0.6920, -0.6952, -0.6983, -0.7010, -0.7026, -0.7045, -0.7062,
2:         -0.7081, -0.7096, -0.7018, -0.6985, -0.6940, -0.6891, -0.6844, -0.6821,
2:         -0.6803], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2043, -0.2541, -0.3194, -0.4953, -0.6846, -0.6846, -0.6215, -0.7145,
2:         -0.8252, -0.8229, -0.9192, -1.2092, -1.4461, -1.5567, -1.6696, -1.6242,
2:         -1.1992, -0.6492, -0.2828, -0.3183, -0.3603, -0.4776, -0.6115, -0.6558,
2:         -0.6946], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.4794, 0.5153, 0.5407, 0.5584, 0.5716, 0.5818, 0.5891, 0.5964, 0.6089,
2:         0.6294, 0.6558, 0.6874, 0.7261, 0.7722, 0.8230, 0.8817, 0.9530, 1.0321,
2:         1.1094, 1.1818, 1.2499, 1.3095, 1.3600, 1.4128, 1.4727],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2038,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1990,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2489,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,
2:         -0.1871,     nan,     nan,     nan,     nan,     nan,  0.1053,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1134,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1657,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2370,
2:             nan,     nan,     nan,     nan,     nan, -0.2394,     nan,     nan,
2:             nan,     nan,     nan, -0.2513, -0.2489,     nan,     nan,     nan,
2:         -0.2370,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1562,     nan,     nan, -0.2489,     nan, -0.2299,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2204,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2299,     nan,
2:             nan,     nan, -0.1705,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.1467,     nan, -0.1610,     nan,
2:             nan, -0.1657,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.0397, -0.1182,     nan,     nan, -0.0849, -0.0730,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,  1.3204])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 9, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([2.3413, 2.4337, 2.5079, 2.5612, 2.6016, 2.6273, 2.6679, 2.6849, 2.7111,
2:         2.7284, 2.7324, 2.7287, 2.7182, 2.7064, 2.6834, 2.6665, 2.6535, 2.6505,
2:         2.2742, 2.3628, 2.4106, 2.4365, 2.4452, 2.4497, 2.4558],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([2.9027, 2.9726, 3.0493, 3.1194, 3.1880, 3.2489, 3.3041, 3.3547, 3.3975,
2:         3.4322, 3.4531, 3.4616, 3.4398, 3.3959, 3.3177, 3.2351, 3.1460, 3.0513,
2:         3.0206, 3.0897, 3.1654, 3.2407, 3.3216, 3.3921, 3.4502],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6530, -0.6447, -0.6399, -0.6323, -0.6238, -0.6114, -0.6019, -0.5904,
2:         -0.5759, -0.5636, -0.5486, -0.5344, -0.5200, -0.5108, -0.5049, -0.5057,
2:         -0.5097, -0.5193, -0.6243, -0.6167, -0.6080, -0.5964, -0.5857, -0.5719,
2:         -0.5578], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.5370, -0.7698, -1.0635, -1.3229, -1.4835, -1.6276, -1.6811, -1.6834,
2:         -1.7380, -1.8218, -2.0052, -2.1570, -2.0278, -1.7967, -1.6156, -1.4685,
2:         -1.4098, -1.3622, -0.9132, -1.1208, -1.3908, -1.6956, -1.9247, -2.0567,
2:         -2.0446], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-1.2717, -1.2626, -1.2436, -1.2197, -1.1893, -1.1556, -1.1218, -1.0894,
2:         -1.0609, -1.0370, -1.0158, -0.9966, -0.9758, -0.9544, -0.9336, -0.9162,
2:         -0.9032, -0.8967, -0.8946, -0.8965, -0.9043, -0.9161, -0.9297, -0.9424,
2:         -0.9517], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1610, -0.1653, -0.1709, -0.1729, -0.1742, -0.1764, -0.1692, -0.1655,
2:         -0.1578, -0.1666, -0.1653, -0.1719, -0.1793, -0.1786, -0.1728, -0.1763,
2:         -0.1663, -0.1579, -0.1628, -0.1686, -0.1670, -0.1695, -0.1709, -0.1676,
2:         -0.1658], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 9 [1/5 (20%)]	Loss: nan : nan :: 0.13587 (1.71 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 9 [2/5 (40%)]	Loss: nan : nan :: 0.13872 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 9 [3/5 (60%)]	Loss: nan : nan :: 0.13613 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 9 [4/5 (80%)]	Loss: nan : nan :: 0.13354 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch9.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch9.mod
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 9 : nan
0: validation loss for velocity_u : 0.04159511998295784
0: validation loss for velocity_v : 0.07642268389463425
0: validation loss for specific_humidity : 0.027864085510373116
0: validation loss for velocity_z : 0.5482432246208191
0: validation loss for temperature : 0.07020406424999237
0: validation loss for total_precip : nan
0: 10 : 23:21:47 :: batch_size = 96, lr = 1.6414931416261842e-05
3: 10 : 23:21:47 :: batch_size = 96, lr = 1.6414931416261842e-05
2: 10 : 23:21:47 :: batch_size = 96, lr = 1.6414931416261842e-05
1: 10 : 23:21:47 :: batch_size = 96, lr = 1.6414931416261842e-05
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 10, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 10, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 10, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.3211, -0.3231, -0.3153, -0.3044, -0.2946, -0.2955, -0.3078, -0.3313,
1:         -0.3626, -0.3929, -0.4208, -0.4342, -0.4271, -0.4004, -0.3635, -0.3399,
1:         -0.3330, -0.3357, -0.2656, -0.2645, -0.2554, -0.2422, -0.2300, -0.2284,
1:         -0.2396], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.3689, -0.3594, -0.3598, -0.3663, -0.3772, -0.3825, -0.3847, -0.3831,
1:         -0.3750, -0.3624, -0.3497, -0.3458, -0.3470, -0.3503, -0.3600, -0.3713,
1:         -0.3760, -0.3691, -0.3768, -0.3689, -0.3671, -0.3707, -0.3770, -0.3807,
1:         -0.3829], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.8565, 1.9181, 1.9402, 1.9308, 1.9130, 1.8893, 1.8686, 1.8004, 1.6221,
1:         1.3252, 0.8761, 0.3446, 0.1553, 0.0765, 0.3373, 0.7338, 1.1228, 1.2596,
1:         1.8806, 1.8988, 1.9206, 1.9238, 1.9078, 1.8997, 1.8833],
1:        device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-7.4828e-02, -6.8230e-02, -7.8127e-02, -7.8127e-02, -1.4350e-02,
1:         -3.3540e-03, -7.1529e-02, -1.2541e-01, -1.9139e-01, -1.1331e-01,
1:         -4.6238e-02, -5.5234e-05,  7.0319e-02,  1.6378e-01,  4.1449e-01,
1:          5.1346e-01,  2.7045e-01, -1.9848e-02, -1.2981e-01, -1.6170e-01,
1:         -1.5840e-01, -1.5290e-01, -5.9434e-02,  4.8327e-02,  8.7416e-03],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-1.5584, -1.5070, -1.4196, -1.3213, -1.2372, -1.1578, -1.0941, -1.0013,
1:         -0.8841, -0.7473, -0.5949, -0.4790, -0.4216, -0.4513, -0.5157, -0.5942,
1:         -0.6838, -0.7244, -0.7390, -0.7409, -0.7374, -0.7294, -0.7130, -0.7026,
1:         -0.6806], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 10, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2333,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2405,     nan,
1:             nan, -0.2058,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2321,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2297,     nan,     nan,     nan, -0.2441,     nan,     nan, -0.2357,
1:             nan,     nan,     nan,     nan,     nan, -0.2441,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.1291,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan, -0.2405,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2441,     nan,
1:             nan,     nan, -0.2201,     nan,     nan, -0.2309,     nan, -0.2441,
1:             nan,     nan,     nan,     nan,     nan, -0.2417,     nan, -0.2405,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2309,     nan,     nan,     nan,     nan,
1:             nan, -0.2429,     nan, -0.2381,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2441, -0.2441,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2010,     nan, -0.2441,     nan,     nan,     nan,
1:             nan,     nan, -0.2261,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2405, -0.2417,     nan,     nan,     nan,     nan,
1:             nan, -0.2429,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2357,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 10, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.0223, -0.0027, -0.0248, -0.0460, -0.0639, -0.0840, -0.1002, -0.1175,
1:         -0.1312, -0.1388, -0.1427, -0.1404, -0.1376, -0.1369, -0.1430, -0.1524,
1:         -0.1690, -0.1876,  0.0326,  0.0121, -0.0071, -0.0243, -0.0440, -0.0619,
1:         -0.0793], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6626, -0.6381, -0.6235, -0.6249, -0.6344, -0.6358, -0.6354, -0.6231,
1:         -0.6007, -0.5747, -0.5461, -0.5166, -0.4934, -0.4694, -0.4499, -0.4306,
1:         -0.4100, -0.3876, -0.6975, -0.6725, -0.6613, -0.6586, -0.6609, -0.6623,
1:         -0.6569], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.8719, 1.9491, 2.0592, 2.1714, 2.2522, 2.2933, 2.2650, 2.1793, 2.0204,
1:         1.8087, 1.5869, 1.3831, 1.2365, 1.1649, 1.1700, 1.2315, 1.3220, 1.4008,
1:         1.8551, 1.9295, 2.0363, 2.1461, 2.2451, 2.3021, 2.3110],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.5442, 0.5776, 0.5669, 0.5370, 0.4814, 0.4273, 0.3970, 0.3776, 0.3766,
1:         0.3759, 0.3597, 0.3524, 0.3259, 0.2568, 0.1598, 0.0878, 0.0545, 0.0101,
1:         0.5139, 0.5571, 0.5704, 0.5738, 0.5262, 0.4659, 0.4264],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.7300, -0.6847, -0.6461, -0.6235, -0.6055, -0.5792, -0.5363, -0.4746,
1:         -0.4030, -0.3426, -0.3086, -0.3074, -0.3316, -0.3746, -0.4300, -0.4933,
1:         -0.5591, -0.6107, -0.6217, -0.5703, -0.4465, -0.2700, -0.0815,  0.0753,
1:          0.1643], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1529, -0.1596, -0.1682, -0.1672, -0.1684, -0.1690, -0.1703, -0.1657,
1:         -0.1638, -0.1628, -0.1576, -0.1679, -0.1724, -0.1738, -0.1707, -0.1735,
1:         -0.1712, -0.1626, -0.1617, -0.1629, -0.1644, -0.1663, -0.1654, -0.1684,
1:         -0.1670], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([-0.3211, -0.3231, -0.3153, -0.3044, -0.2946, -0.2955, -0.3078, -0.3313,
2:         -0.3626, -0.3929, -0.4208, -0.4342, -0.4271, -0.4004, -0.3635, -0.3399,
2:         -0.3330, -0.3357, -0.2656, -0.2645, -0.2554, -0.2422, -0.2300, -0.2284,
2:         -0.2396], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.3689, -0.3594, -0.3598, -0.3663, -0.3772, -0.3825, -0.3847, -0.3831,
2:         -0.3750, -0.3624, -0.3497, -0.3458, -0.3470, -0.3503, -0.3600, -0.3713,
2:         -0.3760, -0.3691, -0.3768, -0.3689, -0.3671, -0.3707, -0.3770, -0.3807,
2:         -0.3829], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.8565, 1.9181, 1.9402, 1.9308, 1.9130, 1.8893, 1.8686, 1.8004, 1.6221,
2:         1.3252, 0.8761, 0.3446, 0.1553, 0.0765, 0.3373, 0.7338, 1.1228, 1.2596,
2:         1.8806, 1.8988, 1.9206, 1.9238, 1.9078, 1.8997, 1.8833],
2:        device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-7.4828e-02, -6.8230e-02, -7.8127e-02, -7.8127e-02, -1.4350e-02,
2:         -3.3540e-03, -7.1529e-02, -1.2541e-01, -1.9139e-01, -1.1331e-01,
2:         -4.6238e-02, -5.5234e-05,  7.0319e-02,  1.6378e-01,  4.1449e-01,
2:          5.1346e-01,  2.7045e-01, -1.9848e-02, -1.2981e-01, -1.6170e-01,
2:         -1.5840e-01, -1.5290e-01, -5.9434e-02,  4.8327e-02,  8.7416e-03],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-1.5584, -1.5070, -1.4196, -1.3213, -1.2372, -1.1578, -1.0941, -1.0013,
2:         -0.8841, -0.7473, -0.5949, -0.4790, -0.4216, -0.4513, -0.5157, -0.5942,
2:         -0.6838, -0.7244, -0.7390, -0.7409, -0.7374, -0.7294, -0.7130, -0.7026,
2:         -0.6806], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 10, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2105,     nan,     nan, -0.2273, -0.2022,
2:             nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2082, -0.2333,     nan, -0.2429,     nan,     nan,
2:             nan,     nan, -0.2369,     nan,     nan,     nan, -0.2333,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2261,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2441,     nan,
2:         -0.2082,     nan, -0.2177,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2441,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2393, -0.2237,     nan,     nan, -0.2441,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2369,     nan,     nan,     nan, -0.2441,
2:         -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.1351,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2082,     nan,     nan,     nan,     nan, -0.2441,     nan,
2:             nan,     nan,     nan,     nan, -0.2345,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2429,     nan, -0.2441,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2429,     nan,     nan, -0.2441,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2345,
2:             nan,     nan, -0.2441,     nan,     nan, -0.2381,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 10, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0120, -0.0016, -0.0192, -0.0401, -0.0638, -0.0895, -0.1146, -0.1354,
2:         -0.1502, -0.1571, -0.1563, -0.1493, -0.1409, -0.1364, -0.1388, -0.1484,
2:         -0.1639, -0.1793,  0.0349,  0.0240,  0.0085, -0.0108, -0.0380, -0.0656,
2:         -0.0917], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6182, -0.6079, -0.6031, -0.6054, -0.6071, -0.6031, -0.5957, -0.5811,
2:         -0.5639, -0.5444, -0.5242, -0.5090, -0.4935, -0.4785, -0.4615, -0.4476,
2:         -0.4299, -0.4105, -0.6556, -0.6466, -0.6453, -0.6429, -0.6415, -0.6362,
2:         -0.6281], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.9125, 1.9913, 2.0934, 2.2070, 2.2859, 2.3243, 2.2913, 2.1953, 2.0181,
2:         1.7874, 1.5448, 1.3222, 1.1652, 1.0834, 1.1002, 1.1826, 1.3069, 1.4228,
2:         1.8812, 1.9553, 2.0600, 2.1678, 2.2607, 2.3185, 2.3219],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.5659,  0.6639,  0.6766,  0.6637,  0.6167,  0.5399,  0.4736,  0.4268,
2:          0.4195,  0.4466,  0.4654,  0.4522,  0.3573,  0.2063,  0.0590, -0.0288,
2:         -0.0410,  0.0122,  0.5550,  0.6803,  0.7369,  0.7361,  0.6810,  0.5946,
2:          0.5144], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.7115, -0.6574, -0.6228, -0.6130, -0.6140, -0.6021, -0.5652, -0.4995,
2:         -0.4174, -0.3451, -0.3043, -0.3009, -0.3275, -0.3693, -0.4169, -0.4644,
2:         -0.5124, -0.5501, -0.5566, -0.5089, -0.3932, -0.2239, -0.0403,  0.1118,
2:          0.1985], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1583, -0.1635, -0.1691, -0.1682, -0.1720, -0.1720, -0.1713, -0.1640,
2:         -0.1611, -0.1671, -0.1644, -0.1702, -0.1720, -0.1747, -0.1716, -0.1714,
2:         -0.1683, -0.1590, -0.1675, -0.1692, -0.1677, -0.1677, -0.1682, -0.1660,
2:         -0.1647], device='cuda:2', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([-0.3211, -0.3231, -0.3153, -0.3044, -0.2946, -0.2955, -0.3078, -0.3313,
3:         -0.3626, -0.3929, -0.4208, -0.4342, -0.4271, -0.4004, -0.3635, -0.3399,
3:         -0.3330, -0.3357, -0.2656, -0.2645, -0.2554, -0.2422, -0.2300, -0.2284,
3:         -0.2396], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.3689, -0.3594, -0.3598, -0.3663, -0.3772, -0.3825, -0.3847, -0.3831,
3:         -0.3750, -0.3624, -0.3497, -0.3458, -0.3470, -0.3503, -0.3600, -0.3713,
3:         -0.3760, -0.3691, -0.3768, -0.3689, -0.3671, -0.3707, -0.3770, -0.3807,
3:         -0.3829], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.8565, 1.9181, 1.9402, 1.9308, 1.9130, 1.8893, 1.8686, 1.8004, 1.6221,
3:         1.3252, 0.8761, 0.3446, 0.1553, 0.0765, 0.3373, 0.7338, 1.1228, 1.2596,
3:         1.8806, 1.8988, 1.9206, 1.9238, 1.9078, 1.8997, 1.8833],
3:        device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-7.4828e-02, -6.8230e-02, -7.8127e-02, -7.8127e-02, -1.4350e-02,
3:         -3.3540e-03, -7.1529e-02, -1.2541e-01, -1.9139e-01, -1.1331e-01,
3:         -4.6238e-02, -5.5234e-05,  7.0319e-02,  1.6378e-01,  4.1449e-01,
3:          5.1346e-01,  2.7045e-01, -1.9848e-02, -1.2981e-01, -1.6170e-01,
3:         -1.5840e-01, -1.5290e-01, -5.9434e-02,  4.8327e-02,  8.7416e-03],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-1.5584, -1.5070, -1.4196, -1.3213, -1.2372, -1.1578, -1.0941, -1.0013,
3:         -0.8841, -0.7473, -0.5949, -0.4790, -0.4216, -0.4513, -0.5157, -0.5942,
3:         -0.6838, -0.7244, -0.7390, -0.7409, -0.7374, -0.7294, -0.7130, -0.7026,
3:         -0.6806], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 10, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan, -0.2357,     nan,     nan,     nan,     nan,     nan, -0.2321,
3:         -0.2333,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2213,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2297,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2441,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2429,
3:             nan, -0.2213,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1051,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2261,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2285,     nan,     nan, -0.2441,     nan,
3:             nan, -0.2381,     nan,     nan, -0.2237, -0.2309,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2441,     nan,     nan,
3:             nan,     nan, -0.2417,     nan,     nan, -0.2441,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2441,     nan,     nan, -0.2441,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2441,     nan,     nan,     nan,
3:         -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2441,     nan,     nan, -0.2333,     nan,     nan,     nan, -0.2393,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2345,
3:         -0.2381,     nan, -0.2429,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2369,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 10, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0294,  0.0083, -0.0159, -0.0361, -0.0583, -0.0835, -0.1059, -0.1285,
3:         -0.1439, -0.1538, -0.1562, -0.1534, -0.1467, -0.1421, -0.1416, -0.1476,
3:         -0.1631, -0.1831,  0.0462,  0.0282,  0.0105, -0.0073, -0.0305, -0.0559,
3:         -0.0799], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6377, -0.6243, -0.6167, -0.6164, -0.6197, -0.6180, -0.6125, -0.5997,
3:         -0.5807, -0.5600, -0.5360, -0.5108, -0.4847, -0.4574, -0.4325, -0.4121,
3:         -0.3950, -0.3787, -0.6704, -0.6579, -0.6536, -0.6505, -0.6497, -0.6485,
3:         -0.6400], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.7830, 1.9138, 2.0606, 2.1952, 2.2797, 2.3227, 2.2866, 2.1961, 2.0311,
3:         1.8090, 1.5655, 1.3410, 1.1702, 1.0817, 1.0843, 1.1558, 1.2601, 1.3562,
3:         1.7844, 1.9000, 2.0388, 2.1624, 2.2580, 2.3090, 2.3180],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.6007,  0.6907,  0.7146,  0.6861,  0.6396,  0.5537,  0.4539,  0.3918,
3:          0.3706,  0.3800,  0.3952,  0.3953,  0.3419,  0.2449,  0.1497,  0.0650,
3:          0.0088, -0.0157,  0.5519,  0.6363,  0.6885,  0.7014,  0.6816,  0.5939,
3:          0.4868], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.7937, -0.7273, -0.6740, -0.6425, -0.6219, -0.5949, -0.5511, -0.4882,
3:         -0.4162, -0.3575, -0.3262, -0.3248, -0.3436, -0.3738, -0.4120, -0.4635,
3:         -0.5253, -0.5845, -0.6084, -0.5656, -0.4412, -0.2538, -0.0514,  0.1129,
3:          0.2037], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1563, -0.1624, -0.1705, -0.1716, -0.1724, -0.1740, -0.1753, -0.1673,
3:         -0.1670, -0.1609, -0.1589, -0.1671, -0.1718, -0.1755, -0.1708, -0.1753,
3:         -0.1717, -0.1611, -0.1568, -0.1580, -0.1602, -0.1643, -0.1675, -0.1666,
3:         -0.1658], device='cuda:3', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 10, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3211, -0.3231, -0.3153, -0.3044, -0.2946, -0.2955, -0.3078, -0.3313,
0:         -0.3626, -0.3929, -0.4208, -0.4342, -0.4271, -0.4004, -0.3635, -0.3399,
0:         -0.3330, -0.3357, -0.2656, -0.2645, -0.2554, -0.2422, -0.2300, -0.2284,
0:         -0.2396], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3689, -0.3594, -0.3598, -0.3663, -0.3772, -0.3825, -0.3847, -0.3831,
0:         -0.3750, -0.3624, -0.3497, -0.3458, -0.3470, -0.3503, -0.3600, -0.3713,
0:         -0.3760, -0.3691, -0.3768, -0.3689, -0.3671, -0.3707, -0.3770, -0.3807,
0:         -0.3829], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.8565, 1.9181, 1.9402, 1.9308, 1.9130, 1.8893, 1.8686, 1.8004, 1.6221,
0:         1.3252, 0.8761, 0.3446, 0.1553, 0.0765, 0.3373, 0.7338, 1.1228, 1.2596,
0:         1.8806, 1.8988, 1.9206, 1.9238, 1.9078, 1.8997, 1.8833],
0:        device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-7.4828e-02, -6.8230e-02, -7.8127e-02, -7.8127e-02, -1.4350e-02,
0:         -3.3540e-03, -7.1529e-02, -1.2541e-01, -1.9139e-01, -1.1331e-01,
0:         -4.6238e-02, -5.5234e-05,  7.0319e-02,  1.6378e-01,  4.1449e-01,
0:          5.1346e-01,  2.7045e-01, -1.9848e-02, -1.2981e-01, -1.6170e-01,
0:         -1.5840e-01, -1.5290e-01, -5.9434e-02,  4.8327e-02,  8.7416e-03],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.5584, -1.5070, -1.4196, -1.3213, -1.2372, -1.1578, -1.0941, -1.0013,
0:         -0.8841, -0.7473, -0.5949, -0.4790, -0.4216, -0.4513, -0.5157, -0.5942,
0:         -0.6838, -0.7244, -0.7390, -0.7409, -0.7374, -0.7294, -0.7130, -0.7026,
0:         -0.6806], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 10, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2441,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2213,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2333,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1554, -0.0500, -0.1051,     nan, -0.2441,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2261,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2309,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2405,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2309,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2441,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2441,     nan,     nan,     nan,
0:             nan, -0.2369,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2441,     nan,     nan, -0.2405,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 10, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0183, -0.0012, -0.0202, -0.0401, -0.0613, -0.0846, -0.1063, -0.1255,
0:         -0.1372, -0.1443, -0.1465, -0.1474, -0.1494, -0.1519, -0.1596, -0.1698,
0:         -0.1854, -0.2004,  0.0359,  0.0188,  0.0012, -0.0167, -0.0414, -0.0658,
0:         -0.0864], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6341, -0.6207, -0.6153, -0.6167, -0.6236, -0.6292, -0.6299, -0.6195,
0:         -0.5996, -0.5740, -0.5451, -0.5198, -0.4984, -0.4781, -0.4578, -0.4418,
0:         -0.4257, -0.4123, -0.6603, -0.6473, -0.6403, -0.6406, -0.6433, -0.6470,
0:         -0.6428], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.9820, 2.0416, 2.1238, 2.2162, 2.2836, 2.3149, 2.2782, 2.1838, 2.0192,
0:         1.8015, 1.5824, 1.3832, 1.2462, 1.1755, 1.1729, 1.2253, 1.3041, 1.3833,
0:         1.9553, 2.0057, 2.0915, 2.1893, 2.2870, 2.3427, 2.3491],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6099, 0.6692, 0.6908, 0.6640, 0.5953, 0.4996, 0.4223, 0.3919, 0.3901,
0:         0.4050, 0.4373, 0.4339, 0.3535, 0.2658, 0.1803, 0.0961, 0.0552, 0.0362,
0:         0.5676, 0.6452, 0.7002, 0.6961, 0.6295, 0.5260, 0.4351],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.7181, -0.6771, -0.6479, -0.6324, -0.6195, -0.5935, -0.5450, -0.4720,
0:         -0.3888, -0.3201, -0.2836, -0.2857, -0.3181, -0.3702, -0.4305, -0.4955,
0:         -0.5597, -0.6108, -0.6249, -0.5746, -0.4503, -0.2664, -0.0675,  0.0974,
0:          0.1924], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1524, -0.1590, -0.1652, -0.1664, -0.1722, -0.1738, -0.1720, -0.1647,
0:         -0.1630, -0.1610, -0.1590, -0.1668, -0.1711, -0.1758, -0.1709, -0.1755,
0:         -0.1729, -0.1626, -0.1583, -0.1586, -0.1630, -0.1637, -0.1667, -0.1689,
0:         -0.1693], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 10 [1/5 (20%)]	Loss: nan : nan :: 0.14330 (1.59 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 10 [2/5 (40%)]	Loss: nan : nan :: 0.14118 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 10 [3/5 (60%)]	Loss: nan : nan :: 0.13554 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 10 [4/5 (80%)]	Loss: nan : nan :: 0.14411 (10.42 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch10.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch10.mod
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 10 : nan
0: validation loss for velocity_u : 0.03349858149886131
0: validation loss for velocity_v : 0.061558566987514496
0: validation loss for specific_humidity : 0.02550867386162281
0: validation loss for velocity_z : 0.47609689831733704
0: validation loss for temperature : 0.08355420082807541
0: validation loss for total_precip : nan
3: 11 : 23:28:33 :: batch_size = 96, lr = 1.601456723537741e-05
0: 11 : 23:28:33 :: batch_size = 96, lr = 1.601456723537741e-05
2: 11 : 23:28:33 :: batch_size = 96, lr = 1.601456723537741e-05
1: 11 : 23:28:33 :: batch_size = 96, lr = 1.601456723537741e-05
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 11, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 11, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] INPUT BATCH
0: Epoch 11, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 11, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5854, -0.5873, -0.5800, -0.5634, -0.5383, -0.5066, -0.4702, -0.4298,
0:         -0.3862, -0.3403, -0.2941, -0.2500, -0.2103, -0.1769, -0.1515, -0.1352,
0:         -0.1289, -0.1325, -0.5937, -0.6057, -0.6075, -0.5982, -0.5784, -0.5496,
0:         -0.5143], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6771,  0.6647,  0.6373,  0.5938,  0.5346,  0.4605,  0.3717,  0.2695,
0:          0.1555,  0.0333, -0.0933, -0.2208, -0.3473, -0.4722, -0.5951, -0.7132,
0:         -0.8222, -0.9163,  0.7041,  0.6934,  0.6690,  0.6316,  0.5814,  0.5185,
0:          0.4424], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6225, -0.6055, -0.5885, -0.5611, -0.5286, -0.4973, -0.4689, -0.4453,
0:         -0.4339, -0.4467, -0.4596, -0.4658, -0.4713, -0.4767, -0.4656, -0.4546,
0:         -0.4292, -0.3341, -0.6500, -0.6332, -0.6163, -0.5919, -0.5642, -0.5364,
0:         -0.4929], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8433, -1.0269, -1.1531, -1.1641, -1.0269, -0.7681, -0.4406, -0.1042,
0:          0.1967,  0.4357,  0.6017,  0.6924,  0.7123,  0.6880,  0.6393,  0.5840,
0:          0.5198,  0.4401, -0.7216, -0.9252, -1.0557, -1.0734, -0.9738, -0.7946,
0:         -0.5755], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.0607, 1.1025, 1.1442, 1.1812, 1.2108, 1.2317, 1.2424, 1.2423, 1.2309,
0:         1.2088, 1.1761, 1.1335, 1.0830, 1.0262, 0.9659, 0.9058, 0.8491, 0.7976,
0:         0.7508, 0.7055, 0.6591, 0.6098, 0.5590, 0.5113, 0.4711],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
3:      first 25 values: tensor([-0.5854, -0.5873, -0.5800, -0.5634, -0.5383, -0.5066, -0.4702, -0.4298,
3:         -0.3862, -0.3403, -0.2941, -0.2500, -0.2103, -0.1769, -0.1515, -0.1352,
3:         -0.1289, -0.1325, -0.5937, -0.6057, -0.6075, -0.5982, -0.5784, -0.5496,
3:         -0.5143], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.6771,  0.6647,  0.6373,  0.5938,  0.5346,  0.4605,  0.3717,  0.2695,
3:          0.1555,  0.0333, -0.0933, -0.2208, -0.3473, -0.4722, -0.5951, -0.7132,
3:         -0.8222, -0.9163,  0.7041,  0.6934,  0.6690,  0.6316,  0.5814,  0.5185,
3:          0.4424], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.6225, -0.6055, -0.5885, -0.5611, -0.5286, -0.4973, -0.4689, -0.4453,
3:         -0.4339, -0.4467, -0.4596, -0.4658, -0.4713, -0.4767, -0.4656, -0.4546,
3:         -0.4292, -0.3341, -0.6500, -0.6332, -0.6163, -0.5919, -0.5642, -0.5364,
3:         -0.4929], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.8433, -1.0269, -1.1531, -1.1641, -1.0269, -0.7681, -0.4406, -0.1042,
3:          0.1967,  0.4357,  0.6017,  0.6924,  0.7123,  0.6880,  0.6393,  0.5840,
3:          0.5198,  0.4401, -0.7216, -0.9252, -1.0557, -1.0734, -0.9738, -0.7946,
3:         -0.5755], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0: [DEBUG] TARGET BATCH
0: Epoch 11, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3:      first 25 values: tensor([1.0607, 1.1025, 1.1442, 1.1812, 1.2108, 1.2317, 1.2424, 1.2423, 1.2309,
3:         1.2088, 1.1761, 1.1335, 1.0830, 1.0262, 0.9659, 0.9058, 0.8491, 0.7976,
3:         0.7508, 0.7055, 0.6591, 0.6098, 0.5590, 0.5113, 0.4711],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 11, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan, -0.2438,
0:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2371,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2416,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,
0:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2394,     nan, -0.2438,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
0:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2394,     nan,     nan,     nan,
3:         -0.2169,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2349,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2438, -0.2438,     nan,     nan,     nan,
3:         -0.2438,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2427,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,
3:             nan,     nan,     nan,     nan, -0.2438, -0.2438, -0.2438,     nan,
3:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 11, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:             nan,     nan, -0.2438])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 11, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0024, 0.0201, 0.0422, 0.0656, 0.0895, 0.1100, 0.1287, 0.1396, 0.1450,
0:         0.1474, 0.1453, 0.1383, 0.1336, 0.1305, 0.1287, 0.1332, 0.1439, 0.1576,
0:         0.0200, 0.0421, 0.0674, 0.0942, 0.1169, 0.1354, 0.1532],
3:      first 25 pred values: tensor([-0.0080,  0.0129,  0.0356,  0.0585,  0.0804,  0.0990,  0.1160,  0.1274,
3:          0.1332,  0.1353,  0.1327,  0.1253,  0.1179,  0.1108,  0.1070,  0.1092,
3:          0.1178,  0.1319,  0.0129,  0.0374,  0.0639,  0.0885,  0.1112,  0.1291,
3:          0.1468], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.3426,  0.3094,  0.2782,  0.2476,  0.2159,  0.1839,  0.1514,  0.1190,
3:          0.0870,  0.0561,  0.0234, -0.0074, -0.0391, -0.0711, -0.1017, -0.1266,
3:         -0.1481, -0.1641,  0.3697,  0.3329,  0.2979,  0.2652,  0.2376,  0.2073,
3:          0.1772], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3552,  0.3238,  0.2936,  0.2619,  0.2308,  0.1966,  0.1608,  0.1229,
0:          0.0872,  0.0531,  0.0217, -0.0077, -0.0354, -0.0657, -0.0953, -0.1205,
0:         -0.1425, -0.1621,  0.3703,  0.3358,  0.3028,  0.2739,  0.2477,  0.2189,
0:          0.1894], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0011, -0.0268, -0.0571, -0.0776, -0.0965, -0.1040, -0.1052, -0.0994,
3:         -0.0961, -0.0947, -0.0951, -0.0983, -0.0987, -0.1022, -0.1050, -0.1059,
3:         -0.1065, -0.1004,  0.0604,  0.0353,  0.0101, -0.0185, -0.0434, -0.0617,
3:         -0.0688], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0112, -0.0237, -0.0350, -0.0439, -0.0582, -0.0666, -0.0710, -0.0673,
0:         -0.0653, -0.0597, -0.0604, -0.0567, -0.0555, -0.0582, -0.0599, -0.0570,
0:         -0.0511, -0.0437,  0.0379,  0.0213,  0.0024, -0.0198, -0.0447, -0.0639,
0:         -0.0725], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.4247, 0.5273, 0.5893, 0.6172, 0.6205, 0.6240, 0.6046, 0.5396, 0.4908,
3:         0.4385, 0.3647, 0.3164, 0.2832, 0.2231, 0.1830, 0.1992, 0.2519, 0.2724,
3:         0.4335, 0.5040, 0.5408, 0.5727, 0.5632, 0.5436, 0.5285],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4760, 0.5849, 0.6481, 0.7037, 0.7195, 0.6743, 0.5759, 0.4419, 0.3435,
0:         0.2578, 0.1611, 0.1236, 0.1162, 0.0885, 0.1056, 0.1648, 0.2285, 0.2795,
0:         0.3996, 0.4800, 0.5413, 0.6404, 0.6958, 0.6856, 0.6411],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([ 1.6671e-02,  2.2721e-02,  2.9334e-02,  3.2309e-02,  3.1286e-02,
3:          2.5547e-02,  1.4686e-02, -3.4883e-04, -1.5610e-02, -3.2896e-02,
3:         -5.1671e-02, -7.2907e-02, -9.6593e-02, -1.2394e-01, -1.5082e-01,
3:         -1.7912e-01, -2.0543e-01, -2.3144e-01, -2.5792e-01, -2.8477e-01,
3:         -3.0984e-01, -3.3210e-01, -3.5073e-01, -3.6542e-01, -3.8167e-01],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0061,  0.0034,  0.0115,  0.0140,  0.0120,  0.0062, -0.0032, -0.0141,
0:         -0.0260, -0.0389, -0.0534, -0.0704, -0.0911, -0.1166, -0.1449, -0.1743,
0:         -0.2035, -0.2325, -0.2614, -0.2898, -0.3165, -0.3384, -0.3566, -0.3735,
0:         -0.3920], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1631, -0.1649, -0.1728, -0.1685, -0.1702, -0.1676, -0.1671, -0.1603,
3:         -0.1607, -0.1692, -0.1674, -0.1712, -0.1700, -0.1714, -0.1669, -0.1700,
3:         -0.1682, -0.1607, -0.1685, -0.1682, -0.1677, -0.1673, -0.1659, -0.1656,
3:         -0.1665], device='cuda:3', grad_fn=<SliceBackward0>)
0:      first 25 pred values: tensor([-0.1735, -0.1751, -0.1786, -0.1760, -0.1730, -0.1717, -0.1717, -0.1637,
0:         -0.1662, -0.1779, -0.1736, -0.1762, -0.1757, -0.1729, -0.1657, -0.1704,
0:         -0.1665, -0.1629, -0.1739, -0.1735, -0.1706, -0.1683, -0.1649, -0.1623,
0:         -0.1615], device='cuda:0', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([-0.5854, -0.5873, -0.5800, -0.5634, -0.5383, -0.5066, -0.4702, -0.4298,
2:         -0.3862, -0.3403, -0.2941, -0.2500, -0.2103, -0.1769, -0.1515, -0.1352,
2:         -0.1289, -0.1325, -0.5937, -0.6057, -0.6075, -0.5982, -0.5784, -0.5496,
2:         -0.5143], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.6771,  0.6647,  0.6373,  0.5938,  0.5346,  0.4605,  0.3717,  0.2695,
2:          0.1555,  0.0333, -0.0933, -0.2208, -0.3473, -0.4722, -0.5951, -0.7132,
2:         -0.8222, -0.9163,  0.7041,  0.6934,  0.6690,  0.6316,  0.5814,  0.5185,
2:          0.4424], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.6225, -0.6055, -0.5885, -0.5611, -0.5286, -0.4973, -0.4689, -0.4453,
2:         -0.4339, -0.4467, -0.4596, -0.4658, -0.4713, -0.4767, -0.4656, -0.4546,
2:         -0.4292, -0.3341, -0.6500, -0.6332, -0.6163, -0.5919, -0.5642, -0.5364,
2:         -0.4929], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.8433, -1.0269, -1.1531, -1.1641, -1.0269, -0.7681, -0.4406, -0.1042,
2:          0.1967,  0.4357,  0.6017,  0.6924,  0.7123,  0.6880,  0.6393,  0.5840,
2:          0.5198,  0.4401, -0.7216, -0.9252, -1.0557, -1.0734, -0.9738, -0.7946,
2:         -0.5755], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([1.0607, 1.1025, 1.1442, 1.1812, 1.2108, 1.2317, 1.2424, 1.2423, 1.2309,
2:         1.2088, 1.1761, 1.1335, 1.0830, 1.0262, 0.9659, 0.9058, 0.8491, 0.7976,
2:         0.7508, 0.7055, 0.6591, 0.6098, 0.5590, 0.5113, 0.4711],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 11, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2259,     nan, -0.2438,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2371,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
2:             nan, -0.2394,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,
2:         -0.2438,     nan,     nan,     nan, -0.2438,     nan,     nan, -0.2438,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
2:         -0.2405,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2438, -0.2438,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2438])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 11, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0235, -0.0055,  0.0187,  0.0462,  0.0727,  0.0930,  0.1096,  0.1185,
2:          0.1219,  0.1233,  0.1211,  0.1178,  0.1153,  0.1139,  0.1149,  0.1218,
2:          0.1327,  0.1470, -0.0004,  0.0217,  0.0482,  0.0765,  0.1036,  0.1240,
2:          0.1410], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.3635,  0.3341,  0.3052,  0.2748,  0.2417,  0.2061,  0.1663,  0.1300,
2:          0.0958,  0.0629,  0.0323,  0.0033, -0.0250, -0.0531, -0.0819, -0.1090,
2:         -0.1365, -0.1640,  0.3833,  0.3511,  0.3172,  0.2884,  0.2594,  0.2269,
2:          0.1912], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0310,  0.0072, -0.0175, -0.0379, -0.0622, -0.0730, -0.0812, -0.0737,
2:         -0.0689, -0.0600, -0.0549, -0.0455, -0.0367, -0.0311, -0.0262, -0.0232,
2:         -0.0239, -0.0205,  0.0749,  0.0575,  0.0341,  0.0014, -0.0267, -0.0533,
2:         -0.0673], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.4662, 0.5066, 0.5385, 0.5401, 0.5261, 0.5164, 0.4741, 0.3926, 0.3293,
2:         0.2793, 0.2196, 0.1859, 0.1631, 0.1187, 0.1041, 0.1360, 0.1911, 0.2093,
2:         0.3886, 0.4030, 0.4241, 0.4433, 0.4395, 0.4306, 0.4074],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([ 0.0073,  0.0111,  0.0148,  0.0154,  0.0142,  0.0107,  0.0051, -0.0024,
2:         -0.0131, -0.0280, -0.0475, -0.0714, -0.0972, -0.1235, -0.1484, -0.1723,
2:         -0.1963, -0.2213, -0.2494, -0.2775, -0.3025, -0.3222, -0.3376, -0.3512,
2:         -0.3688], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1514, -0.1540, -0.1568, -0.1528, -0.1569, -0.1618, -0.1621, -0.1563,
2:         -0.1557, -0.1600, -0.1557, -0.1601, -0.1595, -0.1624, -0.1568, -0.1632,
2:         -0.1626, -0.1543, -0.1569, -0.1549, -0.1545, -0.1527, -0.1535, -0.1544,
2:         -0.1563], device='cuda:2', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-0.5854, -0.5873, -0.5800, -0.5634, -0.5383, -0.5066, -0.4702, -0.4298,
1:         -0.3862, -0.3403, -0.2941, -0.2500, -0.2103, -0.1769, -0.1515, -0.1352,
1:         -0.1289, -0.1325, -0.5937, -0.6057, -0.6075, -0.5982, -0.5784, -0.5496,
1:         -0.5143], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.6771,  0.6647,  0.6373,  0.5938,  0.5346,  0.4605,  0.3717,  0.2695,
1:          0.1555,  0.0333, -0.0933, -0.2208, -0.3473, -0.4722, -0.5951, -0.7132,
1:         -0.8222, -0.9163,  0.7041,  0.6934,  0.6690,  0.6316,  0.5814,  0.5185,
1:          0.4424], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.6225, -0.6055, -0.5885, -0.5611, -0.5286, -0.4973, -0.4689, -0.4453,
1:         -0.4339, -0.4467, -0.4596, -0.4658, -0.4713, -0.4767, -0.4656, -0.4546,
1:         -0.4292, -0.3341, -0.6500, -0.6332, -0.6163, -0.5919, -0.5642, -0.5364,
1:         -0.4929], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.8433, -1.0269, -1.1531, -1.1641, -1.0269, -0.7681, -0.4406, -0.1042,
1:          0.1967,  0.4357,  0.6017,  0.6924,  0.7123,  0.6880,  0.6393,  0.5840,
1:          0.5198,  0.4401, -0.7216, -0.9252, -1.0557, -1.0734, -0.9738, -0.7946,
1:         -0.5755], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([1.0607, 1.1025, 1.1442, 1.1812, 1.2108, 1.2317, 1.2424, 1.2423, 1.2309,
1:         1.2088, 1.1761, 1.1335, 1.0830, 1.0262, 0.9659, 0.9058, 0.8491, 0.7976,
1:         0.7508, 0.7055, 0.6591, 0.6098, 0.5590, 0.5113, 0.4711],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 11, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan, -0.1900, -0.1766,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2371,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2304,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
1:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
1:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2438, -0.2438,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2438, -0.2438,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2438,     nan,     nan,     nan, -0.2438,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2416,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan, -0.2438,
1:         -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 11, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.0028, 0.0155, 0.0344, 0.0591, 0.0848, 0.1063, 0.1245, 0.1328, 0.1351,
1:         0.1343, 0.1302, 0.1246, 0.1219, 0.1217, 0.1212, 0.1265, 0.1324, 0.1379,
1:         0.0237, 0.0382, 0.0597, 0.0859, 0.1110, 0.1320, 0.1519],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.3572,  0.3251,  0.2928,  0.2606,  0.2261,  0.1929,  0.1544,  0.1153,
1:          0.0784,  0.0423,  0.0085, -0.0211, -0.0490, -0.0763, -0.1031, -0.1278,
1:         -0.1509, -0.1731,  0.3705,  0.3323,  0.2943,  0.2615,  0.2336,  0.2024,
1:          0.1716], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.0390, -0.0571, -0.0686, -0.0714, -0.0766, -0.0745, -0.0728, -0.0644,
1:         -0.0555, -0.0477, -0.0409, -0.0330, -0.0262, -0.0247, -0.0228, -0.0223,
1:         -0.0187, -0.0111,  0.0223,  0.0039, -0.0079, -0.0232, -0.0369, -0.0505,
1:         -0.0586], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.3753, 0.4434, 0.5047, 0.5391, 0.5217, 0.4819, 0.4024, 0.2903, 0.2293,
1:         0.2110, 0.1961, 0.1916, 0.1840, 0.1297, 0.0864, 0.1081, 0.1350, 0.1338,
1:         0.3563, 0.3991, 0.4438, 0.5065, 0.5033, 0.4560, 0.3952],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([ 0.0139,  0.0215,  0.0278,  0.0285,  0.0242,  0.0159,  0.0062, -0.0049,
1:         -0.0151, -0.0285, -0.0470, -0.0705, -0.0972, -0.1260, -0.1528, -0.1786,
1:         -0.2013, -0.2230, -0.2457, -0.2697, -0.2927, -0.3122, -0.3303, -0.3494,
1:         -0.3728], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1649, -0.1674, -0.1729, -0.1696, -0.1715, -0.1710, -0.1729, -0.1700,
1:         -0.1676, -0.1671, -0.1654, -0.1685, -0.1714, -0.1731, -0.1716, -0.1725,
1:         -0.1715, -0.1649, -0.1653, -0.1666, -0.1641, -0.1668, -0.1648, -0.1665,
1:         -0.1653], device='cuda:1', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 11 [1/5 (20%)]	Loss: nan : nan :: 0.14116 (1.61 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 11 [2/5 (40%)]	Loss: nan : nan :: 0.14632 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 11 [3/5 (60%)]	Loss: nan : nan :: 0.13633 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 11 [4/5 (80%)]	Loss: nan : nan :: 0.14329 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch11.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch11.mod
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 11 : nan
0: validation loss for velocity_u : 0.036461591720581055
0: validation loss for velocity_v : 0.06819206476211548
0: validation loss for specific_humidity : 0.026185354217886925
0: validation loss for velocity_z : 0.48749810457229614
0: validation loss for temperature : 0.08050397038459778
0: validation loss for total_precip : nan
2: 12 : 23:35:23 :: batch_size = 96, lr = 1.5623968034514547e-05
0: 12 : 23:35:23 :: batch_size = 96, lr = 1.5623968034514547e-05
3: 12 : 23:35:23 :: batch_size = 96, lr = 1.5623968034514547e-05
1: 12 : 23:35:23 :: batch_size = 96, lr = 1.5623968034514547e-05
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 12, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 12, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] INPUT BATCH
0: Epoch 12, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.4959, 1.4608, 1.4204, 1.3753, 1.3285, 1.2827, 1.2387, 1.1986, 1.1607,
1:         1.1185, 1.0735, 1.0292, 0.9786, 0.9146, 0.8426, 0.7765, 0.7401, 0.7546,
1:         1.5435, 1.5154, 1.4811, 1.4402, 1.3974, 1.3552, 1.3128],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.0347, 1.0203, 1.0105, 1.0084, 1.0143, 1.0271, 1.0466, 1.0727, 1.1033,
1:         1.1422, 1.1946, 1.2528, 1.3078, 1.3616, 1.4128, 1.4518, 1.4690, 1.4508,
1:         1.0241, 1.0014, 0.9825, 0.9693, 0.9640, 0.9648, 0.9710],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.6539, -0.6605, -0.6659, -0.6711, -0.6755, -0.6792, -0.6831, -0.6875,
1:         -0.6920, -0.6970, -0.7007, -0.7036, -0.7058, -0.7113, -0.7082, -0.6966,
1:         -0.6213, -0.5213, -0.6493, -0.6565, -0.6636, -0.6693, -0.6741, -0.6790,
1:         -0.6833], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.3305,  0.3090,  0.2993,  0.3176,  0.4179,  0.6045,  0.7847,  0.9314,
1:          1.0543,  0.9950,  0.6692,  0.3068,  0.0328, -0.2972, -0.5669, -0.5442,
1:         -0.4202, -0.3026,  0.2809,  0.2734,  0.2540,  0.2583,  0.3381,  0.5236,
1:          0.7264], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.0585, 0.0760, 0.0930, 0.1115, 0.1324, 0.1566, 0.1866, 0.2236, 0.2663,
1:         0.3123, 0.3562, 0.3923, 0.4180, 0.4319, 0.4292, 0.4055, 0.3639, 0.3229,
1:         0.3192, 0.3785, 0.4769, 0.5589, 0.6042, 0.6409, 0.6949],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 12, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan, -0.1447,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2270, -0.2181, -0.1981,
1:             nan, -0.0847,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2270,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1759,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1603,
1:             nan,     nan,     nan,     nan,     nan, -0.2381,     nan, -0.2137,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2292,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2337,     nan,
1:         -0.2025,     nan,     nan,     nan, -0.2181,     nan,     nan,     nan,
1:         -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.0535,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1581,     nan,     nan, -0.0202,     nan,     nan,
1:             nan,     nan, -0.2137,     nan,     nan, -0.1736,     nan, -0.1825,
1:             nan, -0.1425,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2315,     nan, -0.1848,     nan,     nan,     nan, -0.1670, -0.2181,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 12, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.4457, 1.4154, 1.3864, 1.3463, 1.2969, 1.2382, 1.1746, 1.0995, 1.0180,
1:         0.9310, 0.8419, 0.7547, 0.6833, 0.6290, 0.5931, 0.5694, 0.5576, 0.5488,
1:         1.5765, 1.5580, 1.5352, 1.5038, 1.4573, 1.4015, 1.3370],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.3423, -0.2993, -0.2434, -0.1799, -0.1074, -0.0159,  0.0859,  0.2042,
1:          0.3265,  0.4451,  0.5570,  0.6588,  0.7494,  0.8315,  0.9013,  0.9617,
1:          1.0073,  1.0495, -0.4226, -0.3861, -0.3396, -0.2845, -0.2175, -0.1385,
1:         -0.0441], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 values: tensor([1.4959, 1.4608, 1.4204, 1.3753, 1.3285, 1.2827, 1.2387, 1.1986, 1.1607,
3:         1.1185, 1.0735, 1.0292, 0.9786, 0.9146, 0.8426, 0.7765, 0.7401, 0.7546,
3:         1.5435, 1.5154, 1.4811, 1.4402, 1.3974, 1.3552, 1.3128],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 pred values: tensor([-0.6434, -0.6465, -0.6511, -0.6541, -0.6591, -0.6609, -0.6618, -0.6578,
1:         -0.6497, -0.6385, -0.6236, -0.6061, -0.5875, -0.5689, -0.5560, -0.5435,
1:         -0.5357, -0.5304, -0.6266, -0.6283, -0.6321, -0.6379, -0.6458, -0.6533,
1:         -0.6564], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 values: tensor([1.0347, 1.0203, 1.0105, 1.0084, 1.0143, 1.0271, 1.0466, 1.0727, 1.1033,
3:         1.1422, 1.1946, 1.2528, 1.3078, 1.3616, 1.4128, 1.4518, 1.4690, 1.4508,
3:         1.0241, 1.0014, 0.9825, 0.9693, 0.9640, 0.9648, 0.9710],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 pred values: tensor([ 0.0640, -0.0515, -0.1268, -0.2083, -0.2340, -0.2844, -0.4626, -0.5929,
1:         -0.5799, -0.5444, -0.4613, -0.3025, -0.2822, -0.3737, -0.3088, -0.2138,
1:         -0.2338, -0.2640,  0.0810,  0.0221,  0.0615,  0.0163, -0.0259, -0.0793,
1:         -0.2990], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 values: tensor([-0.6539, -0.6605, -0.6659, -0.6711, -0.6755, -0.6792, -0.6831, -0.6875,
3:         -0.6920, -0.6970, -0.7007, -0.7036, -0.7058, -0.7113, -0.7082, -0.6966,
3:         -0.6213, -0.5213, -0.6493, -0.6565, -0.6636, -0.6693, -0.6741, -0.6790,
3:         -0.6833], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 pred values: tensor([-1.4822, -1.5115, -1.5380, -1.5648, -1.5864, -1.6097, -1.6361, -1.6674,
1:         -1.7022, -1.7366, -1.7656, -1.7836, -1.7855, -1.7711, -1.7448, -1.7143,
1:         -1.6889, -1.6726, -1.6616, -1.6516, -1.6417, -1.6363, -1.6415, -1.6596,
1:         -1.6892], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 values: tensor([ 0.3305,  0.3090,  0.2993,  0.3176,  0.4179,  0.6045,  0.7847,  0.9314,
3:          1.0543,  0.9950,  0.6692,  0.3068,  0.0328, -0.2972, -0.5669, -0.5442,
3:         -0.4202, -0.3026,  0.2809,  0.2734,  0.2540,  0.2583,  0.3381,  0.5236,
3:          0.7264], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 pred values: tensor([-0.1691, -0.1715, -0.1786, -0.1786, -0.1805, -0.1779, -0.1808, -0.1753,
1:         -0.1719, -0.1778, -0.1730, -0.1776, -0.1801, -0.1816, -0.1766, -0.1809,
1:         -0.1772, -0.1712, -0.1769, -0.1788, -0.1761, -0.1809, -0.1762, -0.1760,
1:         -0.1776], device='cuda:1', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([0.0585, 0.0760, 0.0930, 0.1115, 0.1324, 0.1566, 0.1866, 0.2236, 0.2663,
3:         0.3123, 0.3562, 0.3923, 0.4180, 0.4319, 0.4292, 0.4055, 0.3639, 0.3229,
3:         0.3192, 0.3785, 0.4769, 0.5589, 0.6042, 0.6409, 0.6949],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 12, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([-0.2159,     nan,     nan,     nan,  0.0710,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1625,     nan,     nan,     nan,     nan,     nan,
3:             nan,  0.2289,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2337,     nan,     nan,     nan,
3:         -0.1381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2248,     nan,     nan,     nan,     nan,  0.2956,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.0891,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2270, -0.2292,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  0.0932,     nan,     nan, -0.0958,     nan,     nan,  0.3645,
3:             nan,     nan,     nan,     nan,     nan, -0.1002,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2092,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2159,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2337,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:          0.0532,     nan,     nan,     nan,     nan,     nan,     nan, -0.0535,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.0914,  0.1510,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2226,     nan,     nan,     nan,     nan,     nan, -0.1825,
3:         -0.1358,     nan,     nan, -0.2070, -0.2114,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2292,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.0869,     nan,     nan, -0.2181,
3:             nan,     nan, -0.2270])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 12, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.4198, 1.3986, 1.3809, 1.3537, 1.3143, 1.2594, 1.1972, 1.1173, 1.0318,
3:         0.9416, 0.8488, 0.7536, 0.6697, 0.5977, 0.5419, 0.5036, 0.4841, 0.4776,
3:         1.5627, 1.5516, 1.5360, 1.5117, 1.4752, 1.4249, 1.3604],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.3198, -0.2777, -0.2276, -0.1722, -0.1072, -0.0197,  0.0827,  0.2087,
3:          0.3454,  0.4837,  0.6119,  0.7258,  0.8186,  0.8946,  0.9500,  0.9941,
3:          1.0259,  1.0550, -0.3907, -0.3586, -0.3170, -0.2709, -0.2135, -0.1402,
3:         -0.0465], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6355, -0.6366, -0.6410, -0.6446, -0.6508, -0.6552, -0.6588, -0.6543,
3:         -0.6447, -0.6314, -0.6138, -0.5942, -0.5739, -0.5573, -0.5465, -0.5389,
3:         -0.5353, -0.5318, -0.6269, -0.6250, -0.6264, -0.6305, -0.6382, -0.6480,
3:         -0.6529], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.1290, -0.0353, -0.1526, -0.1951, -0.2176, -0.2893, -0.4066, -0.5222,
3:         -0.5841, -0.5937, -0.5328, -0.3747, -0.3239, -0.3965, -0.3019, -0.1724,
3:         -0.1826, -0.1850,  0.1314,  0.0529,  0.0105, -0.0298, -0.0542, -0.1396,
3:         -0.3112], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-1.5586, -1.5696, -1.5818, -1.5966, -1.6075, -1.6184, -1.6347, -1.6614,
3:         -1.6968, -1.7338, -1.7641, -1.7776, -1.7715, -1.7483, -1.7146, -1.6817,
3:         -1.6581, -1.6468, -1.6422, -1.6395, -1.6365, -1.6372, -1.6481, -1.6721,
3:         -1.7069], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1745, -0.1771, -0.1830, -0.1805, -0.1815, -0.1791, -0.1821, -0.1780,
3:         -0.1768, -0.1789, -0.1740, -0.1792, -0.1796, -0.1801, -0.1759, -0.1814,
3:         -0.1779, -0.1738, -0.1761, -0.1738, -0.1718, -0.1749, -0.1742, -0.1717,
3:         -0.1734], device='cuda:3', grad_fn=<SliceBackward0>)
0:      first 25 values: tensor([1.4959, 1.4608, 1.4204, 1.3753, 1.3285, 1.2827, 1.2387, 1.1986, 1.1607,
0:         1.1185, 1.0735, 1.0292, 0.9786, 0.9146, 0.8426, 0.7765, 0.7401, 0.7546,
0:         1.5435, 1.5154, 1.4811, 1.4402, 1.3974, 1.3552, 1.3128],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0347, 1.0203, 1.0105, 1.0084, 1.0143, 1.0271, 1.0466, 1.0727, 1.1033,
0:         1.1422, 1.1946, 1.2528, 1.3078, 1.3616, 1.4128, 1.4518, 1.4690, 1.4508,
0:         1.0241, 1.0014, 0.9825, 0.9693, 0.9640, 0.9648, 0.9710],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6539, -0.6605, -0.6659, -0.6711, -0.6755, -0.6792, -0.6831, -0.6875,
0:         -0.6920, -0.6970, -0.7007, -0.7036, -0.7058, -0.7113, -0.7082, -0.6966,
0:         -0.6213, -0.5213, -0.6493, -0.6565, -0.6636, -0.6693, -0.6741, -0.6790,
0:         -0.6833], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3305,  0.3090,  0.2993,  0.3176,  0.4179,  0.6045,  0.7847,  0.9314,
0:          1.0543,  0.9950,  0.6692,  0.3068,  0.0328, -0.2972, -0.5669, -0.5442,
0:         -0.4202, -0.3026,  0.2809,  0.2734,  0.2540,  0.2583,  0.3381,  0.5236,
0:          0.7264], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.0585, 0.0760, 0.0930, 0.1115, 0.1324, 0.1566, 0.1866, 0.2236, 0.2663,
0:         0.3123, 0.3562, 0.3923, 0.4180, 0.4319, 0.4292, 0.4055, 0.3639, 0.3229,
0:         0.3192, 0.3785, 0.4769, 0.5589, 0.6042, 0.6409, 0.6949],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 12, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.1447,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1870,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1336,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2315,     nan, -0.1825,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2270,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2270,     nan, -0.0513,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.1355,     nan, -0.2070,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2270,     nan,
0:             nan, -0.1647,     nan,     nan,     nan,  0.1221,     nan,     nan,
0:             nan,     nan, -0.2137,     nan, -0.1670,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2270,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1781,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1981,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1492,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1536, -0.1581,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2070,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1136,     nan,     nan,     nan, -0.2292,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2270,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2159,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 12, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.3854, 1.3710, 1.3486, 1.3129, 1.2625, 1.2070, 1.1496, 1.0835, 1.0162,
0:         0.9390, 0.8536, 0.7606, 0.6813, 0.6135, 0.5650, 0.5329, 0.5192, 0.5094,
0:         1.5279, 1.5193, 1.5012, 1.4717, 1.4261, 1.3707, 1.3118],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3556, -0.3092, -0.2504, -0.1839, -0.1100, -0.0209,  0.0799,  0.1991,
0:          0.3267,  0.4518,  0.5712,  0.6818,  0.7785,  0.8650,  0.9355,  0.9962,
0:          1.0377,  1.0732, -0.4251, -0.3898, -0.3429, -0.2869, -0.2196, -0.1397,
0:         -0.0450], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6388, -0.6472, -0.6568, -0.6633, -0.6724, -0.6781, -0.6815, -0.6787,
0:         -0.6696, -0.6550, -0.6331, -0.6106, -0.5879, -0.5681, -0.5548, -0.5428,
0:         -0.5324, -0.5232, -0.6221, -0.6274, -0.6348, -0.6426, -0.6511, -0.6596,
0:         -0.6674], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0713, -0.1438, -0.1860, -0.1665, -0.1815, -0.2924, -0.4205, -0.5186,
0:         -0.5988, -0.6179, -0.5141, -0.3298, -0.2871, -0.3534, -0.2315, -0.0955,
0:         -0.1253, -0.1487, -0.0653, -0.0459,  0.0114,  0.0366, -0.0133, -0.1498,
0:         -0.3326], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.5132, -1.5254, -1.5442, -1.5676, -1.5870, -1.6053, -1.6265, -1.6569,
0:         -1.6942, -1.7325, -1.7628, -1.7765, -1.7702, -1.7511, -1.7225, -1.6967,
0:         -1.6773, -1.6665, -1.6595, -1.6514, -1.6429, -1.6373, -1.6429, -1.6621,
0:         -1.6906], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1675, -0.1701, -0.1733, -0.1735, -0.1733, -0.1718, -0.1730, -0.1688,
0:         -0.1669, -0.1697, -0.1643, -0.1706, -0.1713, -0.1734, -0.1669, -0.1735,
0:         -0.1703, -0.1655, -0.1656, -0.1682, -0.1638, -0.1655, -0.1681, -0.1641,
0:         -0.1675], device='cuda:0', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 12, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.4959, 1.4608, 1.4204, 1.3753, 1.3285, 1.2827, 1.2387, 1.1986, 1.1607,
2:         1.1185, 1.0735, 1.0292, 0.9786, 0.9146, 0.8426, 0.7765, 0.7401, 0.7546,
2:         1.5435, 1.5154, 1.4811, 1.4402, 1.3974, 1.3552, 1.3128],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.0347, 1.0203, 1.0105, 1.0084, 1.0143, 1.0271, 1.0466, 1.0727, 1.1033,
2:         1.1422, 1.1946, 1.2528, 1.3078, 1.3616, 1.4128, 1.4518, 1.4690, 1.4508,
2:         1.0241, 1.0014, 0.9825, 0.9693, 0.9640, 0.9648, 0.9710],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.6539, -0.6605, -0.6659, -0.6711, -0.6755, -0.6792, -0.6831, -0.6875,
2:         -0.6920, -0.6970, -0.7007, -0.7036, -0.7058, -0.7113, -0.7082, -0.6966,
2:         -0.6213, -0.5213, -0.6493, -0.6565, -0.6636, -0.6693, -0.6741, -0.6790,
2:         -0.6833], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.3305,  0.3090,  0.2993,  0.3176,  0.4179,  0.6045,  0.7847,  0.9314,
2:          1.0543,  0.9950,  0.6692,  0.3068,  0.0328, -0.2972, -0.5669, -0.5442,
2:         -0.4202, -0.3026,  0.2809,  0.2734,  0.2540,  0.2583,  0.3381,  0.5236,
2:          0.7264], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.0585, 0.0760, 0.0930, 0.1115, 0.1324, 0.1566, 0.1866, 0.2236, 0.2663,
2:         0.3123, 0.3562, 0.3923, 0.4180, 0.4319, 0.4292, 0.4055, 0.3639, 0.3229,
2:         0.3192, 0.3785, 0.4769, 0.5589, 0.6042, 0.6409, 0.6949],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 12, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0176,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1047,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2248,     nan,     nan,     nan,     nan,     nan, -0.2203,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:          0.2066,     nan, -0.1759,     nan,     nan, -0.0891,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1603,
2:             nan, -0.1314,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.1959,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1469,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2270,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0535,
2:             nan,     nan,     nan,     nan,     nan, -0.1269,     nan,     nan,
2:             nan, -0.1692, -0.2048,     nan,     nan,     nan,     nan,     nan,
2:         -0.1936,     nan,     nan, -0.2048,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1736,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2270,     nan,     nan,     nan,
2:             nan,     nan, -0.0914, -0.1314,     nan,     nan,     nan, -0.2270,
2:             nan,     nan,     nan,     nan,     nan, -0.1247,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 12, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.3954, 1.3689, 1.3404, 1.2992, 1.2513, 1.1950, 1.1385, 1.0680, 0.9941,
2:         0.9118, 0.8241, 0.7345, 0.6550, 0.5928, 0.5455, 0.5177, 0.5001, 0.4847,
2:         1.5452, 1.5371, 1.5205, 1.4879, 1.4439, 1.3874, 1.3242],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.3228, -0.2808, -0.2248, -0.1615, -0.0917, -0.0056,  0.0938,  0.2121,
2:          0.3417,  0.4744,  0.6002,  0.7111,  0.8069,  0.8866,  0.9536,  1.0094,
2:          1.0506,  1.0824, -0.3981, -0.3655, -0.3173, -0.2616, -0.1966, -0.1214,
2:         -0.0327], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6341, -0.6353, -0.6408, -0.6462, -0.6539, -0.6601, -0.6639, -0.6625,
2:         -0.6549, -0.6434, -0.6243, -0.6054, -0.5855, -0.5692, -0.5586, -0.5490,
2:         -0.5420, -0.5377, -0.6262, -0.6244, -0.6284, -0.6346, -0.6443, -0.6538,
2:         -0.6608], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0902, -0.0067, -0.0766, -0.1297, -0.1911, -0.2866, -0.4585, -0.5609,
2:         -0.5413, -0.5464, -0.4851, -0.3272, -0.3210, -0.3735, -0.2561, -0.1770,
2:         -0.2332, -0.2532,  0.0975,  0.0386,  0.0527,  0.0336, -0.0330, -0.1309,
2:         -0.3509], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-1.5474, -1.5545, -1.5699, -1.5938, -1.6163, -1.6383, -1.6611, -1.6894,
2:         -1.7212, -1.7529, -1.7784, -1.7916, -1.7881, -1.7713, -1.7451, -1.7182,
2:         -1.6952, -1.6785, -1.6672, -1.6582, -1.6525, -1.6510, -1.6585, -1.6743,
2:         -1.6968], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1693, -0.1730, -0.1773, -0.1733, -0.1733, -0.1763, -0.1760, -0.1731,
2:         -0.1726, -0.1750, -0.1717, -0.1739, -0.1748, -0.1762, -0.1742, -0.1776,
2:         -0.1744, -0.1675, -0.1702, -0.1718, -0.1696, -0.1721, -0.1691, -0.1700,
2:         -0.1713], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 12 [1/5 (20%)]	Loss: nan : nan :: 0.15972 (1.75 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 12 [2/5 (40%)]	Loss: nan : nan :: 0.13215 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 12 [3/5 (60%)]	Loss: nan : nan :: 0.14467 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 12 [4/5 (80%)]	Loss: nan : nan :: 0.15552 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch12.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch12.mod
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 12 : nan
0: validation loss for velocity_u : 0.03948700800538063
0: validation loss for velocity_v : 0.0689389780163765
0: validation loss for specific_humidity : 0.024966750293970108
0: validation loss for velocity_z : 0.520601749420166
0: validation loss for temperature : 0.07014208287000656
0: validation loss for total_precip : nan
3: 13 : 23:42:07 :: batch_size = 96, lr = 1.5242895643428828e-05
0: 13 : 23:42:07 :: batch_size = 96, lr = 1.5242895643428828e-05
1: 13 : 23:42:07 :: batch_size = 96, lr = 1.5242895643428828e-05
2: 13 : 23:42:07 :: batch_size = 96, lr = 1.5242895643428828e-05
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 13, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-2.5299, -2.5480, -2.5642, -2.5784, -2.5912, -2.6023, -2.6118, -2.6199,
3:         -2.6261, -2.6305, -2.6328, -2.6330, -2.6305, -2.6254, -2.6181, -2.6082,
3:         -2.5966, -2.5833, -2.4801, -2.4990, -2.5165, -2.5326, -2.5471, -2.5604,
3:         -2.5720], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.2082, 1.1702, 1.1318, 1.0934, 1.0546, 1.0155, 0.9763, 0.9368, 0.8971,
3:         0.8576, 0.8179, 0.7782, 0.7383, 0.6984, 0.6583, 0.6182, 0.5780, 0.5379,
3:         1.2252, 1.1898, 1.1542, 1.1186, 1.0826, 1.0464, 1.0099],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7784, -0.7783, -0.7782, -0.7781, -0.7780, -0.7779, -0.7778, -0.7777,
3:         -0.7775, -0.7774, -0.7772, -0.7771, -0.7770, -0.7769, -0.7768, -0.7767,
3:         -0.7766, -0.7766, -0.7784, -0.7784, -0.7783, -0.7782, -0.7781, -0.7781,
3:         -0.7780], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.3743, 1.4633, 1.5534, 1.6491, 1.7514, 1.8605, 1.9762, 2.0907, 2.2009,
3:         2.2943, 2.3611, 2.3911, 2.3755, 2.3077, 2.1898, 2.0251, 1.8260, 1.6079,
3:         1.3298, 1.4021, 1.4822, 1.5779, 1.6902, 1.8171, 1.9517],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.6570, -0.6732, -0.6900, -0.7072, -0.7242, -0.7415, -0.7589, -0.7756,
3:         -0.7923, -0.8082, -0.8230, -0.8370, -0.8493, -0.8600, -0.8688, -0.8756,
3:         -0.8806, -0.8835, -0.8848, -0.8848, -0.8831, -0.8806, -0.8769, -0.8723,
3:         -0.8676], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 13, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2432,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2421,     nan,     nan, -0.2421,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2373,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2397,     nan,
3:             nan,     nan,     nan, -0.2385,     nan, -0.2385,     nan, -0.2432,
3:             nan, -0.2432,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2468,     nan,     nan, -0.2456,     nan,     nan,     nan,     nan,
3:         -0.2432,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2397,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2397,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2373,     nan,     nan, -0.2385,     nan, -0.2397,     nan,     nan,
3:             nan, -0.2362,     nan,     nan,     nan, -0.2385,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2444,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2421, -0.2421,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2397,     nan,     nan,     nan, -0.2409, -0.2409,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2362,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2338,     nan,
3:         -0.2350,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2362, -0.2421,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 13, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.4741, -1.4609, -1.4484, -1.4343, -1.4174, -1.4084, -1.3967, -1.3895,
3:         -1.3803, -1.3660, -1.3460, -1.3174, -1.2869, -1.2590, -1.2379, -1.2181,
3:         -1.2028, -1.1882, -1.4953, -1.4843, -1.4752, -1.4638, -1.4518, -1.4421,
3:         -1.4307], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.4973, -1.5100, -1.5202, -1.5276, -1.5333, -1.5350, -1.5364, -1.5353,
3:         -1.5350, -1.5330, -1.5353, -1.5380, -1.5426, -1.5503, -1.5564, -1.5615,
3:         -1.5649, -1.5693, -1.4832, -1.4926, -1.5003, -1.5084, -1.5137, -1.5161,
3:         -1.5178], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.7655, -0.7662, -0.7664, -0.7645, -0.7639, -0.7623, -0.7630, -0.7630,
3:         -0.7641, -0.7632, -0.7613, -0.7583, -0.7564, -0.7543, -0.7530, -0.7520,
3:         -0.7511, -0.7510, -0.7668, -0.7694, -0.7693, -0.7666, -0.7648, -0.7624,
3:         -0.7604], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0278,  0.1219,  0.1617,  0.2173,  0.3071,  0.3762,  0.4159,  0.4244,
3:          0.4396,  0.4270,  0.3615,  0.3096,  0.2562,  0.1596,  0.0635,  0.0066,
3:         -0.0304, -0.0650,  0.0071,  0.0795,  0.1078,  0.1604,  0.2436,  0.3134,
3:          0.3763], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-1.0832, -1.0776, -1.0744, -1.0751, -1.0771, -1.0791, -1.0823, -1.0863,
3:         -1.0916, -1.0964, -1.0988, -1.0974, -1.0910, -1.0798, -1.0652, -1.0473,
3:         -1.0262, -1.0035, -0.9801, -0.9567, -0.9333, -0.9073, -0.8787, -0.8477,
3:         -0.8149], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1518, -0.1519, -0.1565, -0.1570, -0.1587, -0.1671, -0.1744, -0.1739,
3:         -0.1755, -0.1591, -0.1531, -0.1566, -0.1576, -0.1631, -0.1631, -0.1695,
3:         -0.1738, -0.1691, -0.1589, -0.1598, -0.1574, -0.1569, -0.1549, -0.1573,
3:         -0.1626], device='cuda:3', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 13, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-2.5299, -2.5480, -2.5642, -2.5784, -2.5912, -2.6023, -2.6118, -2.6199,
2:         -2.6261, -2.6305, -2.6328, -2.6330, -2.6305, -2.6254, -2.6181, -2.6082,
2:         -2.5966, -2.5833, -2.4801, -2.4990, -2.5165, -2.5326, -2.5471, -2.5604,
2:         -2.5720], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.2082, 1.1702, 1.1318, 1.0934, 1.0546, 1.0155, 0.9763, 0.9368, 0.8971,
2:         0.8576, 0.8179, 0.7782, 0.7383, 0.6984, 0.6583, 0.6182, 0.5780, 0.5379,
2:         1.2252, 1.1898, 1.1542, 1.1186, 1.0826, 1.0464, 1.0099],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7784, -0.7783, -0.7782, -0.7781, -0.7780, -0.7779, -0.7778, -0.7777,
2:         -0.7775, -0.7774, -0.7772, -0.7771, -0.7770, -0.7769, -0.7768, -0.7767,
2:         -0.7766, -0.7766, -0.7784, -0.7784, -0.7783, -0.7782, -0.7781, -0.7781,
2:         -0.7780], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.3743, 1.4633, 1.5534, 1.6491, 1.7514, 1.8605, 1.9762, 2.0907, 2.2009,
2:         2.2943, 2.3611, 2.3911, 2.3755, 2.3077, 2.1898, 2.0251, 1.8260, 1.6079,
2:         1.3298, 1.4021, 1.4822, 1.5779, 1.6902, 1.8171, 1.9517],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.6570, -0.6732, -0.6900, -0.7072, -0.7242, -0.7415, -0.7589, -0.7756,
2:         -0.7923, -0.8082, -0.8230, -0.8370, -0.8493, -0.8600, -0.8688, -0.8756,
2:         -0.8806, -0.8835, -0.8848, -0.8848, -0.8831, -0.8806, -0.8769, -0.8723,
2:         -0.8676], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 13, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2421,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2421, -0.2421,     nan, -0.2421,     nan,     nan,
2:             nan,     nan,     nan, -0.2421,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2385,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2444,
2:             nan, -0.2421,     nan, -0.2409, -0.2409,     nan,     nan,     nan,
2:             nan,     nan, -0.2409, -0.2409,     nan, -0.2397,     nan,     nan,
2:         -0.2397, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2385,     nan,
2:         -0.2373,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2373,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2444,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2397,
2:             nan,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,
2:             nan,     nan, -0.2385,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2373,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2338,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2373,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 13, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.4574, -1.4481, -1.4372, -1.4255, -1.4101, -1.3990, -1.3857, -1.3739,
2:         -1.3609, -1.3409, -1.3164, -1.2921, -1.2676, -1.2445, -1.2231, -1.1999,
2:         -1.1793, -1.1579, -1.4891, -1.4811, -1.4739, -1.4633, -1.4526, -1.4401,
2:         -1.4250], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.5300, -1.5374, -1.5433, -1.5454, -1.5466, -1.5477, -1.5447, -1.5398,
2:         -1.5353, -1.5332, -1.5330, -1.5371, -1.5420, -1.5486, -1.5523, -1.5568,
2:         -1.5631, -1.5761, -1.5295, -1.5363, -1.5396, -1.5382, -1.5353, -1.5315,
2:         -1.5284], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.7677, -0.7656, -0.7646, -0.7607, -0.7596, -0.7573, -0.7570, -0.7541,
2:         -0.7542, -0.7532, -0.7529, -0.7521, -0.7534, -0.7538, -0.7543, -0.7571,
2:         -0.7584, -0.7610, -0.7643, -0.7644, -0.7629, -0.7602, -0.7592, -0.7580,
2:         -0.7552], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0719,  0.1621,  0.2010,  0.2696,  0.3517,  0.4067,  0.4536,  0.4503,
2:          0.4494,  0.4322,  0.3544,  0.2930,  0.2245,  0.1174,  0.0146, -0.0611,
2:         -0.1194, -0.1755,  0.0661,  0.1168,  0.1249,  0.1777,  0.2741,  0.3415,
2:          0.3963], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-1.0537, -1.0507, -1.0501, -1.0535, -1.0574, -1.0607, -1.0624, -1.0635,
2:         -1.0660, -1.0688, -1.0715, -1.0705, -1.0643, -1.0547, -1.0426, -1.0300,
2:         -1.0164, -0.9995, -0.9787, -0.9551, -0.9295, -0.9019, -0.8724, -0.8424,
2:         -0.8128], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1264, -0.1336, -0.1411, -0.1477, -0.1521, -0.1592, -0.1588, -0.1520,
2:         -0.1520, -0.1362, -0.1315, -0.1384, -0.1431, -0.1497, -0.1480, -0.1532,
2:         -0.1524, -0.1441, -0.1308, -0.1337, -0.1332, -0.1344, -0.1365, -0.1381,
2:         -0.1463], device='cuda:2', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 13, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 13, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.5299, -2.5480, -2.5642, -2.5784, -2.5912, -2.6023, -2.6118, -2.6199,
0:         -2.6261, -2.6305, -2.6328, -2.6330, -2.6305, -2.6254, -2.6181, -2.6082,
0:         -2.5966, -2.5833, -2.4801, -2.4990, -2.5165, -2.5326, -2.5471, -2.5604,
0:         -2.5720], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2082, 1.1702, 1.1318, 1.0934, 1.0546, 1.0155, 0.9763, 0.9368, 0.8971,
0:         0.8576, 0.8179, 0.7782, 0.7383, 0.6984, 0.6583, 0.6182, 0.5780, 0.5379,
0:         1.2252, 1.1898, 1.1542, 1.1186, 1.0826, 1.0464, 1.0099],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7784, -0.7783, -0.7782, -0.7781, -0.7780, -0.7779, -0.7778, -0.7777,
0:         -0.7775, -0.7774, -0.7772, -0.7771, -0.7770, -0.7769, -0.7768, -0.7767,
0:         -0.7766, -0.7766, -0.7784, -0.7784, -0.7783, -0.7782, -0.7781, -0.7781,
0:         -0.7780], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3743, 1.4633, 1.5534, 1.6491, 1.7514, 1.8605, 1.9762, 2.0907, 2.2009,
0:         2.2943, 2.3611, 2.3911, 2.3755, 2.3077, 2.1898, 2.0251, 1.8260, 1.6079,
0:         1.3298, 1.4021, 1.4822, 1.5779, 1.6902, 1.8171, 1.9517],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.6570, -0.6732, -0.6900, -0.7072, -0.7242, -0.7415, -0.7589, -0.7756,
0:         -0.7923, -0.8082, -0.8230, -0.8370, -0.8493, -0.8600, -0.8688, -0.8756,
0:         -0.8806, -0.8835, -0.8848, -0.8848, -0.8831, -0.8806, -0.8769, -0.8723,
0:         -0.8676], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 13, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2432,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2409,     nan, -0.2409, -0.2397,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2397,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2385,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2421,     nan,     nan,
0:             nan,     nan,     nan, -0.2456, -0.2456,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2385,     nan,     nan,     nan, -0.2397, -0.2385,     nan,     nan,
0:             nan,     nan,     nan, -0.2385,     nan,     nan,     nan,     nan,
0:         -0.2362,     nan,     nan, -0.2373,     nan,     nan,     nan,     nan,
0:         -0.2373,     nan,     nan,     nan,     nan,     nan, -0.2373,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2397,
0:             nan,     nan,     nan, -0.2444,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2432,     nan,     nan,     nan,     nan,
0:             nan, -0.2421, -0.2421,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2397, -0.2397, -0.2409,     nan, -0.2397,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2338,     nan, -0.2338,     nan,     nan,
0:         -0.2338,     nan,     nan, -0.2338,     nan,     nan,     nan,     nan,
0:             nan, -0.2385,     nan,     nan, -0.2385,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 13, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.4652, -1.4535, -1.4436, -1.4318, -1.4172, -1.4080, -1.3984, -1.3900,
0:         -1.3798, -1.3613, -1.3381, -1.3140, -1.2875, -1.2638, -1.2444, -1.2222,
0:         -1.2006, -1.1777, -1.4867, -1.4803, -1.4728, -1.4605, -1.4504, -1.4402,
0:         -1.4284], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.5136, -1.5296, -1.5429, -1.5508, -1.5557, -1.5552, -1.5564, -1.5510,
0:         -1.5435, -1.5388, -1.5353, -1.5371, -1.5412, -1.5439, -1.5418, -1.5402,
0:         -1.5401, -1.5484, -1.5057, -1.5174, -1.5286, -1.5323, -1.5321, -1.5312,
0:         -1.5309], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7590, -0.7580, -0.7575, -0.7560, -0.7558, -0.7552, -0.7568, -0.7561,
0:         -0.7585, -0.7587, -0.7582, -0.7559, -0.7551, -0.7538, -0.7549, -0.7561,
0:         -0.7600, -0.7622, -0.7595, -0.7598, -0.7599, -0.7591, -0.7591, -0.7581,
0:         -0.7570], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1146,  0.1590,  0.2072,  0.2984,  0.4055,  0.4480,  0.4693,  0.4962,
0:          0.5186,  0.5092,  0.4487,  0.3768,  0.3009,  0.2050,  0.1048,  0.0259,
0:         -0.0416, -0.0966,  0.1406,  0.1738,  0.1985,  0.2415,  0.3211,  0.3673,
0:          0.4012], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.0883, -1.0825, -1.0764, -1.0739, -1.0730, -1.0735, -1.0736, -1.0730,
0:         -1.0733, -1.0751, -1.0774, -1.0763, -1.0719, -1.0640, -1.0532, -1.0414,
0:         -1.0288, -1.0155, -1.0000, -0.9822, -0.9619, -0.9385, -0.9131, -0.8867,
0:         -0.8594], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1496, -0.1494, -0.1559, -0.1598, -0.1587, -0.1639, -0.1672, -0.1669,
0:         -0.1703, -0.1600, -0.1529, -0.1566, -0.1547, -0.1591, -0.1562, -0.1611,
0:         -0.1619, -0.1571, -0.1605, -0.1584, -0.1532, -0.1513, -0.1496, -0.1497,
0:         -0.1520], device='cuda:0', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-2.5299, -2.5480, -2.5642, -2.5784, -2.5912, -2.6023, -2.6118, -2.6199,
1:         -2.6261, -2.6305, -2.6328, -2.6330, -2.6305, -2.6254, -2.6181, -2.6082,
1:         -2.5966, -2.5833, -2.4801, -2.4990, -2.5165, -2.5326, -2.5471, -2.5604,
1:         -2.5720], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.2082, 1.1702, 1.1318, 1.0934, 1.0546, 1.0155, 0.9763, 0.9368, 0.8971,
1:         0.8576, 0.8179, 0.7782, 0.7383, 0.6984, 0.6583, 0.6182, 0.5780, 0.5379,
1:         1.2252, 1.1898, 1.1542, 1.1186, 1.0826, 1.0464, 1.0099],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7784, -0.7783, -0.7782, -0.7781, -0.7780, -0.7779, -0.7778, -0.7777,
1:         -0.7775, -0.7774, -0.7772, -0.7771, -0.7770, -0.7769, -0.7768, -0.7767,
1:         -0.7766, -0.7766, -0.7784, -0.7784, -0.7783, -0.7782, -0.7781, -0.7781,
1:         -0.7780], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.3743, 1.4633, 1.5534, 1.6491, 1.7514, 1.8605, 1.9762, 2.0907, 2.2009,
1:         2.2943, 2.3611, 2.3911, 2.3755, 2.3077, 2.1898, 2.0251, 1.8260, 1.6079,
1:         1.3298, 1.4021, 1.4822, 1.5779, 1.6902, 1.8171, 1.9517],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.6570, -0.6732, -0.6900, -0.7072, -0.7242, -0.7415, -0.7589, -0.7756,
1:         -0.7923, -0.8082, -0.8230, -0.8370, -0.8493, -0.8600, -0.8688, -0.8756,
1:         -0.8806, -0.8835, -0.8848, -0.8848, -0.8831, -0.8806, -0.8769, -0.8723,
1:         -0.8676], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 13, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan, -0.2432,     nan,     nan,     nan, -0.2432,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2432,     nan,     nan,     nan,     nan, -0.2421,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2421,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2373,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2385,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2432,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2444,     nan,
1:             nan,     nan, -0.2421,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2397, -0.2397,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2397,     nan,     nan, -0.2373,
1:             nan,     nan,     nan,     nan, -0.2385,     nan,     nan, -0.2362,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2409,     nan,     nan,     nan,     nan,     nan, -0.2397,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2421,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2385,
1:         -0.2373,     nan,     nan,     nan,     nan, -0.2362,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2362,     nan, -0.2338,
1:             nan,     nan,     nan,     nan, -0.2338,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2338,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2421,     nan,     nan,     nan,     nan,
1:         -0.2409, -0.2397,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 13, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.4731, -1.4573, -1.4422, -1.4300, -1.4169, -1.4091, -1.4015, -1.3950,
1:         -1.3866, -1.3697, -1.3465, -1.3196, -1.2947, -1.2724, -1.2563, -1.2374,
1:         -1.2198, -1.2006, -1.5096, -1.4959, -1.4807, -1.4662, -1.4540, -1.4442,
1:         -1.4344], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.4648, -1.4758, -1.4864, -1.4959, -1.5032, -1.5083, -1.5133, -1.5184,
1:         -1.5215, -1.5250, -1.5280, -1.5303, -1.5336, -1.5399, -1.5427, -1.5463,
1:         -1.5490, -1.5600, -1.4678, -1.4747, -1.4832, -1.4902, -1.4930, -1.4962,
1:         -1.4989], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.7748, -0.7725, -0.7692, -0.7653, -0.7639, -0.7618, -0.7618, -0.7608,
1:         -0.7604, -0.7591, -0.7577, -0.7563, -0.7562, -0.7563, -0.7569, -0.7589,
1:         -0.7608, -0.7638, -0.7781, -0.7769, -0.7744, -0.7707, -0.7694, -0.7668,
1:         -0.7651], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.0284,  0.0539,  0.0907,  0.1528,  0.2429,  0.3053,  0.3502,  0.3726,
1:          0.4074,  0.4082,  0.3231,  0.2351,  0.1519,  0.0473, -0.0554, -0.1434,
1:         -0.2017, -0.2288, -0.0610, -0.0051,  0.0303,  0.0895,  0.1802,  0.2520,
1:          0.2992], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-1.1028, -1.0951, -1.0875, -1.0865, -1.0899, -1.0950, -1.0984, -1.0986,
1:         -1.0949, -1.0914, -1.0916, -1.0930, -1.0926, -1.0872, -1.0743, -1.0552,
1:         -1.0328, -1.0092, -0.9865, -0.9660, -0.9450, -0.9232, -0.8980, -0.8666,
1:         -0.8313], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1267, -0.1310, -0.1380, -0.1430, -0.1483, -0.1551, -0.1586, -0.1576,
1:         -0.1585, -0.1358, -0.1324, -0.1400, -0.1467, -0.1502, -0.1501, -0.1610,
1:         -0.1592, -0.1562, -0.1386, -0.1395, -0.1411, -0.1428, -0.1459, -0.1479,
1:         -0.1536], device='cuda:1', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 13 [1/5 (20%)]	Loss: nan : nan :: 0.14063 (1.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 13 [2/5 (40%)]	Loss: nan : nan :: 0.14659 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 13 [3/5 (60%)]	Loss: nan : nan :: 0.14092 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 13 [4/5 (80%)]	Loss: nan : nan :: 0.14123 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch13.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch13.mod
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 13 : nan
0: validation loss for velocity_u : 0.03103816881775856
0: validation loss for velocity_v : 0.0589674711227417
0: validation loss for specific_humidity : 0.024134790524840355
0: validation loss for velocity_z : 0.42346101999282837
0: validation loss for temperature : 0.07268287241458893
0: validation loss for total_precip : nan
0: 14 : 23:48:47 :: batch_size = 96, lr = 1.4871117700906175e-05
3: 14 : 23:48:47 :: batch_size = 96, lr = 1.4871117700906175e-05
2: 14 : 23:48:47 :: batch_size = 96, lr = 1.4871117700906175e-05
1: 14 : 23:48:47 :: batch_size = 96, lr = 1.4871117700906175e-05
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 14, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] INPUT BATCH
0: Epoch 14, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 14, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1178, -0.1151, -0.1574, -0.1905, -0.1557, -0.1105, -0.1025, -0.1208,
2:         -0.1321, -0.1291, -0.1416, -0.1428, -0.1071, -0.0887, -0.0922, -0.0873,
2:         -0.0925, -0.0937, -0.2170, -0.2073, -0.2183, -0.2277, -0.2102, -0.1832,
2:         -0.1734], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.3136, 0.3638, 0.4238, 0.4481, 0.4645, 0.5139, 0.5893, 0.6580, 0.6902,
2:         0.6855, 0.6419, 0.5666, 0.5083, 0.4710, 0.4175, 0.3736, 0.3575, 0.3335,
2:         0.2596, 0.3380, 0.4182, 0.4494, 0.4728, 0.5282, 0.6032],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.1771, 1.1908, 1.1704, 1.2030, 1.2053, 1.1584, 1.0191, 0.8591, 0.7105,
2:         0.7300, 0.8064, 0.8655, 0.9009, 0.9119, 1.0237, 1.0754, 1.0161, 0.9307,
2:         1.2576, 1.1679, 1.1301, 1.1111, 1.0783, 0.8524, 0.6984],
2:        device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.9198, -0.8810, -0.4112, -0.0668, -0.3769, -0.8673, -0.3884, -0.1169,
2:         -0.6438, -0.4682, -0.2082, -0.4454, -0.1626, -0.2424, -1.0726, -1.4854,
2:         -1.9278, -2.2631, -0.5526, -0.5001, -0.3040,  0.1043, -0.6598, -2.1354,
2:         -1.7363], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.5066, 0.4864, 0.4470, 0.3955, 0.4015, 0.4946, 0.5829, 0.6023, 0.5805,
2:         0.5643, 0.5509, 0.5401, 0.5737, 0.5994, 0.5410, 0.4972, 0.5375, 0.5734,
2:         0.5742, 0.5850, 0.6073, 0.6326, 0.6643, 0.7013, 0.7260],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
0:      first 25 values: tensor([-0.1178, -0.1151, -0.1574, -0.1905, -0.1557, -0.1105, -0.1025, -0.1208,
0:         -0.1321, -0.1291, -0.1416, -0.1428, -0.1071, -0.0887, -0.0922, -0.0873,
0:         -0.0925, -0.0937, -0.2170, -0.2073, -0.2183, -0.2277, -0.2102, -0.1832,
0:         -0.1734], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3136, 0.3638, 0.4238, 0.4481, 0.4645, 0.5139, 0.5893, 0.6580, 0.6902,
0:         0.6855, 0.6419, 0.5666, 0.5083, 0.4710, 0.4175, 0.3736, 0.3575, 0.3335,
0:         0.2596, 0.3380, 0.4182, 0.4494, 0.4728, 0.5282, 0.6032],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] TARGET BATCH
2: Epoch 14, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0:      first 25 values: tensor([1.1771, 1.1908, 1.1704, 1.2030, 1.2053, 1.1584, 1.0191, 0.8591, 0.7105,
0:         0.7300, 0.8064, 0.8655, 0.9009, 0.9119, 1.0237, 1.0754, 1.0161, 0.9307,
0:         1.2576, 1.1679, 1.1301, 1.1111, 1.0783, 0.8524, 0.6984],
0:        device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9198, -0.8810, -0.4112, -0.0668, -0.3769, -0.8673, -0.3884, -0.1169,
0:         -0.6438, -0.4682, -0.2082, -0.4454, -0.1626, -0.2424, -1.0726, -1.4854,
0:         -1.9278, -2.2631, -0.5526, -0.5001, -0.3040,  0.1043, -0.6598, -2.1354,
0:         -1.7363], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.5066, 0.4864, 0.4470, 0.3955, 0.4015, 0.4946, 0.5829, 0.6023, 0.5805,
0:         0.5643, 0.5509, 0.5401, 0.5737, 0.5994, 0.5410, 0.4972, 0.5375, 0.5734,
0:         0.5742, 0.5850, 0.6073, 0.6326, 0.6643, 0.7013, 0.7260],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan, -0.2318,     nan,     nan,     nan,     nan,     nan,
2:         -0.2071,     nan,     nan, -0.1965,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2471,     nan,     nan,     nan,     nan,
2:             nan,     nan,  1.1893,     nan,     nan,     nan,     nan,     nan,
2:             nan,  0.6850,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,  0.8754,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2048,     nan,     nan,     nan,     nan, -0.1472,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2553,     nan,     nan,     nan,     nan,     nan, -0.2577,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2412,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:          1.6582,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.1801,     nan,     nan,  0.3359,     nan,     nan,  0.3653,     nan,
2:             nan,     nan,     nan,     nan, -0.1789,     nan, -0.2565,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2577,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,  0.0762,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,  0.3512,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1354,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1848,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 14, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.1779, 0.2076, 0.2332, 0.2504, 0.2586, 0.2646, 0.2675, 0.2697, 0.2766,
2:         0.2887, 0.3070, 0.3280, 0.3491, 0.3693, 0.3828, 0.3935, 0.4006, 0.4065,
2:         0.0939, 0.1214, 0.1430, 0.1541, 0.1581, 0.1599, 0.1690],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6069, -0.6070, -0.6132, -0.6280, -0.6451, -0.6598, -0.6670, -0.6705,
2:         -0.6694, -0.6670, -0.6631, -0.6569, -0.6472, -0.6389, -0.6327, -0.6346,
2:         -0.6435, -0.6494, -0.6263, -0.6308, -0.6393, -0.6494, -0.6572, -0.6624,
2:         -0.6635], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0: [DEBUG] TARGET BATCH
0: Epoch 14, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2:      first 25 pred values: tensor([0.6298, 0.6589, 0.6744, 0.6837, 0.6697, 0.6410, 0.6050, 0.5732, 0.5316,
2:         0.4971, 0.4699, 0.4430, 0.4245, 0.4124, 0.4072, 0.4122, 0.4130, 0.4295,
2:         0.6903, 0.7288, 0.7619, 0.7747, 0.7741, 0.7568, 0.7382],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2577, -0.2318,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2471,     nan,     nan,     nan,     nan,
0:             nan,     nan,  1.1893,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2071,     nan,     nan,  2.1413,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.7755,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0261,  0.0679,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2577,     nan,     nan, -0.2577,     nan,     nan,     nan,     nan,
0:         -0.1295,     nan,     nan,     nan, -0.2553,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  2.1096,     nan,     nan,     nan,     nan, -0.2447,
0:             nan,     nan,     nan,     nan, -0.1789,     nan,     nan, -0.2565,
0:         -0.2471,     nan, -0.2494,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2001,     nan,     nan,     nan,     nan, -0.1143,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2577,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2577,     nan, -0.2577,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2224,
0:             nan,     nan,     nan,     nan, -0.0567,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2565,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 14, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.1985,  0.1480,  0.1025,  0.0402, -0.0069, -0.0538, -0.0924, -0.0982,
2:         -0.0831, -0.0922, -0.1029, -0.0644,  0.0138,  0.0669,  0.0103, -0.0414,
2:          0.0454,  0.0941,  0.1097, -0.0493, -0.1634, -0.2280, -0.2316, -0.2527,
2:         -0.3030], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.1815, 0.2135, 0.2363, 0.2480, 0.2540, 0.2576, 0.2659, 0.2758, 0.2893,
0:         0.3060, 0.3247, 0.3435, 0.3598, 0.3789, 0.3893, 0.4007, 0.4072, 0.4120,
0:         0.1027, 0.1322, 0.1514, 0.1582, 0.1583, 0.1605, 0.1736],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.4323, 0.4078, 0.3955, 0.3865, 0.3901, 0.3954, 0.3974, 0.3951, 0.3982,
2:         0.4179, 0.4563, 0.5118, 0.5727, 0.6275, 0.6723, 0.7103, 0.7420, 0.7712,
2:         0.7964, 0.8139, 0.8272, 0.8439, 0.8671, 0.8969, 0.9266],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.6091, -0.6174, -0.6309, -0.6459, -0.6646, -0.6808, -0.6860, -0.6828,
0:         -0.6780, -0.6716, -0.6656, -0.6645, -0.6604, -0.6552, -0.6515, -0.6509,
0:         -0.6575, -0.6643, -0.6369, -0.6443, -0.6543, -0.6685, -0.6825, -0.6936,
0:         -0.6987], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1684, -0.1714, -0.1783, -0.1785, -0.1779, -0.1754, -0.1772, -0.1709,
2:         -0.1696, -0.1719, -0.1701, -0.1749, -0.1784, -0.1757, -0.1727, -0.1769,
2:         -0.1717, -0.1683, -0.1688, -0.1703, -0.1682, -0.1725, -0.1703, -0.1722,
2:         -0.1743], device='cuda:2', grad_fn=<SliceBackward0>)
0:      first 25 pred values: tensor([0.6497, 0.6936, 0.7159, 0.7275, 0.7083, 0.6800, 0.6379, 0.6056, 0.5669,
0:         0.5327, 0.5025, 0.4876, 0.4759, 0.4724, 0.4798, 0.4876, 0.4907, 0.5065,
0:         0.7160, 0.7589, 0.7901, 0.8064, 0.7986, 0.7797, 0.7640],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1278,  0.0855,  0.1732,  0.1463,  0.0915,  0.0800, -0.0155, -0.0879,
0:         -0.0376, -0.0093, -0.0301, -0.0259,  0.0250,  0.0351, -0.0083, -0.0051,
0:          0.0808,  0.1927, -0.0511, -0.1346, -0.0815, -0.1234, -0.1447, -0.1199,
0:         -0.2179], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4944, 0.4629, 0.4495, 0.4491, 0.4579, 0.4666, 0.4714, 0.4736, 0.4840,
0:         0.5091, 0.5478, 0.5958, 0.6465, 0.6905, 0.7294, 0.7618, 0.7895, 0.8126,
0:         0.8316, 0.8471, 0.8614, 0.8819, 0.9062, 0.9333, 0.9595],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1768, -0.1774, -0.1806, -0.1769, -0.1779, -0.1791, -0.1833, -0.1804,
0:         -0.1842, -0.1822, -0.1748, -0.1763, -0.1751, -0.1740, -0.1731, -0.1823,
0:         -0.1815, -0.1816, -0.1776, -0.1768, -0.1701, -0.1707, -0.1686, -0.1679,
0:         -0.1732], device='cuda:0', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-0.1178, -0.1151, -0.1574, -0.1905, -0.1557, -0.1105, -0.1025, -0.1208,
1:         -0.1321, -0.1291, -0.1416, -0.1428, -0.1071, -0.0887, -0.0922, -0.0873,
1:         -0.0925, -0.0937, -0.2170, -0.2073, -0.2183, -0.2277, -0.2102, -0.1832,
1:         -0.1734], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.3136, 0.3638, 0.4238, 0.4481, 0.4645, 0.5139, 0.5893, 0.6580, 0.6902,
1:         0.6855, 0.6419, 0.5666, 0.5083, 0.4710, 0.4175, 0.3736, 0.3575, 0.3335,
1:         0.2596, 0.3380, 0.4182, 0.4494, 0.4728, 0.5282, 0.6032],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.1771, 1.1908, 1.1704, 1.2030, 1.2053, 1.1584, 1.0191, 0.8591, 0.7105,
1:         0.7300, 0.8064, 0.8655, 0.9009, 0.9119, 1.0237, 1.0754, 1.0161, 0.9307,
1:         1.2576, 1.1679, 1.1301, 1.1111, 1.0783, 0.8524, 0.6984],
1:        device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.9198, -0.8810, -0.4112, -0.0668, -0.3769, -0.8673, -0.3884, -0.1169,
1:         -0.6438, -0.4682, -0.2082, -0.4454, -0.1626, -0.2424, -1.0726, -1.4854,
1:         -1.9278, -2.2631, -0.5526, -0.5001, -0.3040,  0.1043, -0.6598, -2.1354,
1:         -1.7363], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.5066, 0.4864, 0.4470, 0.3955, 0.4015, 0.4946, 0.5829, 0.6023, 0.5805,
1:         0.5643, 0.5509, 0.5401, 0.5737, 0.5994, 0.5410, 0.4972, 0.5375, 0.5734,
1:         0.5742, 0.5850, 0.6073, 0.6326, 0.6643, 0.7013, 0.7260],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 14, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2330,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.1813,     nan, -0.0884,     nan,
1:             nan,     nan,  1.0153,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,  0.0750,     nan,     nan,     nan,  0.7755,
1:             nan,     nan,  0.0550,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2483,     nan,     nan,  2.3670,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2577,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2506,     nan,     nan,
1:             nan,     nan,     nan, -0.2577,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,  0.3629,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2036,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2471,     nan,     nan, -0.1413,     nan, -0.1084,     nan,     nan,
1:         -0.2577,     nan,     nan,     nan,     nan,     nan,  0.6039,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2577,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2577,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2577,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan, -0.2224,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2541,     nan,  1.1610,     nan,     nan,
1:         -0.2565,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 14, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.2085, 0.2369, 0.2586, 0.2729, 0.2820, 0.2865, 0.2894, 0.2942, 0.3002,
1:         0.3124, 0.3297, 0.3501, 0.3703, 0.3863, 0.3938, 0.4007, 0.4008, 0.4042,
1:         0.1185, 0.1407, 0.1589, 0.1693, 0.1740, 0.1794, 0.1898],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6326, -0.6342, -0.6394, -0.6508, -0.6662, -0.6812, -0.6901, -0.6909,
1:         -0.6902, -0.6890, -0.6858, -0.6828, -0.6768, -0.6659, -0.6538, -0.6438,
1:         -0.6383, -0.6360, -0.6466, -0.6492, -0.6546, -0.6581, -0.6657, -0.6683,
1:         -0.6731], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.6026, 0.6272, 0.6390, 0.6433, 0.6241, 0.5874, 0.5500, 0.5208, 0.4883,
1:         0.4572, 0.4322, 0.4089, 0.3895, 0.3712, 0.3553, 0.3511, 0.3440, 0.3562,
1:         0.6520, 0.6829, 0.7097, 0.7202, 0.7183, 0.7044, 0.6881],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.2182,  0.1788,  0.1970,  0.1348,  0.0273, -0.0095, -0.0701, -0.0909,
1:         -0.0290, -0.0215, -0.0702, -0.0943, -0.0181,  0.0374, -0.0192, -0.0718,
1:         -0.0249,  0.0184,  0.0892,  0.0247,  0.0028, -0.0548, -0.1279, -0.1232,
1:         -0.1691], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([0.3844, 0.3556, 0.3521, 0.3645, 0.3901, 0.4131, 0.4236, 0.4246, 0.4274,
1:         0.4434, 0.4771, 0.5255, 0.5802, 0.6338, 0.6834, 0.7288, 0.7678, 0.8014,
1:         0.8277, 0.8472, 0.8631, 0.8804, 0.9012, 0.9288, 0.9564],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1768, -0.1789, -0.1823, -0.1791, -0.1790, -0.1759, -0.1747, -0.1678,
1:         -0.1638, -0.1787, -0.1764, -0.1790, -0.1794, -0.1776, -0.1729, -0.1747,
1:         -0.1691, -0.1619, -0.1748, -0.1763, -0.1704, -0.1726, -0.1720, -0.1689,
1:         -0.1704], device='cuda:1', grad_fn=<SliceBackward0>)
3: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 14, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1178, -0.1151, -0.1574, -0.1905, -0.1557, -0.1105, -0.1025, -0.1208,
3:         -0.1321, -0.1291, -0.1416, -0.1428, -0.1071, -0.0887, -0.0922, -0.0873,
3:         -0.0925, -0.0937, -0.2170, -0.2073, -0.2183, -0.2277, -0.2102, -0.1832,
3:         -0.1734], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.3136, 0.3638, 0.4238, 0.4481, 0.4645, 0.5139, 0.5893, 0.6580, 0.6902,
3:         0.6855, 0.6419, 0.5666, 0.5083, 0.4710, 0.4175, 0.3736, 0.3575, 0.3335,
3:         0.2596, 0.3380, 0.4182, 0.4494, 0.4728, 0.5282, 0.6032],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.1771, 1.1908, 1.1704, 1.2030, 1.2053, 1.1584, 1.0191, 0.8591, 0.7105,
3:         0.7300, 0.8064, 0.8655, 0.9009, 0.9119, 1.0237, 1.0754, 1.0161, 0.9307,
3:         1.2576, 1.1679, 1.1301, 1.1111, 1.0783, 0.8524, 0.6984],
3:        device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.9198, -0.8810, -0.4112, -0.0668, -0.3769, -0.8673, -0.3884, -0.1169,
3:         -0.6438, -0.4682, -0.2082, -0.4454, -0.1626, -0.2424, -1.0726, -1.4854,
3:         -1.9278, -2.2631, -0.5526, -0.5001, -0.3040,  0.1043, -0.6598, -2.1354,
3:         -1.7363], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.5066, 0.4864, 0.4470, 0.3955, 0.4015, 0.4946, 0.5829, 0.6023, 0.5805,
3:         0.5643, 0.5509, 0.5401, 0.5737, 0.5994, 0.5410, 0.4972, 0.5375, 0.5734,
3:         0.5742, 0.5850, 0.6073, 0.6326, 0.6643, 0.7013, 0.7260],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 14, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan, -0.1954,     nan,     nan,
3:             nan, -0.2530,     nan,     nan,     nan,     nan, -0.2459,     nan,
3:             nan, -0.0238,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.0884,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,  1.9909,  2.1413,
3:             nan,     nan,     nan,     nan,  0.0562,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2248,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.1072, -0.1107,
3:             nan,     nan,     nan,     nan, -0.0261,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2577,     nan,     nan,     nan, -0.2541,     nan,     nan, -0.2577,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  1.4102,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2142,     nan,     nan,
3:             nan,     nan,     nan, -0.1413,     nan, -0.1084,     nan,     nan,
3:             nan,     nan, -0.2001,     nan,     nan,     nan,     nan, -0.1143,
3:             nan,     nan,     nan,     nan,     nan, -0.2577, -0.2577,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2577,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.1307,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2541,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 14, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.1797, 0.2114, 0.2348, 0.2482, 0.2539, 0.2611, 0.2699, 0.2815, 0.2970,
3:         0.3164, 0.3378, 0.3595, 0.3806, 0.3990, 0.4132, 0.4233, 0.4326, 0.4395,
3:         0.0890, 0.1224, 0.1441, 0.1546, 0.1582, 0.1628, 0.1767],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6143, -0.6214, -0.6326, -0.6461, -0.6601, -0.6741, -0.6836, -0.6881,
3:         -0.6909, -0.6874, -0.6819, -0.6767, -0.6700, -0.6646, -0.6638, -0.6684,
3:         -0.6779, -0.6865, -0.6230, -0.6358, -0.6477, -0.6594, -0.6660, -0.6712,
3:         -0.6758], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.6805, 0.6823, 0.6690, 0.6535, 0.6160, 0.5767, 0.5326, 0.5014, 0.4673,
3:         0.4377, 0.4175, 0.4056, 0.3978, 0.3970, 0.3928, 0.3892, 0.3805, 0.3838,
3:         0.7233, 0.7397, 0.7484, 0.7396, 0.7222, 0.6949, 0.6769],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.1880,  0.2151,  0.2396,  0.1848,  0.0386, -0.0792, -0.1267, -0.0764,
3:          0.0594,  0.0903,  0.0210,  0.0459,  0.1669,  0.2470,  0.2487,  0.2050,
3:          0.1581,  0.1246,  0.0098, -0.0419, -0.0575, -0.0876, -0.1387, -0.1795,
3:         -0.2275], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([0.4711, 0.4154, 0.3899, 0.3896, 0.4129, 0.4359, 0.4508, 0.4533, 0.4597,
3:         0.4795, 0.5164, 0.5651, 0.6163, 0.6627, 0.7047, 0.7430, 0.7785, 0.8097,
3:         0.8393, 0.8644, 0.8870, 0.9084, 0.9309, 0.9557, 0.9799],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1717, -0.1754, -0.1774, -0.1778, -0.1733, -0.1717, -0.1746, -0.1729,
3:         -0.1742, -0.1733, -0.1704, -0.1724, -0.1723, -0.1705, -0.1674, -0.1743,
3:         -0.1720, -0.1699, -0.1686, -0.1694, -0.1659, -0.1643, -0.1628, -0.1618,
3:         -0.1647], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 14 [1/5 (20%)]	Loss: nan : nan :: 0.14014 (1.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 14 [2/5 (40%)]	Loss: nan : nan :: 0.13499 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 14 [3/5 (60%)]	Loss: nan : nan :: 0.13872 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 14 [4/5 (80%)]	Loss: nan : nan :: 0.16372 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch14.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch14.mod
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 14 : nan
0: validation loss for velocity_u : 0.03776537626981735
0: validation loss for velocity_v : 0.06693394482135773
0: validation loss for specific_humidity : 0.02488962933421135
0: validation loss for velocity_z : 0.49294787645339966
0: validation loss for temperature : 0.07221364974975586
0: validation loss for total_precip : nan
3: 15 : 23:55:36 :: batch_size = 96, lr = 1.4508407513079195e-05
0: 15 : 23:55:36 :: batch_size = 96, lr = 1.4508407513079195e-05
2: 15 : 23:55:36 :: batch_size = 96, lr = 1.4508407513079195e-05
1: 15 : 23:55:36 :: batch_size = 96, lr = 1.4508407513079195e-05
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 15, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2058,  0.1375,  0.0629, -0.0151, -0.0948, -0.1756, -0.2574, -0.3402,
0:         -0.4228, -0.5017, -0.5724, -0.6298, -0.6708, -0.6940, -0.7013, -0.6964,
0:         -0.6847, -0.6708,  0.1173,  0.0366, -0.0481, -0.1340, -0.2195, -0.3039,
0:         -0.3866], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4293, -0.6582, -0.9349, -1.2407, -1.5478, -1.8251, -2.0443, -2.1875,
0:         -2.2507, -2.2420, -2.1797, -2.0838, -1.9738, -1.8631, -1.7609, -1.6713,
0:         -1.5955, -1.5324, -0.6783, -0.9564, -1.2701, -1.5926, -1.8906, -2.1328,
0:         -2.2971], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7715, -0.7739, -0.7768, -0.7794, -0.7743, -0.7690, -0.7635, -0.7359,
0:         -0.7088, -0.6816, -0.6596, -0.6375, -0.6161, -0.6138, -0.6116, -0.6091,
0:         -0.6073, -0.6054, -0.7749, -0.7761, -0.7774, -0.7788, -0.7694, -0.7584,
0:         -0.7474], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.9807, 2.0142, 2.0856, 2.2016, 2.3333, 2.4203, 2.4002, 2.2373, 1.9406,
0:         1.5590, 1.1663, 0.8226, 0.5727, 0.4187, 0.3473, 0.3362, 0.3563, 0.3897,
0:         1.9116, 1.9763, 2.0901, 2.2373, 2.3668, 2.4159, 2.3288],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.8838, -0.8198, -0.7405, -0.6504, -0.5557, -0.4629, -0.3781, -0.3058,
0:         -0.2471, -0.2018, -0.1675, -0.1415, -0.1212, -0.1043, -0.0896, -0.0758,
0:         -0.0620, -0.0474, -0.0315, -0.0135,  0.0064,  0.0283,  0.0513,  0.0748,
0:          0.0985], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 15, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2388,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2250,
0:             nan, -0.2250,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2273, -0.2284,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2330,     nan,     nan,     nan,     nan, -0.2319,     nan,
0:             nan,     nan,     nan, -0.2388,     nan,     nan,     nan,     nan,
0:         -0.2376,     nan,     nan,     nan,     nan, -0.2353,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2353, -0.2330,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2284,     nan,     nan,
0:             nan,     nan, -0.2296,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2307,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2284,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2135,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1848,     nan,
0:             nan, -0.1848,     nan,     nan, -0.1905,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2135,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2204,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1928,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 15, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1842, 0.2107, 0.2337, 0.2541, 0.2741, 0.2944, 0.3194, 0.3427, 0.3610,
0:         0.3760, 0.3855, 0.3932, 0.4032, 0.4139, 0.4283, 0.4410, 0.4575, 0.4786,
0:         0.1572, 0.1829, 0.2060, 0.2272, 0.2450, 0.2643, 0.2852],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7861, -0.8113, -0.8330, -0.8486, -0.8625, -0.8693, -0.8738, -0.8738,
0:         -0.8710, -0.8629, -0.8496, -0.8331, -0.8110, -0.7793, -0.7402, -0.6954,
0:         -0.6472, -0.5953, -0.7734, -0.7903, -0.8040, -0.8143, -0.8180, -0.8183,
0:         -0.8163], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5941, -0.5943, -0.5941, -0.5949, -0.5968, -0.5981, -0.5983, -0.5984,
0:         -0.5987, -0.5966, -0.5929, -0.5912, -0.5902, -0.5903, -0.5935, -0.5964,
0:         -0.6006, -0.6026, -0.6088, -0.6105, -0.6092, -0.6100, -0.6109, -0.6115,
0:         -0.6111], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1882, -0.2260, -0.2352, -0.2648, -0.2537, -0.2050, -0.1380, -0.0559,
0:          0.0362,  0.1105,  0.1763,  0.2561,  0.2885,  0.2657,  0.2258,  0.1847,
0:          0.1750,  0.1629, -0.1391, -0.1506, -0.1330, -0.1163, -0.0856, -0.0474,
0:          0.0016], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.2729, 0.2801, 0.2851, 0.2848, 0.2817, 0.2769, 0.2715, 0.2648, 0.2564,
0:         0.2445, 0.2278, 0.2071, 0.1837, 0.1613, 0.1413, 0.1253, 0.1127, 0.1011,
0:         0.0887, 0.0753, 0.0633, 0.0557, 0.0526, 0.0511, 0.0471],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1218, -0.1259, -0.1310, -0.1291, -0.1337, -0.1352, -0.1330, -0.1265,
0:         -0.1236, -0.1338, -0.1263, -0.1308, -0.1310, -0.1360, -0.1319, -0.1363,
0:         -0.1322, -0.1208, -0.1331, -0.1346, -0.1314, -0.1294, -0.1316, -0.1321,
0:         -0.1360], device='cuda:0', grad_fn=<SliceBackward0>)
3: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 15, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: Created sparse mask for total_precip with 10.0% data retained
1:      first 25 values: tensor([ 0.2058,  0.1375,  0.0629, -0.0151, -0.0948, -0.1756, -0.2574, -0.3402,
1:         -0.4228, -0.5017, -0.5724, -0.6298, -0.6708, -0.6940, -0.7013, -0.6964,
1:         -0.6847, -0.6708,  0.1173,  0.0366, -0.0481, -0.1340, -0.2195, -0.3039,
1:         -0.3866], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4293, -0.6582, -0.9349, -1.2407, -1.5478, -1.8251, -2.0443, -2.1875,
1:         -2.2507, -2.2420, -2.1797, -2.0838, -1.9738, -1.8631, -1.7609, -1.6713,
1:         -1.5955, -1.5324, -0.6783, -0.9564, -1.2701, -1.5926, -1.8906, -2.1328,
1:         -2.2971], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7715, -0.7739, -0.7768, -0.7794, -0.7743, -0.7690, -0.7635, -0.7359,
1:         -0.7088, -0.6816, -0.6596, -0.6375, -0.6161, -0.6138, -0.6116, -0.6091,
1:         -0.6073, -0.6054, -0.7749, -0.7761, -0.7774, -0.7788, -0.7694, -0.7584,
1:         -0.7474], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.9807, 2.0142, 2.0856, 2.2016, 2.3333, 2.4203, 2.4002, 2.2373, 1.9406,
1:         1.5590, 1.1663, 0.8226, 0.5727, 0.4187, 0.3473, 0.3362, 0.3563, 0.3897,
1:         1.9116, 1.9763, 2.0901, 2.2373, 2.3668, 2.4159, 2.3288],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.8838, -0.8198, -0.7405, -0.6504, -0.5557, -0.4629, -0.3781, -0.3058,
1:         -0.2471, -0.2018, -0.1675, -0.1415, -0.1212, -0.1043, -0.0896, -0.0758,
1:         -0.0620, -0.0474, -0.0315, -0.0135,  0.0064,  0.0283,  0.0513,  0.0748,
1:          0.0985], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 15, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([-0.2365,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2388,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2284,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2330,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2365,     nan,     nan,     nan,     nan, -0.2376,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2388,
1:         -0.2388,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2296,     nan,     nan, -0.2169,     nan,     nan, -0.2227,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2078,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2307,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2342,     nan,
1:             nan,     nan, -0.2319,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2181, -0.2192, -0.2204,     nan,     nan,     nan,     nan,     nan,
1:         -0.2066, -0.2055,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.1848, -0.1917, -0.1491,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1641,     nan,     nan, -0.1894,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 15, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.1964, 0.2321, 0.2670, 0.2953, 0.3186, 0.3384, 0.3587, 0.3756, 0.3896,
1:         0.4010, 0.4084, 0.4130, 0.4201, 0.4299, 0.4420, 0.4546, 0.4744, 0.4997,
1:         0.1743, 0.2039, 0.2360, 0.2596, 0.2781, 0.2950, 0.3163],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.8222, -0.8444, -0.8597, -0.8690, -0.8780, -0.8782, -0.8762, -0.8700,
1:         -0.8593, -0.8454, -0.8288, -0.8103, -0.7853, -0.7541, -0.7156, -0.6724,
1:         -0.6249, -0.5690, -0.8080, -0.8242, -0.8350, -0.8390, -0.8410, -0.8389,
1:         -0.8330], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5980, -0.5987, -0.5991, -0.5988, -0.6013, -0.6043, -0.6074, -0.6097,
1:         -0.6101, -0.6103, -0.6091, -0.6090, -0.6083, -0.6077, -0.6066, -0.6062,
1:         -0.6066, -0.6065, -0.6087, -0.6101, -0.6090, -0.6081, -0.6096, -0.6107,
1:         -0.6116], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.2580, -0.2548, -0.2023, -0.1885, -0.1408, -0.0709, -0.0183,  0.0274,
1:          0.0784,  0.1206,  0.1584,  0.2326,  0.2723,  0.2413,  0.1998,  0.1872,
1:          0.1855,  0.1401, -0.1032, -0.0780, -0.0343, -0.0221,  0.0211,  0.0747,
1:          0.1087], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([0.2621, 0.2574, 0.2532, 0.2497, 0.2474, 0.2452, 0.2431, 0.2384, 0.2297,
1:         0.2171, 0.2028, 0.1877, 0.1730, 0.1583, 0.1427, 0.1265, 0.1101, 0.0948,
1:         0.0810, 0.0695, 0.0607, 0.0556, 0.0524, 0.0477, 0.0395],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1517, -0.1563, -0.1627, -0.1631, -0.1649, -0.1657, -0.1696, -0.1664,
1:         -0.1649, -0.1561, -0.1514, -0.1553, -0.1594, -0.1641, -0.1615, -0.1700,
1:         -0.1660, -0.1603, -0.1508, -0.1517, -0.1506, -0.1504, -0.1541, -0.1562,
1:         -0.1601], device='cuda:1', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 15, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.2058,  0.1375,  0.0629, -0.0151, -0.0948, -0.1756, -0.2574, -0.3402,
3:         -0.4228, -0.5017, -0.5724, -0.6298, -0.6708, -0.6940, -0.7013, -0.6964,
3:         -0.6847, -0.6708,  0.1173,  0.0366, -0.0481, -0.1340, -0.2195, -0.3039,
3:         -0.3866], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4293, -0.6582, -0.9349, -1.2407, -1.5478, -1.8251, -2.0443, -2.1875,
3:         -2.2507, -2.2420, -2.1797, -2.0838, -1.9738, -1.8631, -1.7609, -1.6713,
3:         -1.5955, -1.5324, -0.6783, -0.9564, -1.2701, -1.5926, -1.8906, -2.1328,
3:         -2.2971], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7715, -0.7739, -0.7768, -0.7794, -0.7743, -0.7690, -0.7635, -0.7359,
3:         -0.7088, -0.6816, -0.6596, -0.6375, -0.6161, -0.6138, -0.6116, -0.6091,
3:         -0.6073, -0.6054, -0.7749, -0.7761, -0.7774, -0.7788, -0.7694, -0.7584,
3:         -0.7474], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.9807, 2.0142, 2.0856, 2.2016, 2.3333, 2.4203, 2.4002, 2.2373, 1.9406,
3:         1.5590, 1.1663, 0.8226, 0.5727, 0.4187, 0.3473, 0.3362, 0.3563, 0.3897,
3:         1.9116, 1.9763, 2.0901, 2.2373, 2.3668, 2.4159, 2.3288],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.8838, -0.8198, -0.7405, -0.6504, -0.5557, -0.4629, -0.3781, -0.3058,
3:         -0.2471, -0.2018, -0.1675, -0.1415, -0.1212, -0.1043, -0.0896, -0.0758,
3:         -0.0620, -0.0474, -0.0315, -0.0135,  0.0064,  0.0283,  0.0513,  0.0748,
3:          0.0985], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 15, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2388,     nan,     nan, -0.2365,     nan, -0.2353,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2250,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2307,     nan,     nan,     nan,     nan,
3:             nan, -0.2330,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2388,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2319,
3:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2250, -0.2273,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2307,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2411,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2250,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2112,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.1859,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.1790,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2238,     nan,     nan, -0.1905,     nan,     nan,     nan,     nan,
3:             nan, -0.2146,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 15, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.2336, 0.2570, 0.2807, 0.3005, 0.3184, 0.3346, 0.3539, 0.3689, 0.3820,
3:         0.3933, 0.4017, 0.4112, 0.4249, 0.4389, 0.4547, 0.4668, 0.4781, 0.4932,
3:         0.2122, 0.2365, 0.2598, 0.2782, 0.2916, 0.3059, 0.3224],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.8652, -0.8801, -0.8852, -0.8859, -0.8862, -0.8825, -0.8811, -0.8788,
3:         -0.8713, -0.8592, -0.8443, -0.8225, -0.7978, -0.7651, -0.7300, -0.6853,
3:         -0.6403, -0.5838, -0.8403, -0.8477, -0.8479, -0.8476, -0.8414, -0.8384,
3:         -0.8327], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6189, -0.6176, -0.6170, -0.6159, -0.6168, -0.6161, -0.6178, -0.6186,
3:         -0.6180, -0.6165, -0.6125, -0.6093, -0.6068, -0.6076, -0.6083, -0.6116,
3:         -0.6146, -0.6171, -0.6231, -0.6225, -0.6206, -0.6186, -0.6174, -0.6160,
3:         -0.6169], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.1202, -0.1723, -0.1694, -0.1620, -0.1135, -0.0851, -0.0619, -0.0308,
3:         -0.0011,  0.0518,  0.1098,  0.1853,  0.2313,  0.2118,  0.1690,  0.1228,
3:          0.0979,  0.0701,  0.0116, -0.0220, -0.0335, -0.0313,  0.0276,  0.0560,
3:          0.0648], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([0.2893, 0.2858, 0.2818, 0.2763, 0.2714, 0.2665, 0.2619, 0.2551, 0.2459,
3:         0.2332, 0.2170, 0.1992, 0.1812, 0.1612, 0.1427, 0.1242, 0.1062, 0.0890,
3:         0.0735, 0.0617, 0.0545, 0.0523, 0.0512, 0.0475, 0.0384],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1578, -0.1591, -0.1657, -0.1664, -0.1691, -0.1669, -0.1692, -0.1632,
3:         -0.1625, -0.1623, -0.1584, -0.1649, -0.1692, -0.1681, -0.1660, -0.1711,
3:         -0.1674, -0.1605, -0.1649, -0.1632, -0.1616, -0.1646, -0.1642, -0.1641,
3:         -0.1651], device='cuda:3', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 15, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.2058,  0.1375,  0.0629, -0.0151, -0.0948, -0.1756, -0.2574, -0.3402,
2:         -0.4228, -0.5017, -0.5724, -0.6298, -0.6708, -0.6940, -0.7013, -0.6964,
2:         -0.6847, -0.6708,  0.1173,  0.0366, -0.0481, -0.1340, -0.2195, -0.3039,
2:         -0.3866], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.4293, -0.6582, -0.9349, -1.2407, -1.5478, -1.8251, -2.0443, -2.1875,
2:         -2.2507, -2.2420, -2.1797, -2.0838, -1.9738, -1.8631, -1.7609, -1.6713,
2:         -1.5955, -1.5324, -0.6783, -0.9564, -1.2701, -1.5926, -1.8906, -2.1328,
2:         -2.2971], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7715, -0.7739, -0.7768, -0.7794, -0.7743, -0.7690, -0.7635, -0.7359,
2:         -0.7088, -0.6816, -0.6596, -0.6375, -0.6161, -0.6138, -0.6116, -0.6091,
2:         -0.6073, -0.6054, -0.7749, -0.7761, -0.7774, -0.7788, -0.7694, -0.7584,
2:         -0.7474], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.9807, 2.0142, 2.0856, 2.2016, 2.3333, 2.4203, 2.4002, 2.2373, 1.9406,
2:         1.5590, 1.1663, 0.8226, 0.5727, 0.4187, 0.3473, 0.3362, 0.3563, 0.3897,
2:         1.9116, 1.9763, 2.0901, 2.2373, 2.3668, 2.4159, 2.3288],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.8838, -0.8198, -0.7405, -0.6504, -0.5557, -0.4629, -0.3781, -0.3058,
2:         -0.2471, -0.2018, -0.1675, -0.1415, -0.1212, -0.1043, -0.0896, -0.0758,
2:         -0.0620, -0.0474, -0.0315, -0.0135,  0.0064,  0.0283,  0.0513,  0.0748,
2:          0.0985], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 15, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2181,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2227,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2330,
2:             nan,     nan,     nan,     nan,     nan, -0.2376, -0.2365,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2158,     nan,     nan,     nan, -0.2261,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2284,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2169,     nan,     nan,     nan,     nan, -0.2032,
2:         -0.2089,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2330,
2:             nan, -0.2238,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1813,
2:             nan,     nan, -0.1836,     nan,     nan,     nan, -0.1641,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1503,
2:             nan,     nan,     nan,     nan, -0.1928,     nan,     nan,     nan,
2:             nan,     nan, -0.1802,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2158,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2089,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2100])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 15, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.1720, 0.2057, 0.2384, 0.2706, 0.2989, 0.3236, 0.3467, 0.3678, 0.3858,
2:         0.3992, 0.4101, 0.4173, 0.4261, 0.4335, 0.4460, 0.4563, 0.4712, 0.4902,
2:         0.1488, 0.1757, 0.2042, 0.2306, 0.2553, 0.2796, 0.3050],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.8762, -0.8868, -0.8895, -0.8852, -0.8825, -0.8772, -0.8724, -0.8655,
2:         -0.8566, -0.8446, -0.8323, -0.8170, -0.7959, -0.7682, -0.7311, -0.6867,
2:         -0.6345, -0.5746, -0.8463, -0.8508, -0.8475, -0.8444, -0.8408, -0.8374,
2:         -0.8309], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6071, -0.6091, -0.6101, -0.6081, -0.6093, -0.6084, -0.6100, -0.6107,
2:         -0.6086, -0.6074, -0.6044, -0.6015, -0.5995, -0.5983, -0.5958, -0.5949,
2:         -0.5929, -0.5923, -0.6134, -0.6157, -0.6138, -0.6115, -0.6104, -0.6089,
2:         -0.6086], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1382, -0.1851, -0.1971, -0.2383, -0.2230, -0.1497, -0.0790, -0.0287,
2:          0.0217,  0.0856,  0.1758,  0.2826,  0.3170,  0.2918,  0.2691,  0.2385,
2:          0.2466,  0.2629, -0.0743, -0.0958, -0.0890, -0.0949, -0.0453,  0.0375,
2:          0.1058], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([0.2890, 0.2882, 0.2892, 0.2883, 0.2852, 0.2805, 0.2758, 0.2715, 0.2667,
2:         0.2579, 0.2440, 0.2241, 0.2017, 0.1789, 0.1580, 0.1390, 0.1213, 0.1035,
2:         0.0860, 0.0705, 0.0587, 0.0522, 0.0483, 0.0432, 0.0341],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1313, -0.1371, -0.1459, -0.1492, -0.1522, -0.1587, -0.1635, -0.1618,
2:         -0.1611, -0.1433, -0.1410, -0.1482, -0.1551, -0.1626, -0.1596, -0.1680,
2:         -0.1637, -0.1572, -0.1506, -0.1532, -0.1536, -0.1552, -0.1592, -0.1592,
2:         -0.1629], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 15 [1/5 (20%)]	Loss: nan : nan :: 0.15345 (1.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 15 [2/5 (40%)]	Loss: nan : nan :: 0.14159 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 15 [3/5 (60%)]	Loss: nan : nan :: 0.15684 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 15 [4/5 (80%)]	Loss: nan : nan :: 0.14293 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch15.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch15.mod
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 15 : nan
0: validation loss for velocity_u : 0.03152585029602051
0: validation loss for velocity_v : 0.058564964681863785
0: validation loss for specific_humidity : 0.02424241602420807
0: validation loss for velocity_z : 0.40813493728637695
0: validation loss for temperature : 0.09757685661315918
0: validation loss for total_precip : nan
1: 16 : 00:02:26 :: batch_size = 96, lr = 1.4154543915199217e-05
2: 16 : 00:02:26 :: batch_size = 96, lr = 1.4154543915199217e-05
3: 16 : 00:02:26 :: batch_size = 96, lr = 1.4154543915199217e-05
0: 16 : 00:02:26 :: batch_size = 96, lr = 1.4154543915199217e-05
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 16, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8708, 0.8867, 0.9018, 0.9173, 0.9318, 0.9446, 0.9578, 0.9707, 0.9838,
0:         0.9971, 1.0110, 1.0259, 1.0420, 1.0593, 1.0769, 1.0936, 1.1097, 1.1245,
0:         0.8331, 0.8489, 0.8646, 0.8812, 0.8963, 0.9099, 0.9231],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4535, 0.4433, 0.4325, 0.4208, 0.4085, 0.3954, 0.3825, 0.3705, 0.3592,
0:         0.3486, 0.3400, 0.3342, 0.3307, 0.3285, 0.3281, 0.3277, 0.3273, 0.3279,
0:         0.4769, 0.4669, 0.4568, 0.4464, 0.4351, 0.4228, 0.4108],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6689, -0.6689, -0.6691, -0.6691, -0.6690, -0.6691, -0.6690, -0.6691,
0:         -0.6691, -0.6690, -0.6691, -0.6687, -0.6688, -0.6688, -0.6693, -0.6702,
0:         -0.6711, -0.6718, -0.6658, -0.6656, -0.6654, -0.6650, -0.6647, -0.6645,
0:         -0.6642], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4998, 0.5824, 0.6041, 0.6650, 0.7475, 0.7975, 0.8475, 0.8475, 0.8605,
0:         0.8779, 0.8562, 0.8388, 0.8279, 0.8605, 0.8736, 0.8453, 0.7953, 0.6780,
0:         0.4390, 0.4933, 0.4825, 0.5368, 0.6150, 0.6389, 0.6780],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.2571, -1.2533, -1.2483, -1.2411, -1.2323, -1.2198, -1.2076, -1.1938,
0:         -1.1778, -1.1622, -1.1457, -1.1281, -1.1111, -1.0895, -1.0643, -1.0361,
0:         -1.0065, -0.9781, -0.9516, -0.9265, -0.9023, -0.8787, -0.8567, -0.8365,
0:         -0.8199], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 16, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2236,     nan,     nan, -0.2213,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2054,     nan,     nan,     nan,     nan,
0:         -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2122,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2009,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2031, -0.1805,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan, -0.1033,
0:             nan,     nan,     nan,     nan, -0.2236,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2349, -0.2304,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2122,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1578,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0988,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 16, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2461, -0.2522, -0.2570, -0.2627, -0.2672, -0.2720, -0.2771, -0.2838,
0:         -0.2902, -0.2957, -0.2994, -0.3016, -0.2995, -0.2952, -0.2865, -0.2739,
0:         -0.2574, -0.2359, -0.1918, -0.1960, -0.2033, -0.2126, -0.2224, -0.2325,
0:         -0.2438], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5608, 0.5643, 0.5703, 0.5784, 0.5888, 0.6004, 0.6170, 0.6339, 0.6536,
0:         0.6773, 0.7022, 0.7255, 0.7485, 0.7638, 0.7734, 0.7796, 0.7850, 0.7927,
0:         0.5733, 0.5724, 0.5762, 0.5837, 0.5965, 0.6122, 0.6284],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6438, -0.6403, -0.6403, -0.6381, -0.6396, -0.6393, -0.6421, -0.6407,
0:         -0.6408, -0.6387, -0.6377, -0.6354, -0.6354, -0.6379, -0.6430, -0.6512,
0:         -0.6609, -0.6705, -0.6465, -0.6406, -0.6356, -0.6303, -0.6273, -0.6243,
0:         -0.6225], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6349, 0.6358, 0.6207, 0.5992, 0.5959, 0.6076, 0.5955, 0.5780, 0.5711,
0:         0.5480, 0.5263, 0.5338, 0.5386, 0.5237, 0.5239, 0.5384, 0.5689, 0.5940,
0:         0.5680, 0.5546, 0.5285, 0.5037, 0.5073, 0.5265, 0.4990],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5380, -0.5396, -0.5435, -0.5505, -0.5558, -0.5602, -0.5633, -0.5650,
0:         -0.5617, -0.5533, -0.5403, -0.5246, -0.5065, -0.4902, -0.4735, -0.4577,
0:         -0.4418, -0.4281, -0.4190, -0.4155, -0.4194, -0.4279, -0.4397, -0.4513,
0:         -0.4609], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1667, -0.1671, -0.1682, -0.1626, -0.1626, -0.1616, -0.1621, -0.1586,
0:         -0.1579, -0.1672, -0.1611, -0.1608, -0.1612, -0.1604, -0.1568, -0.1623,
0:         -0.1629, -0.1591, -0.1614, -0.1570, -0.1539, -0.1538, -0.1531, -0.1526,
0:         -0.1577], device='cuda:0', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 16, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.8708, 0.8867, 0.9018, 0.9173, 0.9318, 0.9446, 0.9578, 0.9707, 0.9838,
1:         0.9971, 1.0110, 1.0259, 1.0420, 1.0593, 1.0769, 1.0936, 1.1097, 1.1245,
1:         0.8331, 0.8489, 0.8646, 0.8812, 0.8963, 0.9099, 0.9231],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.4535, 0.4433, 0.4325, 0.4208, 0.4085, 0.3954, 0.3825, 0.3705, 0.3592,
1:         0.3486, 0.3400, 0.3342, 0.3307, 0.3285, 0.3281, 0.3277, 0.3273, 0.3279,
1:         0.4769, 0.4669, 0.4568, 0.4464, 0.4351, 0.4228, 0.4108],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.6689, -0.6689, -0.6691, -0.6691, -0.6690, -0.6691, -0.6690, -0.6691,
1:         -0.6691, -0.6690, -0.6691, -0.6687, -0.6688, -0.6688, -0.6693, -0.6702,
1:         -0.6711, -0.6718, -0.6658, -0.6656, -0.6654, -0.6650, -0.6647, -0.6645,
1:         -0.6642], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.4998, 0.5824, 0.6041, 0.6650, 0.7475, 0.7975, 0.8475, 0.8475, 0.8605,
1:         0.8779, 0.8562, 0.8388, 0.8279, 0.8605, 0.8736, 0.8453, 0.7953, 0.6780,
1:         0.4390, 0.4933, 0.4825, 0.5368, 0.6150, 0.6389, 0.6780],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-1.2571, -1.2533, -1.2483, -1.2411, -1.2323, -1.2198, -1.2076, -1.1938,
1:         -1.1778, -1.1622, -1.1457, -1.1281, -1.1111, -1.0895, -1.0643, -1.0361,
1:         -1.0065, -0.9781, -0.9516, -0.9265, -0.9023, -0.8787, -0.8567, -0.8365,
1:         -0.8199], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 16, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2372,     nan,
1:         -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2395,     nan,     nan, -0.1850, -0.2009,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2417,     nan, -0.2077,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2236,     nan, -0.2349, -0.2326,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2326,     nan,     nan,     nan,     nan,     nan, -0.2213,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1850, -0.2417,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2417,     nan,     nan,     nan,     nan,     nan, -0.1373,     nan,
1:         -0.2213, -0.2417,     nan,     nan,     nan, -0.2145,     nan, -0.1555,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1600,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.0829,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1646,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1827,     nan,     nan,
1:             nan,     nan, -0.2349,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 16, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.2277, -0.2345, -0.2413, -0.2492, -0.2583, -0.2690, -0.2772, -0.2872,
1:         -0.2927, -0.2967, -0.2988, -0.2979, -0.2912, -0.2806, -0.2620, -0.2415,
1:         -0.2164, -0.1891, -0.1953, -0.1935, -0.1994, -0.2089, -0.2224, -0.2358,
1:         -0.2509], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.5736, 0.5739, 0.5761, 0.5817, 0.5874, 0.6009, 0.6127, 0.6263, 0.6444,
1:         0.6653, 0.6880, 0.7142, 0.7392, 0.7610, 0.7761, 0.7870, 0.7929, 0.7961,
1:         0.5922, 0.5883, 0.5884, 0.5955, 0.6089, 0.6231, 0.6383],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6358, -0.6355, -0.6383, -0.6394, -0.6437, -0.6477, -0.6519, -0.6504,
1:         -0.6492, -0.6493, -0.6492, -0.6495, -0.6490, -0.6523, -0.6556, -0.6581,
1:         -0.6625, -0.6641, -0.6389, -0.6342, -0.6319, -0.6304, -0.6298, -0.6286,
1:         -0.6278], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.6467, 0.6425, 0.6271, 0.6064, 0.6057, 0.5978, 0.5688, 0.5585, 0.5708,
1:         0.5834, 0.5745, 0.5658, 0.5589, 0.5342, 0.5239, 0.5349, 0.5537, 0.5481,
1:         0.6121, 0.5908, 0.5701, 0.5373, 0.5373, 0.5362, 0.4973],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.5194, -0.5249, -0.5366, -0.5487, -0.5564, -0.5591, -0.5578, -0.5551,
1:         -0.5495, -0.5413, -0.5295, -0.5141, -0.4962, -0.4811, -0.4667, -0.4553,
1:         -0.4445, -0.4352, -0.4271, -0.4222, -0.4215, -0.4249, -0.4311, -0.4398,
1:         -0.4507], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1422, -0.1477, -0.1557, -0.1547, -0.1610, -0.1634, -0.1652, -0.1627,
1:         -0.1668, -0.1514, -0.1493, -0.1550, -0.1580, -0.1611, -0.1607, -0.1691,
1:         -0.1730, -0.1655, -0.1495, -0.1493, -0.1513, -0.1505, -0.1566, -0.1568,
1:         -0.1623], device='cuda:1', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 16, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 16, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.8708, 0.8867, 0.9018, 0.9173, 0.9318, 0.9446, 0.9578, 0.9707, 0.9838,
3:         0.9971, 1.0110, 1.0259, 1.0420, 1.0593, 1.0769, 1.0936, 1.1097, 1.1245,
3:         0.8331, 0.8489, 0.8646, 0.8812, 0.8963, 0.9099, 0.9231],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.4535, 0.4433, 0.4325, 0.4208, 0.4085, 0.3954, 0.3825, 0.3705, 0.3592,
3:         0.3486, 0.3400, 0.3342, 0.3307, 0.3285, 0.3281, 0.3277, 0.3273, 0.3279,
3:         0.4769, 0.4669, 0.4568, 0.4464, 0.4351, 0.4228, 0.4108],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.6689, -0.6689, -0.6691, -0.6691, -0.6690, -0.6691, -0.6690, -0.6691,
3:         -0.6691, -0.6690, -0.6691, -0.6687, -0.6688, -0.6688, -0.6693, -0.6702,
3:         -0.6711, -0.6718, -0.6658, -0.6656, -0.6654, -0.6650, -0.6647, -0.6645,
3:         -0.6642], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.4998, 0.5824, 0.6041, 0.6650, 0.7475, 0.7975, 0.8475, 0.8475, 0.8605,
3:         0.8779, 0.8562, 0.8388, 0.8279, 0.8605, 0.8736, 0.8453, 0.7953, 0.6780,
3:         0.4390, 0.4933, 0.4825, 0.5368, 0.6150, 0.6389, 0.6780],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-1.2571, -1.2533, -1.2483, -1.2411, -1.2323, -1.2198, -1.2076, -1.1938,
3:         -1.1778, -1.1622, -1.1457, -1.1281, -1.1111, -1.0895, -1.0643, -1.0361,
3:         -1.0065, -0.9781, -0.9516, -0.9265, -0.9023, -0.8787, -0.8567, -0.8365,
3:         -0.8199], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 16, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2372,
3:             nan,     nan,     nan, -0.2009,     nan,     nan, -0.2372,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2213,     nan,     nan, -0.2145,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2417,     nan, -0.2395,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2372,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2326,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.1918,     nan, -0.1759,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.1986, -0.2417,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.1510,     nan,
3:             nan,     nan, -0.1737,     nan,     nan,     nan, -0.1737,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1464,
3:         -0.0988,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 16, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.2237, -0.2316, -0.2395, -0.2478, -0.2563, -0.2646, -0.2751, -0.2870,
3:         -0.2959, -0.3021, -0.3040, -0.3024, -0.2912, -0.2778, -0.2577, -0.2368,
3:         -0.2152, -0.1908, -0.1767, -0.1814, -0.1908, -0.2029, -0.2155, -0.2280,
3:         -0.2443], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.5905, 0.5866, 0.5872, 0.5952, 0.6031, 0.6151, 0.6276, 0.6430, 0.6616,
3:         0.6842, 0.7080, 0.7316, 0.7524, 0.7716, 0.7886, 0.8023, 0.8122, 0.8172,
3:         0.5963, 0.5892, 0.5892, 0.5982, 0.6120, 0.6266, 0.6392],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6440, -0.6447, -0.6464, -0.6470, -0.6498, -0.6509, -0.6524, -0.6516,
3:         -0.6522, -0.6532, -0.6532, -0.6543, -0.6549, -0.6573, -0.6626, -0.6677,
3:         -0.6738, -0.6780, -0.6472, -0.6424, -0.6394, -0.6371, -0.6347, -0.6326,
3:         -0.6296], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.6793, 0.6835, 0.6726, 0.6753, 0.7025, 0.7232, 0.7141, 0.6793, 0.6678,
3:         0.6616, 0.6361, 0.6511, 0.6768, 0.6661, 0.6727, 0.6917, 0.7129, 0.7123,
3:         0.5831, 0.5780, 0.5616, 0.5512, 0.5651, 0.5869, 0.5824],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.5293, -0.5273, -0.5318, -0.5396, -0.5442, -0.5437, -0.5414, -0.5398,
3:         -0.5367, -0.5326, -0.5256, -0.5137, -0.4961, -0.4790, -0.4623, -0.4495,
3:         -0.4381, -0.4291, -0.4224, -0.4190, -0.4204, -0.4253, -0.4339, -0.4423,
3:         -0.4516], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1451, -0.1528, -0.1585, -0.1591, -0.1604, -0.1630, -0.1659, -0.1577,
3:         -0.1613, -0.1463, -0.1469, -0.1511, -0.1545, -0.1604, -0.1593, -0.1631,
3:         -0.1612, -0.1527, -0.1429, -0.1430, -0.1446, -0.1460, -0.1462, -0.1496,
3:         -0.1533], device='cuda:3', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([0.8708, 0.8867, 0.9018, 0.9173, 0.9318, 0.9446, 0.9578, 0.9707, 0.9838,
2:         0.9971, 1.0110, 1.0259, 1.0420, 1.0593, 1.0769, 1.0936, 1.1097, 1.1245,
2:         0.8331, 0.8489, 0.8646, 0.8812, 0.8963, 0.9099, 0.9231],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.4535, 0.4433, 0.4325, 0.4208, 0.4085, 0.3954, 0.3825, 0.3705, 0.3592,
2:         0.3486, 0.3400, 0.3342, 0.3307, 0.3285, 0.3281, 0.3277, 0.3273, 0.3279,
2:         0.4769, 0.4669, 0.4568, 0.4464, 0.4351, 0.4228, 0.4108],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.6689, -0.6689, -0.6691, -0.6691, -0.6690, -0.6691, -0.6690, -0.6691,
2:         -0.6691, -0.6690, -0.6691, -0.6687, -0.6688, -0.6688, -0.6693, -0.6702,
2:         -0.6711, -0.6718, -0.6658, -0.6656, -0.6654, -0.6650, -0.6647, -0.6645,
2:         -0.6642], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.4998, 0.5824, 0.6041, 0.6650, 0.7475, 0.7975, 0.8475, 0.8475, 0.8605,
2:         0.8779, 0.8562, 0.8388, 0.8279, 0.8605, 0.8736, 0.8453, 0.7953, 0.6780,
2:         0.4390, 0.4933, 0.4825, 0.5368, 0.6150, 0.6389, 0.6780],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-1.2571, -1.2533, -1.2483, -1.2411, -1.2323, -1.2198, -1.2076, -1.1938,
2:         -1.1778, -1.1622, -1.1457, -1.1281, -1.1111, -1.0895, -1.0643, -1.0361,
2:         -1.0065, -0.9781, -0.9516, -0.9265, -0.9023, -0.8787, -0.8567, -0.8365,
2:         -0.8199], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 16, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([-0.2258,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1941, -0.2009,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2009,
2:         -0.2236,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2395,     nan, -0.2417,     nan, -0.2122,     nan,
2:             nan,     nan, -0.2258,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2281,
2:         -0.2326,     nan, -0.2304,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.1873,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.1963,     nan,     nan, -0.1668,     nan, -0.1850,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.1986, -0.2417,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2145,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.1963,     nan,     nan,     nan,
2:         -0.2145,     nan, -0.2168,     nan,     nan,     nan,     nan,     nan,
2:         -0.1419,     nan,     nan,     nan,     nan, -0.2122,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1986,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1827,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 16, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.2517, -0.2533, -0.2534, -0.2540, -0.2580, -0.2621, -0.2694, -0.2791,
2:         -0.2864, -0.2920, -0.2932, -0.2916, -0.2823, -0.2695, -0.2522, -0.2329,
2:         -0.2131, -0.1921, -0.1872, -0.1895, -0.1936, -0.2025, -0.2131, -0.2254,
2:         -0.2388], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.5642, 0.5647, 0.5681, 0.5762, 0.5846, 0.5985, 0.6135, 0.6305, 0.6500,
2:         0.6729, 0.6938, 0.7155, 0.7360, 0.7535, 0.7642, 0.7738, 0.7774, 0.7805,
2:         0.5685, 0.5658, 0.5708, 0.5815, 0.5981, 0.6152, 0.6326],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6427, -0.6436, -0.6452, -0.6465, -0.6466, -0.6468, -0.6473, -0.6467,
2:         -0.6479, -0.6506, -0.6530, -0.6555, -0.6575, -0.6613, -0.6672, -0.6737,
2:         -0.6833, -0.6890, -0.6369, -0.6340, -0.6334, -0.6333, -0.6330, -0.6315,
2:         -0.6306], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.6411, 0.6273, 0.6268, 0.6332, 0.6362, 0.6291, 0.6145, 0.5937, 0.5815,
2:         0.5590, 0.5163, 0.5015, 0.5047, 0.4799, 0.4655, 0.4865, 0.5126, 0.5203,
2:         0.5424, 0.5299, 0.5243, 0.5049, 0.5040, 0.5115, 0.4997],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.5241, -0.5222, -0.5293, -0.5409, -0.5522, -0.5576, -0.5586, -0.5557,
2:         -0.5490, -0.5398, -0.5271, -0.5105, -0.4904, -0.4717, -0.4546, -0.4419,
2:         -0.4313, -0.4235, -0.4184, -0.4175, -0.4216, -0.4298, -0.4405, -0.4510,
2:         -0.4626], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1542, -0.1590, -0.1645, -0.1648, -0.1640, -0.1663, -0.1662, -0.1603,
2:         -0.1594, -0.1600, -0.1562, -0.1656, -0.1692, -0.1688, -0.1639, -0.1677,
2:         -0.1649, -0.1582, -0.1581, -0.1598, -0.1612, -0.1626, -0.1630, -0.1629,
2:         -0.1638], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 16 [1/5 (20%)]	Loss: nan : nan :: 0.14278 (1.71 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 16 [2/5 (40%)]	Loss: nan : nan :: 0.13909 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 16 [3/5 (60%)]	Loss: nan : nan :: 0.13728 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 16 [4/5 (80%)]	Loss: nan : nan :: 0.14100 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch16.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch16.mod
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 16 : nan
0: validation loss for velocity_u : 0.03307100757956505
0: validation loss for velocity_v : 0.06301262974739075
0: validation loss for specific_humidity : 0.023909205570816994
0: validation loss for velocity_z : 0.4263980984687805
0: validation loss for temperature : 0.06732214242219925
0: validation loss for total_precip : nan
2: 17 : 00:09:07 :: batch_size = 96, lr = 1.3809311136779726e-05
1: 17 : 00:09:07 :: batch_size = 96, lr = 1.3809311136779726e-05
0: 17 : 00:09:07 :: batch_size = 96, lr = 1.3809311136779726e-05
3: 17 : 00:09:07 :: batch_size = 96, lr = 1.3809311136779726e-05
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 17, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] INPUT BATCH
0: Epoch 17, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: Created sparse mask for total_precip with 10.0% data retained
1:      first 25 values: tensor([-0.3792, -0.4140, -0.4469, -0.4752, -0.4935, -0.5101, -0.5337, -0.5674,
1:         -0.6126, -0.6626, -0.7077, -0.7282, -0.7254, -0.7233, -0.7478, -0.7867,
1:         -0.8104, -0.8137, -0.3326, -0.3654, -0.4004, -0.4373, -0.4678, -0.4955,
1:         -0.5244], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0996, -0.1315, -0.1728, -0.2260, -0.2826, -0.3380, -0.3899, -0.4289,
1:         -0.4453, -0.4201, -0.3778, -0.3235, -0.2464, -0.1849, -0.1751, -0.2179,
1:         -0.2494, -0.2224, -0.1581, -0.1660, -0.1830, -0.2171, -0.2658, -0.3205,
1:         -0.3784], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.5490, 1.1985, 1.2200, 1.3711, 1.4794, 1.4841, 1.4649, 1.4666, 1.5624,
1:         1.7796, 1.9257, 1.9529, 1.9658, 2.1934, 2.5966, 2.7969, 2.6616, 2.3968,
1:         1.5097, 1.1769, 1.0018, 1.1139, 1.2771, 1.3792, 1.3620],
1:        device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.1461, -0.0224, -0.0710, -0.1332, -0.1128, -0.2270, -0.2202, -0.0902,
1:          0.0862,  0.1382,  0.1766,  0.1461,  0.0624,  0.2569,  0.0308,  0.0432,
1:         -0.0303, -1.1961, -0.0846, -0.1490, -0.2180, -0.2496, -0.1965, -0.2632,
1:         -0.2146], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.6406, -0.3713, -0.2839, -0.3173, -0.3631, -0.3416, -0.2973, -0.2965,
1:         -0.4339, -0.7437, -1.0723, -1.3256, -1.4038, -1.3650, -1.3154, -1.1310,
1:         -0.9226, -0.9983, -1.2659, -1.3679, -1.2608, -0.8799, -0.7143, -0.7163,
1:         -0.4568], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 17, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:          0.1681,     nan,     nan,     nan,     nan,     nan,     nan,  0.6526,
1:             nan,     nan,  0.2113,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  0.3596,     nan,     nan,     nan, -0.2008, -0.1541,
1:             nan,     nan,     nan,     nan,     nan,  0.6946,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1063,     nan,  0.5790,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,  0.6398,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2055,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,  0.3502, -0.0397, -0.0923,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1892,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1343,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.1191, -0.1728,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2265,     nan,     nan,     nan,
1:             nan,  0.0583,     nan,     nan,     nan,     nan, -0.2382,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1133, -0.1495,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2113,     nan, -0.1320,
1:             nan,     nan,     nan,  0.0058,     nan,     nan,     nan, -0.2405,
1:             nan,     nan, -0.0059, -0.0129, -0.0584,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2324,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2335,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 17, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.4520, -0.4688, -0.4904, -0.5188, -0.5557, -0.5965, -0.6303, -0.6560,
1:         -0.6686, -0.6716, -0.6670, -0.6673, -0.6678, -0.6688, -0.6704, -0.6661,
1:         -0.6533, -0.6357, -0.3851, -0.4045, -0.4295, -0.4627, -0.5024, -0.5457,
1:         -0.5855], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1447, -0.1332, -0.1358, -0.1537, -0.1845, -0.2230, -0.2677, -0.3170,
1:         -0.3650, -0.4057, -0.4344, -0.4514, -0.4611, -0.4749, -0.5005, -0.5374,
1:         -0.5764, -0.6051, -0.1737, -0.1583, -0.1527, -0.1590, -0.1741, -0.2035,
1:         -0.2433], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([3.1524, 3.2019, 3.2253, 3.2126, 3.1647, 3.1180, 3.0629, 3.0166, 2.9960,
1:         2.9888, 3.0090, 3.0396, 3.0559, 3.0692, 3.0508, 3.0102, 2.9610, 2.9153,
1:         2.9499, 3.0373, 3.0892, 3.1071, 3.1023, 3.0852, 3.0522],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.3078, -1.4400, -1.6128, -1.7717, -1.5671, -0.9843, -0.3819, -0.1071,
1:         -0.0018,  0.0548, -0.0434, -0.1171, -0.0972, -0.1141, -0.0476,  0.2165,
1:          0.3068,  0.0553, -0.7795, -0.9345, -1.3669, -1.6739, -1.4947, -1.0751,
1:         -0.6286], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-1.4059, -1.4539, -1.5161, -1.5971, -1.6907, -1.7880, -1.8693, -1.9242,
1:         -1.9548, -1.9755, -2.0058, -2.0544, -2.1090, -2.1455, -2.1518, -2.1248,
1:         -2.0838, -2.0465, -2.0173, -1.9839, -1.9336, -1.8539, -1.7495, -1.6292,
1:         -1.5013], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 values: tensor([-0.3792, -0.4140, -0.4469, -0.4752, -0.4935, -0.5101, -0.5337, -0.5674,
0:         -0.6126, -0.6626, -0.7077, -0.7282, -0.7254, -0.7233, -0.7478, -0.7867,
0:         -0.8104, -0.8137, -0.3326, -0.3654, -0.4004, -0.4373, -0.4678, -0.4955,
0:         -0.5244], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 pred values: tensor([-0.1611, -0.1668, -0.1734, -0.1755, -0.1754, -0.1771, -0.1763, -0.1715,
1:         -0.1704, -0.1647, -0.1628, -0.1712, -0.1746, -0.1763, -0.1738, -0.1757,
1:         -0.1715, -0.1673, -0.1617, -0.1660, -0.1649, -0.1694, -0.1693, -0.1683,
1:         -0.1699], device='cuda:1', grad_fn=<SliceBackward0>)
0:      first 25 values: tensor([-0.0996, -0.1315, -0.1728, -0.2260, -0.2826, -0.3380, -0.3899, -0.4289,
0:         -0.4453, -0.4201, -0.3778, -0.3235, -0.2464, -0.1849, -0.1751, -0.2179,
0:         -0.2494, -0.2224, -0.1581, -0.1660, -0.1830, -0.2171, -0.2658, -0.3205,
0:         -0.3784], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.5490, 1.1985, 1.2200, 1.3711, 1.4794, 1.4841, 1.4649, 1.4666, 1.5624,
0:         1.7796, 1.9257, 1.9529, 1.9658, 2.1934, 2.5966, 2.7969, 2.6616, 2.3968,
0:         1.5097, 1.1769, 1.0018, 1.1139, 1.2771, 1.3792, 1.3620],
0:        device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1461, -0.0224, -0.0710, -0.1332, -0.1128, -0.2270, -0.2202, -0.0902,
0:          0.0862,  0.1382,  0.1766,  0.1461,  0.0624,  0.2569,  0.0308,  0.0432,
0:         -0.0303, -1.1961, -0.0846, -0.1490, -0.2180, -0.2496, -0.1965, -0.2632,
0:         -0.2146], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.6406, -0.3713, -0.2839, -0.3173, -0.3631, -0.3416, -0.2973, -0.2965,
0:         -0.4339, -0.7437, -1.0723, -1.3256, -1.4038, -1.3650, -1.3154, -1.1310,
0:         -0.9226, -0.9983, -1.2659, -1.3679, -1.2608, -0.8799, -0.7143, -0.7163,
0:         -0.4568], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 17, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,  0.0490,     nan,     nan,     nan,     nan,     nan, -0.0339,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1681,     nan,     nan,     nan,     nan,     nan,  0.9888,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  1.1710, -0.2008,     nan,
0:             nan,     nan,  0.7693,     nan, -0.0736,     nan,     nan,     nan,
0:             nan,  0.0128,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1576,
0:             nan,     nan,     nan,     nan,  0.9643,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1833,     nan,     nan, -0.0175,     nan,     nan,     nan,     nan,
0:             nan,  1.0017,     nan, -0.1249,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.0315,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2218,     nan,
0:          0.1728,     nan,     nan,     nan,     nan,     nan,     nan, -0.2487,
0:             nan,     nan,     nan,     nan,     nan, -0.1985,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1588, -0.1822, -0.0759,     nan,     nan,
0:         -0.0245,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.0735,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1588,     nan, -0.0386,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1063,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2394,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1798,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 17, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4526, -0.4683, -0.4877, -0.5122, -0.5435, -0.5802, -0.6142, -0.6461,
0:         -0.6645, -0.6741, -0.6762, -0.6781, -0.6807, -0.6792, -0.6758, -0.6674,
0:         -0.6503, -0.6285, -0.3923, -0.4120, -0.4353, -0.4627, -0.4977, -0.5366,
0:         -0.5750], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1573, -0.1472, -0.1462, -0.1564, -0.1794, -0.2180, -0.2663, -0.3191,
0:         -0.3709, -0.4120, -0.4408, -0.4595, -0.4701, -0.4874, -0.5153, -0.5520,
0:         -0.5863, -0.6055, -0.2004, -0.1867, -0.1763, -0.1729, -0.1819, -0.2061,
0:         -0.2500], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.9641, 3.0565, 3.1072, 3.1191, 3.0944, 3.0552, 2.9970, 2.9474, 2.9216,
0:         2.9195, 2.9477, 2.9938, 3.0229, 3.0418, 3.0203, 2.9736, 2.9132, 2.8576,
0:         2.8114, 2.9227, 3.0005, 3.0381, 3.0478, 3.0309, 2.9977],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0575, -1.2224, -1.4672, -1.5645, -1.2655, -0.6570, -0.0348,  0.1862,
0:          0.1727,  0.1428, -0.0110, -0.1348, -0.1448, -0.1614, -0.0968,  0.1382,
0:          0.2322,  0.0496, -0.4624, -0.7598, -1.2948, -1.5078, -1.2076, -0.7396,
0:         -0.2435], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.4114, -1.4514, -1.5101, -1.5882, -1.6810, -1.7731, -1.8510, -1.9017,
0:         -1.9270, -1.9441, -1.9755, -2.0289, -2.0930, -2.1388, -2.1465, -2.1155,
0:         -2.0672, -2.0280, -2.0075, -1.9939, -1.9635, -1.8967, -1.7931, -1.6609,
0:         -1.5149], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1562, -0.1590, -0.1633, -0.1611, -0.1589, -0.1622, -0.1669, -0.1622,
0:         -0.1643, -0.1617, -0.1599, -0.1613, -0.1642, -0.1644, -0.1626, -0.1703,
0:         -0.1644, -0.1603, -0.1620, -0.1626, -0.1604, -0.1612, -0.1603, -0.1584,
0:         -0.1617], device='cuda:0', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 17, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 17, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.3792, -0.4140, -0.4469, -0.4752, -0.4935, -0.5101, -0.5337, -0.5674,
2:         -0.6126, -0.6626, -0.7077, -0.7282, -0.7254, -0.7233, -0.7478, -0.7867,
2:         -0.8104, -0.8137, -0.3326, -0.3654, -0.4004, -0.4373, -0.4678, -0.4955,
2:         -0.5244], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.0996, -0.1315, -0.1728, -0.2260, -0.2826, -0.3380, -0.3899, -0.4289,
2:         -0.4453, -0.4201, -0.3778, -0.3235, -0.2464, -0.1849, -0.1751, -0.2179,
2:         -0.2494, -0.2224, -0.1581, -0.1660, -0.1830, -0.2171, -0.2658, -0.3205,
2:         -0.3784], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.5490, 1.1985, 1.2200, 1.3711, 1.4794, 1.4841, 1.4649, 1.4666, 1.5624,
2:         1.7796, 1.9257, 1.9529, 1.9658, 2.1934, 2.5966, 2.7969, 2.6616, 2.3968,
2:         1.5097, 1.1769, 1.0018, 1.1139, 1.2771, 1.3792, 1.3620],
2:        device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.1461, -0.0224, -0.0710, -0.1332, -0.1128, -0.2270, -0.2202, -0.0902,
2:          0.0862,  0.1382,  0.1766,  0.1461,  0.0624,  0.2569,  0.0308,  0.0432,
2:         -0.0303, -1.1961, -0.0846, -0.1490, -0.2180, -0.2496, -0.1965, -0.2632,
2:         -0.2146], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.6406, -0.3713, -0.2839, -0.3173, -0.3631, -0.3416, -0.2973, -0.2965,
2:         -0.4339, -0.7437, -1.0723, -1.3256, -1.4038, -1.3650, -1.3154, -1.1310,
2:         -0.9226, -0.9983, -1.2659, -1.3679, -1.2608, -0.8799, -0.7143, -0.7163,
2:         -0.4568], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 17, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:          0.0303,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,  0.0046,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1541,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3514,
2:             nan,  0.0128,     nan,     nan,     nan, -0.1063,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,  0.5814,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2265,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.0339,     nan,     nan,     nan,
2:             nan, -0.2347, -0.2347,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2487,
2:             nan,     nan,  0.0268,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1588,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2148,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2405,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1588,     nan, -0.0386,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2359,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2312,     nan,     nan,
2:         -0.2475,     nan,     nan,     nan,     nan, -0.2312, -0.2335,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 17, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.4196, -0.4437, -0.4686, -0.4993, -0.5345, -0.5717, -0.6039, -0.6325,
2:         -0.6511, -0.6600, -0.6616, -0.6646, -0.6651, -0.6615, -0.6577, -0.6459,
2:         -0.6317, -0.6136, -0.3539, -0.3806, -0.4118, -0.4464, -0.4867, -0.5285,
2:         -0.5685], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1214, -0.1089, -0.1104, -0.1278, -0.1595, -0.2060, -0.2590, -0.3149,
2:         -0.3651, -0.4059, -0.4338, -0.4530, -0.4672, -0.4882, -0.5216, -0.5591,
2:         -0.5948, -0.6170, -0.1671, -0.1537, -0.1495, -0.1556, -0.1723, -0.2055,
2:         -0.2512], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([3.0113, 3.0603, 3.0939, 3.1037, 3.0967, 3.0863, 3.0587, 3.0262, 3.0087,
2:         3.0113, 3.0389, 3.0855, 3.1076, 3.1147, 3.0715, 3.0070, 2.9428, 2.8929,
2:         2.8347, 2.9062, 2.9589, 2.9991, 3.0339, 3.0537, 3.0514],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.0454, -1.2312, -1.5542, -1.8768, -1.8325, -1.2602, -0.5152, -0.1071,
2:          0.0341,  0.1066,  0.0527, -0.0322, -0.0682, -0.1441, -0.1194,  0.1470,
2:          0.2874,  0.1142, -0.5849, -0.8109, -1.3835, -1.8477, -1.8081, -1.3308,
2:         -0.7079], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-1.3327, -1.3835, -1.4424, -1.5131, -1.5974, -1.6851, -1.7656, -1.8275,
2:         -1.8657, -1.8914, -1.9234, -1.9690, -2.0232, -2.0646, -2.0782, -2.0595,
2:         -2.0244, -1.9916, -1.9710, -1.9539, -1.9218, -1.8579, -1.7606, -1.6326,
2:         -1.4874], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1556, -0.1582, -0.1629, -0.1639, -0.1614, -0.1606, -0.1640, -0.1594,
2:         -0.1608, -0.1621, -0.1580, -0.1644, -0.1638, -0.1643, -0.1611, -0.1661,
2:         -0.1631, -0.1598, -0.1568, -0.1588, -0.1575, -0.1595, -0.1609, -0.1590,
2:         -0.1601], device='cuda:2', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([-0.3792, -0.4140, -0.4469, -0.4752, -0.4935, -0.5101, -0.5337, -0.5674,
3:         -0.6126, -0.6626, -0.7077, -0.7282, -0.7254, -0.7233, -0.7478, -0.7867,
3:         -0.8104, -0.8137, -0.3326, -0.3654, -0.4004, -0.4373, -0.4678, -0.4955,
3:         -0.5244], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.0996, -0.1315, -0.1728, -0.2260, -0.2826, -0.3380, -0.3899, -0.4289,
3:         -0.4453, -0.4201, -0.3778, -0.3235, -0.2464, -0.1849, -0.1751, -0.2179,
3:         -0.2494, -0.2224, -0.1581, -0.1660, -0.1830, -0.2171, -0.2658, -0.3205,
3:         -0.3784], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.5490, 1.1985, 1.2200, 1.3711, 1.4794, 1.4841, 1.4649, 1.4666, 1.5624,
3:         1.7796, 1.9257, 1.9529, 1.9658, 2.1934, 2.5966, 2.7969, 2.6616, 2.3968,
3:         1.5097, 1.1769, 1.0018, 1.1139, 1.2771, 1.3792, 1.3620],
3:        device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.1461, -0.0224, -0.0710, -0.1332, -0.1128, -0.2270, -0.2202, -0.0902,
3:          0.0862,  0.1382,  0.1766,  0.1461,  0.0624,  0.2569,  0.0308,  0.0432,
3:         -0.0303, -1.1961, -0.0846, -0.1490, -0.2180, -0.2496, -0.1965, -0.2632,
3:         -0.2146], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.6406, -0.3713, -0.2839, -0.3173, -0.3631, -0.3416, -0.2973, -0.2965,
3:         -0.4339, -0.7437, -1.0723, -1.3256, -1.4038, -1.3650, -1.3154, -1.1310,
3:         -0.9226, -0.9983, -1.2659, -1.3679, -1.2608, -0.8799, -0.7143, -0.7163,
3:         -0.4568], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 17, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  0.0116,     nan, -0.0806,  0.2008,
3:             nan,     nan,     nan,  0.0712,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1576,
3:          0.1553,     nan,     nan,  0.3105,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.0397,     nan,
3:             nan,  1.0017,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  0.0992,     nan,  0.4343,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.1378,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2265,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1133,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,  0.7040,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.0059,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2475, -0.2475,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2218, -0.2008,     nan,     nan, -0.2312,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2312,     nan,     nan,
3:             nan,     nan, -0.2347])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 17, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.4498, -0.4644, -0.4864, -0.5150, -0.5509, -0.5913, -0.6211, -0.6469,
3:         -0.6611, -0.6668, -0.6702, -0.6745, -0.6812, -0.6843, -0.6849, -0.6770,
3:         -0.6661, -0.6494, -0.3927, -0.4113, -0.4370, -0.4700, -0.5092, -0.5486,
3:         -0.5844], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.1412, -0.1304, -0.1274, -0.1381, -0.1615, -0.1977, -0.2443, -0.2978,
3:         -0.3494, -0.3939, -0.4266, -0.4468, -0.4593, -0.4770, -0.5064, -0.5408,
3:         -0.5745, -0.5931, -0.1793, -0.1638, -0.1544, -0.1521, -0.1611, -0.1866,
3:         -0.2284], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([3.0890, 3.1312, 3.1591, 3.1558, 3.1320, 3.0992, 3.0468, 3.0003, 2.9798,
3:         2.9901, 3.0382, 3.0996, 3.1465, 3.1754, 3.1554, 3.1082, 3.0563, 3.0122,
3:         2.9246, 3.0016, 3.0517, 3.0781, 3.0938, 3.0789, 3.0423],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.7233, -0.8062, -1.0566, -1.3006, -1.1971, -0.7822, -0.3013, -0.0406,
3:          0.0685,  0.1282,  0.0708,  0.0414,  0.0542, -0.0249,  0.0281,  0.2897,
3:          0.3663,  0.1903, -0.3218, -0.4483, -0.8971, -1.2325, -1.1194, -0.8102,
3:         -0.4379], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-1.3947, -1.4454, -1.5092, -1.5866, -1.6787, -1.7742, -1.8605, -1.9216,
3:         -1.9513, -1.9610, -1.9744, -2.0049, -2.0489, -2.0867, -2.1010, -2.0863,
3:         -2.0610, -2.0438, -2.0407, -2.0344, -2.0009, -1.9219, -1.7989, -1.6463,
3:         -1.4875], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1647, -0.1683, -0.1721, -0.1724, -0.1687, -0.1723, -0.1741, -0.1731,
3:         -0.1741, -0.1698, -0.1663, -0.1712, -0.1724, -0.1727, -0.1710, -0.1761,
3:         -0.1766, -0.1719, -0.1660, -0.1695, -0.1669, -0.1673, -0.1683, -0.1678,
3:         -0.1707], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 17 [1/5 (20%)]	Loss: nan : nan :: 0.16134 (1.77 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 17 [2/5 (40%)]	Loss: nan : nan :: 0.16561 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 17 [3/5 (60%)]	Loss: nan : nan :: 0.16109 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 17 [4/5 (80%)]	Loss: nan : nan :: 0.13824 (10.42 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch17.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch17.mod
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 17 : nan
0: validation loss for velocity_u : 0.03784948214888573
0: validation loss for velocity_v : 0.07083021849393845
0: validation loss for specific_humidity : 0.028242768719792366
0: validation loss for velocity_z : 0.561098039150238
0: validation loss for temperature : 0.07813926786184311
0: validation loss for total_precip : nan
1: 18 : 00:15:44 :: batch_size = 96, lr = 1.3472498670029002e-05
3: 18 : 00:15:44 :: batch_size = 96, lr = 1.3472498670029002e-05
2: 18 : 00:15:44 :: batch_size = 96, lr = 1.3472498670029002e-05
0: 18 : 00:15:44 :: batch_size = 96, lr = 1.3472498670029002e-05
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 18, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 18, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 18, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.0877, 0.0958, 0.0983, 0.1025, 0.1145, 0.1335, 0.1482, 0.1607, 0.1727,
3:         0.1810, 0.1985, 0.2121, 0.2107, 0.2073, 0.2068, 0.2146, 0.2262, 0.2374,
3:         0.1224, 0.1338, 0.1391, 0.1455, 0.1583, 0.1750, 0.1924],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.3268, 0.3492, 0.3655, 0.3687, 0.3633, 0.3662, 0.3703, 0.3635, 0.3519,
3:         0.3411, 0.3374, 0.3326, 0.3216, 0.3111, 0.2935, 0.2696, 0.2464, 0.2232,
3:         0.2806, 0.3200, 0.3527, 0.3701, 0.3778, 0.3902, 0.4055],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.0757, -0.0729, -0.0656, -0.0742, -0.0962, -0.0947, -0.0790, -0.0830,
3:         -0.1202, -0.1592, -0.1949, -0.2374, -0.2566, -0.2824, -0.2871, -0.2891,
3:         -0.2835, -0.2638,  0.4410,  0.4355,  0.4466,  0.4497,  0.4398,  0.4525,
3:          0.4773], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.1650,  0.1728, -0.0464,  0.0420, -0.0251,  0.0386,  0.2399,  0.1359,
3:          0.1493,  0.0263, -0.0486,  0.2388,  0.4703,  0.2220, -0.2566, -0.1828,
3:         -0.2588, -0.1884,  0.2399,  0.2813,  0.0554,  0.0744,  0.0789,  0.0431,
3:          0.2433], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([ 0.3096,  0.4300,  0.5312,  0.5984,  0.6583,  0.7219,  0.7929,  0.8313,
3:          0.8492,  0.8213,  0.7619,  0.7592,  0.7358,  0.6722,  0.5621,  0.4251,
3:          0.3314,  0.2230,  0.1288,  0.0536, -0.0163, -0.0682, -0.1614, -0.1719,
3:         -0.0773], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 18, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan, -0.2409,     nan, -0.2409,     nan,
3:         -0.2409,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,
3:         -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2409, -0.2409,     nan,
3:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2409, -0.2409,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2409, -0.2409,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2409,
3:         -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,
3:             nan, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2409,     nan,     nan, -0.2409,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2409,     nan,     nan, -0.2409,     nan,     nan,     nan,
3:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
3:         -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2409,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2409,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 18, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6473, -0.6469, -0.6399, -0.6346, -0.6282, -0.6258, -0.6262, -0.6339,
3:         -0.6376, -0.6394, -0.6306, -0.6125, -0.5876, -0.5548, -0.5286, -0.5034,
3:         -0.4832, -0.4642, -0.6068, -0.6069, -0.6007, -0.5945, -0.5873, -0.5825,
3:         -0.5801], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.1157, 0.1247, 0.1325, 0.1345, 0.1310, 0.1304, 0.1314, 0.1372, 0.1462,
3:         0.1519, 0.1508, 0.1436, 0.1309, 0.1183, 0.1087, 0.0982, 0.0814, 0.0654,
3:         0.1272, 0.1388, 0.1494, 0.1537, 0.1530, 0.1466, 0.1433],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.4405, 0.4698, 0.4945, 0.5224, 0.5435, 0.5698, 0.5958, 0.6369, 0.6668,
3:         0.6969, 0.7041, 0.6759, 0.6086, 0.5071, 0.3907, 0.2845, 0.2058, 0.1679,
3:         0.4722, 0.4921, 0.5216, 0.5488, 0.5720, 0.5900, 0.6172],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.4441,  0.5111,  0.5355,  0.5180,  0.4448,  0.3171,  0.1731,  0.0258,
3:         -0.0803, -0.1643, -0.2481, -0.2698, -0.2679, -0.2886, -0.2571, -0.1354,
3:          0.0249,  0.1359,  0.4370,  0.5545,  0.6259,  0.6541,  0.6438,  0.5423,
3:          0.4066], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([0.7244, 0.7785, 0.8333, 0.8715, 0.8955, 0.9004, 0.8937, 0.8714, 0.8413,
3:         0.8052, 0.7637, 0.7191, 0.6691, 0.6003, 0.5131, 0.4120, 0.3142, 0.2363,
3:         0.1943, 0.1849, 0.1981, 0.2245, 0.2626, 0.3137, 0.3787],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1533, -0.1583, -0.1637, -0.1635, -0.1639, -0.1651, -0.1648, -0.1595,
3:         -0.1571, -0.1575, -0.1560, -0.1636, -0.1664, -0.1673, -0.1633, -0.1634,
3:         -0.1621, -0.1503, -0.1568, -0.1588, -0.1574, -0.1576, -0.1597, -0.1576,
3:         -0.1589], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([0.0877, 0.0958, 0.0983, 0.1025, 0.1145, 0.1335, 0.1482, 0.1607, 0.1727,
1:         0.1810, 0.1985, 0.2121, 0.2107, 0.2073, 0.2068, 0.2146, 0.2262, 0.2374,
1:         0.1224, 0.1338, 0.1391, 0.1455, 0.1583, 0.1750, 0.1924],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.3268, 0.3492, 0.3655, 0.3687, 0.3633, 0.3662, 0.3703, 0.3635, 0.3519,
1:         0.3411, 0.3374, 0.3326, 0.3216, 0.3111, 0.2935, 0.2696, 0.2464, 0.2232,
1:         0.2806, 0.3200, 0.3527, 0.3701, 0.3778, 0.3902, 0.4055],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0757, -0.0729, -0.0656, -0.0742, -0.0962, -0.0947, -0.0790, -0.0830,
1:         -0.1202, -0.1592, -0.1949, -0.2374, -0.2566, -0.2824, -0.2871, -0.2891,
1:         -0.2835, -0.2638,  0.4410,  0.4355,  0.4466,  0.4497,  0.4398,  0.4525,
1:          0.4773], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.1650,  0.1728, -0.0464,  0.0420, -0.0251,  0.0386,  0.2399,  0.1359,
1:          0.1493,  0.0263, -0.0486,  0.2388,  0.4703,  0.2220, -0.2566, -0.1828,
1:         -0.2588, -0.1884,  0.2399,  0.2813,  0.0554,  0.0744,  0.0789,  0.0431,
1:          0.2433], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([ 0.3096,  0.4300,  0.5312,  0.5984,  0.6583,  0.7219,  0.7929,  0.8313,
1:          0.8492,  0.8213,  0.7619,  0.7592,  0.7358,  0.6722,  0.5621,  0.4251,
1:          0.3314,  0.2230,  0.1288,  0.0536, -0.0163, -0.0682, -0.1614, -0.1719,
1:         -0.0773], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 18, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2409,     nan, -0.2409, -0.2409,     nan, -0.2409,     nan,
1:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2409,     nan, -0.2409,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2409,
1:         -0.2409,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2409,     nan,     nan,     nan,     nan,     nan, -0.2409,
1:             nan,     nan, -0.2409,     nan,     nan, -0.2409,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2409,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2409,     nan,     nan,
1:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2409,     nan,     nan, -0.2409,     nan,     nan,
1:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
1:         -0.2409,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 18, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6597, -0.6543, -0.6479, -0.6446, -0.6413, -0.6391, -0.6366, -0.6370,
1:         -0.6331, -0.6255, -0.6103, -0.5897, -0.5654, -0.5383, -0.5161, -0.4982,
1:         -0.4823, -0.4668, -0.6102, -0.6044, -0.5997, -0.5959, -0.5920, -0.5889,
1:         -0.5843], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.1189, 0.1255, 0.1272, 0.1217, 0.1164, 0.1187, 0.1293, 0.1448, 0.1611,
1:         0.1716, 0.1691, 0.1614, 0.1476, 0.1339, 0.1211, 0.1080, 0.0914, 0.0760,
1:         0.1233, 0.1324, 0.1392, 0.1371, 0.1349, 0.1337, 0.1360],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.4364, 0.4611, 0.4925, 0.5239, 0.5534, 0.5857, 0.6156, 0.6481, 0.6656,
1:         0.6776, 0.6722, 0.6404, 0.5845, 0.4991, 0.4035, 0.3111, 0.2342, 0.1886,
1:         0.4566, 0.4739, 0.5048, 0.5346, 0.5711, 0.6063, 0.6419],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.5926,  0.6567,  0.6156,  0.4893,  0.3774,  0.2502,  0.1262,  0.0120,
1:         -0.0736, -0.1597, -0.2796, -0.3275, -0.3426, -0.3754, -0.3303, -0.2130,
1:         -0.0586,  0.0480,  0.6351,  0.7280,  0.7351,  0.6864,  0.6557,  0.5244,
1:          0.3647], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([0.6933, 0.7446, 0.8091, 0.8671, 0.9071, 0.9168, 0.9047, 0.8759, 0.8413,
1:         0.8068, 0.7723, 0.7381, 0.6996, 0.6449, 0.5721, 0.4765, 0.3694, 0.2694,
1:         0.2013, 0.1787, 0.1974, 0.2476, 0.3143, 0.3866, 0.4666],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1662, -0.1707, -0.1752, -0.1726, -0.1758, -0.1752, -0.1734, -0.1694,
1:         -0.1668, -0.1682, -0.1680, -0.1750, -0.1762, -0.1776, -0.1713, -0.1738,
1:         -0.1708, -0.1626, -0.1658, -0.1671, -0.1657, -0.1672, -0.1678, -0.1660,
1:         -0.1669], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([0.0877, 0.0958, 0.0983, 0.1025, 0.1145, 0.1335, 0.1482, 0.1607, 0.1727,
2:         0.1810, 0.1985, 0.2121, 0.2107, 0.2073, 0.2068, 0.2146, 0.2262, 0.2374,
2:         0.1224, 0.1338, 0.1391, 0.1455, 0.1583, 0.1750, 0.1924],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.3268, 0.3492, 0.3655, 0.3687, 0.3633, 0.3662, 0.3703, 0.3635, 0.3519,
2:         0.3411, 0.3374, 0.3326, 0.3216, 0.3111, 0.2935, 0.2696, 0.2464, 0.2232,
2:         0.2806, 0.3200, 0.3527, 0.3701, 0.3778, 0.3902, 0.4055],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.0757, -0.0729, -0.0656, -0.0742, -0.0962, -0.0947, -0.0790, -0.0830,
2:         -0.1202, -0.1592, -0.1949, -0.2374, -0.2566, -0.2824, -0.2871, -0.2891,
2:         -0.2835, -0.2638,  0.4410,  0.4355,  0.4466,  0.4497,  0.4398,  0.4525,
2:          0.4773], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.1650,  0.1728, -0.0464,  0.0420, -0.0251,  0.0386,  0.2399,  0.1359,
2:          0.1493,  0.0263, -0.0486,  0.2388,  0.4703,  0.2220, -0.2566, -0.1828,
2:         -0.2588, -0.1884,  0.2399,  0.2813,  0.0554,  0.0744,  0.0789,  0.0431,
2:          0.2433], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([ 0.3096,  0.4300,  0.5312,  0.5984,  0.6583,  0.7219,  0.7929,  0.8313,
2:          0.8492,  0.8213,  0.7619,  0.7592,  0.7358,  0.6722,  0.5621,  0.4251,
2:          0.3314,  0.2230,  0.1288,  0.0536, -0.0163, -0.0682, -0.1614, -0.1719,
2:         -0.0773], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 18, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2409, -0.2409,     nan,     nan,     nan, -0.2409,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2409,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2409,     nan,     nan,
2:             nan, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2409,     nan, -0.2409,     nan,     nan,     nan,     nan,
2:             nan, -0.2409, -0.2409,     nan, -0.2409,     nan, -0.2409,     nan,
2:             nan,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2409,     nan,     nan, -0.2409,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2409,     nan, -0.2409,     nan,     nan,
2:             nan,     nan,     nan, -0.2409, -0.2409,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2409,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2409,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2409,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2409,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 18, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6481, -0.6467, -0.6397, -0.6332, -0.6264, -0.6249, -0.6249, -0.6292,
2:         -0.6305, -0.6273, -0.6176, -0.6011, -0.5780, -0.5530, -0.5305, -0.5069,
2:         -0.4883, -0.4658, -0.5989, -0.5980, -0.5917, -0.5841, -0.5782, -0.5754,
2:         -0.5740], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.0916, 0.1134, 0.1262, 0.1288, 0.1232, 0.1191, 0.1220, 0.1302, 0.1436,
2:         0.1547, 0.1573, 0.1538, 0.1453, 0.1357, 0.1273, 0.1175, 0.1014, 0.0896,
2:         0.0990, 0.1261, 0.1440, 0.1477, 0.1434, 0.1346, 0.1294],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.4226, 0.4469, 0.4736, 0.5012, 0.5251, 0.5510, 0.5788, 0.6148, 0.6448,
2:         0.6699, 0.6809, 0.6589, 0.6047, 0.5200, 0.4076, 0.2972, 0.1952, 0.1319,
2:         0.4293, 0.4530, 0.4854, 0.5121, 0.5381, 0.5587, 0.5925],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.6380,  0.6411,  0.5822,  0.4895,  0.3962,  0.2969,  0.1859,  0.0415,
2:         -0.0767, -0.1682, -0.2806, -0.3132, -0.2987, -0.3072, -0.2318, -0.0946,
2:          0.0303,  0.1063,  0.6487,  0.6977,  0.7030,  0.7014,  0.6959,  0.6107,
2:          0.4797], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([0.7321, 0.7920, 0.8524, 0.8926, 0.9068, 0.8897, 0.8574, 0.8163, 0.7780,
2:         0.7466, 0.7172, 0.6855, 0.6456, 0.5879, 0.5142, 0.4244, 0.3284, 0.2369,
2:         0.1688, 0.1348, 0.1366, 0.1753, 0.2402, 0.3171, 0.4050],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1610, -0.1628, -0.1698, -0.1725, -0.1740, -0.1762, -0.1783, -0.1731,
2:         -0.1744, -0.1690, -0.1663, -0.1711, -0.1736, -0.1783, -0.1750, -0.1809,
2:         -0.1774, -0.1747, -0.1689, -0.1714, -0.1695, -0.1732, -0.1744, -0.1735,
2:         -0.1766], device='cuda:2', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 18, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0877, 0.0958, 0.0983, 0.1025, 0.1145, 0.1335, 0.1482, 0.1607, 0.1727,
0:         0.1810, 0.1985, 0.2121, 0.2107, 0.2073, 0.2068, 0.2146, 0.2262, 0.2374,
0:         0.1224, 0.1338, 0.1391, 0.1455, 0.1583, 0.1750, 0.1924],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3268, 0.3492, 0.3655, 0.3687, 0.3633, 0.3662, 0.3703, 0.3635, 0.3519,
0:         0.3411, 0.3374, 0.3326, 0.3216, 0.3111, 0.2935, 0.2696, 0.2464, 0.2232,
0:         0.2806, 0.3200, 0.3527, 0.3701, 0.3778, 0.3902, 0.4055],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0757, -0.0729, -0.0656, -0.0742, -0.0962, -0.0947, -0.0790, -0.0830,
0:         -0.1202, -0.1592, -0.1949, -0.2374, -0.2566, -0.2824, -0.2871, -0.2891,
0:         -0.2835, -0.2638,  0.4410,  0.4355,  0.4466,  0.4497,  0.4398,  0.4525,
0:          0.4773], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1650,  0.1728, -0.0464,  0.0420, -0.0251,  0.0386,  0.2399,  0.1359,
0:          0.1493,  0.0263, -0.0486,  0.2388,  0.4703,  0.2220, -0.2566, -0.1828,
0:         -0.2588, -0.1884,  0.2399,  0.2813,  0.0554,  0.0744,  0.0789,  0.0431,
0:          0.2433], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.3096,  0.4300,  0.5312,  0.5984,  0.6583,  0.7219,  0.7929,  0.8313,
0:          0.8492,  0.8213,  0.7619,  0.7592,  0.7358,  0.6722,  0.5621,  0.4251,
0:          0.3314,  0.2230,  0.1288,  0.0536, -0.0163, -0.0682, -0.1614, -0.1719,
0:         -0.0773], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 18, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2409,     nan, -0.2409,     nan,     nan,     nan,     nan,
0:             nan, -0.2409,     nan, -0.2409, -0.2409,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2409,     nan, -0.2409,     nan,     nan,
0:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,
0:             nan, -0.2409,     nan,     nan,     nan,     nan,     nan, -0.2409,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2409, -0.2409,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2409,     nan,     nan, -0.2409,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2409,     nan,     nan,     nan,     nan, -0.2409,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,
0:         -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2409])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 18, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6523, -0.6456, -0.6365, -0.6315, -0.6280, -0.6289, -0.6336, -0.6419,
0:         -0.6460, -0.6456, -0.6335, -0.6145, -0.5890, -0.5582, -0.5294, -0.5011,
0:         -0.4752, -0.4479, -0.6029, -0.5952, -0.5868, -0.5810, -0.5783, -0.5795,
0:         -0.5824], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1097, 0.1152, 0.1169, 0.1175, 0.1150, 0.1197, 0.1280, 0.1405, 0.1519,
0:         0.1598, 0.1579, 0.1516, 0.1420, 0.1314, 0.1209, 0.1062, 0.0895, 0.0766,
0:         0.1254, 0.1359, 0.1430, 0.1471, 0.1462, 0.1474, 0.1491],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4935, 0.4959, 0.5067, 0.5312, 0.5587, 0.5812, 0.6075, 0.6407, 0.6666,
0:         0.6857, 0.6923, 0.6642, 0.6037, 0.5076, 0.3953, 0.2923, 0.2112, 0.1740,
0:         0.4925, 0.4971, 0.5148, 0.5385, 0.5680, 0.5980, 0.6264],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.5114,  0.5577,  0.5466,  0.4772,  0.3848,  0.2791,  0.1643,  0.0321,
0:         -0.0813, -0.1615, -0.2289, -0.2554, -0.2760, -0.3238, -0.3477, -0.2923,
0:         -0.1285,  0.0321,  0.5569,  0.6210,  0.6398,  0.6256,  0.6099,  0.5164,
0:          0.3850], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.7896, 0.8313, 0.8780, 0.9084, 0.9185, 0.9008, 0.8679, 0.8240, 0.7852,
0:         0.7556, 0.7321, 0.7107, 0.6807, 0.6268, 0.5489, 0.4505, 0.3466, 0.2525,
0:         0.1858, 0.1512, 0.1466, 0.1712, 0.2231, 0.2975, 0.3926],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1481, -0.1494, -0.1588, -0.1590, -0.1613, -0.1628, -0.1656, -0.1586,
0:         -0.1605, -0.1549, -0.1550, -0.1607, -0.1640, -0.1664, -0.1636, -0.1673,
0:         -0.1655, -0.1568, -0.1601, -0.1616, -0.1590, -0.1643, -0.1652, -0.1613,
0:         -0.1651], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 18 [1/5 (20%)]	Loss: nan : nan :: 0.15210 (1.73 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 18 [2/5 (40%)]	Loss: nan : nan :: 0.16107 (10.39 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 18 [3/5 (60%)]	Loss: nan : nan :: 0.13868 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 18 [4/5 (80%)]	Loss: nan : nan :: 0.14196 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch18.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch18.mod
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 18 : nan
0: validation loss for velocity_u : 0.03138778358697891
0: validation loss for velocity_v : 0.05410943925380707
0: validation loss for specific_humidity : 0.02505587972700596
0: validation loss for velocity_z : 0.42116960883140564
0: validation loss for temperature : 0.09353508800268173
0: validation loss for total_precip : nan
1: 19 : 00:22:35 :: batch_size = 96, lr = 1.3143901141491711e-05
3: 19 : 00:22:35 :: batch_size = 96, lr = 1.3143901141491711e-05
0: 19 : 00:22:35 :: batch_size = 96, lr = 1.3143901141491711e-05
2: 19 : 00:22:35 :: batch_size = 96, lr = 1.3143901141491711e-05
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 19, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 19, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.8730, 0.9394, 0.9685, 0.9681, 0.9742, 1.0029, 1.0370, 1.0572, 1.0646,
2:         1.0706, 1.0821, 1.1005, 1.1246, 1.1492, 1.1637, 1.1604, 1.1403, 1.1105,
2:         0.8884, 0.9840, 1.0367, 1.0348, 1.0173, 1.0182, 1.0365],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 6.9541e-01,  6.8055e-01,  6.1305e-01,  5.2201e-01,  4.4336e-01,
2:          3.8226e-01,  3.1909e-01,  2.3879e-01,  1.4507e-01,  5.1150e-02,
2:         -2.9356e-02, -8.5298e-02, -1.0759e-01, -9.2730e-02, -4.9999e-02,
2:         -6.6299e-04,  3.6700e-02,  5.4453e-02,  6.8839e-01,  6.9706e-01,
2:          6.5660e-01,  5.8146e-01,  4.9930e-01,  4.2664e-01,  3.6224e-01],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2412, -0.2453, -0.2327, -0.1975, -0.1667, -0.1385, -0.1174, -0.0912,
2:         -0.0650, -0.0602, -0.0588, -0.0610, -0.0565, -0.0508, -0.0437, -0.0376,
2:         -0.0325, -0.0275, -0.1916, -0.2275, -0.2360, -0.2205, -0.1783, -0.1351,
2:         -0.0857], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.0177,  1.6254,  2.0982,  1.1325, -0.6537, -2.2544, -2.8635, -2.6467,
2:         -2.5249, -2.8501, -3.1810, -3.3632, -3.5588, -3.4694, -2.7864, -1.9470,
2:         -1.4809, -1.3300, -1.7760,  0.5624,  2.3050,  2.5464,  1.0743, -1.1657,
2:         -2.5550], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.0972, 0.1392, 0.1965, 0.2629, 0.3219, 0.3591, 0.3775, 0.3918, 0.4087,
2:         0.4226, 0.4267, 0.4165, 0.3900, 0.3539, 0.3231, 0.3066, 0.3014, 0.2993,
2:         0.2956, 0.2888, 0.2787, 0.2674, 0.2571, 0.2498, 0.2471],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 19, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,  0.1269,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.0064,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.1109,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.0749, -0.1133,     nan,     nan,
2:             nan,     nan,     nan,  0.1797,  0.1593,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.0220,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1841,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1137,
2:             nan,     nan,     nan,     nan, -0.2033,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1505,
2:         -0.0256,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.1385,     nan,
2:             nan,     nan,     nan,     nan, -0.1589,     nan,     nan, -0.1589,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2057,     nan,
2:             nan,     nan,     nan,     nan, -0.0845,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1565,
2:             nan, -0.1949,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1277,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1373,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1217,     nan,     nan,
2:             nan,     nan,     nan, -0.2141,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 19, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.2915, 1.2916, 1.2836, 1.2608, 1.2310, 1.1959, 1.1641, 1.1248, 1.0808,
2:         1.0322, 0.9797, 0.9283, 0.8837, 0.8516, 0.8303, 0.8234, 0.8295, 0.8458,
2:         1.3091, 1.3202, 1.3172, 1.2949, 1.2635, 1.2328, 1.2027],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.7340, -0.7346, -0.7401, -0.7579, -0.7907, -0.8384, -0.8963, -0.9568,
2:         -1.0080, -1.0499, -1.0787, -1.0978, -1.1213, -1.1478, -1.1982, -1.2756,
2:         -1.3758, -1.4832, -0.7540, -0.7599, -0.7749, -0.7929, -0.8205, -0.8649,
2:         -0.9222], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.3248, 0.3229, 0.3183, 0.3177, 0.3146, 0.3107, 0.3021, 0.2936, 0.2815,
2:         0.2718, 0.2590, 0.2467, 0.2287, 0.2054, 0.1816, 0.1544, 0.1240, 0.0957,
2:         0.3429, 0.3320, 0.3224, 0.3195, 0.3167, 0.3111, 0.3089],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.3908e+00, -1.1402e+00, -1.1213e+00, -1.0193e+00, -4.3285e-01,
2:          2.8536e-01,  4.3251e-01,  1.3945e-03, -4.7328e-01, -6.8994e-01,
2:         -5.4521e-01, -2.9517e-01, -9.4469e-02,  5.1035e-02,  1.2369e-01,
2:          1.4759e-01,  7.7156e-02,  1.0559e-01, -3.9624e-01, -3.7850e-01,
2:         -1.1095e+00, -1.6353e+00, -1.1288e+00,  8.2827e-02,  9.1220e-01],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([1.5085, 1.4811, 1.4478, 1.4138, 1.3898, 1.3782, 1.3708, 1.3553, 1.3285,
2:         1.2906, 1.2506, 1.2173, 1.1972, 1.1788, 1.1616, 1.1395, 1.1159, 1.0937,
2:         1.0758, 1.0568, 1.0348, 1.0056, 0.9701, 0.9330, 0.8921],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1775, -0.1788, -0.1832, -0.1822, -0.1821, -0.1834, -0.1847, -0.1811,
2:         -0.1823, -0.1845, -0.1782, -0.1816, -0.1817, -0.1836, -0.1780, -0.1837,
2:         -0.1822, -0.1754, -0.1799, -0.1828, -0.1787, -0.1764, -0.1744, -0.1747,
2:         -0.1757], device='cuda:2', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 19, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.8730, 0.9394, 0.9685, 0.9681, 0.9742, 1.0029, 1.0370, 1.0572, 1.0646,
3:         1.0706, 1.0821, 1.1005, 1.1246, 1.1492, 1.1637, 1.1604, 1.1403, 1.1105,
3:         0.8884, 0.9840, 1.0367, 1.0348, 1.0173, 1.0182, 1.0365],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 6.9541e-01,  6.8055e-01,  6.1305e-01,  5.2201e-01,  4.4336e-01,
3:          3.8226e-01,  3.1909e-01,  2.3879e-01,  1.4507e-01,  5.1150e-02,
3:         -2.9356e-02, -8.5298e-02, -1.0759e-01, -9.2730e-02, -4.9999e-02,
3:         -6.6299e-04,  3.6700e-02,  5.4453e-02,  6.8839e-01,  6.9706e-01,
3:          6.5660e-01,  5.8146e-01,  4.9930e-01,  4.2664e-01,  3.6224e-01],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2412, -0.2453, -0.2327, -0.1975, -0.1667, -0.1385, -0.1174, -0.0912,
3:         -0.0650, -0.0602, -0.0588, -0.0610, -0.0565, -0.0508, -0.0437, -0.0376,
3:         -0.0325, -0.0275, -0.1916, -0.2275, -0.2360, -0.2205, -0.1783, -0.1351,
3:         -0.0857], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.0177,  1.6254,  2.0982,  1.1325, -0.6537, -2.2544, -2.8635, -2.6467,
3:         -2.5249, -2.8501, -3.1810, -3.3632, -3.5588, -3.4694, -2.7864, -1.9470,
3:         -1.4809, -1.3300, -1.7760,  0.5624,  2.3050,  2.5464,  1.0743, -1.1657,
3:         -2.5550], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.0972, 0.1392, 0.1965, 0.2629, 0.3219, 0.3591, 0.3775, 0.3918, 0.4087,
3:         0.4226, 0.4267, 0.4165, 0.3900, 0.3539, 0.3231, 0.3066, 0.3014, 0.2993,
3:         0.2956, 0.2888, 0.2787, 0.2674, 0.2571, 0.2498, 0.2471],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 19, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan, -0.1457,     nan,     nan,     nan, -0.2514,
3:             nan,     nan,  0.1293,     nan, -0.0773, -0.1661,     nan,     nan,
3:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2213,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,  0.1029,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2526,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.0460,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.0268,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.1577,     nan,     nan,     nan,     nan,     nan,  0.0476,     nan,
3:             nan,     nan,     nan, -0.2081,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:          0.0933,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,  0.0776,     nan,     nan,     nan,     nan,     nan,
3:         -0.1865,     nan,     nan,     nan, -0.0352,     nan, -0.1385, -0.1685,
3:             nan, -0.2225,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.1865,     nan,     nan,     nan,     nan, -0.2057,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.1241, -0.1565,
3:         -0.1877,     nan,     nan,     nan,     nan,     nan,     nan, -0.1613,
3:             nan,     nan,     nan, -0.1577,     nan,     nan, -0.2442,     nan,
3:             nan,     nan,     nan,     nan, -0.1769,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1109,     nan,     nan,
3:             nan,     nan,     nan, -0.1829,     nan, -0.1217,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1445,
3:             nan,     nan, -0.1649])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 19, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.3212, 1.3136, 1.2973, 1.2658, 1.2313, 1.1964, 1.1627, 1.1246, 1.0820,
3:         1.0346, 0.9802, 0.9260, 0.8785, 0.8400, 0.8126, 0.7986, 0.7932, 0.8010,
3:         1.3245, 1.3255, 1.3164, 1.2879, 1.2548, 1.2255, 1.2019],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.7995, -0.7975, -0.8023, -0.8110, -0.8266, -0.8461, -0.8762, -0.9140,
3:         -0.9548, -0.9944, -1.0315, -1.0682, -1.1112, -1.1579, -1.2210, -1.3000,
3:         -1.3881, -1.4782, -0.8042, -0.8087, -0.8252, -0.8415, -0.8583, -0.8800,
3:         -0.9094], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.3849, 0.3637, 0.3412, 0.3279, 0.3141, 0.3102, 0.3036, 0.2975, 0.2873,
3:         0.2735, 0.2577, 0.2408, 0.2195, 0.1923, 0.1607, 0.1263, 0.0883, 0.0541,
3:         0.4010, 0.3714, 0.3440, 0.3273, 0.3193, 0.3170, 0.3218],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.4220, -1.0628, -1.0306, -1.0331, -0.5134,  0.2038,  0.3181, -0.2135,
3:         -0.6967, -0.8290, -0.6390, -0.3304, -0.0312,  0.1284,  0.1229,  0.0529,
3:         -0.0455, -0.0436, -0.5380, -0.3305, -0.9546, -1.5766, -1.1613,  0.0345,
3:          0.8546], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([1.4765, 1.4487, 1.4225, 1.4012, 1.3893, 1.3849, 1.3810, 1.3669, 1.3383,
3:         1.2974, 1.2530, 1.2147, 1.1886, 1.1679, 1.1519, 1.1356, 1.1179, 1.0983,
3:         1.0769, 1.0511, 1.0238, 0.9949, 0.9675, 0.9421, 0.9129],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1591, -0.1662, -0.1733, -0.1728, -0.1711, -0.1738, -0.1736, -0.1674,
3:         -0.1652, -0.1690, -0.1673, -0.1732, -0.1759, -0.1769, -0.1735, -0.1761,
3:         -0.1695, -0.1627, -0.1664, -0.1705, -0.1694, -0.1708, -0.1691, -0.1670,
3:         -0.1686], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([0.8730, 0.9394, 0.9685, 0.9681, 0.9742, 1.0029, 1.0370, 1.0572, 1.0646,
1:         1.0706, 1.0821, 1.1005, 1.1246, 1.1492, 1.1637, 1.1604, 1.1403, 1.1105,
1:         0.8884, 0.9840, 1.0367, 1.0348, 1.0173, 1.0182, 1.0365],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 6.9541e-01,  6.8055e-01,  6.1305e-01,  5.2201e-01,  4.4336e-01,
1:          3.8226e-01,  3.1909e-01,  2.3879e-01,  1.4507e-01,  5.1150e-02,
1:         -2.9356e-02, -8.5298e-02, -1.0759e-01, -9.2730e-02, -4.9999e-02,
1:         -6.6299e-04,  3.6700e-02,  5.4453e-02,  6.8839e-01,  6.9706e-01,
1:          6.5660e-01,  5.8146e-01,  4.9930e-01,  4.2664e-01,  3.6224e-01],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2412, -0.2453, -0.2327, -0.1975, -0.1667, -0.1385, -0.1174, -0.0912,
1:         -0.0650, -0.0602, -0.0588, -0.0610, -0.0565, -0.0508, -0.0437, -0.0376,
1:         -0.0325, -0.0275, -0.1916, -0.2275, -0.2360, -0.2205, -0.1783, -0.1351,
1:         -0.0857], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0177,  1.6254,  2.0982,  1.1325, -0.6537, -2.2544, -2.8635, -2.6467,
1:         -2.5249, -2.8501, -3.1810, -3.3632, -3.5588, -3.4694, -2.7864, -1.9470,
1:         -1.4809, -1.3300, -1.7760,  0.5624,  2.3050,  2.5464,  1.0743, -1.1657,
1:         -2.5550], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.0972, 0.1392, 0.1965, 0.2629, 0.3219, 0.3591, 0.3775, 0.3918, 0.4087,
1:         0.4226, 0.4267, 0.4165, 0.3900, 0.3539, 0.3231, 0.3066, 0.3014, 0.2993,
1:         0.2956, 0.2888, 0.2787, 0.2674, 0.2571, 0.2498, 0.2471],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 19, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2478,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2406, -0.2526,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.1697,     nan,     nan,     nan,
1:             nan, -0.0496,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,  0.0872,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1613,     nan,     nan,     nan,     nan,  0.1005,     nan,
1:             nan,     nan,     nan,     nan, -0.2153,     nan,     nan,     nan,
1:             nan,  0.0176,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  0.0776,     nan, -0.0412,     nan, -0.1793, -0.2153,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1937,     nan,     nan,     nan, -0.1301,     nan,
1:         -0.1865,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1637,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.1373,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1145,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1109,     nan,     nan,
1:             nan,     nan,     nan, -0.1829,     nan,     nan,     nan,     nan,
1:             nan, -0.1505,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 19, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.2946, 1.2825, 1.2687, 1.2394, 1.2103, 1.1801, 1.1533, 1.1219, 1.0840,
1:         1.0381, 0.9829, 0.9241, 0.8709, 0.8278, 0.7988, 0.7872, 0.7874, 0.8007,
1:         1.3143, 1.3099, 1.2957, 1.2654, 1.2324, 1.2063, 1.1838],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.7975, -0.7801, -0.7660, -0.7627, -0.7761, -0.8062, -0.8518, -0.8999,
1:         -0.9424, -0.9736, -1.0022, -1.0289, -1.0638, -1.1031, -1.1649, -1.2444,
1:         -1.3456, -1.4584, -0.8277, -0.8172, -0.8124, -0.8130, -0.8255, -0.8586,
1:         -0.9041], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.3003, 0.2837, 0.2690, 0.2672, 0.2694, 0.2736, 0.2797, 0.2837, 0.2811,
1:         0.2747, 0.2621, 0.2475, 0.2238, 0.1967, 0.1655, 0.1318, 0.0971, 0.0694,
1:         0.3200, 0.2967, 0.2785, 0.2733, 0.2772, 0.2836, 0.2946],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.6891, -1.2383, -1.0268, -0.8822, -0.3030,  0.4374,  0.5333, -0.0045,
1:         -0.5609, -0.7788, -0.6537, -0.4234, -0.1000,  0.1292,  0.1796,  0.1879,
1:          0.0487,  0.0165, -0.6063, -0.3903, -0.9758, -1.5258, -1.0396,  0.1728,
1:          0.9002], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([1.4646, 1.4393, 1.4128, 1.3878, 1.3707, 1.3621, 1.3560, 1.3415, 1.3146,
1:         1.2781, 1.2411, 1.2132, 1.1963, 1.1804, 1.1605, 1.1328, 1.1035, 1.0771,
1:         1.0587, 1.0403, 1.0183, 0.9886, 0.9535, 0.9151, 0.8760],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1575, -0.1581, -0.1637, -0.1630, -0.1639, -0.1648, -0.1646, -0.1573,
1:         -0.1596, -0.1641, -0.1598, -0.1647, -0.1694, -0.1688, -0.1666, -0.1703,
1:         -0.1686, -0.1601, -0.1642, -0.1655, -0.1644, -0.1659, -0.1671, -0.1662,
1:         -0.1688], device='cuda:1', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 19, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8730, 0.9394, 0.9685, 0.9681, 0.9742, 1.0029, 1.0370, 1.0572, 1.0646,
0:         1.0706, 1.0821, 1.1005, 1.1246, 1.1492, 1.1637, 1.1604, 1.1403, 1.1105,
0:         0.8884, 0.9840, 1.0367, 1.0348, 1.0173, 1.0182, 1.0365],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 6.9541e-01,  6.8055e-01,  6.1305e-01,  5.2201e-01,  4.4336e-01,
0:          3.8226e-01,  3.1909e-01,  2.3879e-01,  1.4507e-01,  5.1150e-02,
0:         -2.9356e-02, -8.5298e-02, -1.0759e-01, -9.2730e-02, -4.9999e-02,
0:         -6.6299e-04,  3.6700e-02,  5.4453e-02,  6.8839e-01,  6.9706e-01,
0:          6.5660e-01,  5.8146e-01,  4.9930e-01,  4.2664e-01,  3.6224e-01],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2412, -0.2453, -0.2327, -0.1975, -0.1667, -0.1385, -0.1174, -0.0912,
0:         -0.0650, -0.0602, -0.0588, -0.0610, -0.0565, -0.0508, -0.0437, -0.0376,
0:         -0.0325, -0.0275, -0.1916, -0.2275, -0.2360, -0.2205, -0.1783, -0.1351,
0:         -0.0857], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0177,  1.6254,  2.0982,  1.1325, -0.6537, -2.2544, -2.8635, -2.6467,
0:         -2.5249, -2.8501, -3.1810, -3.3632, -3.5588, -3.4694, -2.7864, -1.9470,
0:         -1.4809, -1.3300, -1.7760,  0.5624,  2.3050,  2.5464,  1.0743, -1.1657,
0:         -2.5550], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.0972, 0.1392, 0.1965, 0.2629, 0.3219, 0.3591, 0.3775, 0.3918, 0.4087,
0:         0.4226, 0.4267, 0.4165, 0.3900, 0.3539, 0.3231, 0.3066, 0.3014, 0.2993,
0:         0.2956, 0.2888, 0.2787, 0.2674, 0.2571, 0.2498, 0.2471],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 19, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0881,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2406,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0440,
0:             nan,     nan, -0.0316,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2021,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.1797,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1841, -0.2081,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2033,     nan, -0.1205,     nan,
0:             nan,     nan,  0.0368,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1097,     nan, -0.0352,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1721,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1181,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1553,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0833,     nan, -0.1649])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 19, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.3199, 1.3022, 1.2817, 1.2499, 1.2185, 1.1863, 1.1561, 1.1213, 1.0832,
0:         1.0380, 0.9879, 0.9361, 0.8892, 0.8522, 0.8308, 0.8205, 0.8194, 0.8307,
0:         1.3439, 1.3351, 1.3148, 1.2823, 1.2475, 1.2166, 1.1898],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7871, -0.7715, -0.7665, -0.7752, -0.7989, -0.8346, -0.8773, -0.9228,
0:         -0.9621, -0.9913, -1.0165, -1.0444, -1.0789, -1.1206, -1.1754, -1.2453,
0:         -1.3233, -1.4062, -0.8144, -0.8013, -0.8004, -0.8088, -0.8306, -0.8665,
0:         -0.9119], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3262, 0.3120, 0.3024, 0.2988, 0.2993, 0.3003, 0.3005, 0.2979, 0.2883,
0:         0.2766, 0.2608, 0.2397, 0.2164, 0.1905, 0.1644, 0.1394, 0.1104, 0.0855,
0:         0.3481, 0.3243, 0.3093, 0.3028, 0.3029, 0.3023, 0.3058],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.4641e+00, -1.1706e+00, -9.8743e-01, -7.7904e-01, -2.4572e-01,
0:          2.9432e-01,  3.1015e-01, -1.1706e-01, -5.3561e-01, -6.8132e-01,
0:         -5.3591e-01, -3.6336e-01, -1.5433e-01,  5.3496e-04,  8.3423e-03,
0:          5.0892e-02,  8.8069e-03, -7.7897e-02, -5.0876e-01, -4.4240e-01,
0:         -9.6552e-01, -1.3321e+00, -8.4757e-01,  1.3190e-01,  7.4219e-01],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.4925, 1.4663, 1.4350, 1.4034, 1.3818, 1.3710, 1.3652, 1.3543, 1.3315,
0:         1.2968, 1.2598, 1.2281, 1.2061, 1.1865, 1.1649, 1.1384, 1.1109, 1.0853,
0:         1.0670, 1.0507, 1.0331, 1.0081, 0.9758, 0.9384, 0.8969],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1569, -0.1589, -0.1639, -0.1620, -0.1638, -0.1670, -0.1693, -0.1651,
0:         -0.1665, -0.1633, -0.1582, -0.1641, -0.1668, -0.1701, -0.1699, -0.1744,
0:         -0.1732, -0.1655, -0.1623, -0.1652, -0.1628, -0.1674, -0.1681, -0.1707,
0:         -0.1730], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 19 [1/5 (20%)]	Loss: nan : nan :: 0.15796 (1.69 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 19 [2/5 (40%)]	Loss: nan : nan :: 0.14409 (10.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 19 [3/5 (60%)]	Loss: nan : nan :: 0.13149 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 19 [4/5 (80%)]	Loss: nan : nan :: 0.14455 (10.42 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch19.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch19.mod
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 19 : nan
0: validation loss for velocity_u : 0.03232402354478836
0: validation loss for velocity_v : 0.05782534182071686
0: validation loss for specific_humidity : 0.025552093982696533
0: validation loss for velocity_z : 0.4532736837863922
0: validation loss for temperature : 0.07919619977474213
0: validation loss for total_precip : nan
3: 20 : 00:29:26 :: batch_size = 96, lr = 1.2823318186821183e-05
0: 20 : 00:29:26 :: batch_size = 96, lr = 1.2823318186821183e-05
2: 20 : 00:29:26 :: batch_size = 96, lr = 1.2823318186821183e-05
1: 20 : 00:29:26 :: batch_size = 96, lr = 1.2823318186821183e-05
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 20, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 20, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0624, 0.1135, 0.1638, 0.2095, 0.2703, 0.3518, 0.4254, 0.4719, 0.5151,
0:         0.5825, 0.6613, 0.7211, 0.7570, 0.7841, 0.8081, 0.8212, 0.8213, 0.8164,
0:         0.0163, 0.0649, 0.1178, 0.1638, 0.2141, 0.2818, 0.3562],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5716, -0.5925, -0.6200, -0.6565, -0.7061, -0.7628, -0.8114, -0.8483,
0:         -0.8870, -0.9342, -0.9777, -1.0096, -1.0340, -1.0515, -1.0555, -1.0492,
0:         -1.0461, -1.0490, -0.4413, -0.4701, -0.5072, -0.5474, -0.5935, -0.6458,
0:         -0.7017], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1220, -0.1054, -0.0810, -0.0543, -0.0283, -0.0349, -0.0112,  0.0262,
0:          0.0421,  0.0472,  0.0080, -0.0093, -0.0443, -0.0501, -0.0262,  0.0048,
0:          0.0559,  0.0756, -0.0787, -0.0627, -0.0542, -0.0231,  0.0050,  0.0097,
0:          0.0091], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3873,  0.5483,  0.6179,  0.6940,  0.7005,  0.5287,  0.2459,  0.0501,
0:          0.1153,  0.3873,  0.5309,  0.2633, -0.3132, -0.9028, -1.2487, -1.1965,
0:         -0.8114, -0.3763,  0.4134,  0.5004,  0.5330,  0.5809,  0.5809,  0.4591,
0:          0.3111], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.2170, 0.2597, 0.3013, 0.3430, 0.3777, 0.4052, 0.4325, 0.4586, 0.4761,
0:         0.4856, 0.4933, 0.4998, 0.5026, 0.4979, 0.4840, 0.4653, 0.4537, 0.4553,
0:         0.4656, 0.4773, 0.4918, 0.5144, 0.5450, 0.5753, 0.5973],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 20, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([   nan,    nan,    nan,    nan,    nan,    nan, 1.3054,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan, 2.4541,    nan,    nan,    nan,
0:         1.0168,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan, 1.8428,    nan,    nan,    nan,    nan,    nan,
0:         0.5240,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.7932,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:         0.0675,    nan,    nan,    nan, 2.4741,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan, 0.4418,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan, 0.2775,    nan,    nan, 0.9428,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan, 1.3101,    nan,    nan,
0:         0.6037,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 0.7891, 2.1890,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan, 0.1426,    nan,    nan,    nan,
0:            nan,    nan, 0.5908,    nan, 0.1907,    nan,    nan, 0.6941,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.3183, 2.1549,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan, 1.4192,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 1.0297,    nan,    nan,    nan,    nan,    nan, 5.3653,    nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 20, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.5410, 1.5687, 1.5851, 1.5764, 1.5650, 1.5607, 1.5738, 1.6102, 1.6640,
0:         1.7370, 1.8199, 1.8910, 1.9332, 1.9274, 1.8575, 1.7282, 1.5627, 1.3908,
0:         1.4598, 1.4939, 1.5012, 1.4835, 1.4604, 1.4463, 1.4584],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9601, 0.9207, 0.8710, 0.8096, 0.7529, 0.7194, 0.7251, 0.7709, 0.8563,
0:         0.9568, 1.0522, 1.1286, 1.1772, 1.2177, 1.2496, 1.2583, 1.2456, 1.1941,
0:         0.9055, 0.8672, 0.8188, 0.7737, 0.7305, 0.7057, 0.7155],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6041, -0.5957, -0.5854, -0.5750, -0.5709, -0.5689, -0.5727, -0.5786,
0:         -0.5889, -0.5998, -0.6126, -0.6254, -0.6354, -0.6434, -0.6458, -0.6467,
0:         -0.6400, -0.6316, -0.5783, -0.5657, -0.5512, -0.5418, -0.5411, -0.5426,
0:         -0.5507], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0791, -1.0172, -0.8041, -0.4447,  0.0763,  0.5302,  0.8556,  1.1473,
0:          1.2380,  1.4013,  2.0028,  2.4990,  2.5922,  2.3860,  1.7396,  0.7375,
0:         -0.6448, -1.9248, -0.9015, -0.7553, -0.4802, -0.3600, -0.2128,  0.2038,
0:          0.7940], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.2017, -1.2362, -1.2847, -1.3417, -1.3984, -1.4496, -1.4879, -1.5063,
0:         -1.5076, -1.4947, -1.4732, -1.4458, -1.4087, -1.3573, -1.2955, -1.2413,
0:         -1.2184, -1.2404, -1.3028, -1.3810, -1.4476, -1.4801, -1.4708, -1.4254,
0:         -1.3550], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 values: tensor([0.0624, 0.1135, 0.1638, 0.2095, 0.2703, 0.3518, 0.4254, 0.4719, 0.5151,
2:         0.5825, 0.6613, 0.7211, 0.7570, 0.7841, 0.8081, 0.8212, 0.8213, 0.8164,
2:         0.0163, 0.0649, 0.1178, 0.1638, 0.2141, 0.2818, 0.3562],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 pred values: tensor([-0.1250, -0.1296, -0.1392, -0.1408, -0.1486, -0.1542, -0.1572, -0.1529,
0:         -0.1548, -0.1352, -0.1339, -0.1418, -0.1461, -0.1529, -0.1505, -0.1581,
0:         -0.1577, -0.1465, -0.1388, -0.1444, -0.1424, -0.1438, -0.1485, -0.1459,
0:         -0.1502], device='cuda:0', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([-0.5716, -0.5925, -0.6200, -0.6565, -0.7061, -0.7628, -0.8114, -0.8483,
2:         -0.8870, -0.9342, -0.9777, -1.0096, -1.0340, -1.0515, -1.0555, -1.0492,
2:         -1.0461, -1.0490, -0.4413, -0.4701, -0.5072, -0.5474, -0.5935, -0.6458,
2:         -0.7017], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1220, -0.1054, -0.0810, -0.0543, -0.0283, -0.0349, -0.0112,  0.0262,
2:          0.0421,  0.0472,  0.0080, -0.0093, -0.0443, -0.0501, -0.0262,  0.0048,
2:          0.0559,  0.0756, -0.0787, -0.0627, -0.0542, -0.0231,  0.0050,  0.0097,
2:          0.0091], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.3873,  0.5483,  0.6179,  0.6940,  0.7005,  0.5287,  0.2459,  0.0501,
2:          0.1153,  0.3873,  0.5309,  0.2633, -0.3132, -0.9028, -1.2487, -1.1965,
2:         -0.8114, -0.3763,  0.4134,  0.5004,  0.5330,  0.5809,  0.5809,  0.4591,
2:          0.3111], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.2170, 0.2597, 0.3013, 0.3430, 0.3777, 0.4052, 0.4325, 0.4586, 0.4761,
2:         0.4856, 0.4933, 0.4998, 0.5026, 0.4979, 0.4840, 0.4653, 0.4537, 0.4553,
2:         0.4656, 0.4773, 0.4918, 0.5144, 0.5450, 0.5753, 0.5973],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 20, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([   nan,    nan,    nan, 2.1479,    nan, 1.5154,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan, 1.6492, 3.0643, 2.4541,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:         0.8020,    nan,    nan, 0.5486, 1.4744,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan, 1.1341,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.7985,    nan,
2:            nan,    nan, 0.2024,    nan,    nan,    nan,    nan,    nan, 1.4932,
2:         0.3820,    nan, 0.1813,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan, 3.1417,    nan,    nan,    nan,    nan, 0.8936,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan, 0.9628, 0.3785,    nan,    nan, 1.3113,    nan,    nan, 2.1878,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan, 1.0813,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan, 3.2532,    nan,    nan,    nan,    nan,    nan,
2:            nan,    nan,    nan,    nan,    nan, 2.1174,    nan,    nan,    nan,
2:            nan,    nan, 2.0740, 4.1473,    nan,    nan,    nan,    nan,    nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 20, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.5880, 1.6098, 1.6181, 1.6019, 1.5814, 1.5686, 1.5820, 1.6222, 1.6894,
2:         1.7728, 1.8641, 1.9390, 1.9811, 1.9743, 1.9033, 1.7744, 1.5914, 1.3991,
2:         1.4894, 1.5231, 1.5296, 1.5146, 1.4888, 1.4762, 1.4916],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.8665, 0.8354, 0.7963, 0.7557, 0.7157, 0.6987, 0.7117, 0.7585, 0.8391,
2:         0.9433, 1.0442, 1.1308, 1.1927, 1.2460, 1.2927, 1.3208, 1.3254, 1.2859,
2:         0.8311, 0.7973, 0.7584, 0.7222, 0.6915, 0.6758, 0.6925],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.5691, -0.5644, -0.5616, -0.5564, -0.5549, -0.5556, -0.5600, -0.5648,
2:         -0.5736, -0.5827, -0.5924, -0.6021, -0.6096, -0.6151, -0.6185, -0.6217,
2:         -0.6230, -0.6232, -0.5616, -0.5547, -0.5458, -0.5410, -0.5423, -0.5474,
2:         -0.5530], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.2387, -1.1516, -0.9819, -0.6377, -0.1532,  0.1976,  0.3588,  0.4928,
2:          0.5906,  1.0175,  2.0223,  2.8201,  3.1054,  3.1233,  2.5474,  1.3525,
2:         -0.2555, -1.7100, -1.3069, -1.1011, -0.7144, -0.5044, -0.4146, -0.0252,
2:          0.6709], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-1.1763, -1.2080, -1.2601, -1.3228, -1.3831, -1.4315, -1.4655, -1.4833,
2:         -1.4857, -1.4763, -1.4573, -1.4269, -1.3813, -1.3231, -1.2588, -1.2105,
2:         -1.1978, -1.2300, -1.2983, -1.3807, -1.4491, -1.4846, -1.4805, -1.4400,
2:         -1.3736], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.0937, -0.1017, -0.1134, -0.1233, -0.1372, -0.1430, -0.1454, -0.1348,
2:         -0.1370, -0.1058, -0.1103, -0.1252, -0.1308, -0.1414, -0.1419, -0.1447,
2:         -0.1425, -0.1282, -0.1115, -0.1211, -0.1242, -0.1309, -0.1357, -0.1371,
2:         -0.1397], device='cuda:2', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 20, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 20, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.0624, 0.1135, 0.1638, 0.2095, 0.2703, 0.3518, 0.4254, 0.4719, 0.5151,
3:         0.5825, 0.6613, 0.7211, 0.7570, 0.7841, 0.8081, 0.8212, 0.8213, 0.8164,
3:         0.0163, 0.0649, 0.1178, 0.1638, 0.2141, 0.2818, 0.3562],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.5716, -0.5925, -0.6200, -0.6565, -0.7061, -0.7628, -0.8114, -0.8483,
3:         -0.8870, -0.9342, -0.9777, -1.0096, -1.0340, -1.0515, -1.0555, -1.0492,
3:         -1.0461, -1.0490, -0.4413, -0.4701, -0.5072, -0.5474, -0.5935, -0.6458,
3:         -0.7017], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1220, -0.1054, -0.0810, -0.0543, -0.0283, -0.0349, -0.0112,  0.0262,
3:          0.0421,  0.0472,  0.0080, -0.0093, -0.0443, -0.0501, -0.0262,  0.0048,
3:          0.0559,  0.0756, -0.0787, -0.0627, -0.0542, -0.0231,  0.0050,  0.0097,
3:          0.0091], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.3873,  0.5483,  0.6179,  0.6940,  0.7005,  0.5287,  0.2459,  0.0501,
3:          0.1153,  0.3873,  0.5309,  0.2633, -0.3132, -0.9028, -1.2487, -1.1965,
3:         -0.8114, -0.3763,  0.4134,  0.5004,  0.5330,  0.5809,  0.5809,  0.4591,
3:          0.3111], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.2170, 0.2597, 0.3013, 0.3430, 0.3777, 0.4052, 0.4325, 0.4586, 0.4761,
3:         0.4856, 0.4933, 0.4998, 0.5026, 0.4979, 0.4840, 0.4653, 0.4537, 0.4553,
3:         0.4656, 0.4773, 0.4918, 0.5144, 0.5450, 0.5753, 0.5973],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 20, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([   nan,    nan,    nan,    nan,    nan, 1.5154,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan, 1.2409,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.7005, 2.3673,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan, 0.8208,    nan,    nan,
3:            nan,    nan,    nan,    nan, 1.5823,    nan, 1.1892,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.2206,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan, 0.4418,    nan, 0.7774,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan, 0.5850,    nan,    nan,    nan, 1.6340,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan, 0.5685,    nan,    nan,    nan,    nan,    nan,
3:         1.2796,    nan,    nan,    nan, 1.5260,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:         0.3644,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan, 0.2705,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan, 0.0968, 0.3444, 0.6941,    nan,
3:         0.7070, 0.9628,    nan,    nan,    nan, 1.3113,    nan,    nan, 2.1878,
3:            nan,    nan,    nan,    nan,    nan, 2.5069,    nan,    nan,    nan,
3:            nan,    nan,    nan, 0.5486,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan, 1.3629,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan, 1.3347, 1.2092,    nan,    nan,    nan,
3:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
3:            nan,    nan, 2.0740,    nan,    nan,    nan,    nan,    nan,    nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 20, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.4746, 1.5179, 1.5425, 1.5308, 1.5066, 1.4918, 1.5017, 1.5418, 1.6133,
3:         1.6985, 1.7901, 1.8685, 1.9171, 1.9178, 1.8610, 1.7465, 1.5863, 1.4101,
3:         1.3723, 1.4236, 1.4441, 1.4355, 1.4095, 1.3958, 1.4077],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.8977, 0.8751, 0.8425, 0.8005, 0.7514, 0.7328, 0.7461, 0.8035, 0.8986,
3:         1.0069, 1.1120, 1.1977, 1.2499, 1.2883, 1.3141, 1.3099, 1.2756, 1.2041,
3:         0.8710, 0.8368, 0.7991, 0.7633, 0.7324, 0.7195, 0.7436],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5788, -0.5847, -0.5903, -0.5889, -0.5906, -0.5941, -0.6016, -0.6112,
3:         -0.6233, -0.6348, -0.6434, -0.6513, -0.6564, -0.6615, -0.6623, -0.6608,
3:         -0.6539, -0.6451, -0.5668, -0.5694, -0.5681, -0.5679, -0.5695, -0.5745,
3:         -0.5841], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.3247, -1.2035, -1.0967, -0.8762, -0.3964,  0.2046,  0.7113,  1.0352,
3:          1.1154,  1.2495,  1.9469,  2.7533,  3.0256,  2.8243,  2.1417,  1.0826,
3:         -0.3100, -1.6184, -1.1492, -0.9434, -0.6955, -0.5998, -0.4637,  0.1283,
3:          1.0489], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-1.1594, -1.1903, -1.2398, -1.2989, -1.3547, -1.4009, -1.4342, -1.4537,
3:         -1.4619, -1.4596, -1.4485, -1.4231, -1.3821, -1.3270, -1.2658, -1.2199,
3:         -1.2085, -1.2385, -1.3015, -1.3756, -1.4378, -1.4691, -1.4628, -1.4210,
3:         -1.3515], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1379, -0.1466, -0.1561, -0.1579, -0.1594, -0.1632, -0.1588, -0.1513,
3:         -0.1464, -0.1469, -0.1460, -0.1533, -0.1601, -0.1623, -0.1583, -0.1606,
3:         -0.1558, -0.1423, -0.1465, -0.1502, -0.1479, -0.1509, -0.1540, -0.1535,
3:         -0.1522], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([0.0624, 0.1135, 0.1638, 0.2095, 0.2703, 0.3518, 0.4254, 0.4719, 0.5151,
1:         0.5825, 0.6613, 0.7211, 0.7570, 0.7841, 0.8081, 0.8212, 0.8213, 0.8164,
1:         0.0163, 0.0649, 0.1178, 0.1638, 0.2141, 0.2818, 0.3562],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.5716, -0.5925, -0.6200, -0.6565, -0.7061, -0.7628, -0.8114, -0.8483,
1:         -0.8870, -0.9342, -0.9777, -1.0096, -1.0340, -1.0515, -1.0555, -1.0492,
1:         -1.0461, -1.0490, -0.4413, -0.4701, -0.5072, -0.5474, -0.5935, -0.6458,
1:         -0.7017], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1220, -0.1054, -0.0810, -0.0543, -0.0283, -0.0349, -0.0112,  0.0262,
1:          0.0421,  0.0472,  0.0080, -0.0093, -0.0443, -0.0501, -0.0262,  0.0048,
1:          0.0559,  0.0756, -0.0787, -0.0627, -0.0542, -0.0231,  0.0050,  0.0097,
1:          0.0091], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.3873,  0.5483,  0.6179,  0.6940,  0.7005,  0.5287,  0.2459,  0.0501,
1:          0.1153,  0.3873,  0.5309,  0.2633, -0.3132, -0.9028, -1.2487, -1.1965,
1:         -0.8114, -0.3763,  0.4134,  0.5004,  0.5330,  0.5809,  0.5809,  0.4591,
1:          0.3111], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.2170, 0.2597, 0.3013, 0.3430, 0.3777, 0.4052, 0.4325, 0.4586, 0.4761,
1:         0.4856, 0.4933, 0.4998, 0.5026, 0.4979, 0.4840, 0.4653, 0.4537, 0.4553,
1:         0.4656, 0.4773, 0.4918, 0.5144, 0.5450, 0.5753, 0.5973],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 20, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan, 0.4770,    nan,    nan,    nan, 1.6832,    nan,    nan,
1:            nan,    nan,    nan, 1.6492,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan, 2.3896,    nan,    nan,
1:         0.8020,    nan, 2.7017,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan, 2.9951,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan, 0.9804,    nan,    nan, 2.7932,
1:            nan, 1.1341,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan, 2.2347,    nan, 2.4741,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan, 0.5298,    nan,    nan,    nan, 0.7985,    nan,
1:            nan, 0.1942,    nan,    nan,    nan, 0.9147,    nan,    nan,    nan,
1:            nan,    nan, 0.1813,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan, 1.5753,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan, 1.5260,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan, 1.5858,    nan, 1.3101,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan, 0.3444,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan, 0.3925,    nan,    nan,    nan,    nan, 1.6609,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan, 2.9294,    nan,    nan, 1.3664,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.6633,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
1:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 20, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.5208, 1.5668, 1.5987, 1.6049, 1.6012, 1.6069, 1.6247, 1.6615, 1.7213,
1:         1.7877, 1.8671, 1.9416, 1.9884, 1.9921, 1.9321, 1.7982, 1.6120, 1.4121,
1:         1.4271, 1.4743, 1.4939, 1.4925, 1.4855, 1.4882, 1.5111],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.8543, 0.8445, 0.8252, 0.7903, 0.7480, 0.7271, 0.7379, 0.7907, 0.8852,
1:         0.9925, 1.0907, 1.1679, 1.2135, 1.2490, 1.2785, 1.2871, 1.2691, 1.2100,
1:         0.8155, 0.8022, 0.7798, 0.7480, 0.7148, 0.6921, 0.7109],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5555, -0.5562, -0.5597, -0.5567, -0.5570, -0.5603, -0.5674, -0.5753,
1:         -0.5878, -0.6015, -0.6150, -0.6249, -0.6316, -0.6354, -0.6368, -0.6385,
1:         -0.6392, -0.6373, -0.5391, -0.5385, -0.5371, -0.5358, -0.5396, -0.5459,
1:         -0.5531], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.2138, -1.2627, -1.2399, -1.0410, -0.6233, -0.0645,  0.4412,  0.8398,
1:          0.9428,  1.0622,  1.8169,  2.5628,  2.7580,  2.5327,  1.7828,  0.6130,
1:         -0.7751, -1.8440, -1.1979, -1.0386, -0.8074, -0.7221, -0.6536, -0.1724,
1:          0.5595], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-1.1452, -1.1735, -1.2235, -1.2868, -1.3510, -1.4054, -1.4438, -1.4636,
1:         -1.4685, -1.4641, -1.4538, -1.4347, -1.4000, -1.3470, -1.2812, -1.2262,
1:         -1.2044, -1.2287, -1.2907, -1.3670, -1.4305, -1.4609, -1.4526, -1.4109,
1:         -1.3460], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1365, -0.1399, -0.1441, -0.1415, -0.1454, -0.1520, -0.1544, -0.1508,
1:         -0.1506, -0.1440, -0.1409, -0.1456, -0.1478, -0.1522, -0.1533, -0.1562,
1:         -0.1584, -0.1490, -0.1432, -0.1462, -0.1406, -0.1424, -0.1446, -0.1493,
1:         -0.1552], device='cuda:1', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 20 [1/5 (20%)]	Loss: nan : nan :: 0.13785 (1.85 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 20 [2/5 (40%)]	Loss: nan : nan :: 0.14439 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 20 [3/5 (60%)]	Loss: nan : nan :: 0.13329 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 20 [4/5 (80%)]	Loss: nan : nan :: 0.13833 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch20.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch20.mod
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 20 : nan
0: validation loss for velocity_u : 0.03709892928600311
0: validation loss for velocity_v : 0.0664421021938324
0: validation loss for specific_humidity : 0.025450443848967552
0: validation loss for velocity_z : 0.5101673007011414
0: validation loss for temperature : 0.08576454222202301
0: validation loss for total_precip : nan
1: 21 : 00:36:04 :: batch_size = 96, lr = 1.2510554328606033e-05
2: 21 : 00:36:04 :: batch_size = 96, lr = 1.2510554328606033e-05
0: 21 : 00:36:04 :: batch_size = 96, lr = 1.2510554328606033e-05
3: 21 : 00:36:04 :: batch_size = 96, lr = 1.2510554328606033e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 21, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2027, -0.2199, -0.2413, -0.2560, -0.2503, -0.2162, -0.1794, -0.1556,
0:         -0.1433, -0.1338, -0.1197, -0.1031, -0.0805, -0.0557, -0.0298, -0.0029,
0:          0.0210,  0.0427, -0.2144, -0.1925, -0.2127, -0.2390, -0.2344, -0.1988,
0:         -0.1656], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0153,  0.0094, -0.0122, -0.0290, -0.0281, -0.0147,  0.0006,  0.0065,
0:         -0.0022, -0.0251, -0.0591, -0.1020, -0.1476, -0.1905, -0.2288, -0.2600,
0:         -0.2828, -0.2963,  0.0822,  0.0649,  0.0332,  0.0108,  0.0006,  0.0004,
0:          0.0047], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0418, -0.0007, -0.1028, -0.3382, -0.4723, -0.4820, -0.3839, -0.2698,
0:         -0.1836, -0.1481, -0.1277, -0.1634, -0.2106, -0.2672, -0.3244, -0.3822,
0:         -0.4328, -0.4717,  0.1432, -0.1045, -0.3549, -0.4682, -0.4363, -0.2928,
0:         -0.1433], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1505, -0.1319, -0.1729, -0.0976, -0.2094, -0.4841, -0.7033, -0.7565,
0:         -0.7377, -0.7565, -0.7476, -0.7886, -0.8262, -0.8694, -0.8561, -0.7764,
0:         -0.6491, -0.4807, -0.0633, -0.4996, -0.3390, -0.1740, -0.3357, -0.5904,
0:         -0.7676], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.8849, -0.7922, -0.7638, -0.7209, -0.6876, -0.6973, -0.7117, -0.7758,
0:         -0.8770, -1.0068, -1.1342, -1.2197, -1.2757, -1.2670, -1.2105, -1.1198,
0:         -0.9969, -0.8678, -0.7438, -0.6378, -0.5634, -0.5203, -0.5051, -0.5103,
0:         -0.5143], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 21, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
0:             nan, -0.2436, -0.2436,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2436,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,
0:             nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,
0:         -0.2436,     nan,     nan,     nan,     nan,     nan, -0.2436, -0.2436,
0:             nan,     nan,     nan,     nan, -0.2436, -0.2436,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2436, -0.2436, -0.2436,     nan,     nan,     nan,
0:             nan, -0.2436, -0.2436,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
0:         -0.2436, -0.2436,     nan, -0.2436,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 21, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3127, 0.2856, 0.2527, 0.2243, 0.2060, 0.1992, 0.1977, 0.1985, 0.1943,
0:         0.1851, 0.1670, 0.1423, 0.1146, 0.0886, 0.0690, 0.0577, 0.0586, 0.0668,
0:         0.3296, 0.2947, 0.2545, 0.2221, 0.2013, 0.1925, 0.1910],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2187, 0.2290, 0.2436, 0.2570, 0.2651, 0.2724, 0.2781, 0.2819, 0.2884,
0:         0.2975, 0.3072, 0.3205, 0.3349, 0.3485, 0.3603, 0.3662, 0.3634, 0.3524,
0:         0.2463, 0.2607, 0.2794, 0.2974, 0.3119, 0.3189, 0.3245],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.6291,  0.6158,  0.5878,  0.5549,  0.5169,  0.4846,  0.4504,  0.4012,
0:          0.3229,  0.2241,  0.1078,  0.0018, -0.0692, -0.0886, -0.0514,  0.0292,
0:          0.1254,  0.2102,  0.5336,  0.5036,  0.4645,  0.4259,  0.3876,  0.3525,
0:          0.3210], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0552, -0.0162,  0.0051,  0.0200,  0.0237, -0.0117, -0.0685, -0.1381,
0:         -0.1978, -0.2174, -0.1840, -0.0672,  0.0630,  0.1198,  0.1445,  0.1474,
0:          0.0687, -0.0421, -0.2193, -0.1776, -0.1160, -0.0712, -0.0618, -0.1166,
0:         -0.1916], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.9326, -1.9709, -2.0159, -2.0733, -2.1453, -2.2284, -2.3032, -2.3513,
0:         -2.3658, -2.3502, -2.3219, -2.2974, -2.2941, -2.3195, -2.3676, -2.4300,
0:         -2.4918, -2.5370, -2.5507, -2.5245, -2.4630, -2.3665, -2.2495, -2.1202,
0:         -1.9802], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1690, -0.1731, -0.1813, -0.1829, -0.1822, -0.1823, -0.1837, -0.1799,
0:         -0.1757, -0.1708, -0.1692, -0.1732, -0.1781, -0.1814, -0.1758, -0.1822,
0:         -0.1793, -0.1719, -0.1645, -0.1669, -0.1679, -0.1678, -0.1700, -0.1700,
0:         -0.1725], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 21, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 21, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2027, -0.2199, -0.2413, -0.2560, -0.2503, -0.2162, -0.1794, -0.1556,
1:         -0.1433, -0.1338, -0.1197, -0.1031, -0.0805, -0.0557, -0.0298, -0.0029,
1:          0.0210,  0.0427, -0.2144, -0.1925, -0.2127, -0.2390, -0.2344, -0.1988,
1:         -0.1656], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.0153,  0.0094, -0.0122, -0.0290, -0.0281, -0.0147,  0.0006,  0.0065,
1:         -0.0022, -0.0251, -0.0591, -0.1020, -0.1476, -0.1905, -0.2288, -0.2600,
1:         -0.2828, -0.2963,  0.0822,  0.0649,  0.0332,  0.0108,  0.0006,  0.0004,
1:          0.0047], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0418, -0.0007, -0.1028, -0.3382, -0.4723, -0.4820, -0.3839, -0.2698,
1:         -0.1836, -0.1481, -0.1277, -0.1634, -0.2106, -0.2672, -0.3244, -0.3822,
1:         -0.4328, -0.4717,  0.1432, -0.1045, -0.3549, -0.4682, -0.4363, -0.2928,
1:         -0.1433], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.1505, -0.1319, -0.1729, -0.0976, -0.2094, -0.4841, -0.7033, -0.7565,
1:         -0.7377, -0.7565, -0.7476, -0.7886, -0.8262, -0.8694, -0.8561, -0.7764,
1:         -0.6491, -0.4807, -0.0633, -0.4996, -0.3390, -0.1740, -0.3357, -0.5904,
1:         -0.7676], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.8849, -0.7922, -0.7638, -0.7209, -0.6876, -0.6973, -0.7117, -0.7758,
1:         -0.8770, -1.0068, -1.1342, -1.2197, -1.2757, -1.2670, -1.2105, -1.1198,
1:         -0.9969, -0.8678, -0.7438, -0.6378, -0.5634, -0.5203, -0.5051, -0.5103,
1:         -0.5143], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 21, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2436, -0.2436,     nan,     nan,     nan,     nan,     nan,
1:         -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2436, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436,
1:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2436, -0.2436,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2436, -0.2436,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2436,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
1:             nan,     nan,     nan, -0.2436,     nan,     nan,     nan, -0.2436,
1:             nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2436,     nan, -0.2436,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan, -0.2436,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2436,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 21, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.2918, 0.2598, 0.2307, 0.2043, 0.1888, 0.1827, 0.1792, 0.1748, 0.1643,
1:         0.1508, 0.1330, 0.1111, 0.0885, 0.0687, 0.0487, 0.0368, 0.0361, 0.0425,
1:         0.3033, 0.2674, 0.2315, 0.2015, 0.1834, 0.1761, 0.1747],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.2289, 0.2311, 0.2441, 0.2603, 0.2743, 0.2838, 0.2862, 0.2884, 0.2939,
1:         0.3033, 0.3143, 0.3285, 0.3425, 0.3556, 0.3698, 0.3800, 0.3849, 0.3779,
1:         0.2537, 0.2597, 0.2767, 0.3011, 0.3225, 0.3342, 0.3368],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.6013,  0.5908,  0.5670,  0.5382,  0.5086,  0.4830,  0.4519,  0.4072,
1:          0.3392,  0.2555,  0.1560,  0.0690,  0.0030, -0.0158,  0.0113,  0.0763,
1:          0.1549,  0.2348,  0.5332,  0.5039,  0.4665,  0.4311,  0.3960,  0.3645,
1:          0.3331], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1106, -0.1065, -0.0987, -0.0928, -0.1058, -0.1331, -0.1690, -0.2137,
1:         -0.2281, -0.2068, -0.1640, -0.0596,  0.0570,  0.1293,  0.1772,  0.1700,
1:          0.1120,  0.0321, -0.2414, -0.2277, -0.1583, -0.1174, -0.1319, -0.1827,
1:         -0.2358], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-1.9060, -1.9480, -2.0026, -2.0666, -2.1395, -2.2173, -2.2893, -2.3450,
1:         -2.3744, -2.3783, -2.3646, -2.3434, -2.3323, -2.3437, -2.3763, -2.4284,
1:         -2.4823, -2.5222, -2.5334, -2.5111, -2.4577, -2.3739, -2.2677, -2.1436,
1:         -2.0026], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1658, -0.1707, -0.1765, -0.1731, -0.1744, -0.1747, -0.1765, -0.1754,
1:         -0.1717, -0.1751, -0.1711, -0.1784, -0.1775, -0.1803, -0.1721, -0.1764,
1:         -0.1732, -0.1682, -0.1729, -0.1728, -0.1739, -0.1732, -0.1722, -0.1695,
1:         -0.1667], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([-0.2027, -0.2199, -0.2413, -0.2560, -0.2503, -0.2162, -0.1794, -0.1556,
2:         -0.1433, -0.1338, -0.1197, -0.1031, -0.0805, -0.0557, -0.0298, -0.0029,
2:          0.0210,  0.0427, -0.2144, -0.1925, -0.2127, -0.2390, -0.2344, -0.1988,
2:         -0.1656], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.0153,  0.0094, -0.0122, -0.0290, -0.0281, -0.0147,  0.0006,  0.0065,
2:         -0.0022, -0.0251, -0.0591, -0.1020, -0.1476, -0.1905, -0.2288, -0.2600,
2:         -0.2828, -0.2963,  0.0822,  0.0649,  0.0332,  0.0108,  0.0006,  0.0004,
2:          0.0047], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.0418, -0.0007, -0.1028, -0.3382, -0.4723, -0.4820, -0.3839, -0.2698,
2:         -0.1836, -0.1481, -0.1277, -0.1634, -0.2106, -0.2672, -0.3244, -0.3822,
2:         -0.4328, -0.4717,  0.1432, -0.1045, -0.3549, -0.4682, -0.4363, -0.2928,
2:         -0.1433], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.1505, -0.1319, -0.1729, -0.0976, -0.2094, -0.4841, -0.7033, -0.7565,
2:         -0.7377, -0.7565, -0.7476, -0.7886, -0.8262, -0.8694, -0.8561, -0.7764,
2:         -0.6491, -0.4807, -0.0633, -0.4996, -0.3390, -0.1740, -0.3357, -0.5904,
2:         -0.7676], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.8849, -0.7922, -0.7638, -0.7209, -0.6876, -0.6973, -0.7117, -0.7758,
2:         -0.8770, -1.0068, -1.1342, -1.2197, -1.2757, -1.2670, -1.2105, -1.1198,
2:         -0.9969, -0.8678, -0.7438, -0.6378, -0.5634, -0.5203, -0.5051, -0.5103,
2:         -0.5143], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 21, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2436, -0.2436,     nan,
2:             nan,     nan, -0.2436,     nan,     nan,     nan, -0.2436, -0.2436,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2436,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,
2:         -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,
2:         -0.2436,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2436, -0.2436,     nan,     nan,     nan,     nan, -0.2436,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,
2:             nan,     nan, -0.2436, -0.2436,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436,
2:             nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
2:         -0.2436,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
2:         -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436,
2:         -0.2436,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 21, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.3182, 0.2935, 0.2689, 0.2417, 0.2219, 0.2085, 0.1982, 0.1890, 0.1800,
2:         0.1699, 0.1573, 0.1388, 0.1169, 0.0941, 0.0730, 0.0593, 0.0572, 0.0642,
2:         0.3318, 0.3030, 0.2683, 0.2354, 0.2117, 0.1955, 0.1870],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.2272, 0.2307, 0.2443, 0.2565, 0.2697, 0.2829, 0.2888, 0.2932, 0.3011,
2:         0.3071, 0.3162, 0.3293, 0.3416, 0.3510, 0.3561, 0.3587, 0.3544, 0.3440,
2:         0.2523, 0.2605, 0.2765, 0.2947, 0.3149, 0.3262, 0.3340],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.6808,  0.6645,  0.6370,  0.6048,  0.5650,  0.5258,  0.4809,  0.4155,
2:          0.3329,  0.2349,  0.1284,  0.0328, -0.0394, -0.0698, -0.0517,  0.0104,
2:          0.0927,  0.1754,  0.5936,  0.5564,  0.5155,  0.4773,  0.4362,  0.3944,
2:          0.3466], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1114, -0.0670, -0.0451, -0.0307, -0.0429, -0.0914, -0.1552, -0.2078,
2:         -0.2140, -0.1999, -0.1455, -0.0520,  0.0070,  0.0437,  0.0755,  0.0780,
2:          0.0340, -0.0566, -0.2281, -0.1796, -0.1221, -0.0876, -0.1034, -0.1597,
2:         -0.2314], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-1.9135, -1.9425, -1.9866, -2.0492, -2.1289, -2.2157, -2.2938, -2.3422,
2:         -2.3543, -2.3373, -2.3099, -2.2874, -2.2881, -2.3182, -2.3735, -2.4420,
2:         -2.5063, -2.5478, -2.5536, -2.5221, -2.4570, -2.3616, -2.2442, -2.1103,
2:         -1.9624], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1580, -0.1654, -0.1735, -0.1746, -0.1811, -0.1853, -0.1871, -0.1806,
2:         -0.1773, -0.1615, -0.1639, -0.1722, -0.1795, -0.1845, -0.1806, -0.1869,
2:         -0.1804, -0.1733, -0.1613, -0.1675, -0.1684, -0.1754, -0.1791, -0.1794,
2:         -0.1809], device='cuda:2', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 21, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2027, -0.2199, -0.2413, -0.2560, -0.2503, -0.2162, -0.1794, -0.1556,
3:         -0.1433, -0.1338, -0.1197, -0.1031, -0.0805, -0.0557, -0.0298, -0.0029,
3:          0.0210,  0.0427, -0.2144, -0.1925, -0.2127, -0.2390, -0.2344, -0.1988,
3:         -0.1656], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.0153,  0.0094, -0.0122, -0.0290, -0.0281, -0.0147,  0.0006,  0.0065,
3:         -0.0022, -0.0251, -0.0591, -0.1020, -0.1476, -0.1905, -0.2288, -0.2600,
3:         -0.2828, -0.2963,  0.0822,  0.0649,  0.0332,  0.0108,  0.0006,  0.0004,
3:          0.0047], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.0418, -0.0007, -0.1028, -0.3382, -0.4723, -0.4820, -0.3839, -0.2698,
3:         -0.1836, -0.1481, -0.1277, -0.1634, -0.2106, -0.2672, -0.3244, -0.3822,
3:         -0.4328, -0.4717,  0.1432, -0.1045, -0.3549, -0.4682, -0.4363, -0.2928,
3:         -0.1433], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.1505, -0.1319, -0.1729, -0.0976, -0.2094, -0.4841, -0.7033, -0.7565,
3:         -0.7377, -0.7565, -0.7476, -0.7886, -0.8262, -0.8694, -0.8561, -0.7764,
3:         -0.6491, -0.4807, -0.0633, -0.4996, -0.3390, -0.1740, -0.3357, -0.5904,
3:         -0.7676], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.8849, -0.7922, -0.7638, -0.7209, -0.6876, -0.6973, -0.7117, -0.7758,
3:         -0.8770, -1.0068, -1.1342, -1.2197, -1.2757, -1.2670, -1.2105, -1.1198,
3:         -0.9969, -0.8678, -0.7438, -0.6378, -0.5634, -0.5203, -0.5051, -0.5103,
3:         -0.5143], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 21, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([-0.2436,     nan,     nan,     nan, -0.2436,     nan, -0.2436, -0.2436,
3:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2436, -0.2436,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2436, -0.2436,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2436,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 21, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.2962, 0.2721, 0.2435, 0.2141, 0.1918, 0.1776, 0.1701, 0.1646, 0.1615,
3:         0.1565, 0.1493, 0.1340, 0.1150, 0.0961, 0.0759, 0.0636, 0.0643, 0.0751,
3:         0.3134, 0.2845, 0.2487, 0.2152, 0.1897, 0.1744, 0.1681],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.2161, 0.2218, 0.2363, 0.2543, 0.2675, 0.2755, 0.2746, 0.2715, 0.2726,
3:         0.2800, 0.2911, 0.3079, 0.3249, 0.3429, 0.3583, 0.3677, 0.3677, 0.3550,
3:         0.2367, 0.2465, 0.2667, 0.2924, 0.3142, 0.3223, 0.3213],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.6044,  0.6053,  0.5880,  0.5627,  0.5240,  0.4891,  0.4392,  0.3783,
3:          0.2910,  0.1925,  0.0909,  0.0095, -0.0397, -0.0344,  0.0159,  0.1011,
3:          0.1932,  0.2778,  0.5168,  0.4993,  0.4705,  0.4360,  0.3964,  0.3492,
3:          0.3029], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0073, -0.0173, -0.0351, -0.0455, -0.0401, -0.0666, -0.1320, -0.1963,
3:         -0.2470, -0.2286, -0.1552, -0.0729,  0.0121,  0.0566,  0.0754,  0.0758,
3:          0.0318, -0.0251, -0.0979, -0.1158, -0.0887, -0.0819, -0.0710, -0.1041,
3:         -0.1791], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-1.8962, -1.9416, -1.9948, -2.0580, -2.1307, -2.2087, -2.2762, -2.3183,
3:         -2.3313, -2.3197, -2.2984, -2.2775, -2.2705, -2.2815, -2.3128, -2.3621,
3:         -2.4206, -2.4700, -2.4935, -2.4791, -2.4250, -2.3325, -2.2151, -2.0851,
3:         -1.9458], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1727, -0.1753, -0.1810, -0.1829, -0.1838, -0.1841, -0.1849, -0.1797,
3:         -0.1784, -0.1767, -0.1724, -0.1799, -0.1833, -0.1859, -0.1834, -0.1870,
3:         -0.1840, -0.1778, -0.1729, -0.1744, -0.1749, -0.1751, -0.1787, -0.1783,
3:         -0.1822], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 21 [1/5 (20%)]	Loss: nan : nan :: 0.14830 (1.69 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 21 [2/5 (40%)]	Loss: nan : nan :: 0.14854 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 21 [3/5 (60%)]	Loss: nan : nan :: 0.13729 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 21 [4/5 (80%)]	Loss: nan : nan :: 0.14118 (10.42 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch21.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch21.mod
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 21 : nan
0: validation loss for velocity_u : 0.03400622680783272
0: validation loss for velocity_v : 0.06809690594673157
0: validation loss for specific_humidity : 0.02656138502061367
0: validation loss for velocity_z : 0.5028497576713562
0: validation loss for temperature : 0.08462858945131302
0: validation loss for total_precip : nan
0: 22 : 00:42:41 :: batch_size = 96, lr = 1.2205418857176618e-05
1: 22 : 00:42:41 :: batch_size = 96, lr = 1.2205418857176618e-05
2: 22 : 00:42:41 :: batch_size = 96, lr = 1.2205418857176618e-05
3: 22 : 00:42:41 :: batch_size = 96, lr = 1.2205418857176618e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 22, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0160, -0.0037, -0.0227, -0.0419, -0.0627, -0.0861, -0.1127, -0.1423,
0:         -0.1729, -0.2014, -0.2236, -0.2366, -0.2408, -0.2392, -0.2356, -0.2334,
0:         -0.2352, -0.2430,  0.0533,  0.0297,  0.0074, -0.0139, -0.0358, -0.0600,
0:         -0.0877], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0614,  0.0204, -0.0394, -0.1140, -0.1976, -0.2865, -0.3791, -0.4758,
0:         -0.5754, -0.6733, -0.7612, -0.8296, -0.8700, -0.8751, -0.8407, -0.7671,
0:         -0.6608, -0.5358, -0.0159, -0.0486, -0.1003, -0.1679, -0.2470, -0.3340,
0:         -0.4272], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5410, -0.5409, -0.5408, -0.5451, -0.5510, -0.5622, -0.5788, -0.5962,
0:         -0.6166, -0.6367, -0.6543, -0.6722, -0.6855, -0.6975, -0.7053, -0.7091,
0:         -0.7119, -0.7123, -0.5372, -0.5371, -0.5370, -0.5413, -0.5469, -0.5581,
0:         -0.5749], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6078, -0.4382, -0.2771, -0.1569, -0.0796, -0.0216,  0.0363,  0.0986,
0:          0.1501,  0.1802,  0.1866,  0.1652,  0.1093,  0.0299, -0.0474, -0.1032,
0:         -0.1290, -0.1333, -0.5713, -0.4253, -0.2685, -0.1354, -0.0409,  0.0235,
0:          0.0686], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.1723, 1.1084, 1.0497, 0.9977, 0.9516, 0.9109, 0.8779, 0.8535, 0.8364,
0:         0.8231, 0.8078, 0.7857, 0.7511, 0.7003, 0.6311, 0.5468, 0.4555, 0.3677,
0:         0.2921, 0.2332, 0.1916, 0.1663, 0.1541, 0.1516, 0.1551],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 22, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.1126,     nan,     nan,     nan,
0:             nan,     nan, -0.1418,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1149,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1171, -0.1081,     nan,     nan,     nan, -0.1441,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1328,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1418,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1575,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1733,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2114,     nan,
0:             nan,     nan,     nan,     nan, -0.1441,     nan,     nan,     nan,
0:         -0.1239,     nan,     nan,     nan,     nan,     nan,     nan, -0.1486,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1912,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1688,     nan,
0:             nan,     nan,     nan,     nan, -0.1822,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2159,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 22, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8016, 0.7934, 0.7845, 0.7762, 0.7625, 0.7484, 0.7309, 0.7143, 0.7011,
0:         0.6870, 0.6731, 0.6602, 0.6493, 0.6382, 0.6295, 0.6173, 0.6058, 0.5953,
0:         0.8280, 0.8163, 0.8059, 0.7931, 0.7797, 0.7630, 0.7484],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1685, 0.1886, 0.2050, 0.2210, 0.2410, 0.2692, 0.3044, 0.3458, 0.3905,
0:         0.4329, 0.4742, 0.5119, 0.5410, 0.5633, 0.5757, 0.5849, 0.5901, 0.5948,
0:         0.1100, 0.1343, 0.1568, 0.1764, 0.2024, 0.2326, 0.2691],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6620, -0.6606, -0.6619, -0.6608, -0.6633, -0.6663, -0.6676, -0.6646,
0:         -0.6620, -0.6573, -0.6523, -0.6470, -0.6442, -0.6437, -0.6435, -0.6462,
0:         -0.6474, -0.6484, -0.6607, -0.6602, -0.6592, -0.6596, -0.6624, -0.6647,
0:         -0.6660], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0667,  0.0517,  0.0249, -0.0197, -0.0563, -0.0825, -0.1213, -0.1513,
0:         -0.1533, -0.1642, -0.1574, -0.1131, -0.0803, -0.0717, -0.0701, -0.0669,
0:         -0.0581, -0.0489,  0.0694,  0.0567,  0.0287, -0.0237, -0.0563, -0.0714,
0:         -0.0989], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.2701, -0.2879, -0.2952, -0.2978, -0.2949, -0.2938, -0.2938, -0.2938,
0:         -0.2933, -0.2918, -0.2902, -0.2909, -0.2953, -0.3023, -0.3132, -0.3261,
0:         -0.3421, -0.3595, -0.3771, -0.3901, -0.3996, -0.4037, -0.4057, -0.4075,
0:         -0.4073], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1543, -0.1601, -0.1669, -0.1712, -0.1694, -0.1704, -0.1665, -0.1574,
0:         -0.1560, -0.1606, -0.1583, -0.1649, -0.1665, -0.1694, -0.1644, -0.1659,
0:         -0.1611, -0.1531, -0.1581, -0.1618, -0.1587, -0.1606, -0.1599, -0.1582,
0:         -0.1588], device='cuda:0', grad_fn=<SliceBackward0>)
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 22, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.0160, -0.0037, -0.0227, -0.0419, -0.0627, -0.0861, -0.1127, -0.1423,
3:         -0.1729, -0.2014, -0.2236, -0.2366, -0.2408, -0.2392, -0.2356, -0.2334,
3:         -0.2352, -0.2430,  0.0533,  0.0297,  0.0074, -0.0139, -0.0358, -0.0600,
3:         -0.0877], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.0614,  0.0204, -0.0394, -0.1140, -0.1976, -0.2865, -0.3791, -0.4758,
3:         -0.5754, -0.6733, -0.7612, -0.8296, -0.8700, -0.8751, -0.8407, -0.7671,
3:         -0.6608, -0.5358, -0.0159, -0.0486, -0.1003, -0.1679, -0.2470, -0.3340,
3:         -0.4272], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.5410, -0.5409, -0.5408, -0.5451, -0.5510, -0.5622, -0.5788, -0.5962,
3:         -0.6166, -0.6367, -0.6543, -0.6722, -0.6855, -0.6975, -0.7053, -0.7091,
3:         -0.7119, -0.7123, -0.5372, -0.5371, -0.5370, -0.5413, -0.5469, -0.5581,
3:         -0.5749], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.6078, -0.4382, -0.2771, -0.1569, -0.0796, -0.0216,  0.0363,  0.0986,
3:          0.1501,  0.1802,  0.1866,  0.1652,  0.1093,  0.0299, -0.0474, -0.1032,
3:         -0.1290, -0.1333, -0.5713, -0.4253, -0.2685, -0.1354, -0.0409,  0.0235,
3:          0.0686], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([1.1723, 1.1084, 1.0497, 0.9977, 0.9516, 0.9109, 0.8779, 0.8535, 0.8364,
3:         0.8231, 0.8078, 0.7857, 0.7511, 0.7003, 0.6311, 0.5468, 0.4555, 0.3677,
3:         0.2921, 0.2332, 0.1916, 0.1663, 0.1541, 0.1516, 0.1551],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 22, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan, -0.1351,     nan,     nan, -0.1126,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1575,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1912, -0.1912,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.1261,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.1486,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.1553,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.1822,     nan,
3:             nan,     nan, -0.1980,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2092, -0.2069, -0.1688,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1194, -0.1890,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.1980,     nan,     nan,     nan, -0.1935,     nan, -0.1935,
3:             nan,     nan, -0.2069,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2114,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 22, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.8281, 0.8171, 0.8055, 0.7928, 0.7760, 0.7574, 0.7388, 0.7167, 0.6954,
3:         0.6759, 0.6539, 0.6344, 0.6203, 0.6106, 0.6034, 0.5951, 0.5879, 0.5764,
3:         0.8432, 0.8326, 0.8195, 0.8032, 0.7852, 0.7669, 0.7520],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.2004, 0.2159, 0.2318, 0.2490, 0.2695, 0.2991, 0.3331, 0.3708, 0.4101,
3:         0.4499, 0.4837, 0.5164, 0.5424, 0.5629, 0.5741, 0.5807, 0.5839, 0.5851,
3:         0.1294, 0.1464, 0.1673, 0.1894, 0.2180, 0.2490, 0.2876],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6633, -0.6634, -0.6652, -0.6647, -0.6645, -0.6635, -0.6614, -0.6573,
3:         -0.6539, -0.6523, -0.6499, -0.6488, -0.6492, -0.6485, -0.6476, -0.6476,
3:         -0.6473, -0.6471, -0.6622, -0.6624, -0.6622, -0.6630, -0.6644, -0.6635,
3:         -0.6609], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0721,  0.0649,  0.0386,  0.0194,  0.0086, -0.0093, -0.0508, -0.0988,
3:         -0.1166, -0.1273, -0.1173, -0.0761, -0.0537, -0.0418, -0.0297, -0.0202,
3:         -0.0173, -0.0437,  0.1012,  0.0909,  0.0572,  0.0395,  0.0228, -0.0040,
3:         -0.0407], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.2638, -0.2742, -0.2776, -0.2779, -0.2748, -0.2731, -0.2709, -0.2682,
3:         -0.2663, -0.2655, -0.2671, -0.2722, -0.2810, -0.2922, -0.3055, -0.3194,
3:         -0.3364, -0.3556, -0.3749, -0.3901, -0.4004, -0.4040, -0.4045, -0.4060,
3:         -0.4075], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1471, -0.1538, -0.1620, -0.1599, -0.1629, -0.1640, -0.1624, -0.1538,
3:         -0.1544, -0.1553, -0.1534, -0.1593, -0.1627, -0.1638, -0.1607, -0.1626,
3:         -0.1562, -0.1452, -0.1570, -0.1540, -0.1543, -0.1551, -0.1548, -0.1541,
3:         -0.1523], device='cuda:3', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 22, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 22, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.0160, -0.0037, -0.0227, -0.0419, -0.0627, -0.0861, -0.1127, -0.1423,
2:         -0.1729, -0.2014, -0.2236, -0.2366, -0.2408, -0.2392, -0.2356, -0.2334,
2:         -0.2352, -0.2430,  0.0533,  0.0297,  0.0074, -0.0139, -0.0358, -0.0600,
2:         -0.0877], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.0614,  0.0204, -0.0394, -0.1140, -0.1976, -0.2865, -0.3791, -0.4758,
2:         -0.5754, -0.6733, -0.7612, -0.8296, -0.8700, -0.8751, -0.8407, -0.7671,
2:         -0.6608, -0.5358, -0.0159, -0.0486, -0.1003, -0.1679, -0.2470, -0.3340,
2:         -0.4272], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.5410, -0.5409, -0.5408, -0.5451, -0.5510, -0.5622, -0.5788, -0.5962,
2:         -0.6166, -0.6367, -0.6543, -0.6722, -0.6855, -0.6975, -0.7053, -0.7091,
2:         -0.7119, -0.7123, -0.5372, -0.5371, -0.5370, -0.5413, -0.5469, -0.5581,
2:         -0.5749], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.6078, -0.4382, -0.2771, -0.1569, -0.0796, -0.0216,  0.0363,  0.0986,
2:          0.1501,  0.1802,  0.1866,  0.1652,  0.1093,  0.0299, -0.0474, -0.1032,
2:         -0.1290, -0.1333, -0.5713, -0.4253, -0.2685, -0.1354, -0.0409,  0.0235,
2:          0.0686], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([1.1723, 1.1084, 1.0497, 0.9977, 0.9516, 0.9109, 0.8779, 0.8535, 0.8364,
2:         0.8231, 0.8078, 0.7857, 0.7511, 0.7003, 0.6311, 0.5468, 0.4555, 0.3677,
2:         0.2921, 0.2332, 0.1916, 0.1663, 0.1541, 0.1516, 0.1551],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 22, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.0857,     nan,     nan, -0.1306,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.1081,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1104,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.1508,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1778,
2:             nan,     nan,     nan,     nan, -0.1396,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.1575,     nan,
2:             nan,     nan, -0.1486, -0.1486,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1553,     nan,     nan,     nan, -0.1643,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1733,
2:             nan,     nan,     nan, -0.1710,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2025,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.1553,     nan,     nan,     nan, -0.1284,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1890,
2:             nan,     nan,     nan,     nan, -0.1688,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1957,     nan,     nan,     nan,     nan, -0.1935,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2182, -0.2182])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 22, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.8116, 0.7971, 0.7836, 0.7703, 0.7531, 0.7375, 0.7200, 0.7010, 0.6851,
2:         0.6689, 0.6508, 0.6341, 0.6219, 0.6130, 0.6064, 0.5978, 0.5903, 0.5830,
2:         0.8344, 0.8192, 0.8068, 0.7905, 0.7743, 0.7571, 0.7428],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.1987, 0.2213, 0.2378, 0.2521, 0.2668, 0.2913, 0.3206, 0.3558, 0.3926,
2:         0.4311, 0.4674, 0.5042, 0.5361, 0.5633, 0.5799, 0.5925, 0.6027, 0.6116,
2:         0.1326, 0.1556, 0.1757, 0.1928, 0.2155, 0.2412, 0.2764],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6617, -0.6631, -0.6655, -0.6640, -0.6628, -0.6621, -0.6585, -0.6547,
2:         -0.6520, -0.6480, -0.6442, -0.6427, -0.6404, -0.6404, -0.6403, -0.6394,
2:         -0.6378, -0.6359, -0.6555, -0.6570, -0.6578, -0.6569, -0.6575, -0.6559,
2:         -0.6524], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0599,  0.0643,  0.0527,  0.0269,  0.0117, -0.0168, -0.0697, -0.1027,
2:         -0.1002, -0.0899, -0.0707, -0.0425, -0.0195, -0.0108, -0.0171, -0.0368,
2:         -0.0616, -0.0888,  0.0913,  0.0864,  0.0742,  0.0473,  0.0446,  0.0270,
2:         -0.0168], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.2861, -0.2914, -0.2891, -0.2847, -0.2785, -0.2776, -0.2799, -0.2842,
2:         -0.2881, -0.2896, -0.2895, -0.2908, -0.2950, -0.3023, -0.3138, -0.3274,
2:         -0.3438, -0.3613, -0.3787, -0.3914, -0.4008, -0.4065, -0.4118, -0.4196,
2:         -0.4274], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1474, -0.1514, -0.1586, -0.1609, -0.1634, -0.1618, -0.1626, -0.1526,
2:         -0.1493, -0.1566, -0.1534, -0.1605, -0.1654, -0.1683, -0.1671, -0.1666,
2:         -0.1613, -0.1506, -0.1592, -0.1624, -0.1624, -0.1630, -0.1642, -0.1653,
2:         -0.1658], device='cuda:2', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([ 0.0160, -0.0037, -0.0227, -0.0419, -0.0627, -0.0861, -0.1127, -0.1423,
1:         -0.1729, -0.2014, -0.2236, -0.2366, -0.2408, -0.2392, -0.2356, -0.2334,
1:         -0.2352, -0.2430,  0.0533,  0.0297,  0.0074, -0.0139, -0.0358, -0.0600,
1:         -0.0877], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.0614,  0.0204, -0.0394, -0.1140, -0.1976, -0.2865, -0.3791, -0.4758,
1:         -0.5754, -0.6733, -0.7612, -0.8296, -0.8700, -0.8751, -0.8407, -0.7671,
1:         -0.6608, -0.5358, -0.0159, -0.0486, -0.1003, -0.1679, -0.2470, -0.3340,
1:         -0.4272], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.5410, -0.5409, -0.5408, -0.5451, -0.5510, -0.5622, -0.5788, -0.5962,
1:         -0.6166, -0.6367, -0.6543, -0.6722, -0.6855, -0.6975, -0.7053, -0.7091,
1:         -0.7119, -0.7123, -0.5372, -0.5371, -0.5370, -0.5413, -0.5469, -0.5581,
1:         -0.5749], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.6078, -0.4382, -0.2771, -0.1569, -0.0796, -0.0216,  0.0363,  0.0986,
1:          0.1501,  0.1802,  0.1866,  0.1652,  0.1093,  0.0299, -0.0474, -0.1032,
1:         -0.1290, -0.1333, -0.5713, -0.4253, -0.2685, -0.1354, -0.0409,  0.0235,
1:          0.0686], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([1.1723, 1.1084, 1.0497, 0.9977, 0.9516, 0.9109, 0.8779, 0.8535, 0.8364,
1:         0.8231, 0.8078, 0.7857, 0.7511, 0.7003, 0.6311, 0.5468, 0.4555, 0.3677,
1:         0.2921, 0.2332, 0.1916, 0.1663, 0.1541, 0.1516, 0.1551],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 22, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1171,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1351,     nan, -0.1261,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1688,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.1643,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1845,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2137,     nan,     nan,     nan,
1:             nan,     nan, -0.1688,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.1890,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1890,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2002, -0.1912,     nan,
1:             nan,     nan, -0.1620,     nan,     nan,     nan,     nan, -0.1890,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.1800,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2114,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 22, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.8250, 0.8135, 0.8008, 0.7903, 0.7753, 0.7562, 0.7382, 0.7139, 0.6950,
1:         0.6789, 0.6625, 0.6477, 0.6349, 0.6228, 0.6105, 0.5977, 0.5892, 0.5788,
1:         0.8435, 0.8330, 0.8206, 0.8079, 0.7929, 0.7738, 0.7552],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.1939, 0.2066, 0.2194, 0.2351, 0.2573, 0.2849, 0.3155, 0.3494, 0.3850,
1:         0.4215, 0.4592, 0.4967, 0.5280, 0.5556, 0.5734, 0.5859, 0.5959, 0.6055,
1:         0.1302, 0.1476, 0.1654, 0.1866, 0.2115, 0.2428, 0.2769],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6538, -0.6545, -0.6570, -0.6582, -0.6606, -0.6604, -0.6596, -0.6561,
1:         -0.6544, -0.6522, -0.6490, -0.6468, -0.6451, -0.6438, -0.6427, -0.6439,
1:         -0.6448, -0.6442, -0.6517, -0.6533, -0.6537, -0.6565, -0.6590, -0.6591,
1:         -0.6578], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.1050,  0.1287,  0.1011,  0.0514,  0.0152, -0.0389, -0.1055, -0.1392,
1:         -0.1353, -0.1336, -0.1240, -0.0868, -0.0600, -0.0498, -0.0284, -0.0007,
1:          0.0177,  0.0006,  0.0984,  0.1084,  0.0838,  0.0431,  0.0232, -0.0183,
1:         -0.0722], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.3177, -0.3180, -0.3095, -0.2983, -0.2881, -0.2831, -0.2826, -0.2841,
1:         -0.2855, -0.2866, -0.2885, -0.2918, -0.2959, -0.3004, -0.3067, -0.3168,
1:         -0.3329, -0.3531, -0.3748, -0.3912, -0.4010, -0.4036, -0.4034, -0.4043,
1:         -0.4063], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1612, -0.1617, -0.1697, -0.1678, -0.1636, -0.1651, -0.1632, -0.1517,
1:         -0.1500, -0.1721, -0.1678, -0.1744, -0.1735, -0.1714, -0.1667, -0.1627,
1:         -0.1570, -0.1474, -0.1710, -0.1727, -0.1704, -0.1686, -0.1673, -0.1647,
1:         -0.1592], device='cuda:1', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 22 [1/5 (20%)]	Loss: nan : nan :: 0.16657 (1.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 22 [2/5 (40%)]	Loss: nan : nan :: 0.16994 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 22 [3/5 (60%)]	Loss: nan : nan :: 0.13987 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 22 [4/5 (80%)]	Loss: nan : nan :: 0.14291 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch22.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch22.mod
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 22 : nan
0: validation loss for velocity_u : 0.033264461904764175
0: validation loss for velocity_v : 0.05998024716973305
0: validation loss for specific_humidity : 0.025577202439308167
0: validation loss for velocity_z : 0.4364522099494934
0: validation loss for temperature : 0.07059825211763382
0: validation loss for total_precip : nan
0: 23 : 00:49:32 :: batch_size = 96, lr = 1.1907725714318652e-05
1: 23 : 00:49:32 :: batch_size = 96, lr = 1.1907725714318652e-05
2: 23 : 00:49:32 :: batch_size = 96, lr = 1.1907725714318652e-05
3: 23 : 00:49:32 :: batch_size = 96, lr = 1.1907725714318652e-05
1: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 23, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1:      first 25 values: tensor([-0.4606, -0.4705, -0.4839, -0.4955, -0.5009, -0.4954, -0.4887, -0.4848,
1:         -0.4880, -0.5054, -0.5225, -0.5302, -0.5444, -0.5696, -0.5807, -0.5850,
1:         -0.6025, -0.6384, -0.4709, -0.4736, -0.4807, -0.4874, -0.4903, -0.4854,
1:         -0.4779], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2074, -0.2097, -0.2201, -0.2344, -0.2507, -0.2625, -0.2677, -0.2719,
1:         -0.2793, -0.2901, -0.3007, -0.3208, -0.3431, -0.3429, -0.3413, -0.3541,
1:         -0.3703, -0.3910, -0.1948, -0.1958, -0.2053, -0.2221, -0.2420, -0.2576,
1:         -0.2667], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.7174, 1.6823, 1.6484, 1.6067, 1.5704, 1.5609, 1.5221, 1.4916, 1.4591,
1:         1.4309, 1.4309, 1.4182, 1.4056, 1.5496, 1.6341, 1.4182, 1.2726, 1.1575,
1:         1.5729, 1.5338, 1.4841, 1.4424, 1.4355, 1.4320, 1.4388],
1:        device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.2369, 0.2592, 0.2056, 0.2615, 0.3621, 0.4604, 0.5253, 0.4582, 0.5298,
1:         0.5096, 0.3196, 0.2905, 0.4336, 0.3487, 0.2056, 0.0513, 0.3263, 0.4515,
1:         0.2548, 0.2235, 0.1944, 0.1698, 0.2659, 0.3733, 0.4448],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-2.1787, -2.1384, -2.1171, -2.1432, -2.1843, -2.2457, -2.2835, -2.2840,
1:         -2.2683, -2.2237, -2.1591, -2.0758, -2.0328, -2.0882, -2.1874, -2.1757,
1:         -2.1574, -2.1420, -1.9147, -1.6921, -1.5316, -1.2525, -0.9577, -0.7916,
1:         -0.7247], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 23, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([ 5.0491e-01,         nan,  1.0639e+00,         nan,         nan,
1:                 nan,         nan,  2.4401e+00,         nan,  4.5794e-01,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan, -1.9495e-01,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan, -2.4192e-01,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,  4.0911e+00,         nan,
1:                 nan,         nan,  1.4678e+00,         nan,         nan,
1:                 nan,         nan,         nan,  9.2060e-01,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,  3.4991e-01,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan, -2.1843e-01,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,  1.5664e+00,
1:                 nan,         nan,         nan,         nan,         nan,
1:          5.2840e-01,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan, -2.3694e-03,
1:                 nan,  3.4521e-01,         nan,         nan,  8.7833e-01,
1:                 nan,  8.6894e-01,         nan,         nan, -2.3506e-02,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,  3.9453e-01,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,         nan,         nan,         nan,
1:                 nan,         nan,  1.1271e-01,         nan, -2.2313e-01,
1:                 nan, -1.5502e-01,         nan,         nan,         nan,
1:                 nan,         nan,         nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 23, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.2944, -0.2942, -0.3035, -0.3226, -0.3508, -0.3894, -0.4296, -0.4672,
1:         -0.4849, -0.4781, -0.4464, -0.3998, -0.3518, -0.3172, -0.3078, -0.3210,
1:         -0.3423, -0.3609, -0.2961, -0.2862, -0.2835, -0.2932, -0.3219, -0.3658,
1:         -0.4188], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1863, -0.2042, -0.2168, -0.2166, -0.2106, -0.1954, -0.1744, -0.1519,
1:         -0.1266, -0.1033, -0.0870, -0.0880, -0.1095, -0.1483, -0.1887, -0.2233,
1:         -0.2347, -0.2264, -0.1689, -0.1891, -0.2039, -0.2054, -0.1999, -0.1926,
1:         -0.1826], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.8766, 0.9779, 1.0867, 1.2159, 1.3745, 1.5805, 1.8460, 2.1765, 2.5150,
1:         2.7988, 2.9511, 2.9138, 2.6969, 2.3914, 2.0980, 1.9204, 1.8918, 1.9850,
1:         0.8299, 0.9559, 1.0870, 1.2078, 1.3388, 1.5043, 1.7541],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.0156, -0.2112, -0.6413, -1.1183, -1.4681, -1.5850, -1.5547, -1.3527,
1:         -0.8246, -0.3293, -0.0470,  0.3984,  0.7230,  0.5836,  0.3508,  0.2729,
1:          0.2748,  0.2517,  0.1959, -0.0829, -0.5608, -1.0577, -1.3961, -1.5120,
1:         -1.5224], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.0049,  0.0560,  0.0676, -0.0109, -0.2014, -0.4805, -0.8053, -1.1211,
1:         -1.3795, -1.5602, -1.6501, -1.6386, -1.5290, -1.3499, -1.1406, -0.9585,
1:         -0.8183, -0.7100, -0.5982, -0.4630, -0.3298, -0.2508, -0.2733, -0.4028,
1:         -0.6113], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1374, -0.1419, -0.1502, -0.1526, -0.1587, -0.1617, -0.1643, -0.1596,
1:         -0.1582, -0.1441, -0.1442, -0.1528, -0.1598, -0.1603, -0.1612, -0.1661,
1:         -0.1634, -0.1561, -0.1516, -0.1532, -0.1523, -0.1545, -0.1559, -0.1573,
1:         -0.1605], device='cuda:1', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 23, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 23, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4606, -0.4705, -0.4839, -0.4955, -0.5009, -0.4954, -0.4887, -0.4848,
0:         -0.4880, -0.5054, -0.5225, -0.5302, -0.5444, -0.5696, -0.5807, -0.5850,
0:         -0.6025, -0.6384, -0.4709, -0.4736, -0.4807, -0.4874, -0.4903, -0.4854,
0:         -0.4779], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2074, -0.2097, -0.2201, -0.2344, -0.2507, -0.2625, -0.2677, -0.2719,
0:         -0.2793, -0.2901, -0.3007, -0.3208, -0.3431, -0.3429, -0.3413, -0.3541,
0:         -0.3703, -0.3910, -0.1948, -0.1958, -0.2053, -0.2221, -0.2420, -0.2576,
0:         -0.2667], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.7174, 1.6823, 1.6484, 1.6067, 1.5704, 1.5609, 1.5221, 1.4916, 1.4591,
0:         1.4309, 1.4309, 1.4182, 1.4056, 1.5496, 1.6341, 1.4182, 1.2726, 1.1575,
0:         1.5729, 1.5338, 1.4841, 1.4424, 1.4355, 1.4320, 1.4388],
0:        device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2369, 0.2592, 0.2056, 0.2615, 0.3621, 0.4604, 0.5253, 0.4582, 0.5298,
0:         0.5096, 0.3196, 0.2905, 0.4336, 0.3487, 0.2056, 0.0513, 0.3263, 0.4515,
0:         0.2548, 0.2235, 0.1944, 0.1698, 0.2659, 0.3733, 0.4448],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-2.1787, -2.1384, -2.1171, -2.1432, -2.1843, -2.2457, -2.2835, -2.2840,
0:         -2.2683, -2.2237, -2.1591, -2.0758, -2.0328, -2.0882, -2.1874, -2.1757,
0:         -2.1574, -2.1420, -1.9147, -1.6921, -1.5316, -1.2525, -0.9577, -0.7916,
0:         -0.7247], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 23, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,  3.1869,     nan,     nan,  2.4401,
0:          1.1414,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.8877,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.1479,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.0681,     nan, -0.2466,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2255,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2419,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0634,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1034,     nan,
0:         -0.1245,     nan,     nan,     nan,  3.1822,     nan,     nan,     nan,
0:             nan,  1.3974,  0.7280,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          1.8671,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.8572,     nan,     nan,  0.4180,     nan, -0.1903,     nan,
0:             nan,     nan,     nan,  0.6623,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2020,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.0423,     nan,
0:             nan,     nan,  1.0662,  1.3433,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  2.1888,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.9582,     nan,     nan,     nan, -0.0235,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.3945,     nan,     nan,     nan,
0:             nan,     nan, -0.0423,     nan,     nan,     nan,     nan,  0.0282,
0:             nan,     nan,     nan,     nan,  0.0305,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 23, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3393, -0.3332, -0.3354, -0.3466, -0.3677, -0.4013, -0.4341, -0.4666,
0:         -0.4839, -0.4776, -0.4471, -0.4012, -0.3521, -0.3148, -0.3013, -0.3101,
0:         -0.3343, -0.3594, -0.3330, -0.3128, -0.3032, -0.3059, -0.3270, -0.3643,
0:         -0.4104], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1634, -0.1928, -0.2107, -0.2127, -0.2066, -0.1888, -0.1631, -0.1313,
0:         -0.0939, -0.0585, -0.0365, -0.0372, -0.0643, -0.1134, -0.1676, -0.2130,
0:         -0.2326, -0.2276, -0.1475, -0.1815, -0.2059, -0.2112, -0.2080, -0.1997,
0:         -0.1855], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8879, 0.9697, 1.0578, 1.1772, 1.3357, 1.5424, 1.8017, 2.1225, 2.4541,
0:         2.7287, 2.8795, 2.8620, 2.6846, 2.4339, 2.2007, 2.0655, 2.0568, 2.1441,
0:         0.8647, 0.9710, 1.0838, 1.2049, 1.3424, 1.5091, 1.7489],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0167, -0.1622, -0.6107, -1.1472, -1.5617, -1.7417, -1.7708, -1.5813,
0:         -0.9697, -0.4507, -0.2204,  0.2954,  0.7287,  0.5635,  0.2865,  0.2479,
0:          0.3039,  0.2867,  0.1446, -0.0374, -0.4959, -1.0119, -1.4259, -1.6260,
0:         -1.7177], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0410,  0.0141,  0.0246, -0.0469, -0.2189, -0.4834, -0.8075, -1.1439,
0:         -1.4329, -1.6321, -1.7129, -1.6676, -1.5183, -1.3090, -1.0933, -0.9131,
0:         -0.7762, -0.6624, -0.5438, -0.4160, -0.3062, -0.2601, -0.3064, -0.4385,
0:         -0.6305], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1412, -0.1426, -0.1495, -0.1524, -0.1563, -0.1615, -0.1653, -0.1637,
0:         -0.1636, -0.1489, -0.1442, -0.1500, -0.1548, -0.1611, -0.1603, -0.1666,
0:         -0.1687, -0.1592, -0.1521, -0.1498, -0.1482, -0.1509, -0.1564, -0.1594,
0:         -0.1613], device='cuda:0', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([-0.4606, -0.4705, -0.4839, -0.4955, -0.5009, -0.4954, -0.4887, -0.4848,
3:         -0.4880, -0.5054, -0.5225, -0.5302, -0.5444, -0.5696, -0.5807, -0.5850,
3:         -0.6025, -0.6384, -0.4709, -0.4736, -0.4807, -0.4874, -0.4903, -0.4854,
3:         -0.4779], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2074, -0.2097, -0.2201, -0.2344, -0.2507, -0.2625, -0.2677, -0.2719,
3:         -0.2793, -0.2901, -0.3007, -0.3208, -0.3431, -0.3429, -0.3413, -0.3541,
3:         -0.3703, -0.3910, -0.1948, -0.1958, -0.2053, -0.2221, -0.2420, -0.2576,
3:         -0.2667], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.7174, 1.6823, 1.6484, 1.6067, 1.5704, 1.5609, 1.5221, 1.4916, 1.4591,
3:         1.4309, 1.4309, 1.4182, 1.4056, 1.5496, 1.6341, 1.4182, 1.2726, 1.1575,
3:         1.5729, 1.5338, 1.4841, 1.4424, 1.4355, 1.4320, 1.4388],
3:        device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.2369, 0.2592, 0.2056, 0.2615, 0.3621, 0.4604, 0.5253, 0.4582, 0.5298,
3:         0.5096, 0.3196, 0.2905, 0.4336, 0.3487, 0.2056, 0.0513, 0.3263, 0.4515,
3:         0.2548, 0.2235, 0.1944, 0.1698, 0.2659, 0.3733, 0.4448],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-2.1787, -2.1384, -2.1171, -2.1432, -2.1843, -2.2457, -2.2835, -2.2840,
3:         -2.2683, -2.2237, -2.1591, -2.0758, -2.0328, -2.0882, -2.1874, -2.1757,
3:         -2.1574, -2.1420, -1.9147, -1.6921, -1.5316, -1.2525, -0.9577, -0.7916,
3:         -0.7247], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 23, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  0.4579,     nan,     nan,     nan,  2.7947,  3.1447,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,  0.0681, -0.2466,     nan,     nan, -0.2161,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2255,     nan,     nan,     nan,  0.2254,  0.2395,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1809,
3:         -0.1245,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.1949,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2208,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.1456,     nan, -0.2161,     nan,  0.0094,     nan,     nan,     nan,
3:             nan,     nan,  1.0662,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  1.1414,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,  2.1888,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2067,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.0423,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.0352,     nan,     nan,     nan, -0.0423,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 23, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.3281, -0.3298, -0.3370, -0.3501, -0.3716, -0.4059, -0.4429, -0.4791,
3:         -0.5013, -0.5003, -0.4757, -0.4313, -0.3803, -0.3390, -0.3190, -0.3234,
3:         -0.3428, -0.3645, -0.3223, -0.3137, -0.3112, -0.3152, -0.3354, -0.3716,
3:         -0.4190], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.1811, -0.2022, -0.2119, -0.2089, -0.2016, -0.1898, -0.1709, -0.1442,
3:         -0.1127, -0.0821, -0.0631, -0.0669, -0.0932, -0.1371, -0.1847, -0.2204,
3:         -0.2357, -0.2318, -0.1696, -0.1941, -0.2082, -0.2082, -0.2025, -0.1973,
3:         -0.1895], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.9526, 1.0423, 1.1430, 1.2564, 1.3923, 1.5625, 1.7849, 2.0706, 2.3846,
3:         2.6709, 2.8499, 2.8649, 2.7145, 2.4702, 2.2205, 2.0654, 2.0315, 2.1025,
3:         0.8821, 1.0000, 1.1219, 1.2423, 1.3679, 1.5193, 1.7305],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.0050, -0.1443, -0.5217, -1.0382, -1.4977, -1.7334, -1.7423, -1.4956,
3:         -0.9011, -0.3874, -0.1369,  0.3170,  0.6534,  0.5013,  0.3102,  0.3063,
3:          0.3410,  0.3061,  0.1574, -0.0145, -0.4035, -0.9022, -1.3167, -1.5271,
3:         -1.5744], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-6.8872e-02,  9.7516e-04,  2.5961e-02, -3.5727e-02, -2.0639e-01,
3:         -4.6726e-01, -7.8517e-01, -1.1039e+00, -1.3752e+00, -1.5660e+00,
3:         -1.6541e+00, -1.6344e+00, -1.5158e+00, -1.3321e+00, -1.1279e+00,
3:         -9.4637e-01, -7.9769e-01, -6.6679e-01, -5.2918e-01, -3.8064e-01,
3:         -2.5338e-01, -2.0016e-01, -2.5808e-01, -4.2179e-01, -6.6031e-01],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1140, -0.1197, -0.1300, -0.1355, -0.1447, -0.1554, -0.1616, -0.1583,
3:         -0.1629, -0.1245, -0.1236, -0.1318, -0.1402, -0.1495, -0.1539, -0.1636,
3:         -0.1662, -0.1606, -0.1259, -0.1317, -0.1298, -0.1352, -0.1439, -0.1504,
3:         -0.1578], device='cuda:3', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 23, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.4606, -0.4705, -0.4839, -0.4955, -0.5009, -0.4954, -0.4887, -0.4848,
2:         -0.4880, -0.5054, -0.5225, -0.5302, -0.5444, -0.5696, -0.5807, -0.5850,
2:         -0.6025, -0.6384, -0.4709, -0.4736, -0.4807, -0.4874, -0.4903, -0.4854,
2:         -0.4779], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2074, -0.2097, -0.2201, -0.2344, -0.2507, -0.2625, -0.2677, -0.2719,
2:         -0.2793, -0.2901, -0.3007, -0.3208, -0.3431, -0.3429, -0.3413, -0.3541,
2:         -0.3703, -0.3910, -0.1948, -0.1958, -0.2053, -0.2221, -0.2420, -0.2576,
2:         -0.2667], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.7174, 1.6823, 1.6484, 1.6067, 1.5704, 1.5609, 1.5221, 1.4916, 1.4591,
2:         1.4309, 1.4309, 1.4182, 1.4056, 1.5496, 1.6341, 1.4182, 1.2726, 1.1575,
2:         1.5729, 1.5338, 1.4841, 1.4424, 1.4355, 1.4320, 1.4388],
2:        device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.2369, 0.2592, 0.2056, 0.2615, 0.3621, 0.4604, 0.5253, 0.4582, 0.5298,
2:         0.5096, 0.3196, 0.2905, 0.4336, 0.3487, 0.2056, 0.0513, 0.3263, 0.4515,
2:         0.2548, 0.2235, 0.1944, 0.1698, 0.2659, 0.3733, 0.4448],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-2.1787, -2.1384, -2.1171, -2.1432, -2.1843, -2.2457, -2.2835, -2.2840,
2:         -2.2683, -2.2237, -2.1591, -2.0758, -2.0328, -2.0882, -2.1874, -2.1757,
2:         -2.1574, -2.1420, -1.9147, -1.6921, -1.5316, -1.2525, -0.9577, -0.7916,
2:         -0.7247], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 23, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([        nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,  7.4446e-01,         nan,         nan,         nan,
2:                 nan,         nan,         nan,  1.8551e-01,         nan,
2:                 nan,         nan,  1.5148e+00,         nan,         nan,
2:                 nan,         nan,         nan, -1.9495e-01,         nan,
2:          6.3390e-02,         nan,         nan,         nan,         nan,
2:                 nan,         nan, -2.4662e-01,         nan,         nan,
2:                 nan,  3.8984e-01,  4.5325e-01,         nan,  9.6269e-02,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,  2.3277e-03,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:          3.5369e+00,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,  1.7144e+00,         nan,
2:                 nan,         nan,         nan,         nan, -4.2294e-02,
2:          1.1271e-01,         nan,  6.3408e-01,         nan,         nan,
2:          7.6560e-01,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,  4.6762e-03,
2:                 nan,         nan, -1.4563e-01,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:          5.3995e-02,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,  9.1121e-01,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:          3.2877e-01,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:          5.6598e-01,         nan,         nan,         nan,         nan,
2:          1.1271e-01,         nan,         nan,         nan,         nan,
2:                 nan,         nan, -2.0669e-01,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan,         nan,         nan,
2:                 nan,         nan,         nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 23, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.3093, -0.3091, -0.3166, -0.3329, -0.3588, -0.3970, -0.4358, -0.4711,
2:         -0.4904, -0.4820, -0.4496, -0.3988, -0.3456, -0.3053, -0.2887, -0.2967,
2:         -0.3209, -0.3466, -0.3130, -0.2992, -0.2931, -0.2994, -0.3243, -0.3653,
2:         -0.4168], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1882, -0.2125, -0.2215, -0.2149, -0.2018, -0.1845, -0.1659, -0.1462,
2:         -0.1234, -0.1038, -0.0900, -0.0934, -0.1161, -0.1534, -0.1951, -0.2263,
2:         -0.2418, -0.2354, -0.1818, -0.2092, -0.2214, -0.2137, -0.1968, -0.1832,
2:         -0.1733], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.1146, 1.1912, 1.2844, 1.3965, 1.5462, 1.7457, 2.0033, 2.3115, 2.6170,
2:         2.8610, 2.9828, 2.9440, 2.7625, 2.5171, 2.2803, 2.1391, 2.1041, 2.1609,
2:         0.9950, 1.0943, 1.2049, 1.3208, 1.4562, 1.6415, 1.9037],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.3275, -0.6060, -1.1585, -1.7863, -2.1983, -2.2637, -2.0813, -1.6923,
2:         -1.0072, -0.4117, -0.1030,  0.3433,  0.6176,  0.4301,  0.2180,  0.1509,
2:          0.1573,  0.1623, -0.0769, -0.3993, -0.9646, -1.5641, -1.9312, -1.9798,
2:         -1.8542], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-6.4215e-02,  1.1735e-03,  2.3617e-02, -3.8600e-02, -2.0707e-01,
2:         -4.6833e-01, -7.9019e-01, -1.1231e+00, -1.4137e+00, -1.6258e+00,
2:         -1.7337e+00, -1.7237e+00, -1.6085e+00, -1.4215e+00, -1.2123e+00,
2:         -1.0271e+00, -8.7958e-01, -7.5268e-01, -6.1884e-01, -4.6814e-01,
2:         -3.2609e-01, -2.4926e-01, -2.7498e-01, -4.0313e-01, -6.0817e-01],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1029, -0.1098, -0.1218, -0.1250, -0.1336, -0.1460, -0.1507, -0.1479,
2:         -0.1474, -0.1125, -0.1128, -0.1230, -0.1343, -0.1436, -0.1438, -0.1526,
2:         -0.1551, -0.1483, -0.1137, -0.1216, -0.1230, -0.1269, -0.1389, -0.1421,
2:         -0.1509], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 23 [1/5 (20%)]	Loss: nan : nan :: 0.13533 (1.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 23 [2/5 (40%)]	Loss: nan : nan :: 0.14078 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 23 [3/5 (60%)]	Loss: nan : nan :: 0.13524 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 23 [4/5 (80%)]	Loss: nan : nan :: 0.14507 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch23.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch23.mod
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 23 : nan
0: validation loss for velocity_u : 0.03485869616270065
0: validation loss for velocity_v : 0.07089004665613174
0: validation loss for specific_humidity : 0.02492610365152359
0: validation loss for velocity_z : 0.4937003552913666
0: validation loss for temperature : 0.08329412341117859
0: validation loss for total_precip : nan
1: 24 : 00:56:11 :: batch_size = 96, lr = 1.1617293379823076e-05
2: 24 : 00:56:11 :: batch_size = 96, lr = 1.1617293379823076e-05
0: 24 : 00:56:11 :: batch_size = 96, lr = 1.1617293379823076e-05
3: 24 : 00:56:11 :: batch_size = 96, lr = 1.1617293379823076e-05
1: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 24, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.1080,  0.1375,  0.1662,  0.1944,  0.2217,  0.2485,  0.2745,  0.2997,
1:          0.3235,  0.3461,  0.3678,  0.3890,  0.4098,  0.4295,  0.4484,  0.4670,
1:          0.4859,  0.5050, -0.3828, -0.3519, -0.3213, -0.2912, -0.2612, -0.2313,
1:         -0.2015], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2837, -0.2665, -0.2499, -0.2342, -0.2199, -0.2068, -0.1939, -0.1809,
1:         -0.1674, -0.1541, -0.1407, -0.1264, -0.1112, -0.0955, -0.0791, -0.0613,
1:         -0.0416, -0.0198, -0.2130, -0.1977, -0.1830, -0.1691, -0.1560, -0.1442,
1:         -0.1330], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7569, -0.7567, -0.7565, -0.7565, -0.7566, -0.7565, -0.7564, -0.7563,
1:         -0.7562, -0.7566, -0.7574, -0.7583, -0.7604, -0.7621, -0.7645, -0.7678,
1:         -0.7709, -0.7742, -0.7707, -0.7706, -0.7704, -0.7702, -0.7699, -0.7697,
1:         -0.7694], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.6133,  0.5915,  0.5697,  0.5435,  0.5151,  0.4802,  0.4322,  0.3820,
1:          0.3340,  0.2882,  0.2424,  0.1944,  0.1485,  0.1027,  0.0591,  0.0111,
1:         -0.0326, -0.0740,  0.5391,  0.5566,  0.5587,  0.5522,  0.5457,  0.5435,
1:          0.5347], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([1.1736, 1.1725, 1.1714, 1.1707, 1.1700, 1.1694, 1.1689, 1.1684, 1.1675,
1:         1.1671, 1.1672, 1.1672, 1.1675, 1.1676, 1.1684, 1.1685, 1.1685, 1.1674,
1:         1.1662, 1.1647, 1.1626, 1.1603, 1.1578, 1.1550, 1.1520],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 24, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2229,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2012, -0.1966,     nan,     nan,
1:             nan,     nan, -0.2149,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1818,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1818,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.1944,     nan,     nan,     nan, -0.2309,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2126,     nan,     nan,     nan, -0.1898,     nan,
1:             nan, -0.2343,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2252,     nan,
1:             nan, -0.2149,     nan,     nan,     nan, -0.2263,     nan,     nan,
1:         -0.2286,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2103,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2411, -0.2389,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2343,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2331,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2331,     nan,     nan, -0.2195,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1966,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2012,     nan,     nan,
1:             nan, -0.1784, -0.1715,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2354,     nan,     nan,     nan,     nan,     nan,
1:         -0.2103,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2491,     nan,     nan, -0.2468,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 24, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.0112, 0.9811, 0.9596, 0.9364, 0.9119, 0.8832, 0.8645, 0.8406, 0.8243,
1:         0.8173, 0.8070, 0.7928, 0.7827, 0.7718, 0.7604, 0.7513, 0.7425, 0.7346,
1:         1.1313, 1.1201, 1.1073, 1.0881, 1.0581, 1.0225, 0.9892],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.1846, -1.1590, -1.1410, -1.1315, -1.1324, -1.1266, -1.1195, -1.1085,
1:         -1.0980, -1.0862, -1.0812, -1.0724, -1.0640, -1.0522, -1.0412, -1.0302,
1:         -1.0063, -0.9776, -1.1905, -1.1684, -1.1506, -1.1448, -1.1431, -1.1404,
1:         -1.1287], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.1680, 0.1681, 0.1556, 0.1375, 0.1143, 0.0948, 0.0751, 0.0680, 0.0628,
1:         0.0623, 0.0681, 0.0762, 0.0812, 0.0887, 0.0982, 0.1039, 0.1143, 0.1297,
1:         0.0562, 0.0747, 0.0852, 0.0920, 0.0925, 0.0877, 0.0839],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.0621,  0.0719,  0.0619,  0.0520,  0.0429,  0.0296,  0.0196, -0.0150,
1:         -0.0303, -0.0312, -0.0466, -0.0564, -0.0643, -0.0856, -0.0878, -0.0474,
1:          0.0014,  0.0259,  0.0383,  0.0384,  0.0431,  0.0426,  0.0451,  0.0329,
1:          0.0201], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([1.2114, 1.2061, 1.1977, 1.1852, 1.1717, 1.1601, 1.1502, 1.1428, 1.1356,
1:         1.1271, 1.1182, 1.1103, 1.1063, 1.1049, 1.1063, 1.1072, 1.1072, 1.1051,
1:         1.1030, 1.1015, 1.1026, 1.1046, 1.1070, 1.1076, 1.1052],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1582, -0.1595, -0.1627, -0.1622, -0.1623, -0.1642, -0.1654, -0.1605,
1:         -0.1592, -0.1646, -0.1590, -0.1656, -0.1685, -0.1691, -0.1653, -0.1679,
1:         -0.1674, -0.1603, -0.1630, -0.1632, -0.1628, -0.1613, -0.1641, -0.1637,
1:         -0.1646], device='cuda:1', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 24, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: Created sparse mask for total_precip with 10.0% data retained
2:      first 25 values: tensor([ 0.1080,  0.1375,  0.1662,  0.1944,  0.2217,  0.2485,  0.2745,  0.2997,
2:          0.3235,  0.3461,  0.3678,  0.3890,  0.4098,  0.4295,  0.4484,  0.4670,
2:          0.4859,  0.5050, -0.3828, -0.3519, -0.3213, -0.2912, -0.2612, -0.2313,
2:         -0.2015], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2837, -0.2665, -0.2499, -0.2342, -0.2199, -0.2068, -0.1939, -0.1809,
2:         -0.1674, -0.1541, -0.1407, -0.1264, -0.1112, -0.0955, -0.0791, -0.0613,
2:         -0.0416, -0.0198, -0.2130, -0.1977, -0.1830, -0.1691, -0.1560, -0.1442,
2:         -0.1330], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7569, -0.7567, -0.7565, -0.7565, -0.7566, -0.7565, -0.7564, -0.7563,
2:         -0.7562, -0.7566, -0.7574, -0.7583, -0.7604, -0.7621, -0.7645, -0.7678,
2:         -0.7709, -0.7742, -0.7707, -0.7706, -0.7704, -0.7702, -0.7699, -0.7697,
2:         -0.7694], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.6133,  0.5915,  0.5697,  0.5435,  0.5151,  0.4802,  0.4322,  0.3820,
2:          0.3340,  0.2882,  0.2424,  0.1944,  0.1485,  0.1027,  0.0591,  0.0111,
2:         -0.0326, -0.0740,  0.5391,  0.5566,  0.5587,  0.5522,  0.5457,  0.5435,
2:          0.5347], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([1.1736, 1.1725, 1.1714, 1.1707, 1.1700, 1.1694, 1.1689, 1.1684, 1.1675,
2:         1.1671, 1.1672, 1.1672, 1.1675, 1.1676, 1.1684, 1.1685, 1.1685, 1.1674,
2:         1.1662, 1.1647, 1.1626, 1.1603, 1.1578, 1.1550, 1.1520],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 24, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2240,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2092,     nan,     nan, -0.1898,     nan,
2:         -0.1807, -0.1818,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.1955,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1852, -0.2366,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2309,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2297,     nan,     nan,     nan,     nan, -0.2252,     nan,
2:             nan,     nan, -0.2263,     nan,     nan,     nan, -0.2309,     nan,
2:         -0.2286,     nan,     nan,     nan,     nan,     nan, -0.2001,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2103,     nan,     nan, -0.1864,     nan,     nan,     nan,     nan,
2:         -0.2320,     nan,     nan,     nan,     nan,     nan,     nan, -0.2457,
2:         -0.2434,     nan, -0.2343,     nan,     nan,     nan, -0.2149,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2434,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2309,     nan,     nan,     nan,     nan,     nan, -0.2309,
2:             nan,     nan,     nan,     nan, -0.1966,     nan,     nan,     nan,
2:         -0.2012,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1875,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2149,     nan,     nan,     nan,     nan, -0.1966,
2:             nan, -0.2423,     nan, -0.2354,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 24, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.9738, 0.9405, 0.9111, 0.8871, 0.8673, 0.8492, 0.8353, 0.8171, 0.8023,
2:         0.7933, 0.7800, 0.7583, 0.7415, 0.7209, 0.7088, 0.7006, 0.6960, 0.6965,
2:         1.1169, 1.0998, 1.0792, 1.0551, 1.0275, 0.9936, 0.9600],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.1442, -1.1256, -1.1154, -1.1165, -1.1231, -1.1201, -1.1131, -1.1005,
2:         -1.0899, -1.0764, -1.0678, -1.0542, -1.0441, -1.0382, -1.0386, -1.0440,
2:         -1.0358, -1.0179, -1.1461, -1.1274, -1.1198, -1.1256, -1.1325, -1.1310,
2:         -1.1211], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.1188, 0.1160, 0.1056, 0.0869, 0.0703, 0.0614, 0.0568, 0.0655, 0.0758,
2:         0.0852, 0.0940, 0.1025, 0.1022, 0.1014, 0.1008, 0.0984, 0.1018, 0.1116,
2:         0.0162, 0.0357, 0.0452, 0.0486, 0.0470, 0.0481, 0.0526],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0084,  0.0078,  0.0077,  0.0085, -0.0040, -0.0382, -0.0879, -0.1408,
2:         -0.1447, -0.1320, -0.1261, -0.1091, -0.0762, -0.0613, -0.0694, -0.0559,
2:         -0.0518, -0.0978,  0.0306,  0.0473,  0.0404,  0.0275,  0.0154,  0.0009,
2:         -0.0323], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([1.1982, 1.1921, 1.1862, 1.1755, 1.1661, 1.1566, 1.1493, 1.1425, 1.1347,
2:         1.1261, 1.1185, 1.1133, 1.1137, 1.1170, 1.1210, 1.1226, 1.1210, 1.1178,
2:         1.1161, 1.1171, 1.1197, 1.1233, 1.1263, 1.1269, 1.1240],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1803, -0.1821, -0.1894, -0.1863, -0.1857, -0.1837, -0.1861, -0.1797,
2:         -0.1756, -0.1851, -0.1810, -0.1844, -0.1865, -0.1868, -0.1818, -0.1836,
2:         -0.1790, -0.1731, -0.1853, -0.1831, -0.1782, -0.1806, -0.1785, -0.1759,
2:         -0.1762], device='cuda:2', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 24, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 24, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1080,  0.1375,  0.1662,  0.1944,  0.2217,  0.2485,  0.2745,  0.2997,
0:          0.3235,  0.3461,  0.3678,  0.3890,  0.4098,  0.4295,  0.4484,  0.4670,
0:          0.4859,  0.5050, -0.3828, -0.3519, -0.3213, -0.2912, -0.2612, -0.2313,
0:         -0.2015], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2837, -0.2665, -0.2499, -0.2342, -0.2199, -0.2068, -0.1939, -0.1809,
0:         -0.1674, -0.1541, -0.1407, -0.1264, -0.1112, -0.0955, -0.0791, -0.0613,
0:         -0.0416, -0.0198, -0.2130, -0.1977, -0.1830, -0.1691, -0.1560, -0.1442,
0:         -0.1330], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7569, -0.7567, -0.7565, -0.7565, -0.7566, -0.7565, -0.7564, -0.7563,
0:         -0.7562, -0.7566, -0.7574, -0.7583, -0.7604, -0.7621, -0.7645, -0.7678,
0:         -0.7709, -0.7742, -0.7707, -0.7706, -0.7704, -0.7702, -0.7699, -0.7697,
0:         -0.7694], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6133,  0.5915,  0.5697,  0.5435,  0.5151,  0.4802,  0.4322,  0.3820,
0:          0.3340,  0.2882,  0.2424,  0.1944,  0.1485,  0.1027,  0.0591,  0.0111,
0:         -0.0326, -0.0740,  0.5391,  0.5566,  0.5587,  0.5522,  0.5457,  0.5435,
0:          0.5347], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.1736, 1.1725, 1.1714, 1.1707, 1.1700, 1.1694, 1.1689, 1.1684, 1.1675,
0:         1.1671, 1.1672, 1.1672, 1.1675, 1.1676, 1.1684, 1.1685, 1.1685, 1.1674,
0:         1.1662, 1.1647, 1.1626, 1.1603, 1.1578, 1.1550, 1.1520],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 24, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2252,     nan,     nan,     nan, -0.2217,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2229,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1944,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2309,
0:             nan,     nan,     nan,     nan,     nan, -0.1989,     nan,     nan,
0:         -0.2309,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2286,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2240,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2320,     nan, -0.2183, -0.2138,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2263,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2172,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1966,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2012,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2446,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2491,     nan,     nan,     nan,     nan,
0:             nan, -0.2354,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 24, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0304, 0.9932, 0.9618, 0.9360, 0.9094, 0.8835, 0.8640, 0.8359, 0.8172,
0:         0.8056, 0.7901, 0.7697, 0.7545, 0.7422, 0.7368, 0.7340, 0.7357, 0.7358,
0:         1.1810, 1.1585, 1.1307, 1.1034, 1.0673, 1.0317, 0.9938],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1905, -1.1717, -1.1611, -1.1609, -1.1635, -1.1587, -1.1506, -1.1355,
0:         -1.1254, -1.1163, -1.1156, -1.1143, -1.1099, -1.1051, -1.1032, -1.1056,
0:         -1.1001, -1.0880, -1.2038, -1.1788, -1.1667, -1.1660, -1.1699, -1.1654,
0:         -1.1579], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0603,  0.0683,  0.0634,  0.0482,  0.0291,  0.0121, -0.0024, -0.0047,
0:         -0.0027,  0.0035,  0.0112,  0.0215,  0.0280,  0.0355,  0.0444,  0.0513,
0:          0.0579,  0.0723, -0.0300,  0.0011,  0.0204,  0.0316,  0.0322,  0.0271,
0:          0.0270], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0201, -0.0173, -0.0459, -0.0689, -0.0504, -0.0568, -0.0976, -0.1274,
0:         -0.1321, -0.1259, -0.1106, -0.1068, -0.1030, -0.1016, -0.1015, -0.0730,
0:         -0.0413, -0.0428,  0.0411,  0.0318,  0.0196, -0.0144, -0.0035, -0.0097,
0:         -0.0542], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.1926, 1.1817, 1.1742, 1.1679, 1.1635, 1.1591, 1.1532, 1.1470, 1.1386,
0:         1.1291, 1.1209, 1.1150, 1.1150, 1.1181, 1.1243, 1.1264, 1.1234, 1.1153,
0:         1.1075, 1.1042, 1.1047, 1.1066, 1.1068, 1.1046, 1.0994],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1601, -0.1662, -0.1747, -0.1751, -0.1795, -0.1771, -0.1778, -0.1701,
0:         -0.1682, -0.1650, -0.1640, -0.1706, -0.1749, -0.1797, -0.1748, -0.1777,
0:         -0.1745, -0.1652, -0.1625, -0.1642, -0.1651, -0.1693, -0.1721, -0.1739,
0:         -0.1743], device='cuda:0', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([ 0.1080,  0.1375,  0.1662,  0.1944,  0.2217,  0.2485,  0.2745,  0.2997,
3:          0.3235,  0.3461,  0.3678,  0.3890,  0.4098,  0.4295,  0.4484,  0.4670,
3:          0.4859,  0.5050, -0.3828, -0.3519, -0.3213, -0.2912, -0.2612, -0.2313,
3:         -0.2015], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2837, -0.2665, -0.2499, -0.2342, -0.2199, -0.2068, -0.1939, -0.1809,
3:         -0.1674, -0.1541, -0.1407, -0.1264, -0.1112, -0.0955, -0.0791, -0.0613,
3:         -0.0416, -0.0198, -0.2130, -0.1977, -0.1830, -0.1691, -0.1560, -0.1442,
3:         -0.1330], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7569, -0.7567, -0.7565, -0.7565, -0.7566, -0.7565, -0.7564, -0.7563,
3:         -0.7562, -0.7566, -0.7574, -0.7583, -0.7604, -0.7621, -0.7645, -0.7678,
3:         -0.7709, -0.7742, -0.7707, -0.7706, -0.7704, -0.7702, -0.7699, -0.7697,
3:         -0.7694], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.6133,  0.5915,  0.5697,  0.5435,  0.5151,  0.4802,  0.4322,  0.3820,
3:          0.3340,  0.2882,  0.2424,  0.1944,  0.1485,  0.1027,  0.0591,  0.0111,
3:         -0.0326, -0.0740,  0.5391,  0.5566,  0.5587,  0.5522,  0.5457,  0.5435,
3:          0.5347], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([1.1736, 1.1725, 1.1714, 1.1707, 1.1700, 1.1694, 1.1689, 1.1684, 1.1675,
3:         1.1671, 1.1672, 1.1672, 1.1675, 1.1676, 1.1684, 1.1685, 1.1685, 1.1674,
3:         1.1662, 1.1647, 1.1626, 1.1603, 1.1578, 1.1550, 1.1520],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 24, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2217,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.1898,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1989,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2069,     nan, -0.1898,     nan,
3:             nan,     nan,     nan,     nan, -0.2046,     nan,     nan, -0.1590,
3:             nan,     nan, -0.2297,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2058,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1944,
3:         -0.1875,     nan,     nan,     nan, -0.1784, -0.2286,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2411,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2343,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2183,     nan,
3:             nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2400,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.1966,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.1829,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2331,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2468,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 24, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.9316, 0.9084, 0.8875, 0.8692, 0.8505, 0.8304, 0.8126, 0.7863, 0.7716,
3:         0.7622, 0.7523, 0.7388, 0.7327, 0.7238, 0.7187, 0.7120, 0.7089, 0.7069,
3:         1.0778, 1.0661, 1.0507, 1.0340, 1.0103, 0.9796, 0.9455],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.1149, -1.1019, -1.0941, -1.0971, -1.1020, -1.0987, -1.0956, -1.0907,
3:         -1.0932, -1.0975, -1.1035, -1.0988, -1.0898, -1.0807, -1.0808, -1.0828,
3:         -1.0760, -1.0527, -1.1267, -1.1111, -1.1073, -1.1131, -1.1212, -1.1185,
3:         -1.1133], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0898,  0.0998,  0.1027,  0.0986,  0.0888,  0.0761,  0.0613,  0.0557,
3:          0.0526,  0.0554,  0.0618,  0.0716,  0.0751,  0.0790,  0.0816,  0.0762,
3:          0.0751,  0.0786, -0.0025,  0.0278,  0.0530,  0.0744,  0.0838,  0.0835,
3:          0.0830], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.1068, -0.0886, -0.1027, -0.1089, -0.0867, -0.0833, -0.1015, -0.1437,
3:         -0.1601, -0.1670, -0.1796, -0.1645, -0.1572, -0.1615, -0.1544, -0.1181,
3:         -0.0526, -0.0181, -0.1300, -0.1066, -0.0901, -0.0874, -0.0589, -0.0516,
3:         -0.0606], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([1.1979, 1.1933, 1.1887, 1.1807, 1.1739, 1.1691, 1.1661, 1.1623, 1.1571,
3:         1.1495, 1.1422, 1.1379, 1.1383, 1.1395, 1.1400, 1.1362, 1.1298, 1.1231,
3:         1.1186, 1.1181, 1.1192, 1.1205, 1.1198, 1.1184, 1.1151],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1618, -0.1632, -0.1703, -0.1643, -0.1624, -0.1654, -0.1664, -0.1617,
3:         -0.1599, -0.1679, -0.1610, -0.1631, -0.1638, -0.1654, -0.1611, -0.1654,
3:         -0.1632, -0.1568, -0.1676, -0.1648, -0.1609, -0.1587, -0.1561, -0.1558,
3:         -0.1597], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 24 [1/5 (20%)]	Loss: nan : nan :: 0.16110 (1.60 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 24 [2/5 (40%)]	Loss: nan : nan :: 0.14581 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 24 [3/5 (60%)]	Loss: nan : nan :: 0.14479 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 24 [4/5 (80%)]	Loss: nan : nan :: 0.13536 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch24.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch24.mod
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 24 : nan
0: validation loss for velocity_u : 0.03261122107505798
0: validation loss for velocity_v : 0.06292132288217545
0: validation loss for specific_humidity : 0.027096586301922798
0: validation loss for velocity_z : 0.49662935733795166
0: validation loss for temperature : 0.08095527440309525
0: validation loss for total_precip : nan
3: 25 : 01:02:50 :: batch_size = 96, lr = 1.1333944760803001e-05
1: 25 : 01:02:50 :: batch_size = 96, lr = 1.1333944760803001e-05
2: 25 : 01:02:50 :: batch_size = 96, lr = 1.1333944760803001e-05
0: 25 : 01:02:50 :: batch_size = 96, lr = 1.1333944760803001e-05
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 25, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 25, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0838, -0.0868, -0.0895, -0.0917, -0.0935, -0.0944, -0.0948, -0.0943,
0:         -0.0928, -0.0905, -0.0876, -0.0840, -0.0796, -0.0744, -0.0678, -0.0598,
0:         -0.0499, -0.0380, -0.0835, -0.0824, -0.0802, -0.0768, -0.0721, -0.0660,
0:         -0.0589], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7949, -0.8149, -0.8320, -0.8458, -0.8569, -0.8649, -0.8698, -0.8720,
0:         -0.8709, -0.8664, -0.8582, -0.8462, -0.8307, -0.8116, -0.7897, -0.7648,
0:         -0.7373, -0.7070, -0.7592, -0.7765, -0.7910, -0.8023, -0.8105, -0.8155,
0:         -0.8175], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1435,  0.1438,  0.1350,  0.1242,  0.1136,  0.1025,  0.0801,  0.0567,
0:          0.0336,  0.0108, -0.0240, -0.0586, -0.0930, -0.1244, -0.1632, -0.2018,
0:         -0.2404, -0.2704, -0.0022, -0.0059, -0.0122, -0.0267, -0.0412, -0.0554,
0:         -0.0733], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0046, -0.0181, -0.0271, -0.0361, -0.0496, -0.0699, -0.0901, -0.1103,
0:         -0.1261, -0.1373, -0.1441, -0.1486, -0.1576, -0.1666, -0.1778, -0.1846,
0:         -0.1846, -0.1801,  0.0921,  0.0831,  0.0696,  0.0471,  0.0201, -0.0136,
0:         -0.0429], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.6478, 0.6495, 0.6517, 0.6547, 0.6576, 0.6609, 0.6640, 0.6675, 0.6707,
0:         0.6741, 0.6770, 0.6799, 0.6823, 0.6841, 0.6851, 0.6856, 0.6859, 0.6856,
0:         0.6854, 0.6850, 0.6851, 0.6852, 0.6856, 0.6866, 0.6886],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 25, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1772,     nan,     nan,     nan,
0:             nan,     nan, -0.2299,     nan, -0.1680,     nan,     nan,     nan,
0:         -0.2161,     nan,     nan,     nan,     nan, -0.1909,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2390,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2253, -0.2276,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2253,     nan,     nan,     nan,
0:             nan,     nan, -0.2230,     nan,     nan,     nan, -0.2253,     nan,
0:             nan,     nan,     nan,     nan, -0.2344,     nan,     nan,     nan,
0:             nan,     nan, -0.1978,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2207,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2253,     nan,     nan,     nan, -0.2367,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2390, -0.2390,     nan,     nan, -0.2367,     nan,     nan, -0.2413,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2161,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1886,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2276,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 25, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2236, -0.2306, -0.2390, -0.2465, -0.2492, -0.2536, -0.2587, -0.2664,
0:         -0.2775, -0.2857, -0.2899, -0.2934, -0.2993, -0.3039, -0.3095, -0.3138,
0:         -0.3164, -0.3158, -0.2246, -0.2364, -0.2505, -0.2612, -0.2705, -0.2769,
0:         -0.2816], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3839, -0.3925, -0.4127, -0.4353, -0.4539, -0.4631, -0.4655, -0.4661,
0:         -0.4683, -0.4740, -0.4853, -0.4964, -0.5055, -0.5074, -0.5089, -0.5119,
0:         -0.5219, -0.5346, -0.4019, -0.4132, -0.4335, -0.4571, -0.4744, -0.4856,
0:         -0.4876], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3672, -0.3571, -0.3441, -0.3229, -0.3019, -0.2805, -0.2620, -0.2390,
0:         -0.2189, -0.2025, -0.1849, -0.1677, -0.1518, -0.1349, -0.1213, -0.1091,
0:         -0.0998, -0.0899, -0.3904, -0.3802, -0.3633, -0.3412, -0.3170, -0.2959,
0:         -0.2703], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0479,  0.0476,  0.0409,  0.0284,  0.0486,  0.0663,  0.0455, -0.0067,
0:         -0.0142, -0.0185, -0.0625, -0.0626, -0.0547, -0.0613, -0.0516, -0.0525,
0:         -0.0302, -0.0045,  0.0841,  0.0796,  0.0550,  0.0231,  0.0203,  0.0146,
0:         -0.0149], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4964, 0.4990, 0.5037, 0.5078, 0.5142, 0.5217, 0.5311, 0.5408, 0.5505,
0:         0.5614, 0.5725, 0.5855, 0.6000, 0.6147, 0.6293, 0.6437, 0.6567, 0.6694,
0:         0.6822, 0.6951, 0.7063, 0.7168, 0.7261, 0.7338, 0.7413],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1576, -0.1601, -0.1674, -0.1690, -0.1682, -0.1724, -0.1744, -0.1703,
0:         -0.1693, -0.1608, -0.1586, -0.1655, -0.1708, -0.1744, -0.1709, -0.1771,
0:         -0.1722, -0.1670, -0.1639, -0.1644, -0.1620, -0.1671, -0.1671, -0.1687,
0:         -0.1697], device='cuda:0', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 25, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0838, -0.0868, -0.0895, -0.0917, -0.0935, -0.0944, -0.0948, -0.0943,
1:         -0.0928, -0.0905, -0.0876, -0.0840, -0.0796, -0.0744, -0.0678, -0.0598,
1:         -0.0499, -0.0380, -0.0835, -0.0824, -0.0802, -0.0768, -0.0721, -0.0660,
1:         -0.0589], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7949, -0.8149, -0.8320, -0.8458, -0.8569, -0.8649, -0.8698, -0.8720,
1:         -0.8709, -0.8664, -0.8582, -0.8462, -0.8307, -0.8116, -0.7897, -0.7648,
1:         -0.7373, -0.7070, -0.7592, -0.7765, -0.7910, -0.8023, -0.8105, -0.8155,
1:         -0.8175], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.1435,  0.1438,  0.1350,  0.1242,  0.1136,  0.1025,  0.0801,  0.0567,
1:          0.0336,  0.0108, -0.0240, -0.0586, -0.0930, -0.1244, -0.1632, -0.2018,
1:         -0.2404, -0.2704, -0.0022, -0.0059, -0.0122, -0.0267, -0.0412, -0.0554,
1:         -0.0733], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.0046, -0.0181, -0.0271, -0.0361, -0.0496, -0.0699, -0.0901, -0.1103,
1:         -0.1261, -0.1373, -0.1441, -0.1486, -0.1576, -0.1666, -0.1778, -0.1846,
1:         -0.1846, -0.1801,  0.0921,  0.0831,  0.0696,  0.0471,  0.0201, -0.0136,
1:         -0.0429], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.6478, 0.6495, 0.6517, 0.6547, 0.6576, 0.6609, 0.6640, 0.6675, 0.6707,
1:         0.6741, 0.6770, 0.6799, 0.6823, 0.6841, 0.6851, 0.6856, 0.6859, 0.6856,
1:         0.6854, 0.6850, 0.6851, 0.6852, 0.6856, 0.6866, 0.6886],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 25, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([-0.2253,     nan, -0.2344,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.1749,     nan,     nan, -0.2115,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.1612,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2344,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2253,     nan,     nan,     nan,
1:         -0.2253,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.1978,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2207,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2230,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2184,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2390,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2299,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2413,     nan, -0.2001, -0.2024,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2001,
1:         -0.2047,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2092,     nan,     nan,     nan,     nan, -0.2276, -0.2024,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2321,     nan,     nan,     nan, -0.2344,
1:             nan,     nan,     nan,     nan, -0.2321,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 25, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.2574, -0.2667, -0.2734, -0.2791, -0.2785, -0.2821, -0.2840, -0.2921,
1:         -0.2986, -0.3026, -0.3033, -0.3051, -0.3092, -0.3098, -0.3164, -0.3231,
1:         -0.3334, -0.3442, -0.2558, -0.2653, -0.2746, -0.2813, -0.2885, -0.2935,
1:         -0.2994], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.3516, -0.3656, -0.3906, -0.4163, -0.4377, -0.4468, -0.4505, -0.4511,
1:         -0.4497, -0.4498, -0.4520, -0.4546, -0.4605, -0.4640, -0.4700, -0.4758,
1:         -0.4830, -0.4895, -0.3697, -0.3861, -0.4136, -0.4390, -0.4578, -0.4686,
1:         -0.4714], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.3758, -0.3661, -0.3514, -0.3329, -0.3131, -0.2929, -0.2734, -0.2481,
1:         -0.2254, -0.2026, -0.1808, -0.1597, -0.1438, -0.1276, -0.1166, -0.1043,
1:         -0.0932, -0.0789, -0.3914, -0.3794, -0.3632, -0.3465, -0.3281, -0.3104,
1:         -0.2874], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.0657, 0.0936, 0.0871, 0.0863, 0.1135, 0.1112, 0.0913, 0.0563, 0.0557,
1:         0.0667, 0.0391, 0.0383, 0.0424, 0.0304, 0.0318, 0.0385, 0.0564, 0.0749,
1:         0.0575, 0.0816, 0.0621, 0.0533, 0.0593, 0.0398, 0.0249],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([0.4856, 0.4819, 0.4833, 0.4878, 0.4969, 0.5072, 0.5179, 0.5272, 0.5364,
1:         0.5462, 0.5564, 0.5676, 0.5793, 0.5920, 0.6052, 0.6196, 0.6331, 0.6460,
1:         0.6575, 0.6685, 0.6768, 0.6851, 0.6939, 0.7042, 0.7191],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1373, -0.1413, -0.1447, -0.1407, -0.1424, -0.1472, -0.1493, -0.1458,
1:         -0.1475, -0.1460, -0.1413, -0.1463, -0.1506, -0.1526, -0.1498, -0.1517,
1:         -0.1538, -0.1481, -0.1523, -0.1498, -0.1471, -0.1465, -0.1460, -0.1490,
1:         -0.1525], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([-0.0838, -0.0868, -0.0895, -0.0917, -0.0935, -0.0944, -0.0948, -0.0943,
2:         -0.0928, -0.0905, -0.0876, -0.0840, -0.0796, -0.0744, -0.0678, -0.0598,
2:         -0.0499, -0.0380, -0.0835, -0.0824, -0.0802, -0.0768, -0.0721, -0.0660,
2:         -0.0589], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7949, -0.8149, -0.8320, -0.8458, -0.8569, -0.8649, -0.8698, -0.8720,
2:         -0.8709, -0.8664, -0.8582, -0.8462, -0.8307, -0.8116, -0.7897, -0.7648,
2:         -0.7373, -0.7070, -0.7592, -0.7765, -0.7910, -0.8023, -0.8105, -0.8155,
2:         -0.8175], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.1435,  0.1438,  0.1350,  0.1242,  0.1136,  0.1025,  0.0801,  0.0567,
2:          0.0336,  0.0108, -0.0240, -0.0586, -0.0930, -0.1244, -0.1632, -0.2018,
2:         -0.2404, -0.2704, -0.0022, -0.0059, -0.0122, -0.0267, -0.0412, -0.0554,
2:         -0.0733], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.0046, -0.0181, -0.0271, -0.0361, -0.0496, -0.0699, -0.0901, -0.1103,
2:         -0.1261, -0.1373, -0.1441, -0.1486, -0.1576, -0.1666, -0.1778, -0.1846,
2:         -0.1846, -0.1801,  0.0921,  0.0831,  0.0696,  0.0471,  0.0201, -0.0136,
2:         -0.0429], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.6478, 0.6495, 0.6517, 0.6547, 0.6576, 0.6609, 0.6640, 0.6675, 0.6707,
2:         0.6741, 0.6770, 0.6799, 0.6823, 0.6841, 0.6851, 0.6856, 0.6859, 0.6856,
2:         0.6854, 0.6850, 0.6851, 0.6852, 0.6856, 0.6866, 0.6886],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 25, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([-0.2253,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2161,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2207, -0.2276,     nan, -0.2390,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2230,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2253,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2459,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2230,     nan,     nan,     nan, -0.1909,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2367, -0.2024, -0.2047,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2299,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2390,     nan,     nan,     nan,     nan, -0.2367,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2367,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2276,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2184,     nan,     nan,     nan,     nan, -0.2276,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2344,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 25, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.2267, -0.2366, -0.2460, -0.2556, -0.2594, -0.2621, -0.2623, -0.2649,
2:         -0.2665, -0.2693, -0.2705, -0.2752, -0.2827, -0.2886, -0.2955, -0.2997,
2:         -0.2987, -0.2954, -0.2209, -0.2324, -0.2487, -0.2608, -0.2721, -0.2780,
2:         -0.2804], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.3502, -0.3681, -0.3963, -0.4221, -0.4425, -0.4517, -0.4547, -0.4534,
2:         -0.4503, -0.4509, -0.4569, -0.4640, -0.4731, -0.4776, -0.4799, -0.4804,
2:         -0.4859, -0.4950, -0.3776, -0.3962, -0.4241, -0.4481, -0.4649, -0.4748,
2:         -0.4741], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.3665, -0.3540, -0.3355, -0.3138, -0.2941, -0.2723, -0.2553, -0.2357,
2:         -0.2189, -0.2017, -0.1806, -0.1597, -0.1418, -0.1260, -0.1158, -0.1101,
2:         -0.1063, -0.1021, -0.3851, -0.3693, -0.3484, -0.3275, -0.3049, -0.2850,
2:         -0.2642], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0390,  0.0627,  0.0605,  0.0707,  0.0910,  0.0801,  0.0529,  0.0042,
2:         -0.0244, -0.0276, -0.0506, -0.0537, -0.0460, -0.0420, -0.0470, -0.0585,
2:         -0.0480, -0.0486,  0.0133,  0.0394,  0.0341,  0.0338,  0.0420,  0.0182,
2:         -0.0141], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([0.4534, 0.4578, 0.4649, 0.4702, 0.4775, 0.4851, 0.4949, 0.5056, 0.5164,
2:         0.5262, 0.5343, 0.5413, 0.5496, 0.5610, 0.5745, 0.5907, 0.6047, 0.6165,
2:         0.6268, 0.6364, 0.6465, 0.6576, 0.6708, 0.6850, 0.7008],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1444, -0.1484, -0.1534, -0.1559, -0.1556, -0.1565, -0.1612, -0.1542,
2:         -0.1535, -0.1487, -0.1468, -0.1529, -0.1572, -0.1625, -0.1601, -0.1611,
2:         -0.1586, -0.1521, -0.1485, -0.1509, -0.1496, -0.1543, -0.1557, -0.1570,
2:         -0.1572], device='cuda:2', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 25, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.0838, -0.0868, -0.0895, -0.0917, -0.0935, -0.0944, -0.0948, -0.0943,
3:         -0.0928, -0.0905, -0.0876, -0.0840, -0.0796, -0.0744, -0.0678, -0.0598,
3:         -0.0499, -0.0380, -0.0835, -0.0824, -0.0802, -0.0768, -0.0721, -0.0660,
3:         -0.0589], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7949, -0.8149, -0.8320, -0.8458, -0.8569, -0.8649, -0.8698, -0.8720,
3:         -0.8709, -0.8664, -0.8582, -0.8462, -0.8307, -0.8116, -0.7897, -0.7648,
3:         -0.7373, -0.7070, -0.7592, -0.7765, -0.7910, -0.8023, -0.8105, -0.8155,
3:         -0.8175], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.1435,  0.1438,  0.1350,  0.1242,  0.1136,  0.1025,  0.0801,  0.0567,
3:          0.0336,  0.0108, -0.0240, -0.0586, -0.0930, -0.1244, -0.1632, -0.2018,
3:         -0.2404, -0.2704, -0.0022, -0.0059, -0.0122, -0.0267, -0.0412, -0.0554,
3:         -0.0733], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.0046, -0.0181, -0.0271, -0.0361, -0.0496, -0.0699, -0.0901, -0.1103,
3:         -0.1261, -0.1373, -0.1441, -0.1486, -0.1576, -0.1666, -0.1778, -0.1846,
3:         -0.1846, -0.1801,  0.0921,  0.0831,  0.0696,  0.0471,  0.0201, -0.0136,
3:         -0.0429], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.6478, 0.6495, 0.6517, 0.6547, 0.6576, 0.6609, 0.6640, 0.6675, 0.6707,
3:         0.6741, 0.6770, 0.6799, 0.6823, 0.6841, 0.6851, 0.6856, 0.6859, 0.6856,
3:         0.6854, 0.6850, 0.6851, 0.6852, 0.6856, 0.6866, 0.6886],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 25, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1772, -0.1749,     nan, -0.1955,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2207,     nan,     nan,     nan,     nan,     nan, -0.2161,
3:             nan,     nan, -0.2207,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2253,     nan,     nan, -0.2276,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2230, -0.2253,     nan,     nan,     nan,
3:         -0.2253,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1978,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.1932,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.1909,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2024,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2367,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2367,     nan,     nan,
3:             nan,     nan,     nan, -0.2321,     nan, -0.2344,     nan,     nan,
3:             nan,     nan, -0.2413,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1772,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2024,
3:             nan,     nan,     nan,     nan, -0.2207,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2367,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2344,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 25, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.2640, -0.2718, -0.2788, -0.2879, -0.2914, -0.2961, -0.2970, -0.3000,
3:         -0.3006, -0.3017, -0.2996, -0.3031, -0.3093, -0.3132, -0.3225, -0.3264,
3:         -0.3290, -0.3298, -0.2621, -0.2706, -0.2810, -0.2921, -0.3026, -0.3103,
3:         -0.3124], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.3798, -0.3921, -0.4121, -0.4354, -0.4570, -0.4705, -0.4799, -0.4860,
3:         -0.4875, -0.4864, -0.4883, -0.4905, -0.4948, -0.4941, -0.4941, -0.4927,
3:         -0.4947, -0.5011, -0.3901, -0.4077, -0.4320, -0.4573, -0.4769, -0.4933,
3:         -0.5020], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.3764, -0.3653, -0.3512, -0.3307, -0.3123, -0.2917, -0.2722, -0.2488,
3:         -0.2279, -0.2078, -0.1876, -0.1666, -0.1501, -0.1335, -0.1202, -0.1103,
3:         -0.0992, -0.0902, -0.3848, -0.3733, -0.3579, -0.3400, -0.3210, -0.3023,
3:         -0.2794], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0333,  0.0562,  0.0666,  0.0744,  0.1036,  0.1043,  0.0801,  0.0346,
3:          0.0209,  0.0266,  0.0037, -0.0010, -0.0122, -0.0377, -0.0455, -0.0473,
3:         -0.0274, -0.0062,  0.0177,  0.0235,  0.0206,  0.0215,  0.0347,  0.0118,
3:         -0.0177], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([0.5135, 0.5027, 0.4959, 0.4932, 0.4989, 0.5076, 0.5188, 0.5274, 0.5348,
3:         0.5431, 0.5528, 0.5644, 0.5772, 0.5904, 0.6022, 0.6144, 0.6265, 0.6402,
3:         0.6561, 0.6729, 0.6882, 0.7025, 0.7158, 0.7270, 0.7391],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1290, -0.1292, -0.1387, -0.1381, -0.1394, -0.1446, -0.1460, -0.1378,
3:         -0.1416, -0.1319, -0.1304, -0.1353, -0.1378, -0.1390, -0.1380, -0.1400,
3:         -0.1412, -0.1301, -0.1311, -0.1280, -0.1255, -0.1275, -0.1305, -0.1293,
3:         -0.1327], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 25 [1/5 (20%)]	Loss: nan : nan :: 0.14307 (2.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 25 [2/5 (40%)]	Loss: nan : nan :: 0.16251 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 25 [3/5 (60%)]	Loss: nan : nan :: 0.14548 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 25 [4/5 (80%)]	Loss: nan : nan :: 0.14472 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch25.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch25.mod
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 25 : nan
0: validation loss for velocity_u : 0.0386132076382637
0: validation loss for velocity_v : 0.06906379759311676
0: validation loss for specific_humidity : 0.023253386840224266
0: validation loss for velocity_z : 0.5022721886634827
0: validation loss for temperature : 0.06889386475086212
0: validation loss for total_precip : nan
3: 26 : 01:09:25 :: batch_size = 96, lr = 1.1057507083710246e-05
2: 26 : 01:09:25 :: batch_size = 96, lr = 1.1057507083710246e-05
0: 26 : 01:09:25 :: batch_size = 96, lr = 1.1057507083710246e-05
1: 26 : 01:09:25 :: batch_size = 96, lr = 1.1057507083710246e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 26, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.4216, 1.3727, 1.3334, 1.3031, 1.2791, 1.2582, 1.2382, 1.2169, 1.1940,
0:         1.1703, 1.1485, 1.1305, 1.1163, 1.1037, 1.0884, 1.0669, 1.0375, 1.0020,
0:         1.4579, 1.4030, 1.3576, 1.3222, 1.2951, 1.2743, 1.2571],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.3942, -2.3554, -2.3159, -2.2754, -2.2335, -2.1902, -2.1458, -2.0996,
0:         -2.0495, -1.9939, -1.9318, -1.8613, -1.7801, -1.6874, -1.5827, -1.4674,
0:         -1.3444, -1.2214, -2.4364, -2.3954, -2.3537, -2.3113, -2.2684, -2.2249,
0:         -2.1812], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6806, -0.6829, -0.6841, -0.6852, -0.6862, -0.6872, -0.6881, -0.6889,
0:         -0.6889, -0.6893, -0.6890, -0.6880, -0.6874, -0.6872, -0.6868, -0.6863,
0:         -0.6861, -0.6858, -0.6730, -0.6781, -0.6820, -0.6837, -0.6852, -0.6859,
0:         -0.6868], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5704,  0.6284,  0.6485,  0.6395,  0.6150,  0.5726,  0.5235,  0.4789,
0:          0.4521,  0.4298,  0.3941,  0.3473,  0.3138,  0.2959,  0.2223,  0.0148,
0:         -0.3333, -0.7237,  0.2714,  0.4164,  0.5548,  0.6306,  0.6328,  0.6038,
0:          0.5771], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1303, -0.1675, -0.1986, -0.2258, -0.2510, -0.2764, -0.3023, -0.3283,
0:         -0.3544, -0.3814, -0.4105, -0.4429, -0.4795, -0.5219, -0.5721, -0.6302,
0:         -0.6931, -0.7539, -0.8049, -0.8413, -0.8619, -0.8703, -0.8718, -0.8720,
0:         -0.8750], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 26, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2341,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan,     nan,
0:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2434,     nan,     nan, -0.2364,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2434,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2399,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2422,
0:             nan,     nan,     nan, -0.2410,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2387,     nan,     nan,     nan, -0.2434,     nan,
0:         -0.2434,     nan,     nan, -0.2434,     nan, -0.2352,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2434,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2434,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 26, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 2.4396e-01,  2.4590e-01,  2.5321e-01,  2.5540e-01,  2.5205e-01,
0:          2.3661e-01,  2.1547e-01,  1.8086e-01,  1.3750e-01,  9.1350e-02,
0:          4.6468e-02, -9.9204e-05, -4.0654e-02, -7.7490e-02, -1.1368e-01,
0:         -1.4410e-01, -1.7085e-01, -1.9273e-01,  2.4562e-01,  2.5117e-01,
0:          2.5625e-01,  2.5890e-01,  2.5812e-01,  2.5054e-01,  2.3191e-01],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2728, -1.3742, -1.4843, -1.6000, -1.7246, -1.8394, -1.9521, -2.0505,
0:         -2.1281, -2.1758, -2.1937, -2.1870, -2.1669, -2.1384, -2.1131, -2.0996,
0:         -2.0894, -2.0864, -1.2191, -1.3205, -1.4334, -1.5572, -1.6839, -1.8086,
0:         -1.9303], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0313, -0.0185, -0.0804, -0.1329, -0.1804, -0.2122, -0.2302, -0.2415,
0:         -0.2496, -0.2602, -0.2797, -0.3047, -0.3398, -0.3746, -0.4052, -0.4309,
0:         -0.4590, -0.4866,  0.0797,  0.0259, -0.0388, -0.1013, -0.1570, -0.1969,
0:         -0.2213], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4360, 0.5127, 0.6023, 0.6632, 0.7181, 0.8613, 1.0947, 1.3008, 1.4810,
0:         1.6498, 1.7433, 1.7525, 1.5966, 1.2618, 0.8810, 0.5987, 0.4415, 0.3372,
0:         0.3063, 0.3865, 0.4972, 0.6105, 0.7253, 0.9127, 1.1400],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.8533, 0.8618, 0.8734, 0.8854, 0.8930, 0.8942, 0.8898, 0.8805, 0.8715,
0:         0.8638, 0.8601, 0.8595, 0.8613, 0.8651, 0.8729, 0.8845, 0.9021, 0.9224,
0:         0.9442, 0.9612, 0.9737, 0.9803, 0.9834, 0.9821, 0.9758],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1521, -0.1554, -0.1580, -0.1542, -0.1549, -0.1547, -0.1548, -0.1468,
0:         -0.1467, -0.1600, -0.1582, -0.1586, -0.1593, -0.1570, -0.1519, -0.1533,
0:         -0.1521, -0.1439, -0.1606, -0.1599, -0.1544, -0.1539, -0.1513, -0.1470,
0:         -0.1478], device='cuda:0', grad_fn=<SliceBackward0>)
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 26, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 26, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 26, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.4216, 1.3727, 1.3334, 1.3031, 1.2791, 1.2582, 1.2382, 1.2169, 1.1940,
3:         1.1703, 1.1485, 1.1305, 1.1163, 1.1037, 1.0884, 1.0669, 1.0375, 1.0020,
3:         1.4579, 1.4030, 1.3576, 1.3222, 1.2951, 1.2743, 1.2571],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-2.3942, -2.3554, -2.3159, -2.2754, -2.2335, -2.1902, -2.1458, -2.0996,
3:         -2.0495, -1.9939, -1.9318, -1.8613, -1.7801, -1.6874, -1.5827, -1.4674,
3:         -1.3444, -1.2214, -2.4364, -2.3954, -2.3537, -2.3113, -2.2684, -2.2249,
3:         -2.1812], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.6806, -0.6829, -0.6841, -0.6852, -0.6862, -0.6872, -0.6881, -0.6889,
3:         -0.6889, -0.6893, -0.6890, -0.6880, -0.6874, -0.6872, -0.6868, -0.6863,
3:         -0.6861, -0.6858, -0.6730, -0.6781, -0.6820, -0.6837, -0.6852, -0.6859,
3:         -0.6868], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.5704,  0.6284,  0.6485,  0.6395,  0.6150,  0.5726,  0.5235,  0.4789,
3:          0.4521,  0.4298,  0.3941,  0.3473,  0.3138,  0.2959,  0.2223,  0.0148,
3:         -0.3333, -0.7237,  0.2714,  0.4164,  0.5548,  0.6306,  0.6328,  0.6038,
3:          0.5771], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.1303, -0.1675, -0.1986, -0.2258, -0.2510, -0.2764, -0.3023, -0.3283,
3:         -0.3544, -0.3814, -0.4105, -0.4429, -0.4795, -0.5219, -0.5721, -0.6302,
3:         -0.6931, -0.7539, -0.8049, -0.8413, -0.8619, -0.8703, -0.8718, -0.8720,
3:         -0.8750], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 26, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
3:             nan, -0.2352,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2434,     nan,     nan,     nan,
3:         -0.2422,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan,     nan,
3:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2434,     nan,     nan,     nan,
3:         -0.2434,     nan,     nan,     nan,     nan,     nan,     nan, -0.2422,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2364,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2422,     nan,     nan,
3:             nan, -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2364,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2376,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2434,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
3:             nan, -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2329,     nan,     nan,     nan,     nan,     nan, -0.2434,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 26, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.2544,  0.2598,  0.2681,  0.2738,  0.2757,  0.2665,  0.2463,  0.2124,
3:          0.1636,  0.1122,  0.0577,  0.0039, -0.0421, -0.0843, -0.1210, -0.1520,
3:         -0.1797, -0.2037,  0.2554,  0.2657,  0.2758,  0.2831,  0.2851,  0.2793,
3:          0.2583], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.2723, -1.3561, -1.4537, -1.5640, -1.6857, -1.8064, -1.9244, -2.0276,
3:         -2.1081, -2.1578, -2.1809, -2.1755, -2.1507, -2.1187, -2.0862, -2.0615,
3:         -2.0424, -2.0346, -1.2250, -1.3148, -1.4200, -1.5356, -1.6590, -1.7885,
3:         -1.9128], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0678,  0.0069, -0.0620, -0.1177, -0.1659, -0.1923, -0.2056, -0.2116,
3:         -0.2167, -0.2246, -0.2428, -0.2673, -0.3030, -0.3395, -0.3800, -0.4147,
3:         -0.4566, -0.4921,  0.1139,  0.0505, -0.0171, -0.0840, -0.1373, -0.1731,
3:         -0.1936], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.3955, 0.4920, 0.5653, 0.6135, 0.6958, 0.7837, 0.9046, 1.0991, 1.3273,
3:         1.5488, 1.7383, 1.8126, 1.6423, 1.3256, 0.9696, 0.6765, 0.5785, 0.5691,
3:         0.1813, 0.3262, 0.4619, 0.5799, 0.7510, 0.9163, 1.0543],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([0.8559, 0.8651, 0.8756, 0.8834, 0.8868, 0.8835, 0.8781, 0.8699, 0.8633,
3:         0.8593, 0.8572, 0.8547, 0.8518, 0.8507, 0.8531, 0.8630, 0.8817, 0.9053,
3:         0.9315, 0.9524, 0.9659, 0.9714, 0.9735, 0.9728, 0.9686],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1639, -0.1645, -0.1703, -0.1710, -0.1715, -0.1728, -0.1782, -0.1754,
3:         -0.1744, -0.1675, -0.1639, -0.1710, -0.1712, -0.1740, -0.1705, -0.1780,
3:         -0.1780, -0.1724, -0.1674, -0.1680, -0.1641, -0.1679, -0.1681, -0.1696,
3:         -0.1716], device='cuda:3', grad_fn=<SliceBackward0>)
2:      first 25 values: tensor([1.4216, 1.3727, 1.3334, 1.3031, 1.2791, 1.2582, 1.2382, 1.2169, 1.1940,
2:         1.1703, 1.1485, 1.1305, 1.1163, 1.1037, 1.0884, 1.0669, 1.0375, 1.0020,
2:         1.4579, 1.4030, 1.3576, 1.3222, 1.2951, 1.2743, 1.2571],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-2.3942, -2.3554, -2.3159, -2.2754, -2.2335, -2.1902, -2.1458, -2.0996,
2:         -2.0495, -1.9939, -1.9318, -1.8613, -1.7801, -1.6874, -1.5827, -1.4674,
2:         -1.3444, -1.2214, -2.4364, -2.3954, -2.3537, -2.3113, -2.2684, -2.2249,
2:         -2.1812], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.6806, -0.6829, -0.6841, -0.6852, -0.6862, -0.6872, -0.6881, -0.6889,
2:         -0.6889, -0.6893, -0.6890, -0.6880, -0.6874, -0.6872, -0.6868, -0.6863,
2:         -0.6861, -0.6858, -0.6730, -0.6781, -0.6820, -0.6837, -0.6852, -0.6859,
2:         -0.6868], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.5704,  0.6284,  0.6485,  0.6395,  0.6150,  0.5726,  0.5235,  0.4789,
2:          0.4521,  0.4298,  0.3941,  0.3473,  0.3138,  0.2959,  0.2223,  0.0148,
2:         -0.3333, -0.7237,  0.2714,  0.4164,  0.5548,  0.6306,  0.6328,  0.6038,
2:          0.5771], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.1303, -0.1675, -0.1986, -0.2258, -0.2510, -0.2764, -0.3023, -0.3283,
2:         -0.3544, -0.3814, -0.4105, -0.4429, -0.4795, -0.5219, -0.5721, -0.6302,
2:         -0.6931, -0.7539, -0.8049, -0.8413, -0.8619, -0.8703, -0.8718, -0.8720,
2:         -0.8750], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 26, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2376, -0.2434, -0.2434,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2318,     nan, -0.2434,
2:             nan,     nan, -0.2434,     nan, -0.2434,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2434,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan,     nan,
2:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
2:         -0.2399,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2434,     nan,     nan,     nan,     nan,     nan, -0.2434,
2:             nan,     nan, -0.2318, -0.2248,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan, -0.2376,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2434,
2:             nan,     nan, -0.2387,     nan,     nan,     nan,     nan, -0.2434,
2:             nan, -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2434,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 26, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.2150,  0.2236,  0.2365,  0.2432,  0.2443,  0.2306,  0.2091,  0.1734,
2:          0.1272,  0.0777,  0.0263, -0.0228, -0.0659, -0.1016, -0.1321, -0.1558,
2:         -0.1792, -0.2018,  0.2335,  0.2432,  0.2522,  0.2601,  0.2626,  0.2544,
2:          0.2316], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.2350, -1.3332, -1.4411, -1.5547, -1.6745, -1.7953, -1.9117, -2.0173,
2:         -2.0996, -2.1500, -2.1721, -2.1672, -2.1450, -2.1193, -2.0936, -2.0743,
2:         -2.0594, -2.0488, -1.1906, -1.2939, -1.4103, -1.5334, -1.6563, -1.7838,
2:         -1.9080], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0135, -0.0525, -0.1031, -0.1483, -0.1933, -0.2256, -0.2507, -0.2657,
2:         -0.2743, -0.2833, -0.2944, -0.3087, -0.3322, -0.3600, -0.3857, -0.4124,
2:         -0.4447, -0.4764,  0.0424, -0.0035, -0.0559, -0.1099, -0.1625, -0.2018,
2:         -0.2292], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.3627, 0.5139, 0.6425, 0.7783, 0.9097, 0.9657, 1.0349, 1.1567, 1.3372,
2:         1.5449, 1.6985, 1.7425, 1.5930, 1.2678, 0.8971, 0.6345, 0.5506, 0.5527,
2:         0.1445, 0.3195, 0.4740, 0.6462, 0.8178, 0.9167, 1.0090],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([0.8759, 0.8738, 0.8767, 0.8789, 0.8805, 0.8777, 0.8725, 0.8621, 0.8502,
2:         0.8388, 0.8306, 0.8274, 0.8304, 0.8390, 0.8518, 0.8684, 0.8881, 0.9086,
2:         0.9315, 0.9519, 0.9703, 0.9832, 0.9918, 0.9939, 0.9851],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1502, -0.1529, -0.1636, -0.1674, -0.1706, -0.1772, -0.1788, -0.1729,
2:         -0.1729, -0.1528, -0.1512, -0.1601, -0.1672, -0.1720, -0.1698, -0.1761,
2:         -0.1763, -0.1683, -0.1467, -0.1499, -0.1526, -0.1580, -0.1619, -0.1656,
2:         -0.1679], device='cuda:2', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([1.4216, 1.3727, 1.3334, 1.3031, 1.2791, 1.2582, 1.2382, 1.2169, 1.1940,
1:         1.1703, 1.1485, 1.1305, 1.1163, 1.1037, 1.0884, 1.0669, 1.0375, 1.0020,
1:         1.4579, 1.4030, 1.3576, 1.3222, 1.2951, 1.2743, 1.2571],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-2.3942, -2.3554, -2.3159, -2.2754, -2.2335, -2.1902, -2.1458, -2.0996,
1:         -2.0495, -1.9939, -1.9318, -1.8613, -1.7801, -1.6874, -1.5827, -1.4674,
1:         -1.3444, -1.2214, -2.4364, -2.3954, -2.3537, -2.3113, -2.2684, -2.2249,
1:         -2.1812], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.6806, -0.6829, -0.6841, -0.6852, -0.6862, -0.6872, -0.6881, -0.6889,
1:         -0.6889, -0.6893, -0.6890, -0.6880, -0.6874, -0.6872, -0.6868, -0.6863,
1:         -0.6861, -0.6858, -0.6730, -0.6781, -0.6820, -0.6837, -0.6852, -0.6859,
1:         -0.6868], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.5704,  0.6284,  0.6485,  0.6395,  0.6150,  0.5726,  0.5235,  0.4789,
1:          0.4521,  0.4298,  0.3941,  0.3473,  0.3138,  0.2959,  0.2223,  0.0148,
1:         -0.3333, -0.7237,  0.2714,  0.4164,  0.5548,  0.6306,  0.6328,  0.6038,
1:          0.5771], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.1303, -0.1675, -0.1986, -0.2258, -0.2510, -0.2764, -0.3023, -0.3283,
1:         -0.3544, -0.3814, -0.4105, -0.4429, -0.4795, -0.5219, -0.5721, -0.6302,
1:         -0.6931, -0.7539, -0.8049, -0.8413, -0.8619, -0.8703, -0.8718, -0.8720,
1:         -0.8750], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 26, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2341,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2213,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2434,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan, -0.2434,
1:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan,     nan,
1:             nan,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2422, -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2434, -0.2434,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2434,     nan,     nan,     nan, -0.2399,     nan, -0.2434,     nan,
1:             nan, -0.2434,     nan, -0.2434,     nan,     nan,     nan,     nan,
1:         -0.2434,     nan, -0.2434,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan, -0.2434,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2434, -0.2434,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2434,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2434,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2434,     nan, -0.2434, -0.2410,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2434,     nan,     nan,
1:             nan,     nan, -0.2434, -0.2434,     nan,     nan,     nan, -0.2434,
1:         -0.2434,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 26, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.2659,  0.2618,  0.2645,  0.2639,  0.2611,  0.2479,  0.2285,  0.1955,
1:          0.1495,  0.0981,  0.0450, -0.0065, -0.0513, -0.0869, -0.1202, -0.1510,
1:         -0.1825, -0.2129,  0.2620,  0.2641,  0.2648,  0.2663,  0.2649,  0.2569,
1:          0.2377], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.2508, -1.3419, -1.4454, -1.5567, -1.6812, -1.8033, -1.9228, -2.0236,
1:         -2.0989, -2.1432, -2.1605, -2.1557, -2.1431, -2.1245, -2.1068, -2.0941,
1:         -2.0824, -2.0751, -1.2274, -1.3188, -1.4279, -1.5450, -1.6667, -1.7952,
1:         -1.9212], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.0323e-02, -5.5944e-02, -1.1465e-01, -1.6357e-01, -2.1056e-01,
1:         -2.4399e-01, -2.6497e-01, -2.7217e-01, -2.7305e-01, -2.7499e-01,
1:         -2.8633e-01, -3.0374e-01, -3.3489e-01, -3.7028e-01, -4.0420e-01,
1:         -4.3521e-01, -4.6973e-01, -4.9568e-01,  4.9341e-02,  2.2724e-04,
1:         -5.9088e-02, -1.1997e-01, -1.7511e-01, -2.1741e-01, -2.4245e-01],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.5491, 0.6226, 0.6666, 0.7171, 0.7888, 0.8878, 1.0406, 1.1950, 1.3693,
1:         1.5241, 1.6343, 1.7180, 1.6114, 1.3213, 1.0114, 0.7677, 0.6546, 0.6162,
1:         0.3674, 0.4562, 0.5290, 0.6285, 0.7512, 0.8892, 1.0439],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([0.8803, 0.8890, 0.8986, 0.9062, 0.9099, 0.9067, 0.8977, 0.8813, 0.8644,
1:         0.8491, 0.8390, 0.8353, 0.8362, 0.8428, 0.8543, 0.8709, 0.8933, 0.9166,
1:         0.9401, 0.9574, 0.9703, 0.9761, 0.9787, 0.9769, 0.9711],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1499, -0.1518, -0.1551, -0.1534, -0.1510, -0.1531, -0.1519, -0.1438,
1:         -0.1420, -0.1553, -0.1497, -0.1534, -0.1549, -0.1566, -0.1483, -0.1514,
1:         -0.1477, -0.1400, -0.1527, -0.1542, -0.1506, -0.1478, -0.1481, -0.1475,
1:         -0.1488], device='cuda:1', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 26 [1/5 (20%)]	Loss: nan : nan :: 0.14370 (1.40 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 26 [2/5 (40%)]	Loss: nan : nan :: 0.15328 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 26 [3/5 (60%)]	Loss: nan : nan :: 0.13633 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 26 [4/5 (80%)]	Loss: nan : nan :: 0.14699 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch26.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch26.mod
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 26 : nan
0: validation loss for velocity_u : 0.03329486772418022
0: validation loss for velocity_v : 0.05744028463959694
0: validation loss for specific_humidity : 0.02594057098031044
0: validation loss for velocity_z : 0.46734419465065
0: validation loss for temperature : 0.084695965051651
0: validation loss for total_precip : nan
1: 27 : 01:16:18 :: batch_size = 96, lr = 1.0787811788985607e-05
2: 27 : 01:16:18 :: batch_size = 96, lr = 1.0787811788985607e-05
0: 27 : 01:16:18 :: batch_size = 96, lr = 1.0787811788985607e-05
3: 27 : 01:16:18 :: batch_size = 96, lr = 1.0787811788985607e-05
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 27, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([1.0314, 1.0389, 1.0527, 1.0697, 1.0865, 1.1004, 1.1083, 1.1088, 1.1018,
3:         1.0886, 1.0691, 1.0425, 1.0082, 0.9682, 0.9262, 0.8866, 0.8517, 0.8224,
3:         0.9912, 0.9877, 0.9912, 1.0001, 1.0125, 1.0249, 1.0333],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.7728,  0.7120,  0.6464,  0.5783,  0.5096,  0.4426,  0.3792,  0.3223,
3:          0.2748,  0.2385,  0.2124,  0.1914,  0.1685,  0.1380,  0.0964,  0.0438,
3:         -0.0162, -0.0779,  0.8064,  0.7404,  0.6689,  0.5923,  0.5143,  0.4388,
3:          0.3690], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1828, -0.1692, -0.1433, -0.1145, -0.0854, -0.0527, -0.0358, -0.0340,
3:         -0.0352, -0.0493, -0.0783, -0.1098, -0.1480, -0.1780, -0.1985, -0.2011,
3:         -0.2036, -0.1991, -0.2492, -0.2420, -0.2139, -0.1794, -0.1329, -0.0932,
3:         -0.0564], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1357, -0.1467, -0.1666, -0.1843, -0.2020, -0.2418, -0.3258, -0.4187,
3:         -0.4497, -0.3811, -0.2506, -0.1489, -0.1003, -0.0737, -0.0560, -0.0649,
3:         -0.0892, -0.1069, -0.0848, -0.0870, -0.1025, -0.1312, -0.1865, -0.2772,
3:         -0.3833], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.8678, -0.8891, -0.9119, -0.9360, -0.9614, -0.9874, -1.0117, -1.0354,
3:         -1.0606, -1.0866, -1.1103, -1.1309, -1.1492, -1.1653, -1.1781, -1.1866,
3:         -1.1926, -1.1963, -1.1981, -1.1998, -1.2034, -1.2088, -1.2129, -1.2138,
3:         -1.2111], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 27, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2292,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2026,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2314,     nan,     nan,     nan, -0.2137,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1893,
3:             nan,     nan, -0.2004,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2048,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2137,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2314,     nan,
3:             nan, -0.2004,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1982,     nan, -0.2314,     nan,     nan, -0.2292,
3:             nan,     nan,     nan, -0.2092,     nan,     nan,     nan,     nan,
3:         -0.2270,     nan,     nan,     nan,     nan,     nan, -0.2203,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2270,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 27, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.8936, 0.9329, 0.9751, 1.0159, 1.0577, 1.0987, 1.1370, 1.1674, 1.1926,
3:         1.2111, 1.2256, 1.2381, 1.2542, 1.2718, 1.2870, 1.2950, 1.2962, 1.2849,
3:         0.9100, 0.9565, 1.0023, 1.0430, 1.0814, 1.1163, 1.1477],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.8565, 0.9136, 0.9810, 1.0575, 1.1320, 1.2179, 1.3074, 1.4022, 1.5099,
3:         1.6223, 1.7411, 1.8645, 1.9855, 2.0999, 2.1954, 2.2674, 2.3044, 2.3119,
3:         0.8715, 0.9395, 1.0201, 1.1059, 1.1963, 1.2887, 1.3862],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5141, -0.5168, -0.5236, -0.5279, -0.5303, -0.5315, -0.5319, -0.5358,
3:         -0.5401, -0.5447, -0.5501, -0.5584, -0.5692, -0.5867, -0.6046, -0.6217,
3:         -0.6321, -0.6339, -0.5110, -0.5142, -0.5174, -0.5203, -0.5225, -0.5236,
3:         -0.5266], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.4197, 0.3119, 0.2797, 0.3108, 0.4127, 0.5514, 0.6635, 0.7672, 0.8864,
3:         0.9959, 1.0987, 1.2126, 1.2599, 1.2545, 1.3040, 1.3932, 1.4963, 1.5489,
3:         0.3932, 0.3523, 0.3754, 0.4375, 0.5560, 0.7099, 0.8042],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([ 0.3061,  0.3016,  0.2985,  0.2923,  0.2841,  0.2713,  0.2537,  0.2299,
3:          0.2031,  0.1769,  0.1565,  0.1444,  0.1432,  0.1493,  0.1583,  0.1656,
3:          0.1662,  0.1562,  0.1357,  0.1038,  0.0628,  0.0146, -0.0368, -0.0882,
3:         -0.1394], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1765, -0.1765, -0.1801, -0.1734, -0.1743, -0.1727, -0.1753, -0.1705,
3:         -0.1707, -0.1845, -0.1790, -0.1813, -0.1793, -0.1778, -0.1756, -0.1810,
3:         -0.1792, -0.1745, -0.1834, -0.1831, -0.1773, -0.1803, -0.1779, -0.1801,
3:         -0.1803], device='cuda:3', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 27, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: Created sparse mask for total_precip with 10.0% data retained
2:      first 25 values: tensor([1.0314, 1.0389, 1.0527, 1.0697, 1.0865, 1.1004, 1.1083, 1.1088, 1.1018,
2:         1.0886, 1.0691, 1.0425, 1.0082, 0.9682, 0.9262, 0.8866, 0.8517, 0.8224,
2:         0.9912, 0.9877, 0.9912, 1.0001, 1.0125, 1.0249, 1.0333],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.7728,  0.7120,  0.6464,  0.5783,  0.5096,  0.4426,  0.3792,  0.3223,
2:          0.2748,  0.2385,  0.2124,  0.1914,  0.1685,  0.1380,  0.0964,  0.0438,
2:         -0.0162, -0.0779,  0.8064,  0.7404,  0.6689,  0.5923,  0.5143,  0.4388,
2:          0.3690], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1828, -0.1692, -0.1433, -0.1145, -0.0854, -0.0527, -0.0358, -0.0340,
2:         -0.0352, -0.0493, -0.0783, -0.1098, -0.1480, -0.1780, -0.1985, -0.2011,
2:         -0.2036, -0.1991, -0.2492, -0.2420, -0.2139, -0.1794, -0.1329, -0.0932,
2:         -0.0564], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1357, -0.1467, -0.1666, -0.1843, -0.2020, -0.2418, -0.3258, -0.4187,
2:         -0.4497, -0.3811, -0.2506, -0.1489, -0.1003, -0.0737, -0.0560, -0.0649,
2:         -0.0892, -0.1069, -0.0848, -0.0870, -0.1025, -0.1312, -0.1865, -0.2772,
2:         -0.3833], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.8678, -0.8891, -0.9119, -0.9360, -0.9614, -0.9874, -1.0117, -1.0354,
2:         -1.0606, -1.0866, -1.1103, -1.1309, -1.1492, -1.1653, -1.1781, -1.1866,
2:         -1.1926, -1.1963, -1.1981, -1.1998, -1.2034, -1.2088, -1.2129, -1.2138,
2:         -1.2111], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 27, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.1959,     nan,     nan, -0.2292,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2115, -0.2203,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2248,     nan, -0.2292,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2270,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1826,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1982, -0.2159,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.1937,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2004,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2070,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2203,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2314,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2314, -0.2314,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2137,     nan, -0.2203,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2270,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2115,     nan,     nan,     nan,     nan,
2:             nan, -0.1915,     nan, -0.2070,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 27, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.8478, 0.8858, 0.9312, 0.9828, 1.0328, 1.0791, 1.1247, 1.1629, 1.1945,
2:         1.2207, 1.2399, 1.2579, 1.2803, 1.2969, 1.3122, 1.3187, 1.3133, 1.2959,
2:         0.8708, 0.9150, 0.9626, 1.0115, 1.0552, 1.0991, 1.1367],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.8849, 0.9404, 1.0109, 1.0874, 1.1632, 1.2453, 1.3281, 1.4246, 1.5381,
2:         1.6536, 1.7715, 1.8885, 1.9945, 2.0967, 2.1820, 2.2509, 2.2864, 2.2851,
2:         0.9276, 0.9907, 1.0680, 1.1507, 1.2328, 1.3152, 1.4034],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.4918, -0.4936, -0.5012, -0.5089, -0.5145, -0.5219, -0.5239, -0.5282,
2:         -0.5320, -0.5358, -0.5405, -0.5490, -0.5593, -0.5757, -0.5948, -0.6150,
2:         -0.6325, -0.6411, -0.4955, -0.4960, -0.4983, -0.5018, -0.5069, -0.5106,
2:         -0.5133], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.2975, 0.2926, 0.2889, 0.3318, 0.4469, 0.5736, 0.6716, 0.7549, 0.8917,
2:         1.0485, 1.1992, 1.3550, 1.3886, 1.3365, 1.3367, 1.4101, 1.5207, 1.5499,
2:         0.2500, 0.2892, 0.3289, 0.4083, 0.5398, 0.6769, 0.7887],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([ 0.3065,  0.2983,  0.2926,  0.2885,  0.2861,  0.2808,  0.2700,  0.2511,
2:          0.2252,  0.1968,  0.1719,  0.1554,  0.1491,  0.1513,  0.1582,  0.1639,
2:          0.1651,  0.1565,  0.1377,  0.1075,  0.0670,  0.0210, -0.0280, -0.0781,
2:         -0.1263], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1607, -0.1650, -0.1734, -0.1748, -0.1753, -0.1759, -0.1777, -0.1712,
2:         -0.1702, -0.1702, -0.1658, -0.1715, -0.1776, -0.1790, -0.1766, -0.1803,
2:         -0.1779, -0.1680, -0.1676, -0.1708, -0.1716, -0.1736, -0.1753, -0.1765,
2:         -0.1768], device='cuda:2', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 27, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0314, 1.0389, 1.0527, 1.0697, 1.0865, 1.1004, 1.1083, 1.1088, 1.1018,
0:         1.0886, 1.0691, 1.0425, 1.0082, 0.9682, 0.9262, 0.8866, 0.8517, 0.8224,
0:         0.9912, 0.9877, 0.9912, 1.0001, 1.0125, 1.0249, 1.0333],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.7728,  0.7120,  0.6464,  0.5783,  0.5096,  0.4426,  0.3792,  0.3223,
0:          0.2748,  0.2385,  0.2124,  0.1914,  0.1685,  0.1380,  0.0964,  0.0438,
0:         -0.0162, -0.0779,  0.8064,  0.7404,  0.6689,  0.5923,  0.5143,  0.4388,
0:          0.3690], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1828, -0.1692, -0.1433, -0.1145, -0.0854, -0.0527, -0.0358, -0.0340,
0:         -0.0352, -0.0493, -0.0783, -0.1098, -0.1480, -0.1780, -0.1985, -0.2011,
0:         -0.2036, -0.1991, -0.2492, -0.2420, -0.2139, -0.1794, -0.1329, -0.0932,
0:         -0.0564], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1357, -0.1467, -0.1666, -0.1843, -0.2020, -0.2418, -0.3258, -0.4187,
0:         -0.4497, -0.3811, -0.2506, -0.1489, -0.1003, -0.0737, -0.0560, -0.0649,
0:         -0.0892, -0.1069, -0.0848, -0.0870, -0.1025, -0.1312, -0.1865, -0.2772,
0:         -0.3833], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.8678, -0.8891, -0.9119, -0.9360, -0.9614, -0.9874, -1.0117, -1.0354,
0:         -1.0606, -1.0866, -1.1103, -1.1309, -1.1492, -1.1653, -1.1781, -1.1866,
0:         -1.1926, -1.1963, -1.1981, -1.1998, -1.2034, -1.2088, -1.2129, -1.2138,
0:         -1.2111], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 27, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.1582,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2292, -0.2292,     nan,     nan,     nan, -0.2270,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2026,
0:             nan, -0.1982,     nan,     nan,     nan,     nan,     nan, -0.2159,
0:         -0.2203,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2092,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2203,     nan,
0:             nan,     nan, -0.1826,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2026, -0.2004,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2292,
0:             nan,     nan,     nan,     nan, -0.1959,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2137,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2314,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2226, -0.2159,     nan,     nan,     nan,
0:             nan, -0.1471,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 27, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8596, 0.8978, 0.9395, 0.9833, 1.0218, 1.0579, 1.0930, 1.1246, 1.1574,
0:         1.1892, 1.2151, 1.2350, 1.2550, 1.2696, 1.2782, 1.2805, 1.2733, 1.2529,
0:         0.8903, 0.9279, 0.9706, 1.0107, 1.0446, 1.0764, 1.1069],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8804, 0.9336, 1.0032, 1.0824, 1.1619, 1.2463, 1.3321, 1.4288, 1.5389,
0:         1.6525, 1.7730, 1.8922, 2.0019, 2.1071, 2.1970, 2.2708, 2.3177, 2.3339,
0:         0.9299, 0.9920, 1.0696, 1.1531, 1.2367, 1.3230, 1.4099],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5427, -0.5360, -0.5349, -0.5318, -0.5301, -0.5322, -0.5348, -0.5402,
0:         -0.5475, -0.5536, -0.5615, -0.5689, -0.5784, -0.5910, -0.6059, -0.6213,
0:         -0.6316, -0.6297, -0.5456, -0.5393, -0.5326, -0.5293, -0.5304, -0.5309,
0:         -0.5354], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3302, 0.2181, 0.1840, 0.2206, 0.3238, 0.4899, 0.6444, 0.7457, 0.8985,
0:         1.0533, 1.1605, 1.2855, 1.3016, 1.2892, 1.3645, 1.4477, 1.5618, 1.5838,
0:         0.3162, 0.2221, 0.2170, 0.2885, 0.4287, 0.6242, 0.7752],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.2818,  0.2818,  0.2842,  0.2853,  0.2827,  0.2744,  0.2585,  0.2354,
0:          0.2093,  0.1846,  0.1658,  0.1552,  0.1534,  0.1576,  0.1642,  0.1700,
0:          0.1702,  0.1625,  0.1446,  0.1148,  0.0736,  0.0241, -0.0295, -0.0821,
0:         -0.1319], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1544, -0.1607, -0.1694, -0.1727, -0.1770, -0.1792, -0.1796, -0.1755,
0:         -0.1761, -0.1614, -0.1583, -0.1673, -0.1735, -0.1795, -0.1790, -0.1838,
0:         -0.1829, -0.1752, -0.1562, -0.1615, -0.1633, -0.1687, -0.1746, -0.1778,
0:         -0.1794], device='cuda:0', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 27, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.0314, 1.0389, 1.0527, 1.0697, 1.0865, 1.1004, 1.1083, 1.1088, 1.1018,
1:         1.0886, 1.0691, 1.0425, 1.0082, 0.9682, 0.9262, 0.8866, 0.8517, 0.8224,
1:         0.9912, 0.9877, 0.9912, 1.0001, 1.0125, 1.0249, 1.0333],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.7728,  0.7120,  0.6464,  0.5783,  0.5096,  0.4426,  0.3792,  0.3223,
1:          0.2748,  0.2385,  0.2124,  0.1914,  0.1685,  0.1380,  0.0964,  0.0438,
1:         -0.0162, -0.0779,  0.8064,  0.7404,  0.6689,  0.5923,  0.5143,  0.4388,
1:          0.3690], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1828, -0.1692, -0.1433, -0.1145, -0.0854, -0.0527, -0.0358, -0.0340,
1:         -0.0352, -0.0493, -0.0783, -0.1098, -0.1480, -0.1780, -0.1985, -0.2011,
1:         -0.2036, -0.1991, -0.2492, -0.2420, -0.2139, -0.1794, -0.1329, -0.0932,
1:         -0.0564], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1357, -0.1467, -0.1666, -0.1843, -0.2020, -0.2418, -0.3258, -0.4187,
1:         -0.4497, -0.3811, -0.2506, -0.1489, -0.1003, -0.0737, -0.0560, -0.0649,
1:         -0.0892, -0.1069, -0.0848, -0.0870, -0.1025, -0.1312, -0.1865, -0.2772,
1:         -0.3833], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.8678, -0.8891, -0.9119, -0.9360, -0.9614, -0.9874, -1.0117, -1.0354,
1:         -1.0606, -1.0866, -1.1103, -1.1309, -1.1492, -1.1653, -1.1781, -1.1866,
1:         -1.1926, -1.1963, -1.1981, -1.1998, -1.2034, -1.2088, -1.2129, -1.2138,
1:         -1.2111], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 27, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan, -0.1804,     nan,     nan, -0.1760,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2026,     nan, -0.2226,     nan,     nan,
1:             nan,     nan,     nan, -0.2292,     nan,     nan,     nan,     nan,
1:         -0.2137,     nan, -0.2181,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2026,     nan,     nan, -0.2070,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2137,
1:             nan,     nan,     nan, -0.2092,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2226,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1937,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2314,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2292,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2314, -0.2270,
1:             nan,     nan,     nan, -0.1871,     nan, -0.2314,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2181,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2159,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 27, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.9064, 0.9363, 0.9721, 1.0105, 1.0509, 1.0921, 1.1344, 1.1743, 1.2105,
1:         1.2430, 1.2664, 1.2835, 1.3005, 1.3141, 1.3251, 1.3317, 1.3309, 1.3181,
1:         0.9197, 0.9574, 0.9979, 1.0373, 1.0736, 1.1092, 1.1447],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.8769, 0.9331, 1.0053, 1.0892, 1.1758, 1.2648, 1.3541, 1.4527, 1.5641,
1:         1.6741, 1.7859, 1.8943, 1.9934, 2.0858, 2.1646, 2.2257, 2.2608, 2.2638,
1:         0.9135, 0.9759, 1.0600, 1.1557, 1.2496, 1.3431, 1.4364],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5217, -0.5367, -0.5490, -0.5553, -0.5561, -0.5525, -0.5456, -0.5441,
1:         -0.5438, -0.5453, -0.5521, -0.5600, -0.5715, -0.5859, -0.6041, -0.6205,
1:         -0.6327, -0.6366, -0.5259, -0.5348, -0.5395, -0.5414, -0.5402, -0.5349,
1:         -0.5323], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.3427, 0.3167, 0.2964, 0.3492, 0.4636, 0.5759, 0.6757, 0.7722, 0.9079,
1:         1.0222, 1.1095, 1.2161, 1.2681, 1.2983, 1.3962, 1.5066, 1.5875, 1.5923,
1:         0.3838, 0.3983, 0.3961, 0.4636, 0.5718, 0.6858, 0.7794],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([ 0.3083,  0.2998,  0.2952,  0.2914,  0.2858,  0.2755,  0.2590,  0.2364,
1:          0.2116,  0.1877,  0.1685,  0.1565,  0.1540,  0.1582,  0.1687,  0.1790,
1:          0.1846,  0.1787,  0.1600,  0.1280,  0.0846,  0.0334, -0.0235, -0.0815,
1:         -0.1365], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1675, -0.1691, -0.1813, -0.1817, -0.1844, -0.1831, -0.1852, -0.1762,
1:         -0.1744, -0.1707, -0.1675, -0.1735, -0.1787, -0.1829, -0.1782, -0.1800,
1:         -0.1775, -0.1679, -0.1665, -0.1668, -0.1679, -0.1715, -0.1739, -0.1742,
1:         -0.1733], device='cuda:1', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 27 [1/5 (20%)]	Loss: nan : nan :: 0.14282 (1.64 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 27 [2/5 (40%)]	Loss: nan : nan :: 0.14905 (10.52 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 27 [3/5 (60%)]	Loss: nan : nan :: 0.14172 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 27 [4/5 (80%)]	Loss: nan : nan :: 0.14159 (10.42 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch27.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch27.mod
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 27 : nan
0: validation loss for velocity_u : 0.031483814120292664
0: validation loss for velocity_v : 0.06315288692712784
0: validation loss for specific_humidity : 0.025034327059984207
0: validation loss for velocity_z : 0.4351519048213959
0: validation loss for temperature : 0.07134288549423218
0: validation loss for total_precip : nan
2: 28 : 01:23:01 :: batch_size = 96, lr = 1.0524694428278642e-05
0: 28 : 01:23:01 :: batch_size = 96, lr = 1.0524694428278642e-05
1: 28 : 01:23:01 :: batch_size = 96, lr = 1.0524694428278642e-05
3: 28 : 01:23:01 :: batch_size = 96, lr = 1.0524694428278642e-05
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 28, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7443, -0.7475, -0.7572, -0.7654, -0.7615, -0.7456, -0.7314, -0.7356,
0:         -0.7605, -0.7913, -0.8109, -0.8151, -0.8119, -0.8097, -0.8122, -0.8203,
0:         -0.8345, -0.8533, -0.7200, -0.7214, -0.7301, -0.7421, -0.7475, -0.7408,
0:         -0.7309], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2065, -0.2640, -0.3192, -0.3780, -0.4436, -0.5095, -0.5676, -0.6196,
0:         -0.6777, -0.7516, -0.8392, -0.9312, -1.0223, -1.1147, -1.2147, -1.3241,
0:         -1.4405, -1.5608, -0.1719, -0.2240, -0.2776, -0.3363, -0.4030, -0.4740,
0:         -0.5428], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6645, -0.6697, -0.6759, -0.6831, -0.6898, -0.6958, -0.7043, -0.7132,
0:         -0.7204, -0.7275, -0.7308, -0.7344, -0.7385, -0.7409, -0.7431, -0.7445,
0:         -0.7453, -0.7466, -0.6676, -0.6704, -0.6734, -0.6821, -0.6909, -0.6969,
0:         -0.7043], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7849, 0.7481, 0.6835, 0.6067, 0.4831, 0.3383, 0.3094, 0.4931, 0.7982,
0:         1.0310, 1.0900, 1.0031, 0.8539, 0.7236, 0.6802, 0.7503, 0.9096, 1.1201,
0:         0.8929, 0.8962, 0.7381, 0.5889, 0.5310, 0.4619, 0.3405],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.3606, -0.3961, -0.4302, -0.4599, -0.4868, -0.5134, -0.5426, -0.5786,
0:         -0.6240, -0.6769, -0.7338, -0.7889, -0.8327, -0.8563, -0.8609, -0.8567,
0:         -0.8557, -0.8606, -0.8714, -0.8872, -0.9073, -0.9275, -0.9460, -0.9617,
0:         -0.9735], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 28, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2497, -0.2497, -0.2497,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
0:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2497,     nan, -0.2497,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2497,     nan,     nan, -0.2497,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 28, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2958, -1.2756, -1.2494, -1.2139, -1.1726, -1.1322, -1.0863, -1.0521,
0:         -1.0166, -0.9837, -0.9424, -0.9046, -0.8615, -0.8190, -0.7836, -0.7553,
0:         -0.7303, -0.7136, -1.3591, -1.3337, -1.3134, -1.2744, -1.2312, -1.1889,
0:         -1.1506], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1368,  0.1384,  0.1250,  0.0981,  0.0660,  0.0447,  0.0278,  0.0236,
0:          0.0231,  0.0248,  0.0224,  0.0239,  0.0188,  0.0143,  0.0018, -0.0109,
0:         -0.0222, -0.0355,  0.1629,  0.1786,  0.1791,  0.1655,  0.1467,  0.1355,
0:          0.1254], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3700, -0.4054, -0.4387, -0.4687, -0.4985, -0.5236, -0.5513, -0.5716,
0:         -0.5962, -0.6198, -0.6430, -0.6620, -0.6743, -0.6795, -0.6855, -0.6925,
0:         -0.7054, -0.7159, -0.3376, -0.3713, -0.4054, -0.4379, -0.4657, -0.4919,
0:         -0.5115], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1153, -0.0163, -0.0095, -0.0959, -0.1332, -0.1628, -0.1982, -0.1388,
0:         -0.0052,  0.0173, -0.0897, -0.1131, -0.0702, -0.1172, -0.1350, -0.0061,
0:          0.1926,  0.3926,  0.1877,  0.3507,  0.3069,  0.1204,  0.0795,  0.0602,
0:         -0.0523], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3308, -0.3508, -0.3700, -0.3981, -0.4279, -0.4612, -0.4918, -0.5177,
0:         -0.5311, -0.5301, -0.5190, -0.5016, -0.4784, -0.4508, -0.4178, -0.3809,
0:         -0.3417, -0.3044, -0.2683, -0.2339, -0.2002, -0.1725, -0.1557, -0.1473,
0:         -0.1479], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1558, -0.1607, -0.1695, -0.1683, -0.1678, -0.1698, -0.1709, -0.1651,
0:         -0.1628, -0.1696, -0.1661, -0.1713, -0.1728, -0.1758, -0.1717, -0.1762,
0:         -0.1729, -0.1629, -0.1669, -0.1723, -0.1678, -0.1690, -0.1715, -0.1677,
0:         -0.1684], device='cuda:0', grad_fn=<SliceBackward0>)
3: [DEBUG] INPUT BATCH
3: Epoch 28, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch 28, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: [DEBUG] INPUT BATCH
2: Epoch 28, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7443, -0.7475, -0.7572, -0.7654, -0.7615, -0.7456, -0.7314, -0.7356,
3:         -0.7605, -0.7913, -0.8109, -0.8151, -0.8119, -0.8097, -0.8122, -0.8203,
3:         -0.8345, -0.8533, -0.7200, -0.7214, -0.7301, -0.7421, -0.7475, -0.7408,
3:         -0.7309], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2065, -0.2640, -0.3192, -0.3780, -0.4436, -0.5095, -0.5676, -0.6196,
3:         -0.6777, -0.7516, -0.8392, -0.9312, -1.0223, -1.1147, -1.2147, -1.3241,
3:         -1.4405, -1.5608, -0.1719, -0.2240, -0.2776, -0.3363, -0.4030, -0.4740,
3:         -0.5428], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.6645, -0.6697, -0.6759, -0.6831, -0.6898, -0.6958, -0.7043, -0.7132,
3:         -0.7204, -0.7275, -0.7308, -0.7344, -0.7385, -0.7409, -0.7431, -0.7445,
3:         -0.7453, -0.7466, -0.6676, -0.6704, -0.6734, -0.6821, -0.6909, -0.6969,
3:         -0.7043], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.7849, 0.7481, 0.6835, 0.6067, 0.4831, 0.3383, 0.3094, 0.4931, 0.7982,
3:         1.0310, 1.0900, 1.0031, 0.8539, 0.7236, 0.6802, 0.7503, 0.9096, 1.1201,
3:         0.8929, 0.8962, 0.7381, 0.5889, 0.5310, 0.4619, 0.3405],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.3606, -0.3961, -0.4302, -0.4599, -0.4868, -0.5134, -0.5426, -0.5786,
3:         -0.6240, -0.6769, -0.7338, -0.7889, -0.8327, -0.8563, -0.8609, -0.8567,
3:         -0.8557, -0.8606, -0.8714, -0.8872, -0.9073, -0.9275, -0.9460, -0.9617,
3:         -0.9735], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 28, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,
3:             nan,     nan, -0.2497, -0.2497,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2497, -0.2497,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2497,     nan,     nan, -0.2497,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2497,     nan,     nan, -0.2497,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2497, -0.2497,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan, -0.2497,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,
3:             nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 28, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.2644, -1.2379, -1.2087, -1.1768, -1.1469, -1.1174, -1.0815, -1.0459,
3:         -1.0072, -0.9655, -0.9189, -0.8822, -0.8514, -0.8273, -0.8111, -0.7970,
3:         -0.7808, -0.7643, -1.3491, -1.3215, -1.2975, -1.2655, -1.2315, -1.1963,
3:         -1.1624], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.1133,  0.1106,  0.0957,  0.0765,  0.0546,  0.0374,  0.0200,  0.0148,
3:          0.0102,  0.0122,  0.0165,  0.0268,  0.0307,  0.0295,  0.0206,  0.0067,
3:         -0.0056, -0.0267,  0.1387,  0.1490,  0.1477,  0.1407,  0.1305,  0.1270,
3:          0.1139], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.3891, -0.4275, -0.4653, -0.4965, -0.5270, -0.5494, -0.5695, -0.5844,
3:         -0.6034, -0.6196, -0.6347, -0.6486, -0.6625, -0.6745, -0.6907, -0.7057,
3:         -0.7248, -0.7378, -0.3391, -0.3795, -0.4128, -0.4449, -0.4748, -0.5012,
3:         -0.5202], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0552,  0.1634,  0.0876, -0.0523, -0.0283, -0.0451, -0.1249, -0.1480,
3:         -0.0636, -0.0042, -0.1016, -0.1331, -0.0758, -0.1193, -0.1047,  0.0454,
3:          0.2178,  0.3871,  0.2655,  0.4278,  0.3059,  0.0900,  0.1309,  0.1424,
3:         -0.0038], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.3619, -0.3704, -0.3785, -0.3993, -0.4278, -0.4627, -0.4948, -0.5199,
3:         -0.5307, -0.5286, -0.5181, -0.5013, -0.4764, -0.4452, -0.4103, -0.3725,
3:         -0.3315, -0.2868, -0.2361, -0.1832, -0.1310, -0.0892, -0.0617, -0.0439,
3:         -0.0391], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1426, -0.1426, -0.1489, -0.1456, -0.1437, -0.1504, -0.1515, -0.1426,
3:         -0.1464, -0.1483, -0.1424, -0.1449, -0.1415, -0.1454, -0.1417, -0.1452,
3:         -0.1453, -0.1360, -0.1456, -0.1461, -0.1404, -0.1334, -0.1342, -0.1306,
3:         -0.1348], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-0.7443, -0.7475, -0.7572, -0.7654, -0.7615, -0.7456, -0.7314, -0.7356,
1:         -0.7605, -0.7913, -0.8109, -0.8151, -0.8119, -0.8097, -0.8122, -0.8203,
1:         -0.8345, -0.8533, -0.7200, -0.7214, -0.7301, -0.7421, -0.7475, -0.7408,
1:         -0.7309], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2065, -0.2640, -0.3192, -0.3780, -0.4436, -0.5095, -0.5676, -0.6196,
1:         -0.6777, -0.7516, -0.8392, -0.9312, -1.0223, -1.1147, -1.2147, -1.3241,
1:         -1.4405, -1.5608, -0.1719, -0.2240, -0.2776, -0.3363, -0.4030, -0.4740,
1:         -0.5428], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.6645, -0.6697, -0.6759, -0.6831, -0.6898, -0.6958, -0.7043, -0.7132,
1:         -0.7204, -0.7275, -0.7308, -0.7344, -0.7385, -0.7409, -0.7431, -0.7445,
1:         -0.7453, -0.7466, -0.6676, -0.6704, -0.6734, -0.6821, -0.6909, -0.6969,
1:         -0.7043], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.7849, 0.7481, 0.6835, 0.6067, 0.4831, 0.3383, 0.3094, 0.4931, 0.7982,
1:         1.0310, 1.0900, 1.0031, 0.8539, 0.7236, 0.6802, 0.7503, 0.9096, 1.1201,
1:         0.8929, 0.8962, 0.7381, 0.5889, 0.5310, 0.4619, 0.3405],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.3606, -0.3961, -0.4302, -0.4599, -0.4868, -0.5134, -0.5426, -0.5786,
1:         -0.6240, -0.6769, -0.7338, -0.7889, -0.8327, -0.8563, -0.8609, -0.8567,
1:         -0.8557, -0.8606, -0.8714, -0.8872, -0.9073, -0.9275, -0.9460, -0.9617,
1:         -0.9735], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 28, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2:      first 25 values: tensor([-0.7443, -0.7475, -0.7572, -0.7654, -0.7615, -0.7456, -0.7314, -0.7356,
2:         -0.7605, -0.7913, -0.8109, -0.8151, -0.8119, -0.8097, -0.8122, -0.8203,
2:         -0.8345, -0.8533, -0.7200, -0.7214, -0.7301, -0.7421, -0.7475, -0.7408,
2:         -0.7309], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2497,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2497, -0.2497,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan, -0.2497,
1:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2497,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 28, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([-0.2065, -0.2640, -0.3192, -0.3780, -0.4436, -0.5095, -0.5676, -0.6196,
2:         -0.6777, -0.7516, -0.8392, -0.9312, -1.0223, -1.1147, -1.2147, -1.3241,
2:         -1.4405, -1.5608, -0.1719, -0.2240, -0.2776, -0.3363, -0.4030, -0.4740,
2:         -0.5428], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 pred values: tensor([-1.2511, -1.2285, -1.2093, -1.1830, -1.1538, -1.1255, -1.0878, -1.0554,
1:         -1.0134, -0.9658, -0.9130, -0.8654, -0.8227, -0.7908, -0.7674, -0.7504,
1:         -0.7345, -0.7244, -1.3090, -1.2876, -1.2728, -1.2417, -1.2097, -1.1769,
1:         -1.1445], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([-0.6645, -0.6697, -0.6759, -0.6831, -0.6898, -0.6958, -0.7043, -0.7132,
2:         -0.7204, -0.7275, -0.7308, -0.7344, -0.7385, -0.7409, -0.7431, -0.7445,
2:         -0.7453, -0.7466, -0.6676, -0.6704, -0.6734, -0.6821, -0.6909, -0.6969,
2:         -0.7043], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.7849, 0.7481, 0.6835, 0.6067, 0.4831, 0.3383, 0.3094, 0.4931, 0.7982,
2:         1.0310, 1.0900, 1.0031, 0.8539, 0.7236, 0.6802, 0.7503, 0.9096, 1.1201,
2:         0.8929, 0.8962, 0.7381, 0.5889, 0.5310, 0.4619, 0.3405],
2:        device='cuda:2')
1:      first 25 pred values: tensor([0.1029, 0.1039, 0.0876, 0.0636, 0.0410, 0.0325, 0.0306, 0.0354, 0.0367,
1:         0.0400, 0.0366, 0.0348, 0.0319, 0.0293, 0.0267, 0.0223, 0.0179, 0.0061,
1:         0.1405, 0.1531, 0.1508, 0.1367, 0.1255, 0.1228, 0.1206],
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([-0.3606, -0.3961, -0.4302, -0.4599, -0.4868, -0.5134, -0.5426, -0.5786,
2:         -0.6240, -0.6769, -0.7338, -0.7889, -0.8327, -0.8563, -0.8609, -0.8567,
2:         -0.8557, -0.8606, -0.8714, -0.8872, -0.9073, -0.9275, -0.9460, -0.9617,
2:         -0.9735], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 pred values: tensor([-0.3767, -0.4121, -0.4465, -0.4824, -0.5203, -0.5547, -0.5833, -0.6035,
1:         -0.6229, -0.6370, -0.6488, -0.6608, -0.6758, -0.6931, -0.7155, -0.7346,
1:         -0.7594, -0.7753, -0.3359, -0.3683, -0.4007, -0.4399, -0.4755, -0.5073,
1:         -0.5358], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
1:      first 25 pred values: tensor([-0.1105, -0.0682, -0.1159, -0.2080, -0.1604, -0.1882, -0.3155, -0.3235,
1:         -0.1728,  0.0070, -0.0311, -0.1753, -0.1837, -0.2248, -0.1415,  0.1209,
1:          0.2492,  0.3816,  0.1173,  0.2691,  0.1833,  0.0037,  0.0635,  0.0840,
1:         -0.0704], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2: [DEBUG] TARGET BATCH
2: Epoch 28, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.3870, -0.3861, -0.3863, -0.4026, -0.4277, -0.4635, -0.5005, -0.5372,
1:         -0.5591, -0.5615, -0.5478, -0.5206, -0.4855, -0.4481, -0.4141, -0.3802,
1:         -0.3463, -0.3108, -0.2722, -0.2334, -0.1941, -0.1608, -0.1349, -0.1128,
1:         -0.1009], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan, -0.2497, -0.2497,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2497, -0.2497,     nan,     nan,     nan, -0.2497,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,
2:             nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan, -0.2497,
2:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2497,     nan, -0.2497, -0.2497,     nan,     nan, -0.2497,     nan,
2:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2497,     nan,     nan,     nan, -0.2497,
2:         -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 28, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1487, -0.1549, -0.1660, -0.1665, -0.1731, -0.1778, -0.1774, -0.1698,
1:         -0.1673, -0.1539, -0.1559, -0.1608, -0.1691, -0.1727, -0.1730, -0.1732,
1:         -0.1699, -0.1576, -0.1540, -0.1592, -0.1576, -0.1608, -0.1630, -0.1630,
1:         -0.1607], device='cuda:1', grad_fn=<SliceBackward0>)
2:      first 25 pred values: tensor([-1.2497, -1.2272, -1.2032, -1.1706, -1.1355, -1.0997, -1.0585, -1.0246,
2:         -0.9927, -0.9557, -0.9106, -0.8698, -0.8257, -0.7875, -0.7581, -0.7359,
2:         -0.7201, -0.7062, -1.3104, -1.2882, -1.2701, -1.2369, -1.2008, -1.1633,
2:         -1.1263], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.1708,  0.1595,  0.1362,  0.1043,  0.0718,  0.0509,  0.0297,  0.0166,
2:          0.0091,  0.0099,  0.0113,  0.0176,  0.0175,  0.0073, -0.0088, -0.0274,
2:         -0.0451, -0.0640,  0.1775,  0.1751,  0.1631,  0.1456,  0.1296,  0.1174,
2:          0.1058], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.4225, -0.4525, -0.4836, -0.5083, -0.5350, -0.5597, -0.5832, -0.6018,
2:         -0.6242, -0.6428, -0.6646, -0.6809, -0.6935, -0.7012, -0.7116, -0.7195,
2:         -0.7346, -0.7484, -0.3780, -0.4067, -0.4325, -0.4605, -0.4881, -0.5156,
2:         -0.5373], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0100,  0.1289,  0.1545,  0.0514, -0.0217, -0.1055, -0.1768, -0.1386,
2:         -0.0218,  0.0466, -0.0462, -0.1319, -0.0796, -0.0603, -0.0110,  0.1353,
2:          0.2833,  0.4365,  0.1877,  0.3488,  0.2869,  0.1348,  0.1347,  0.1036,
2:         -0.0405], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.3815, -0.3971, -0.4052, -0.4166, -0.4297, -0.4524, -0.4843, -0.5207,
2:         -0.5505, -0.5620, -0.5522, -0.5242, -0.4843, -0.4406, -0.3989, -0.3598,
2:         -0.3243, -0.2917, -0.2596, -0.2272, -0.1932, -0.1603, -0.1309, -0.1020,
2:         -0.0798], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1661, -0.1691, -0.1775, -0.1759, -0.1760, -0.1776, -0.1791, -0.1740,
2:         -0.1733, -0.1701, -0.1673, -0.1736, -0.1777, -0.1767, -0.1730, -0.1775,
2:         -0.1771, -0.1705, -0.1678, -0.1706, -0.1667, -0.1695, -0.1694, -0.1672,
2:         -0.1703], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 28 [1/5 (20%)]	Loss: nan : nan :: 0.14647 (1.79 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 28 [2/5 (40%)]	Loss: nan : nan :: 0.14173 (10.52 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 28 [3/5 (60%)]	Loss: nan : nan :: 0.13794 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 28 [4/5 (80%)]	Loss: nan : nan :: 0.15954 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch28.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch28.mod
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 28 : nan
0: validation loss for velocity_u : 0.0338112935423851
0: validation loss for velocity_v : 0.06138153374195099
0: validation loss for specific_humidity : 0.02515595592558384
0: validation loss for velocity_z : 0.4610745906829834
0: validation loss for temperature : 0.08212052285671234
0: validation loss for total_precip : nan
1: 29 : 01:29:55 :: batch_size = 96, lr = 1.0267994564174285e-05
2: 29 : 01:29:55 :: batch_size = 96, lr = 1.0267994564174285e-05
3: 29 : 01:29:55 :: batch_size = 96, lr = 1.0267994564174285e-05
0: 29 : 01:29:55 :: batch_size = 96, lr = 1.0267994564174285e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 29, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0758, 1.0901, 1.1043, 1.1173, 1.1314, 1.1528, 1.1823, 1.2129, 1.2434,
0:         1.2732, 1.2998, 1.3239, 1.3459, 1.3651, 1.3831, 1.4018, 1.4218, 1.4428,
0:         1.0248, 1.0296, 1.0278, 1.0324, 1.0461, 1.0627, 1.0826],
0:        device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2677, -0.2837, -0.2951, -0.3004, -0.3037, -0.3076, -0.3136, -0.3210,
0:         -0.3330, -0.3519, -0.3743, -0.4019, -0.4363, -0.4723, -0.5082, -0.5419,
0:         -0.5701, -0.5940, -0.2267, -0.2399, -0.2463, -0.2475, -0.2469, -0.2436,
0:         -0.2424], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6343, 0.6195, 0.6050, 0.5913, 0.5815, 0.5750, 0.5700, 0.5676, 0.5669,
0:         0.5657, 0.5624, 0.5563, 0.5413, 0.5235, 0.5038, 0.4818, 0.4497, 0.4111,
0:         0.7257, 0.7095, 0.6902, 0.6706, 0.6557, 0.6440, 0.6371],
0:        device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1801, -0.2890, -0.2501, -0.0990,  0.0755,  0.1699,  0.1933,  0.1944,
0:          0.2055,  0.2688,  0.3622,  0.4044,  0.3499,  0.2499,  0.1810,  0.1444,
0:          0.1244,  0.1377, -0.1501, -0.1923, -0.1368,  0.0255,  0.1488,  0.1344,
0:          0.1021], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.3077, 0.2846, 0.2666, 0.2490, 0.2382, 0.2403, 0.2486, 0.2556, 0.2612,
0:         0.2667, 0.2751, 0.2855, 0.2941, 0.3010, 0.3057, 0.3073, 0.3075, 0.3049,
0:         0.3019, 0.3014, 0.3013, 0.3037, 0.3090, 0.3132, 0.3181],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 29, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2502,     nan,     nan,     nan,     nan,
0:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2502,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2502,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2502,
0:         -0.2502,     nan, -0.2502,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2502,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2502,
0:             nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2502,     nan, -0.2502,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2502,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2502, -0.2502,     nan,     nan,     nan,     nan,     nan, -0.2502,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 29, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1897, 1.2065, 1.2210, 1.2290, 1.2383, 1.2418, 1.2535, 1.2567, 1.2699,
0:         1.2762, 1.2784, 1.2688, 1.2602, 1.2535, 1.2475, 1.2500, 1.2588, 1.2584,
0:         1.2460, 1.2752, 1.2965, 1.3164, 1.3241, 1.3310, 1.3377],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2589, -0.2441, -0.2369, -0.2313, -0.2256, -0.2158, -0.2035, -0.1889,
0:         -0.1749, -0.1589, -0.1491, -0.1421, -0.1370, -0.1288, -0.1159, -0.0995,
0:         -0.0826, -0.0695, -0.2881, -0.2794, -0.2723, -0.2672, -0.2558, -0.2445,
0:         -0.2316], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2944, -0.2840, -0.2736, -0.2624, -0.2494, -0.2349, -0.2230, -0.2016,
0:         -0.1859, -0.1721, -0.1582, -0.1440, -0.1282, -0.1135, -0.0980, -0.0743,
0:         -0.0506, -0.0221, -0.3183, -0.3052, -0.2871, -0.2728, -0.2588, -0.2441,
0:         -0.2259], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6031, 0.5358, 0.4717, 0.4336, 0.3925, 0.3278, 0.2274, 0.1208, 0.0462,
0:         0.0262, 0.0518, 0.1385, 0.2335, 0.2539, 0.2732, 0.3183, 0.3324, 0.3223,
0:         0.5716, 0.5222, 0.5143, 0.5061, 0.4621, 0.3874, 0.2717],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.1889, 0.1992, 0.2174, 0.2409, 0.2676, 0.2929, 0.3149, 0.3324, 0.3477,
0:         0.3599, 0.3690, 0.3773, 0.3869, 0.3970, 0.4069, 0.4141, 0.4176, 0.4170,
0:         0.4152, 0.4158, 0.4182, 0.4202, 0.4182, 0.4133, 0.4065],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1553, -0.1603, -0.1703, -0.1691, -0.1702, -0.1716, -0.1720, -0.1670,
0:         -0.1643, -0.1621, -0.1589, -0.1656, -0.1713, -0.1718, -0.1678, -0.1710,
0:         -0.1690, -0.1592, -0.1568, -0.1588, -0.1601, -0.1595, -0.1617, -0.1640,
0:         -0.1635], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 29, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.0758, 1.0901, 1.1043, 1.1173, 1.1314, 1.1528, 1.1823, 1.2129, 1.2434,
1:         1.2732, 1.2998, 1.3239, 1.3459, 1.3651, 1.3831, 1.4018, 1.4218, 1.4428,
1:         1.0248, 1.0296, 1.0278, 1.0324, 1.0461, 1.0627, 1.0826],
1:        device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2677, -0.2837, -0.2951, -0.3004, -0.3037, -0.3076, -0.3136, -0.3210,
1:         -0.3330, -0.3519, -0.3743, -0.4019, -0.4363, -0.4723, -0.5082, -0.5419,
1:         -0.5701, -0.5940, -0.2267, -0.2399, -0.2463, -0.2475, -0.2469, -0.2436,
1:         -0.2424], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.6343, 0.6195, 0.6050, 0.5913, 0.5815, 0.5750, 0.5700, 0.5676, 0.5669,
1:         0.5657, 0.5624, 0.5563, 0.5413, 0.5235, 0.5038, 0.4818, 0.4497, 0.4111,
1:         0.7257, 0.7095, 0.6902, 0.6706, 0.6557, 0.6440, 0.6371],
1:        device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1801, -0.2890, -0.2501, -0.0990,  0.0755,  0.1699,  0.1933,  0.1944,
1:          0.2055,  0.2688,  0.3622,  0.4044,  0.3499,  0.2499,  0.1810,  0.1444,
1:          0.1244,  0.1377, -0.1501, -0.1923, -0.1368,  0.0255,  0.1488,  0.1344,
1:          0.1021], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.3077, 0.2846, 0.2666, 0.2490, 0.2382, 0.2403, 0.2486, 0.2556, 0.2612,
1:         0.2667, 0.2751, 0.2855, 0.2941, 0.3010, 0.3057, 0.3073, 0.3075, 0.3049,
1:         0.3019, 0.3014, 0.3013, 0.3037, 0.3090, 0.3132, 0.3181],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 29, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan, -0.2502,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2502,     nan,     nan,
1:             nan, -0.2502, -0.2502,     nan,     nan, -0.2502,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2502,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2502,     nan,     nan,     nan,     nan,     nan,
1:         -0.2502, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2502,     nan,     nan, -0.2502,     nan, -0.2502,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2502,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2502,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2502, -0.2502,     nan,
1:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2502,     nan, -0.2502,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2502, -0.2502,     nan,     nan,
1:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2502, -0.2502, -0.2502])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 29, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.2394, 1.2448, 1.2463, 1.2439, 1.2409, 1.2406, 1.2508, 1.2605, 1.2749,
1:         1.2839, 1.2808, 1.2663, 1.2539, 1.2516, 1.2543, 1.2653, 1.2839, 1.2962,
1:         1.3345, 1.3496, 1.3555, 1.3601, 1.3557, 1.3588, 1.3649],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.2560, -0.2485, -0.2427, -0.2327, -0.2199, -0.2080, -0.1926, -0.1757,
1:         -0.1585, -0.1405, -0.1237, -0.1121, -0.1031, -0.0957, -0.0845, -0.0763,
1:         -0.0683, -0.0636, -0.2822, -0.2749, -0.2659, -0.2525, -0.2383, -0.2249,
1:         -0.2145], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.3361, -0.3147, -0.3053, -0.2894, -0.2807, -0.2711, -0.2656, -0.2480,
1:         -0.2384, -0.2201, -0.2002, -0.1773, -0.1570, -0.1386, -0.1261, -0.1077,
1:         -0.0910, -0.0623, -0.3423, -0.3261, -0.3064, -0.2972, -0.2884, -0.2786,
1:         -0.2694], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.3013,  0.2895,  0.2766,  0.2834,  0.3125,  0.2797,  0.1602,  0.0363,
1:         -0.0532, -0.0730, -0.0375,  0.0264,  0.0833,  0.1020,  0.1430,  0.2154,
1:          0.2851,  0.3359,  0.3386,  0.3240,  0.3294,  0.3429,  0.3386,  0.2661,
1:          0.1343], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([0.1736, 0.1854, 0.2066, 0.2345, 0.2662, 0.2952, 0.3186, 0.3353, 0.3462,
1:         0.3522, 0.3557, 0.3596, 0.3671, 0.3762, 0.3865, 0.3950, 0.3990, 0.3997,
1:         0.3994, 0.4015, 0.4047, 0.4082, 0.4102, 0.4119, 0.4137],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1632, -0.1653, -0.1699, -0.1659, -0.1648, -0.1659, -0.1669, -0.1606,
1:         -0.1619, -0.1667, -0.1619, -0.1664, -0.1680, -0.1704, -0.1613, -0.1685,
1:         -0.1647, -0.1616, -0.1603, -0.1622, -0.1591, -0.1610, -0.1607, -0.1613,
1:         -0.1643], device='cuda:1', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 29, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 29, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([1.0758, 1.0901, 1.1043, 1.1173, 1.1314, 1.1528, 1.1823, 1.2129, 1.2434,
2:         1.2732, 1.2998, 1.3239, 1.3459, 1.3651, 1.3831, 1.4018, 1.4218, 1.4428,
2:         1.0248, 1.0296, 1.0278, 1.0324, 1.0461, 1.0627, 1.0826],
2:        device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2677, -0.2837, -0.2951, -0.3004, -0.3037, -0.3076, -0.3136, -0.3210,
2:         -0.3330, -0.3519, -0.3743, -0.4019, -0.4363, -0.4723, -0.5082, -0.5419,
2:         -0.5701, -0.5940, -0.2267, -0.2399, -0.2463, -0.2475, -0.2469, -0.2436,
2:         -0.2424], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.6343, 0.6195, 0.6050, 0.5913, 0.5815, 0.5750, 0.5700, 0.5676, 0.5669,
2:         0.5657, 0.5624, 0.5563, 0.5413, 0.5235, 0.5038, 0.4818, 0.4497, 0.4111,
2:         0.7257, 0.7095, 0.6902, 0.6706, 0.6557, 0.6440, 0.6371],
2:        device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1801, -0.2890, -0.2501, -0.0990,  0.0755,  0.1699,  0.1933,  0.1944,
2:          0.2055,  0.2688,  0.3622,  0.4044,  0.3499,  0.2499,  0.1810,  0.1444,
2:          0.1244,  0.1377, -0.1501, -0.1923, -0.1368,  0.0255,  0.1488,  0.1344,
2:          0.1021], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.3077, 0.2846, 0.2666, 0.2490, 0.2382, 0.2403, 0.2486, 0.2556, 0.2612,
2:         0.2667, 0.2751, 0.2855, 0.2941, 0.3010, 0.3057, 0.3073, 0.3075, 0.3049,
2:         0.3019, 0.3014, 0.3013, 0.3037, 0.3090, 0.3132, 0.3181],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 29, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan, -0.2502,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2502, -0.2502,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2502,     nan,     nan,     nan,     nan, -0.2502,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2502,
2:             nan,     nan,     nan,     nan, -0.2502,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2502,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2502,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2502,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2502,
2:         -0.2502,     nan,     nan,     nan,     nan,     nan, -0.2502,     nan,
2:             nan,     nan,     nan, -0.2502,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2502,     nan,     nan,     nan,
2:         -0.2502,     nan,     nan,     nan,     nan, -0.2502,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2502,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2502, -0.2502,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2502,     nan,     nan,     nan, -0.2502,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2502,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 29, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.1771, 1.1862, 1.1960, 1.2075, 1.2208, 1.2292, 1.2419, 1.2437, 1.2516,
2:         1.2557, 1.2568, 1.2471, 1.2391, 1.2371, 1.2364, 1.2467, 1.2636, 1.2810,
2:         1.2761, 1.2937, 1.3075, 1.3234, 1.3329, 1.3400, 1.3429],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.2582, -0.2532, -0.2477, -0.2367, -0.2225, -0.2067, -0.1897, -0.1738,
2:         -0.1584, -0.1435, -0.1312, -0.1223, -0.1131, -0.1029, -0.0887, -0.0707,
2:         -0.0553, -0.0432, -0.2905, -0.2906, -0.2828, -0.2709, -0.2519, -0.2358,
2:         -0.2214], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.3110, -0.2797, -0.2566, -0.2383, -0.2260, -0.2139, -0.2020, -0.1784,
2:         -0.1570, -0.1318, -0.1097, -0.0877, -0.0738, -0.0653, -0.0550, -0.0380,
2:         -0.0161,  0.0150, -0.3479, -0.3227, -0.2945, -0.2745, -0.2599, -0.2384,
2:         -0.2194], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.4484,  0.4157,  0.4053,  0.3804,  0.3651,  0.3322,  0.2105,  0.0734,
2:         -0.0214, -0.0426,  0.0211,  0.1217,  0.2008,  0.2252,  0.2501,  0.2957,
2:          0.3580,  0.3998,  0.5065,  0.4519,  0.4477,  0.4221,  0.3804,  0.3111,
2:          0.1772], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([0.1517, 0.1636, 0.1869, 0.2173, 0.2502, 0.2786, 0.3013, 0.3206, 0.3368,
2:         0.3510, 0.3638, 0.3769, 0.3920, 0.4083, 0.4246, 0.4362, 0.4392, 0.4338,
2:         0.4241, 0.4168, 0.4122, 0.4116, 0.4120, 0.4138, 0.4156],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1615, -0.1652, -0.1662, -0.1656, -0.1670, -0.1668, -0.1677, -0.1623,
2:         -0.1615, -0.1693, -0.1659, -0.1708, -0.1699, -0.1700, -0.1635, -0.1676,
2:         -0.1664, -0.1593, -0.1685, -0.1689, -0.1665, -0.1649, -0.1620, -0.1606,
2:         -0.1642], device='cuda:2', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([1.0758, 1.0901, 1.1043, 1.1173, 1.1314, 1.1528, 1.1823, 1.2129, 1.2434,
3:         1.2732, 1.2998, 1.3239, 1.3459, 1.3651, 1.3831, 1.4018, 1.4218, 1.4428,
3:         1.0248, 1.0296, 1.0278, 1.0324, 1.0461, 1.0627, 1.0826],
3:        device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2677, -0.2837, -0.2951, -0.3004, -0.3037, -0.3076, -0.3136, -0.3210,
3:         -0.3330, -0.3519, -0.3743, -0.4019, -0.4363, -0.4723, -0.5082, -0.5419,
3:         -0.5701, -0.5940, -0.2267, -0.2399, -0.2463, -0.2475, -0.2469, -0.2436,
3:         -0.2424], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.6343, 0.6195, 0.6050, 0.5913, 0.5815, 0.5750, 0.5700, 0.5676, 0.5669,
3:         0.5657, 0.5624, 0.5563, 0.5413, 0.5235, 0.5038, 0.4818, 0.4497, 0.4111,
3:         0.7257, 0.7095, 0.6902, 0.6706, 0.6557, 0.6440, 0.6371],
3:        device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1801, -0.2890, -0.2501, -0.0990,  0.0755,  0.1699,  0.1933,  0.1944,
3:          0.2055,  0.2688,  0.3622,  0.4044,  0.3499,  0.2499,  0.1810,  0.1444,
3:          0.1244,  0.1377, -0.1501, -0.1923, -0.1368,  0.0255,  0.1488,  0.1344,
3:          0.1021], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.3077, 0.2846, 0.2666, 0.2490, 0.2382, 0.2403, 0.2486, 0.2556, 0.2612,
3:         0.2667, 0.2751, 0.2855, 0.2941, 0.3010, 0.3057, 0.3073, 0.3075, 0.3049,
3:         0.3019, 0.3014, 0.3013, 0.3037, 0.3090, 0.3132, 0.3181],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 29, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2502,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2502,     nan,     nan,     nan,
3:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2502,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2502, -0.2502,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2502,     nan,     nan,     nan, -0.2502,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2502,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2502,     nan,
3:             nan,     nan,     nan, -0.2502,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2502,
3:             nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 29, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.2060, 1.2132, 1.2240, 1.2308, 1.2371, 1.2383, 1.2459, 1.2461, 1.2550,
3:         1.2583, 1.2545, 1.2386, 1.2262, 1.2245, 1.2272, 1.2403, 1.2565, 1.2640,
3:         1.2901, 1.3067, 1.3203, 1.3383, 1.3476, 1.3533, 1.3581],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.2677, -0.2573, -0.2467, -0.2366, -0.2260, -0.2154, -0.2018, -0.1875,
3:         -0.1702, -0.1512, -0.1350, -0.1254, -0.1166, -0.1088, -0.0951, -0.0789,
3:         -0.0625, -0.0453, -0.2953, -0.2899, -0.2780, -0.2649, -0.2499, -0.2368,
3:         -0.2239], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.3300, -0.3021, -0.2797, -0.2619, -0.2445, -0.2269, -0.2130, -0.1893,
3:         -0.1729, -0.1541, -0.1372, -0.1195, -0.1076, -0.0994, -0.0939, -0.0832,
3:         -0.0766, -0.0621, -0.3497, -0.3270, -0.3010, -0.2846, -0.2714, -0.2564,
3:         -0.2395], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.4317, 0.4040, 0.3751, 0.3670, 0.3783, 0.3152, 0.1803, 0.0872, 0.0232,
3:         0.0034, 0.0666, 0.1552, 0.2188, 0.2487, 0.2657, 0.3020, 0.3460, 0.3585,
3:         0.4989, 0.4775, 0.4587, 0.4319, 0.3977, 0.2979, 0.1522],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([0.1597, 0.1732, 0.1964, 0.2266, 0.2597, 0.2913, 0.3176, 0.3384, 0.3533,
3:         0.3638, 0.3716, 0.3803, 0.3918, 0.4035, 0.4155, 0.4238, 0.4276, 0.4275,
3:         0.4259, 0.4248, 0.4243, 0.4236, 0.4202, 0.4153, 0.4077],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1457, -0.1506, -0.1606, -0.1614, -0.1655, -0.1692, -0.1693, -0.1655,
3:         -0.1637, -0.1560, -0.1555, -0.1626, -0.1664, -0.1706, -0.1688, -0.1717,
3:         -0.1709, -0.1593, -0.1561, -0.1598, -0.1622, -0.1638, -0.1674, -0.1667,
3:         -0.1689], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 29 [1/5 (20%)]	Loss: nan : nan :: 0.15776 (1.64 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 29 [2/5 (40%)]	Loss: nan : nan :: 0.14466 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 29 [3/5 (60%)]	Loss: nan : nan :: 0.13947 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 29 [4/5 (80%)]	Loss: nan : nan :: 0.14018 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch29.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch29.mod
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 29 : nan
0: validation loss for velocity_u : 0.03063216246664524
0: validation loss for velocity_v : 0.06149822473526001
0: validation loss for specific_humidity : 0.024688951671123505
0: validation loss for velocity_z : 0.4361953139305115
0: validation loss for temperature : 0.07189616560935974
0: validation loss for total_precip : nan
1: 30 : 01:36:42 :: batch_size = 96, lr = 1.0017555672365157e-05
2: 30 : 01:36:42 :: batch_size = 96, lr = 1.0017555672365157e-05
3: 30 : 01:36:42 :: batch_size = 96, lr = 1.0017555672365157e-05
0: 30 : 01:36:42 :: batch_size = 96, lr = 1.0017555672365157e-05
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 30, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1461, -0.1154, -0.0803, -0.0415, -0.0013,  0.0375,  0.0728,  0.1029,
0:          0.1281,  0.1487,  0.1661,  0.1811,  0.1947,  0.2077,  0.2210,  0.2354,
0:          0.2520,  0.2710, -0.1408, -0.1171, -0.0910, -0.0635, -0.0360, -0.0098,
0:          0.0143], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4050, -0.4409, -0.4732, -0.4976, -0.5107, -0.5107, -0.4974, -0.4721,
0:         -0.4373, -0.3966, -0.3532, -0.3109, -0.2718, -0.2372, -0.2070, -0.1798,
0:         -0.1538, -0.1273, -0.1926, -0.2168, -0.2405, -0.2612, -0.2760, -0.2831,
0:         -0.2814], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7536, -0.7542, -0.7549, -0.7557, -0.7564, -0.7570, -0.7572, -0.7581,
0:         -0.7589, -0.7598, -0.7604, -0.7609, -0.7615, -0.7617, -0.7613, -0.7611,
0:         -0.7604, -0.7594, -0.7562, -0.7564, -0.7566, -0.7568, -0.7570, -0.7574,
0:         -0.7577], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0497,  0.0649,  0.0714,  0.0714,  0.0671,  0.0584,  0.0432,  0.0193,
0:         -0.0068, -0.0220, -0.0090,  0.0432,  0.1279,  0.2257,  0.3039,  0.3430,
0:          0.3343,  0.2865, -0.1154, -0.0872, -0.0633, -0.0437, -0.0242, -0.0111,
0:         -0.0046], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.3541, -1.3855, -1.4168, -1.4475, -1.4773, -1.5066, -1.5361, -1.5654,
0:         -1.5949, -1.6241, -1.6520, -1.6780, -1.7010, -1.7198, -1.7331, -1.7388,
0:         -1.7366, -1.7265, -1.7118, -1.6957, -1.6832, -1.6775, -1.6804, -1.6902,
0:         -1.7029], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 30, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan, -0.2392,
0:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,
0:         -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
0:             nan,     nan, -0.2392,     nan,     nan,     nan,     nan, -0.2392,
0:             nan,     nan,     nan, -0.2392,     nan,     nan, -0.2392,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
0:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2392, -0.2392,     nan,
0:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan, -0.2392,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan, -0.2392,
0:         -0.2392,     nan, -0.2392,     nan,     nan,     nan, -0.2392,     nan,
0:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan, -0.2392,
0:         -0.2392,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
0:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2392,     nan,     nan, -0.2392,     nan,     nan, -0.2392,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 30, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6758, -0.6629, -0.6582, -0.6639, -0.6687, -0.6730, -0.6733, -0.6726,
0:         -0.6718, -0.6699, -0.6685, -0.6698, -0.6696, -0.6681, -0.6646, -0.6555,
0:         -0.6403, -0.6231, -0.6838, -0.6754, -0.6795, -0.6890, -0.7002, -0.7059,
0:         -0.7055], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4756, 1.4118, 1.3443, 1.2791, 1.2185, 1.1573, 1.0993, 1.0449, 0.9999,
0:         0.9691, 0.9352, 0.9002, 0.8576, 0.8063, 0.7522, 0.6975, 0.6465, 0.5930,
0:         1.5329, 1.4653, 1.3991, 1.3413, 1.2878, 1.2365, 1.1811],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7682, -0.7682, -0.7686, -0.7674, -0.7667, -0.7658, -0.7657, -0.7650,
0:         -0.7679, -0.7690, -0.7698, -0.7701, -0.7703, -0.7698, -0.7697, -0.7705,
0:         -0.7725, -0.7756, -0.7658, -0.7665, -0.7666, -0.7647, -0.7643, -0.7622,
0:         -0.7606], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0395,  0.0307,  0.1336,  0.1830,  0.2274,  0.2726,  0.3109,  0.3433,
0:          0.3680,  0.4060,  0.4431,  0.5319,  0.5804,  0.5523,  0.5029,  0.4267,
0:          0.4284,  0.4340, -0.2086, -0.1689,  0.0598,  0.1676,  0.2286,  0.2797,
0:          0.2978], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.1641, 1.1719, 1.1850, 1.1985, 1.2148, 1.2270, 1.2358, 1.2414, 1.2414,
0:         1.2380, 1.2289, 1.2131, 1.1902, 1.1607, 1.1209, 1.0753, 1.0215, 0.9650,
0:         0.9103, 0.8592, 0.8128, 0.7699, 0.7331, 0.7031, 0.6818],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1506, -0.1538, -0.1579, -0.1596, -0.1583, -0.1618, -0.1638, -0.1611,
0:         -0.1582, -0.1570, -0.1526, -0.1548, -0.1556, -0.1588, -0.1564, -0.1638,
0:         -0.1623, -0.1559, -0.1515, -0.1523, -0.1485, -0.1521, -0.1490, -0.1520,
0:         -0.1571], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 30, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1461, -0.1154, -0.0803, -0.0415, -0.0013,  0.0375,  0.0728,  0.1029,
2:          0.1281,  0.1487,  0.1661,  0.1811,  0.1947,  0.2077,  0.2210,  0.2354,
2:          0.2520,  0.2710, -0.1408, -0.1171, -0.0910, -0.0635, -0.0360, -0.0098,
2:          0.0143], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.4050, -0.4409, -0.4732, -0.4976, -0.5107, -0.5107, -0.4974, -0.4721,
2:         -0.4373, -0.3966, -0.3532, -0.3109, -0.2718, -0.2372, -0.2070, -0.1798,
2:         -0.1538, -0.1273, -0.1926, -0.2168, -0.2405, -0.2612, -0.2760, -0.2831,
2:         -0.2814], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7536, -0.7542, -0.7549, -0.7557, -0.7564, -0.7570, -0.7572, -0.7581,
2:         -0.7589, -0.7598, -0.7604, -0.7609, -0.7615, -0.7617, -0.7613, -0.7611,
2:         -0.7604, -0.7594, -0.7562, -0.7564, -0.7566, -0.7568, -0.7570, -0.7574,
2:         -0.7577], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.0497,  0.0649,  0.0714,  0.0714,  0.0671,  0.0584,  0.0432,  0.0193,
2:         -0.0068, -0.0220, -0.0090,  0.0432,  0.1279,  0.2257,  0.3039,  0.3430,
2:          0.3343,  0.2865, -0.1154, -0.0872, -0.0633, -0.0437, -0.0242, -0.0111,
2:         -0.0046], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-1.3541, -1.3855, -1.4168, -1.4475, -1.4773, -1.5066, -1.5361, -1.5654,
2:         -1.5949, -1.6241, -1.6520, -1.6780, -1.7010, -1.7198, -1.7331, -1.7388,
2:         -1.7366, -1.7265, -1.7118, -1.6957, -1.6832, -1.6775, -1.6804, -1.6902,
2:         -1.7029], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 30, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan, -0.2392,     nan,     nan,     nan, -0.2392,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2392, -0.2392,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
2:             nan,     nan, -0.2392,     nan,     nan, -0.2392,     nan,     nan,
2:         -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
2:             nan,     nan, -0.2392])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 30, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6722, -0.6660, -0.6630, -0.6698, -0.6741, -0.6804, -0.6814, -0.6812,
2:         -0.6773, -0.6724, -0.6668, -0.6630, -0.6590, -0.6531, -0.6453, -0.6316,
2:         -0.6144, -0.5942, -0.6870, -0.6817, -0.6853, -0.6922, -0.6988, -0.7035,
2:         -0.7038], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([1.5023, 1.4245, 1.3452, 1.2760, 1.2113, 1.1491, 1.0899, 1.0358, 0.9906,
2:         0.9561, 0.9226, 0.8827, 0.8355, 0.7814, 0.7312, 0.6844, 0.6435, 0.5975,
2:         1.5608, 1.4806, 1.4041, 1.3426, 1.2881, 1.2364, 1.1832],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.7595, -0.7601, -0.7630, -0.7635, -0.7650, -0.7659, -0.7666, -0.7660,
2:         -0.7685, -0.7694, -0.7686, -0.7676, -0.7669, -0.7670, -0.7683, -0.7713,
2:         -0.7757, -0.7804, -0.7597, -0.7616, -0.7634, -0.7638, -0.7645, -0.7637,
2:         -0.7630], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.1292, -0.1415, -0.0190,  0.0496,  0.0886,  0.1371,  0.1971,  0.2617,
2:          0.3738,  0.4986,  0.5334,  0.5753,  0.5934,  0.5364,  0.4928,  0.4643,
2:          0.4640,  0.4515, -0.2416, -0.2232, -0.0450,  0.0620,  0.1370,  0.2282,
2:          0.3151], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([1.2114, 1.2137, 1.2181, 1.2225, 1.2284, 1.2319, 1.2358, 1.2381, 1.2391,
2:         1.2379, 1.2317, 1.2176, 1.1962, 1.1649, 1.1251, 1.0792, 1.0310, 0.9813,
2:         0.9317, 0.8821, 0.8333, 0.7886, 0.7515, 0.7221, 0.7027],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1562, -0.1595, -0.1667, -0.1676, -0.1718, -0.1774, -0.1820, -0.1809,
2:         -0.1850, -0.1644, -0.1597, -0.1633, -0.1658, -0.1686, -0.1695, -0.1774,
2:         -0.1771, -0.1781, -0.1669, -0.1668, -0.1603, -0.1616, -0.1630, -0.1624,
2:         -0.1679], device='cuda:2', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 30, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 30, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1461, -0.1154, -0.0803, -0.0415, -0.0013,  0.0375,  0.0728,  0.1029,
1:          0.1281,  0.1487,  0.1661,  0.1811,  0.1947,  0.2077,  0.2210,  0.2354,
1:          0.2520,  0.2710, -0.1408, -0.1171, -0.0910, -0.0635, -0.0360, -0.0098,
1:          0.0143], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4050, -0.4409, -0.4732, -0.4976, -0.5107, -0.5107, -0.4974, -0.4721,
1:         -0.4373, -0.3966, -0.3532, -0.3109, -0.2718, -0.2372, -0.2070, -0.1798,
1:         -0.1538, -0.1273, -0.1926, -0.2168, -0.2405, -0.2612, -0.2760, -0.2831,
1:         -0.2814], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7536, -0.7542, -0.7549, -0.7557, -0.7564, -0.7570, -0.7572, -0.7581,
1:         -0.7589, -0.7598, -0.7604, -0.7609, -0.7615, -0.7617, -0.7613, -0.7611,
1:         -0.7604, -0.7594, -0.7562, -0.7564, -0.7566, -0.7568, -0.7570, -0.7574,
1:         -0.7577], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.0497,  0.0649,  0.0714,  0.0714,  0.0671,  0.0584,  0.0432,  0.0193,
1:         -0.0068, -0.0220, -0.0090,  0.0432,  0.1279,  0.2257,  0.3039,  0.3430,
1:          0.3343,  0.2865, -0.1154, -0.0872, -0.0633, -0.0437, -0.0242, -0.0111,
1:         -0.0046], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-1.3541, -1.3855, -1.4168, -1.4475, -1.4773, -1.5066, -1.5361, -1.5654,
1:         -1.5949, -1.6241, -1.6520, -1.6780, -1.7010, -1.7198, -1.7331, -1.7388,
1:         -1.7366, -1.7265, -1.7118, -1.6957, -1.6832, -1.6775, -1.6804, -1.6902,
1:         -1.7029], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 30, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan, -0.2392, -0.2392,     nan,     nan,
1:             nan, -0.2392,     nan,     nan, -0.2392,     nan, -0.2392,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
1:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,
1:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan, -0.2392,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2392,     nan, -0.2392,     nan, -0.2392,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
1:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2392,     nan,     nan,     nan, -0.2392,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 30, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6876, -0.6817, -0.6787, -0.6813, -0.6788, -0.6817, -0.6817, -0.6855,
1:         -0.6904, -0.6934, -0.6925, -0.6920, -0.6865, -0.6784, -0.6684, -0.6518,
1:         -0.6311, -0.6096, -0.7050, -0.7050, -0.7111, -0.7165, -0.7198, -0.7226,
1:         -0.7214], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([1.4557, 1.3960, 1.3314, 1.2673, 1.2059, 1.1494, 1.0958, 1.0457, 1.0011,
1:         0.9577, 0.9105, 0.8610, 0.8090, 0.7511, 0.6988, 0.6543, 0.6208, 0.5860,
1:         1.5223, 1.4574, 1.3917, 1.3296, 1.2749, 1.2236, 1.1744],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.7797, -0.7778, -0.7774, -0.7756, -0.7745, -0.7728, -0.7735, -0.7724,
1:         -0.7743, -0.7750, -0.7742, -0.7737, -0.7744, -0.7748, -0.7766, -0.7798,
1:         -0.7844, -0.7896, -0.7793, -0.7793, -0.7776, -0.7756, -0.7744, -0.7726,
1:         -0.7712], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.0639,  0.0846,  0.1470,  0.1581,  0.1757,  0.2021,  0.2490,  0.3178,
1:          0.4136,  0.5387,  0.6329,  0.7029,  0.7364,  0.7082,  0.6407,  0.5878,
1:          0.6148,  0.6264, -0.1301, -0.0930,  0.0574,  0.1440,  0.2124,  0.2780,
1:          0.3272], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([1.1570, 1.1757, 1.1989, 1.2178, 1.2368, 1.2496, 1.2607, 1.2693, 1.2720,
1:         1.2683, 1.2568, 1.2353, 1.2076, 1.1743, 1.1341, 1.0896, 1.0398, 0.9867,
1:         0.9343, 0.8865, 0.8425, 0.8042, 0.7701, 0.7394, 0.7135],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1620, -0.1659, -0.1720, -0.1727, -0.1726, -0.1714, -0.1748, -0.1683,
1:         -0.1656, -0.1666, -0.1633, -0.1681, -0.1696, -0.1714, -0.1653, -0.1718,
1:         -0.1700, -0.1638, -0.1667, -0.1659, -0.1640, -0.1635, -0.1649, -0.1644,
1:         -0.1651], device='cuda:1', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([-0.1461, -0.1154, -0.0803, -0.0415, -0.0013,  0.0375,  0.0728,  0.1029,
3:          0.1281,  0.1487,  0.1661,  0.1811,  0.1947,  0.2077,  0.2210,  0.2354,
3:          0.2520,  0.2710, -0.1408, -0.1171, -0.0910, -0.0635, -0.0360, -0.0098,
3:          0.0143], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4050, -0.4409, -0.4732, -0.4976, -0.5107, -0.5107, -0.4974, -0.4721,
3:         -0.4373, -0.3966, -0.3532, -0.3109, -0.2718, -0.2372, -0.2070, -0.1798,
3:         -0.1538, -0.1273, -0.1926, -0.2168, -0.2405, -0.2612, -0.2760, -0.2831,
3:         -0.2814], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7536, -0.7542, -0.7549, -0.7557, -0.7564, -0.7570, -0.7572, -0.7581,
3:         -0.7589, -0.7598, -0.7604, -0.7609, -0.7615, -0.7617, -0.7613, -0.7611,
3:         -0.7604, -0.7594, -0.7562, -0.7564, -0.7566, -0.7568, -0.7570, -0.7574,
3:         -0.7577], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.0497,  0.0649,  0.0714,  0.0714,  0.0671,  0.0584,  0.0432,  0.0193,
3:         -0.0068, -0.0220, -0.0090,  0.0432,  0.1279,  0.2257,  0.3039,  0.3430,
3:          0.3343,  0.2865, -0.1154, -0.0872, -0.0633, -0.0437, -0.0242, -0.0111,
3:         -0.0046], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-1.3541, -1.3855, -1.4168, -1.4475, -1.4773, -1.5066, -1.5361, -1.5654,
3:         -1.5949, -1.6241, -1.6520, -1.6780, -1.7010, -1.7198, -1.7331, -1.7388,
3:         -1.7366, -1.7265, -1.7118, -1.6957, -1.6832, -1.6775, -1.6804, -1.6902,
3:         -1.7029], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 30, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,
3:             nan, -0.2392, -0.2392,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2392,     nan,     nan,     nan,     nan, -0.2392,     nan, -0.2392,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2392,     nan,     nan, -0.2392,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,
3:             nan,     nan,     nan,     nan, -0.2392,     nan, -0.2392,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2392,     nan, -0.2392,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 30, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.7112, -0.6963, -0.6832, -0.6781, -0.6705, -0.6699, -0.6703, -0.6748,
3:         -0.6829, -0.6890, -0.6932, -0.6951, -0.6920, -0.6870, -0.6779, -0.6632,
3:         -0.6454, -0.6279, -0.7142, -0.7070, -0.7064, -0.7044, -0.7037, -0.7026,
3:         -0.7027], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([1.5032, 1.4396, 1.3738, 1.3110, 1.2492, 1.1871, 1.1204, 1.0577, 1.0045,
3:         0.9611, 0.9191, 0.8759, 0.8281, 0.7800, 0.7333, 0.6901, 0.6474, 0.5950,
3:         1.5554, 1.4846, 1.4170, 1.3618, 1.3101, 1.2561, 1.1965],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.7741, -0.7733, -0.7742, -0.7741, -0.7751, -0.7767, -0.7781, -0.7787,
3:         -0.7813, -0.7808, -0.7786, -0.7761, -0.7745, -0.7729, -0.7737, -0.7761,
3:         -0.7800, -0.7854, -0.7703, -0.7709, -0.7717, -0.7720, -0.7730, -0.7732,
3:         -0.7745], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.0657, -0.0477,  0.0756,  0.1737,  0.2409,  0.3010,  0.3745,  0.4570,
3:          0.5307,  0.5760,  0.5959,  0.6353,  0.6304,  0.5535,  0.4799,  0.4320,
3:          0.4337,  0.4613, -0.1454, -0.1106,  0.0741,  0.2086,  0.2837,  0.3544,
3:          0.4212], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([1.1818, 1.1901, 1.2007, 1.2119, 1.2264, 1.2379, 1.2483, 1.2549, 1.2543,
3:         1.2475, 1.2345, 1.2144, 1.1881, 1.1571, 1.1199, 1.0787, 1.0339, 0.9863,
3:         0.9368, 0.8884, 0.8399, 0.7946, 0.7537, 0.7167, 0.6849],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1607, -0.1629, -0.1685, -0.1698, -0.1715, -0.1725, -0.1772, -0.1736,
3:         -0.1720, -0.1646, -0.1624, -0.1658, -0.1702, -0.1714, -0.1690, -0.1754,
3:         -0.1773, -0.1709, -0.1629, -0.1641, -0.1616, -0.1632, -0.1633, -0.1653,
3:         -0.1701], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 30 [1/5 (20%)]	Loss: nan : nan :: 0.14584 (1.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 30 [2/5 (40%)]	Loss: nan : nan :: 0.16166 (10.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 30 [3/5 (60%)]	Loss: nan : nan :: 0.13641 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 30 [4/5 (80%)]	Loss: nan : nan :: 0.14391 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch30.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch30.mod
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 30 : nan
0: validation loss for velocity_u : 0.03421939164400101
0: validation loss for velocity_v : 0.06500198692083359
0: validation loss for specific_humidity : 0.027523178607225418
0: validation loss for velocity_z : 0.49229446053504944
0: validation loss for temperature : 0.08248211443424225
0: validation loss for total_precip : nan
1: 31 : 01:43:14 :: batch_size = 96, lr = 1e-05
2: 31 : 01:43:14 :: batch_size = 96, lr = 1e-05
3: 31 : 01:43:14 :: batch_size = 96, lr = 1e-05
0: 31 : 01:43:14 :: batch_size = 96, lr = 1e-05
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 31, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 31, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-1.6400, -1.6114, -1.5808, -1.5487, -1.5155, -1.4826, -1.4516, -1.4236,
3:         -1.3986, -1.3760, -1.3549, -1.3343, -1.3145, -1.2953, -1.2771, -1.2601,
3:         -1.2438, -1.2276, -1.5712, -1.5346, -1.4974, -1.4602, -1.4231, -1.3872,
3:         -1.3535], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.0998, 0.1598, 0.2216, 0.2836, 0.3444, 0.4021, 0.4565, 0.5074, 0.5550,
3:         0.5991, 0.6401, 0.6778, 0.7128, 0.7458, 0.7767, 0.8055, 0.8323, 0.8572,
3:         0.1940, 0.2647, 0.3341, 0.4006, 0.4629, 0.5195, 0.5704],
3:        device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.4865, -0.5082, -0.5206, -0.5281, -0.5461, -0.5506, -0.5495, -0.5466,
3:         -0.5400, -0.5352, -0.5321, -0.5248, -0.5177, -0.5107, -0.5037, -0.4978,
3:         -0.4911, -0.4846, -0.5280, -0.5435, -0.5559, -0.5532, -0.5470, -0.5383,
3:         -0.5316], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.2014,  0.2810,  0.1970,  0.0225, -0.1189, -0.1499, -0.0924, -0.0261,
3:         -0.0151, -0.0593, -0.1123, -0.1366, -0.1344, -0.1322, -0.1388, -0.1477,
3:         -0.1388, -0.1101, -0.0902,  0.1020,  0.1329,  0.0335, -0.0814, -0.1167,
3:         -0.0681], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.0703, -0.1305, -0.1937, -0.2562, -0.3146, -0.3678, -0.4150, -0.4582,
3:         -0.4991, -0.5371, -0.5715, -0.6013, -0.6260, -0.6471, -0.6646, -0.6787,
3:         -0.6888, -0.6960, -0.7002, -0.7031, -0.7049, -0.7052, -0.7037, -0.7003,
3:         -0.6960], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 31, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:          1.1268,     nan,     nan,     nan,     nan,  0.1775,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1374,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2115,     nan, -0.1837,     nan, -0.1235,     nan,     nan,     nan,
3:         -0.1004,     nan,     nan,  0.1126,     nan,     nan, -0.1004,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,  0.1358, -0.0170,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,  2.2243,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,  0.3118,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,  0.7633,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,  0.5456,     nan,     nan,
3:             nan,     nan,     nan,  0.2191,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.1143,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.1027])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 31, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-1.3116, -1.2843, -1.2680, -1.2562, -1.2511, -1.2500, -1.2414, -1.2346,
3:         -1.2233, -1.2146, -1.2156, -1.2226, -1.2367, -1.2480, -1.2562, -1.2582,
3:         -1.2660, -1.2728, -1.3493, -1.3266, -1.3076, -1.2924, -1.2811, -1.2715,
3:         -1.2555], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.6790, -0.7093, -0.7562, -0.8047, -0.8429, -0.8592, -0.8577, -0.8316,
3:         -0.7938, -0.7470, -0.6934, -0.6436, -0.5944, -0.5508, -0.5057, -0.4552,
3:         -0.4016, -0.3523, -0.6705, -0.7139, -0.7694, -0.8199, -0.8526, -0.8635,
3:         -0.8539], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.3840, -0.3748, -0.3644, -0.3524, -0.3457, -0.3403, -0.3360, -0.3329,
3:         -0.3323, -0.3344, -0.3354, -0.3419, -0.3484, -0.3542, -0.3634, -0.3718,
3:         -0.3797, -0.3856, -0.3915, -0.3758, -0.3592, -0.3436, -0.3322, -0.3244,
3:         -0.3170], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.9488,  0.7406,  0.4621,  0.1369, -0.1614, -0.2588, -0.2192, -0.1438,
3:          0.0255,  0.1769,  0.2685,  0.3985,  0.4992,  0.5925,  0.6825,  0.6673,
3:          0.6716,  0.6709,  1.4349,  1.3789,  1.1211,  0.7516,  0.3455,  0.0590,
3:         -0.0818], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.1282, -0.1139, -0.1037, -0.1010, -0.1055, -0.1180, -0.1360, -0.1548,
3:         -0.1689, -0.1768, -0.1773, -0.1721, -0.1623, -0.1534, -0.1413, -0.1298,
3:         -0.1178, -0.1073, -0.0965, -0.0859, -0.0766, -0.0704, -0.0699, -0.0728,
3:         -0.0744], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1529, -0.1525, -0.1579, -0.1585, -0.1570, -0.1610, -0.1671, -0.1687,
3:         -0.1707, -0.1552, -0.1491, -0.1518, -0.1568, -0.1541, -0.1539, -0.1645,
3:         -0.1663, -0.1648, -0.1534, -0.1522, -0.1474, -0.1465, -0.1462, -0.1478,
3:         -0.1534], device='cuda:3', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-1.6400, -1.6114, -1.5808, -1.5487, -1.5155, -1.4826, -1.4516, -1.4236,
1:         -1.3986, -1.3760, -1.3549, -1.3343, -1.3145, -1.2953, -1.2771, -1.2601,
1:         -1.2438, -1.2276, -1.5712, -1.5346, -1.4974, -1.4602, -1.4231, -1.3872,
1:         -1.3535], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.0998, 0.1598, 0.2216, 0.2836, 0.3444, 0.4021, 0.4565, 0.5074, 0.5550,
1:         0.5991, 0.6401, 0.6778, 0.7128, 0.7458, 0.7767, 0.8055, 0.8323, 0.8572,
1:         0.1940, 0.2647, 0.3341, 0.4006, 0.4629, 0.5195, 0.5704],
1:        device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4865, -0.5082, -0.5206, -0.5281, -0.5461, -0.5506, -0.5495, -0.5466,
1:         -0.5400, -0.5352, -0.5321, -0.5248, -0.5177, -0.5107, -0.5037, -0.4978,
1:         -0.4911, -0.4846, -0.5280, -0.5435, -0.5559, -0.5532, -0.5470, -0.5383,
1:         -0.5316], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.2014,  0.2810,  0.1970,  0.0225, -0.1189, -0.1499, -0.0924, -0.0261,
1:         -0.0151, -0.0593, -0.1123, -0.1366, -0.1344, -0.1322, -0.1388, -0.1477,
1:         -0.1388, -0.1101, -0.0902,  0.1020,  0.1329,  0.0335, -0.0814, -0.1167,
1:         -0.0681], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.0703, -0.1305, -0.1937, -0.2562, -0.3146, -0.3678, -0.4150, -0.4582,
1:         -0.4991, -0.5371, -0.5715, -0.6013, -0.6260, -0.6471, -0.6646, -0.6787,
1:         -0.6888, -0.6960, -0.7002, -0.7031, -0.7049, -0.7052, -0.7037, -0.7003,
1:         -0.6960], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 31, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,  0.4970,     nan,     nan,     nan,     nan, -0.1235,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  3.0949,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,  2.1456,     nan,     nan,
1:             nan,     nan,     nan,     nan,  1.3028,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.1328,     nan,     nan, -0.1930,     nan,
1:             nan,     nan,     nan,     nan, -0.1235,     nan,     nan,     nan,
1:             nan,     nan,  0.3720,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:          1.7797,     nan,     nan,     nan,     nan,     nan,     nan,  2.0900,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,  1.0434,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,  0.0987,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1837,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  0.6845,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:          0.0409,     nan, -0.0541,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  1.5343,     nan,     nan,     nan,  1.6732,     nan,
1:          2.3262,  2.3053,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,  1.3537,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.1282,     nan,     nan, -0.0957,     nan,     nan,     nan,     nan,
1:             nan, -0.1745,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.1536,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 31, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.3003, -1.2781, -1.2639, -1.2501, -1.2449, -1.2435, -1.2342, -1.2232,
1:         -1.2144, -1.2062, -1.2106, -1.2194, -1.2293, -1.2388, -1.2387, -1.2413,
1:         -1.2528, -1.2689, -1.3346, -1.3169, -1.3053, -1.2932, -1.2852, -1.2772,
1:         -1.2648], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.6831, -0.7295, -0.7886, -0.8394, -0.8773, -0.8889, -0.8844, -0.8614,
1:         -0.8310, -0.7942, -0.7589, -0.7238, -0.6854, -0.6435, -0.5885, -0.5319,
1:         -0.4742, -0.4308, -0.6859, -0.7387, -0.7966, -0.8479, -0.8790, -0.8889,
1:         -0.8805], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.3658, -0.3528, -0.3445, -0.3368, -0.3366, -0.3360, -0.3380, -0.3360,
1:         -0.3355, -0.3342, -0.3358, -0.3382, -0.3429, -0.3481, -0.3553, -0.3639,
1:         -0.3731, -0.3788, -0.3840, -0.3696, -0.3548, -0.3434, -0.3378, -0.3344,
1:         -0.3276], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.9240,  0.7645,  0.4717,  0.1774, -0.0571, -0.1357, -0.0745, -0.0151,
1:          0.1027,  0.2423,  0.3097,  0.4012,  0.4501,  0.4577,  0.5254,  0.5228,
1:          0.4768,  0.4084,  1.2969,  1.3084,  1.0608,  0.7250,  0.3763,  0.1201,
1:          0.0255], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.1011, -0.0934, -0.0889, -0.0899, -0.0955, -0.1072, -0.1229, -0.1399,
1:         -0.1532, -0.1618, -0.1631, -0.1597, -0.1509, -0.1406, -0.1267, -0.1132,
1:         -0.0988, -0.0871, -0.0770, -0.0671, -0.0583, -0.0525, -0.0534, -0.0612,
1:         -0.0749], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1309, -0.1341, -0.1474, -0.1484, -0.1552, -0.1612, -0.1671, -0.1644,
1:         -0.1646, -0.1259, -0.1271, -0.1343, -0.1432, -0.1462, -0.1521, -0.1576,
1:         -0.1594, -0.1544, -0.1225, -0.1233, -0.1251, -0.1302, -0.1355, -0.1389,
1:         -0.1455], device='cuda:1', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 31, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-1.6400, -1.6114, -1.5808, -1.5487, -1.5155, -1.4826, -1.4516, -1.4236,
2:         -1.3986, -1.3760, -1.3549, -1.3343, -1.3145, -1.2953, -1.2771, -1.2601,
2:         -1.2438, -1.2276, -1.5712, -1.5346, -1.4974, -1.4602, -1.4231, -1.3872,
2:         -1.3535], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.0998, 0.1598, 0.2216, 0.2836, 0.3444, 0.4021, 0.4565, 0.5074, 0.5550,
2:         0.5991, 0.6401, 0.6778, 0.7128, 0.7458, 0.7767, 0.8055, 0.8323, 0.8572,
2:         0.1940, 0.2647, 0.3341, 0.4006, 0.4629, 0.5195, 0.5704],
2:        device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.4865, -0.5082, -0.5206, -0.5281, -0.5461, -0.5506, -0.5495, -0.5466,
2:         -0.5400, -0.5352, -0.5321, -0.5248, -0.5177, -0.5107, -0.5037, -0.4978,
2:         -0.4911, -0.4846, -0.5280, -0.5435, -0.5559, -0.5532, -0.5470, -0.5383,
2:         -0.5316], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.2014,  0.2810,  0.1970,  0.0225, -0.1189, -0.1499, -0.0924, -0.0261,
2:         -0.0151, -0.0593, -0.1123, -0.1366, -0.1344, -0.1322, -0.1388, -0.1477,
2:         -0.1388, -0.1101, -0.0902,  0.1020,  0.1329,  0.0335, -0.0814, -0.1167,
2:         -0.0681], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.0703, -0.1305, -0.1937, -0.2562, -0.3146, -0.3678, -0.4150, -0.4582,
2:         -0.4991, -0.5371, -0.5715, -0.6013, -0.6260, -0.6471, -0.6646, -0.6787,
2:         -0.6888, -0.6960, -0.7002, -0.7031, -0.7049, -0.7052, -0.7037, -0.7003,
2:         -0.6960], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 31, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan, -0.1282,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,  3.0949,     nan,     nan,     nan,     nan,     nan,
2:          0.5109,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,  0.3951,     nan,     nan,     nan,     nan,     nan,
2:             nan,  1.3537,     nan,     nan,  0.1821,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.1189,     nan, -0.1328,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.1004,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,  0.1358,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,  1.4463,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,  0.8582,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,  1.5528,     nan,     nan,     nan,     nan,     nan,
2:             nan,  1.1685,  1.1036,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,  0.0802,     nan,     nan,     nan,
2:             nan,     nan, -0.1189,     nan,     nan,     nan,     nan,     nan,
2:         -0.2023,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.0193,     nan,     nan,
2:             nan,     nan, -0.0448,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.0587,     nan,  2.3077,     nan,
2:             nan,  1.9696,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,  1.6547,     nan,     nan,     nan,     nan,
2:          1.1245,     nan,     nan,     nan,     nan,     nan,     nan,  0.0177,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.1559,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 31, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-1.2912, -1.2756, -1.2706, -1.2661, -1.2610, -1.2581, -1.2496, -1.2427,
2:         -1.2353, -1.2299, -1.2318, -1.2369, -1.2430, -1.2513, -1.2568, -1.2629,
2:         -1.2703, -1.2730, -1.3259, -1.3153, -1.3063, -1.2974, -1.2895, -1.2798,
2:         -1.2645], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.6726, -0.6993, -0.7429, -0.7917, -0.8333, -0.8564, -0.8622, -0.8449,
2:         -0.8157, -0.7703, -0.7159, -0.6592, -0.6065, -0.5612, -0.5202, -0.4847,
2:         -0.4441, -0.4076, -0.6575, -0.6987, -0.7549, -0.8068, -0.8449, -0.8593,
2:         -0.8590], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.3490, -0.3419, -0.3367, -0.3273, -0.3226, -0.3190, -0.3194, -0.3181,
2:         -0.3177, -0.3190, -0.3191, -0.3217, -0.3271, -0.3339, -0.3435, -0.3580,
2:         -0.3723, -0.3827, -0.3615, -0.3487, -0.3342, -0.3169, -0.3048, -0.2989,
2:         -0.2922], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.9150,  0.7909,  0.5422,  0.2553,  0.0107, -0.0776, -0.0912, -0.0812,
2:          0.0480,  0.1875,  0.2561,  0.3418,  0.4181,  0.4965,  0.5428,  0.4638,
2:          0.3894,  0.3711,  1.3374,  1.3477,  1.1432,  0.8126,  0.4639,  0.2207,
2:          0.0698], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.1319, -0.1138, -0.1032, -0.0997, -0.1026, -0.1100, -0.1229, -0.1386,
2:         -0.1514, -0.1585, -0.1572, -0.1480, -0.1349, -0.1233, -0.1121, -0.1047,
2:         -0.0981, -0.0939, -0.0903, -0.0872, -0.0828, -0.0776, -0.0739, -0.0732,
2:         -0.0748], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1554, -0.1558, -0.1642, -0.1651, -0.1638, -0.1682, -0.1705, -0.1694,
2:         -0.1717, -0.1559, -0.1534, -0.1590, -0.1615, -0.1654, -0.1639, -0.1728,
2:         -0.1724, -0.1700, -0.1501, -0.1510, -0.1506, -0.1546, -0.1592, -0.1581,
2:         -0.1652], device='cuda:2', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 31, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.6400, -1.6114, -1.5808, -1.5487, -1.5155, -1.4826, -1.4516, -1.4236,
0:         -1.3986, -1.3760, -1.3549, -1.3343, -1.3145, -1.2953, -1.2771, -1.2601,
0:         -1.2438, -1.2276, -1.5712, -1.5346, -1.4974, -1.4602, -1.4231, -1.3872,
0:         -1.3535], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0998, 0.1598, 0.2216, 0.2836, 0.3444, 0.4021, 0.4565, 0.5074, 0.5550,
0:         0.5991, 0.6401, 0.6778, 0.7128, 0.7458, 0.7767, 0.8055, 0.8323, 0.8572,
0:         0.1940, 0.2647, 0.3341, 0.4006, 0.4629, 0.5195, 0.5704],
0:        device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4865, -0.5082, -0.5206, -0.5281, -0.5461, -0.5506, -0.5495, -0.5466,
0:         -0.5400, -0.5352, -0.5321, -0.5248, -0.5177, -0.5107, -0.5037, -0.4978,
0:         -0.4911, -0.4846, -0.5280, -0.5435, -0.5559, -0.5532, -0.5470, -0.5383,
0:         -0.5316], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2014,  0.2810,  0.1970,  0.0225, -0.1189, -0.1499, -0.0924, -0.0261,
0:         -0.0151, -0.0593, -0.1123, -0.1366, -0.1344, -0.1322, -0.1388, -0.1477,
0:         -0.1388, -0.1101, -0.0902,  0.1020,  0.1329,  0.0335, -0.0814, -0.1167,
0:         -0.0681], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0703, -0.1305, -0.1937, -0.2562, -0.3146, -0.3678, -0.4150, -0.4582,
0:         -0.4991, -0.5371, -0.5715, -0.6013, -0.6260, -0.6471, -0.6646, -0.6787,
0:         -0.6888, -0.6960, -0.7002, -0.7031, -0.7049, -0.7052, -0.7037, -0.7003,
0:         -0.6960], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 31, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1282,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  1.0527,     nan,
0:             nan,     nan, -0.0448,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.1821,     nan,     nan,  1.1546,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.1173,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1050,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1004,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.4044,     nan,     nan,     nan,  2.2752,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.0110,
0:             nan,     nan,  1.1036,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.1265,  0.0987,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2023,     nan,     nan, -0.1698,     nan,     nan,
0:             nan, -0.0309,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.7633,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.5410,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  1.1615,     nan,     nan,     nan,
0:          1.1245,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.2516,     nan,     nan,     nan,     nan, -0.0402,
0:             nan,     nan, -0.1120,     nan,     nan, -0.1143,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 31, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2947, -1.2686, -1.2510, -1.2355, -1.2296, -1.2338, -1.2322, -1.2331,
0:         -1.2312, -1.2272, -1.2305, -1.2387, -1.2492, -1.2620, -1.2677, -1.2705,
0:         -1.2752, -1.2773, -1.3147, -1.2979, -1.2834, -1.2702, -1.2659, -1.2639,
0:         -1.2621], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6987, -0.7274, -0.7664, -0.8084, -0.8451, -0.8591, -0.8621, -0.8440,
0:         -0.8135, -0.7740, -0.7327, -0.6915, -0.6515, -0.6117, -0.5629, -0.5056,
0:         -0.4378, -0.3700, -0.6875, -0.7277, -0.7745, -0.8179, -0.8477, -0.8590,
0:         -0.8498], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3862, -0.3738, -0.3622, -0.3511, -0.3495, -0.3492, -0.3530, -0.3559,
0:         -0.3605, -0.3624, -0.3622, -0.3598, -0.3596, -0.3588, -0.3633, -0.3696,
0:         -0.3812, -0.3894, -0.3973, -0.3785, -0.3599, -0.3452, -0.3382, -0.3362,
0:         -0.3345], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.9490,  0.8062,  0.5530,  0.2802,  0.0223, -0.0724, -0.0628, -0.0566,
0:          0.0675,  0.2426,  0.3362,  0.4301,  0.5050,  0.5457,  0.5717,  0.5375,
0:          0.5030,  0.4659,  1.4466,  1.4629,  1.2596,  0.9405,  0.5272,  0.2194,
0:          0.0711], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1241, -0.1063, -0.0935, -0.0888, -0.0918, -0.1049, -0.1247, -0.1450,
0:         -0.1594, -0.1639, -0.1577, -0.1456, -0.1313, -0.1210, -0.1124, -0.1057,
0:         -0.0975, -0.0872, -0.0753, -0.0629, -0.0550, -0.0534, -0.0610, -0.0756,
0:         -0.0903], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1598, -0.1656, -0.1695, -0.1695, -0.1705, -0.1700, -0.1745, -0.1703,
0:         -0.1709, -0.1611, -0.1579, -0.1618, -0.1649, -0.1652, -0.1624, -0.1679,
0:         -0.1669, -0.1637, -0.1560, -0.1567, -0.1544, -0.1544, -0.1556, -0.1546,
0:         -0.1582], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 31 [1/5 (20%)]	Loss: nan : nan :: 0.14903 (1.69 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 31 [2/5 (40%)]	Loss: nan : nan :: 0.14183 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 31 [3/5 (60%)]	Loss: nan : nan :: 0.15388 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 31 [4/5 (80%)]	Loss: nan : nan :: 0.14037 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch31.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch31.mod
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 31 : nan
0: validation loss for velocity_u : 0.029825791716575623
0: validation loss for velocity_v : 0.05361170694231987
0: validation loss for specific_humidity : 0.02357320860028267
0: validation loss for velocity_z : 0.4185003936290741
0: validation loss for temperature : 0.0661621019244194
0: validation loss for total_precip : nan
0: 32 : 01:50:15 :: batch_size = 96, lr = 1e-05
1: 32 : 01:50:15 :: batch_size = 96, lr = 1e-05
2: 32 : 01:50:15 :: batch_size = 96, lr = 1e-05
3: 32 : 01:50:15 :: batch_size = 96, lr = 1e-05
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 32, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1168, -0.1170, -0.1048, -0.0867, -0.0683, -0.0545, -0.0514, -0.0601,
3:         -0.0762, -0.0966, -0.1198, -0.1410, -0.1579, -0.1719, -0.1821, -0.1862,
3:         -0.1850, -0.1817, -0.1278, -0.1193, -0.1009, -0.0798, -0.0627, -0.0545,
3:         -0.0583], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.2847, -0.2096, -0.1116, -0.0063,  0.0945,  0.1820,  0.2495,  0.2947,
3:          0.3175,  0.3185,  0.3014,  0.2710,  0.2302,  0.1829,  0.1367,  0.0986,
3:          0.0690,  0.0475, -0.2660, -0.1621, -0.0432,  0.0720,  0.1709,  0.2476,
3:          0.2979], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.5135, -0.5447, -0.5861, -0.6374, -0.6695, -0.6830, -0.6831, -0.6798,
3:         -0.6724, -0.6644, -0.6577, -0.6541, -0.6552, -0.6576, -0.6625, -0.6681,
3:         -0.6740, -0.6808, -0.5420, -0.5893, -0.6327, -0.6734, -0.6860, -0.6909,
3:         -0.6848], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([0.4169, 0.6124, 0.7029, 0.6996, 0.6466, 0.6035, 0.5638, 0.4721, 0.3617,
3:         0.2988, 0.2778, 0.2690, 0.2756, 0.3010, 0.3728, 0.5240, 0.6875, 0.7504,
3:         0.6356, 0.6963, 0.6300, 0.5837, 0.5715, 0.4942, 0.3562],
3:        device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.4029, 0.3526, 0.3293, 0.3241, 0.3289, 0.3356, 0.3360, 0.3257, 0.3052,
3:         0.2747, 0.2357, 0.1941, 0.1562, 0.1257, 0.1013, 0.0797, 0.0631, 0.0591,
3:         0.0723, 0.0978, 0.1298, 0.1652, 0.2016, 0.2370, 0.2700],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 32, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan, -0.2560,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2560,     nan,     nan,     nan, -0.2560, -0.2560,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2560,     nan, -0.2560,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2560,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2560,     nan,     nan,
3:         -0.2560,     nan,     nan,     nan,     nan,     nan, -0.2560,     nan,
3:             nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2560,
3:         -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2560,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2560,     nan,     nan,
3:             nan, -0.2560,     nan,     nan,     nan,     nan, -0.2560,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2560,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2560, -0.2560,     nan, -0.2560,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2560,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2560,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2560,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2560,     nan,     nan,     nan,     nan,
3:         -0.2560,     nan, -0.2560])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 32, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.3566, 0.3432, 0.3289, 0.3104, 0.2889, 0.2657, 0.2457, 0.2289, 0.2184,
3:         0.2123, 0.2093, 0.2050, 0.1974, 0.1859, 0.1714, 0.1561, 0.1403, 0.1264,
3:         0.3782, 0.3690, 0.3522, 0.3309, 0.3044, 0.2772, 0.2547],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.4496, 0.4544, 0.4554, 0.4531, 0.4526, 0.4627, 0.4745, 0.4867, 0.5027,
3:         0.5149, 0.5261, 0.5373, 0.5477, 0.5568, 0.5651, 0.5713, 0.5742, 0.5754,
3:         0.4259, 0.4343, 0.4422, 0.4446, 0.4522, 0.4600, 0.4721],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.1049, 0.1087, 0.1077, 0.1114, 0.1078, 0.1081, 0.1031, 0.1049, 0.1045,
3:         0.1009, 0.0920, 0.0853, 0.0799, 0.0806, 0.0790, 0.0824, 0.0737, 0.0636,
3:         0.1089, 0.1176, 0.1263, 0.1374, 0.1481, 0.1532, 0.1649],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0830,  0.0936,  0.1491,  0.2267,  0.2995,  0.3394,  0.2911,  0.2002,
3:          0.1627,  0.1209,  0.0560,  0.0467,  0.0289, -0.0381, -0.0492, -0.0192,
3:          0.0009,  0.0008,  0.0936,  0.0927,  0.1412,  0.1996,  0.2331,  0.2563,
3:          0.2383], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.5736, -0.6237, -0.6660, -0.6961, -0.7119, -0.7143, -0.7055, -0.6860,
3:         -0.6530, -0.6102, -0.5635, -0.5196, -0.4798, -0.4456, -0.4096, -0.3692,
3:         -0.3204, -0.2652, -0.2026, -0.1336, -0.0612,  0.0092,  0.0700,  0.1153,
3:          0.1382], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1511, -0.1563, -0.1620, -0.1625, -0.1629, -0.1659, -0.1656, -0.1606,
3:         -0.1643, -0.1584, -0.1548, -0.1618, -0.1640, -0.1677, -0.1645, -0.1662,
3:         -0.1644, -0.1568, -0.1546, -0.1583, -0.1569, -0.1575, -0.1606, -0.1578,
3:         -0.1591], device='cuda:3', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 32, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1168, -0.1170, -0.1048, -0.0867, -0.0683, -0.0545, -0.0514, -0.0601,
1:         -0.0762, -0.0966, -0.1198, -0.1410, -0.1579, -0.1719, -0.1821, -0.1862,
1:         -0.1850, -0.1817, -0.1278, -0.1193, -0.1009, -0.0798, -0.0627, -0.0545,
1:         -0.0583], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.2847, -0.2096, -0.1116, -0.0063,  0.0945,  0.1820,  0.2495,  0.2947,
1:          0.3175,  0.3185,  0.3014,  0.2710,  0.2302,  0.1829,  0.1367,  0.0986,
1:          0.0690,  0.0475, -0.2660, -0.1621, -0.0432,  0.0720,  0.1709,  0.2476,
1:          0.2979], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.5135, -0.5447, -0.5861, -0.6374, -0.6695, -0.6830, -0.6831, -0.6798,
1:         -0.6724, -0.6644, -0.6577, -0.6541, -0.6552, -0.6576, -0.6625, -0.6681,
1:         -0.6740, -0.6808, -0.5420, -0.5893, -0.6327, -0.6734, -0.6860, -0.6909,
1:         -0.6848], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.4169, 0.6124, 0.7029, 0.6996, 0.6466, 0.6035, 0.5638, 0.4721, 0.3617,
1:         0.2988, 0.2778, 0.2690, 0.2756, 0.3010, 0.3728, 0.5240, 0.6875, 0.7504,
1:         0.6356, 0.6963, 0.6300, 0.5837, 0.5715, 0.4942, 0.3562],
1:        device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.4029, 0.3526, 0.3293, 0.3241, 0.3289, 0.3356, 0.3360, 0.3257, 0.3052,
1:         0.2747, 0.2357, 0.1941, 0.1562, 0.1257, 0.1013, 0.0797, 0.0631, 0.0591,
1:         0.0723, 0.0978, 0.1298, 0.1652, 0.2016, 0.2370, 0.2700],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 32, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2560,     nan,     nan,     nan,     nan, -0.2560,     nan,     nan,
1:         -0.2560,     nan,     nan,     nan,     nan,     nan,     nan, -0.2560,
1:             nan,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
1:             nan, -0.2560,     nan, -0.2560,     nan,     nan,     nan, -0.2560,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2560,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2560,     nan,
1:         -0.2560,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2560,     nan, -0.2560,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2560,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2560,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2560,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2560,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2560,     nan, -0.2560,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2560,     nan,
1:             nan,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2560, -0.2560,     nan, -0.2560,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2560])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 32, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.3653, 0.3502, 0.3360, 0.3174, 0.2974, 0.2783, 0.2609, 0.2474, 0.2390,
1:         0.2322, 0.2267, 0.2166, 0.2045, 0.1868, 0.1665, 0.1454, 0.1234, 0.1064,
1:         0.3902, 0.3766, 0.3570, 0.3329, 0.3055, 0.2798, 0.2577],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.4357, 0.4439, 0.4472, 0.4463, 0.4489, 0.4616, 0.4792, 0.4981, 0.5198,
1:         0.5359, 0.5467, 0.5525, 0.5565, 0.5595, 0.5646, 0.5757, 0.5863, 0.5968,
1:         0.4146, 0.4282, 0.4372, 0.4416, 0.4499, 0.4601, 0.4774],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.0883, 0.0808, 0.0755, 0.0860, 0.0966, 0.1151, 0.1286, 0.1416, 0.1451,
1:         0.1363, 0.1158, 0.0988, 0.0875, 0.0880, 0.0996, 0.1141, 0.1181, 0.1123,
1:         0.1037, 0.0979, 0.0989, 0.1148, 0.1415, 0.1698, 0.2015],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.1048,  0.1041,  0.1295,  0.1688,  0.2161,  0.2481,  0.2175,  0.1648,
1:          0.1508,  0.0988,  0.0347,  0.0479,  0.0293, -0.0570, -0.0903, -0.0597,
1:         -0.0191, -0.0023,  0.0907,  0.0834,  0.1025,  0.1368,  0.1695,  0.2065,
1:          0.2034], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.5991, -0.6418, -0.6840, -0.7195, -0.7420, -0.7476, -0.7339, -0.7045,
1:         -0.6619, -0.6163, -0.5747, -0.5400, -0.5099, -0.4795, -0.4438, -0.3996,
1:         -0.3448, -0.2813, -0.2108, -0.1377, -0.0676, -0.0042,  0.0485,  0.0867,
1:          0.1074], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1624, -0.1646, -0.1694, -0.1723, -0.1727, -0.1732, -0.1755, -0.1729,
1:         -0.1722, -0.1689, -0.1651, -0.1693, -0.1711, -0.1741, -0.1715, -0.1776,
1:         -0.1768, -0.1724, -0.1668, -0.1683, -0.1647, -0.1674, -0.1652, -0.1707,
1:         -0.1740], device='cuda:1', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 32, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1168, -0.1170, -0.1048, -0.0867, -0.0683, -0.0545, -0.0514, -0.0601,
0:         -0.0762, -0.0966, -0.1198, -0.1410, -0.1579, -0.1719, -0.1821, -0.1862,
0:         -0.1850, -0.1817, -0.1278, -0.1193, -0.1009, -0.0798, -0.0627, -0.0545,
0:         -0.0583], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2847, -0.2096, -0.1116, -0.0063,  0.0945,  0.1820,  0.2495,  0.2947,
0:          0.3175,  0.3185,  0.3014,  0.2710,  0.2302,  0.1829,  0.1367,  0.0986,
0:          0.0690,  0.0475, -0.2660, -0.1621, -0.0432,  0.0720,  0.1709,  0.2476,
0:          0.2979], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5135, -0.5447, -0.5861, -0.6374, -0.6695, -0.6830, -0.6831, -0.6798,
0:         -0.6724, -0.6644, -0.6577, -0.6541, -0.6552, -0.6576, -0.6625, -0.6681,
0:         -0.6740, -0.6808, -0.5420, -0.5893, -0.6327, -0.6734, -0.6860, -0.6909,
0:         -0.6848], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4169, 0.6124, 0.7029, 0.6996, 0.6466, 0.6035, 0.5638, 0.4721, 0.3617,
0:         0.2988, 0.2778, 0.2690, 0.2756, 0.3010, 0.3728, 0.5240, 0.6875, 0.7504,
0:         0.6356, 0.6963, 0.6300, 0.5837, 0.5715, 0.4942, 0.3562],
0:        device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.4029, 0.3526, 0.3293, 0.3241, 0.3289, 0.3356, 0.3360, 0.3257, 0.3052,
0:         0.2747, 0.2357, 0.1941, 0.1562, 0.1257, 0.1013, 0.0797, 0.0631, 0.0591,
0:         0.0723, 0.0978, 0.1298, 0.1652, 0.2016, 0.2370, 0.2700],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 32, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2560,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2560,     nan,
0:             nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2560,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2560,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2560,     nan,
0:             nan,     nan, -0.2560,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2560,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2560,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2560,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 32, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3711, 0.3611, 0.3487, 0.3291, 0.3052, 0.2810, 0.2617, 0.2466, 0.2388,
0:         0.2350, 0.2337, 0.2329, 0.2276, 0.2182, 0.2000, 0.1781, 0.1536, 0.1305,
0:         0.3898, 0.3814, 0.3650, 0.3407, 0.3122, 0.2827, 0.2605],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4040, 0.4132, 0.4217, 0.4281, 0.4391, 0.4598, 0.4795, 0.4961, 0.5146,
0:         0.5272, 0.5356, 0.5446, 0.5510, 0.5588, 0.5661, 0.5741, 0.5818, 0.5874,
0:         0.3914, 0.4027, 0.4141, 0.4253, 0.4424, 0.4573, 0.4771],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0485, 0.0344, 0.0293, 0.0358, 0.0496, 0.0688, 0.0812, 0.0907, 0.0909,
0:         0.0826, 0.0674, 0.0547, 0.0471, 0.0437, 0.0455, 0.0492, 0.0417, 0.0298,
0:         0.0710, 0.0583, 0.0574, 0.0719, 0.0972, 0.1264, 0.1520],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1242,  0.1264,  0.1443,  0.2001,  0.2582,  0.2515,  0.2112,  0.1635,
0:          0.1125,  0.0574,  0.0133, -0.0032, -0.0182, -0.0434, -0.0354,  0.0174,
0:          0.0641,  0.0810,  0.1432,  0.1293,  0.1388,  0.1821,  0.2324,  0.2429,
0:          0.2186], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.6174, -0.6578, -0.6977, -0.7347, -0.7634, -0.7789, -0.7743, -0.7499,
0:         -0.7061, -0.6521, -0.5969, -0.5475, -0.5056, -0.4717, -0.4377, -0.4002,
0:         -0.3504, -0.2903, -0.2206, -0.1469, -0.0755, -0.0092,  0.0485,  0.0942,
0:          0.1239], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1438, -0.1503, -0.1563, -0.1572, -0.1590, -0.1626, -0.1656, -0.1622,
0:         -0.1635, -0.1522, -0.1530, -0.1575, -0.1607, -0.1646, -0.1617, -0.1680,
0:         -0.1677, -0.1622, -0.1515, -0.1567, -0.1554, -0.1567, -0.1595, -0.1603,
0:         -0.1641], device='cuda:0', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 32, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1168, -0.1170, -0.1048, -0.0867, -0.0683, -0.0545, -0.0514, -0.0601,
2:         -0.0762, -0.0966, -0.1198, -0.1410, -0.1579, -0.1719, -0.1821, -0.1862,
2:         -0.1850, -0.1817, -0.1278, -0.1193, -0.1009, -0.0798, -0.0627, -0.0545,
2:         -0.0583], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.2847, -0.2096, -0.1116, -0.0063,  0.0945,  0.1820,  0.2495,  0.2947,
2:          0.3175,  0.3185,  0.3014,  0.2710,  0.2302,  0.1829,  0.1367,  0.0986,
2:          0.0690,  0.0475, -0.2660, -0.1621, -0.0432,  0.0720,  0.1709,  0.2476,
2:          0.2979], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.5135, -0.5447, -0.5861, -0.6374, -0.6695, -0.6830, -0.6831, -0.6798,
2:         -0.6724, -0.6644, -0.6577, -0.6541, -0.6552, -0.6576, -0.6625, -0.6681,
2:         -0.6740, -0.6808, -0.5420, -0.5893, -0.6327, -0.6734, -0.6860, -0.6909,
2:         -0.6848], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([0.4169, 0.6124, 0.7029, 0.6996, 0.6466, 0.6035, 0.5638, 0.4721, 0.3617,
2:         0.2988, 0.2778, 0.2690, 0.2756, 0.3010, 0.3728, 0.5240, 0.6875, 0.7504,
2:         0.6356, 0.6963, 0.6300, 0.5837, 0.5715, 0.4942, 0.3562],
2:        device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.4029, 0.3526, 0.3293, 0.3241, 0.3289, 0.3356, 0.3360, 0.3257, 0.3052,
2:         0.2747, 0.2357, 0.1941, 0.1562, 0.1257, 0.1013, 0.0797, 0.0631, 0.0591,
2:         0.0723, 0.0978, 0.1298, 0.1652, 0.2016, 0.2370, 0.2700],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 32, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan, -0.2560,     nan,     nan,     nan,     nan,     nan,
2:         -0.2560, -0.2560,     nan,     nan,     nan,     nan, -0.2560, -0.2560,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2560,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2560, -0.2560,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2560,     nan,     nan,     nan,     nan, -0.2560,
2:             nan,     nan,     nan,     nan, -0.2560, -0.2560, -0.2560,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2560,     nan,     nan,     nan,     nan,     nan,     nan, -0.2560,
2:             nan, -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2560,     nan,     nan,     nan,
2:         -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2560,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2560, -0.2560,     nan,
2:         -0.2560,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 32, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.3778, 0.3644, 0.3489, 0.3272, 0.3009, 0.2742, 0.2508, 0.2325, 0.2199,
2:         0.2132, 0.2090, 0.2070, 0.2033, 0.1975, 0.1861, 0.1693, 0.1479, 0.1259,
2:         0.3941, 0.3833, 0.3650, 0.3411, 0.3111, 0.2817, 0.2566],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.4177, 0.4202, 0.4235, 0.4313, 0.4434, 0.4665, 0.4878, 0.5064, 0.5224,
2:         0.5314, 0.5377, 0.5412, 0.5417, 0.5468, 0.5547, 0.5655, 0.5741, 0.5783,
2:         0.4050, 0.4099, 0.4148, 0.4273, 0.4461, 0.4656, 0.4858],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.0282, 0.0374, 0.0511, 0.0749, 0.0978, 0.1205, 0.1429, 0.1573, 0.1613,
2:         0.1517, 0.1308, 0.1103, 0.0978, 0.0928, 0.0931, 0.0999, 0.0956, 0.0850,
2:         0.0598, 0.0747, 0.0920, 0.1149, 0.1416, 0.1738, 0.2029],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.1344,  0.1550,  0.1769,  0.1976,  0.2199,  0.2448,  0.2297,  0.1586,
2:          0.1319,  0.1146,  0.0481,  0.0401,  0.0464, -0.0069, -0.0220,  0.0011,
2:          0.0204,  0.0439,  0.1172,  0.1253,  0.1604,  0.1928,  0.2151,  0.2363,
2:          0.2195], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.5909, -0.6362, -0.6775, -0.7111, -0.7340, -0.7422, -0.7327, -0.7073,
2:         -0.6674, -0.6228, -0.5783, -0.5379, -0.4992, -0.4623, -0.4241, -0.3840,
2:         -0.3367, -0.2820, -0.2175, -0.1461, -0.0740, -0.0078,  0.0470,  0.0869,
2:          0.1078], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1507, -0.1545, -0.1582, -0.1585, -0.1580, -0.1598, -0.1625, -0.1620,
2:         -0.1619, -0.1580, -0.1551, -0.1605, -0.1580, -0.1624, -0.1583, -0.1655,
2:         -0.1658, -0.1620, -0.1579, -0.1601, -0.1568, -0.1557, -0.1559, -0.1566,
2:         -0.1606], device='cuda:2', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 32 [1/5 (20%)]	Loss: nan : nan :: 0.14308 (1.77 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 32 [2/5 (40%)]	Loss: nan : nan :: 0.13856 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 32 [3/5 (60%)]	Loss: nan : nan :: 0.15489 (10.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 32 [4/5 (80%)]	Loss: nan : nan :: 0.14598 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch32.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch32.mod
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 32 : nan
0: validation loss for velocity_u : 0.034369438886642456
0: validation loss for velocity_v : 0.060626257210969925
0: validation loss for specific_humidity : 0.024553583934903145
0: validation loss for velocity_z : 0.45788344740867615
0: validation loss for temperature : 0.07179044187068939
0: validation loss for total_precip : nan
0: 33 : 01:57:01 :: batch_size = 96, lr = 1e-05
3: 33 : 01:57:01 :: batch_size = 96, lr = 1e-05
2: 33 : 01:57:01 :: batch_size = 96, lr = 1e-05
1: 33 : 01:57:01 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 33, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.3547, -1.4451, -1.5105, -1.5484, -1.5595, -1.5475, -1.5224, -1.5010,
0:         -1.4992, -1.5251, -1.5748, -1.6385, -1.7052, -1.7674, -1.8201, -1.8613,
0:         -1.8913, -1.9126, -1.4237, -1.4913, -1.5366, -1.5619, -1.5702, -1.5652,
0:         -1.5534], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1774, -0.2010, -0.2457, -0.2960, -0.3327, -0.3400, -0.3127, -0.2569,
0:         -0.1870, -0.1176, -0.0575, -0.0095,  0.0264,  0.0519,  0.0684,  0.0771,
0:          0.0788,  0.0730, -0.2910, -0.3064, -0.3329, -0.3609, -0.3786, -0.3767,
0:         -0.3515], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3054, -0.3357, -0.3523, -0.3668, -0.3731, -0.3843, -0.3924, -0.3982,
0:         -0.4003, -0.4021, -0.3973, -0.3949, -0.3827, -0.3655, -0.3416, -0.3115,
0:         -0.2817, -0.2523, -0.3398, -0.3639, -0.3758, -0.3889, -0.3905, -0.3900,
0:         -0.3877], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4566,  0.7480,  0.9497,  0.9755,  0.7502,  0.2683, -0.3482, -0.8784,
0:         -1.1261, -1.0398, -0.7282, -0.3662, -0.0770,  0.1102,  0.2201,  0.2806,
0:          0.2985,  0.2806,  0.4622,  0.5933,  0.6572,  0.6426,  0.5126,  0.2358,
0:         -0.1622], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.7487, 0.7172, 0.6704, 0.6179, 0.5712, 0.5408, 0.5351, 0.5551, 0.5930,
0:         0.6354, 0.6694, 0.6893, 0.6954, 0.6908, 0.6798, 0.6655, 0.6509, 0.6379,
0:         0.6264, 0.6157, 0.6045, 0.5930, 0.5818, 0.5709, 0.5606],
0:        device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 33, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2420,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2226, -0.1816,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.4267,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3777,
0:             nan,     nan,     nan,     nan,     nan, -0.2488,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2465,     nan,     nan,     nan,
0:             nan, -0.1668,     nan,     nan, -0.2488,     nan,     nan,     nan,
0:         -0.1645,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0451,  0.1430,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0656,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.2433,     nan,     nan,  0.2365,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1463,     nan,  0.0474,  0.1886,     nan,     nan,     nan,
0:         -0.1076,     nan,     nan,     nan,     nan,     nan,     nan, -0.0779,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1886,     nan,  0.1522,  0.1795,     nan,     nan,     nan,     nan,
0:             nan,  0.1317,     nan,     nan,  0.1522,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1636,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 33, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-2.6496, -2.6796, -2.7047, -2.7226, -2.7353, -2.7466, -2.7520, -2.7558,
0:         -2.7526, -2.7383, -2.7132, -2.6877, -2.6556, -2.6240, -2.5894, -2.5587,
0:         -2.5264, -2.4919, -2.6481, -2.6749, -2.6979, -2.7044, -2.7086, -2.7100,
0:         -2.7082], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5943, -0.5717, -0.5439, -0.5185, -0.5017, -0.4925, -0.4971, -0.5019,
0:         -0.5020, -0.4972, -0.4825, -0.4651, -0.4473, -0.4294, -0.4097, -0.3910,
0:         -0.3722, -0.3563, -0.6198, -0.6005, -0.5746, -0.5485, -0.5304, -0.5222,
0:         -0.5204], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0090,  0.0204,  0.0533,  0.0937,  0.1259,  0.1631,  0.1962,  0.2423,
0:          0.2820,  0.3253,  0.3604,  0.3923,  0.3995,  0.3978,  0.3869,  0.3669,
0:          0.3416,  0.3075,  0.1120,  0.1346,  0.1667,  0.1960,  0.2211,  0.2525,
0:          0.2857], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9741, -1.2270, -1.8306, -2.5738, -2.8845, -2.4063, -1.1279,  0.4677,
0:          1.9156,  2.8159,  3.0549,  2.8717,  2.3330,  1.6970,  1.1752,  0.7295,
0:          0.4051,  0.1344, -1.4961, -1.4077, -1.3784, -1.4289, -1.2957, -0.8430,
0:         -0.0153], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.6260, -0.6034, -0.5717, -0.5345, -0.4989, -0.4703, -0.4501, -0.4397,
0:         -0.4381, -0.4448, -0.4582, -0.4734, -0.4866, -0.4947, -0.4986, -0.4964,
0:         -0.4884, -0.4750, -0.4582, -0.4412, -0.4254, -0.4104, -0.3955, -0.3813,
0:         -0.3672], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1638, -0.1671, -0.1751, -0.1772, -0.1775, -0.1795, -0.1792, -0.1697,
0:         -0.1688, -0.1692, -0.1674, -0.1736, -0.1786, -0.1810, -0.1785, -0.1824,
0:         -0.1755, -0.1667, -0.1691, -0.1705, -0.1721, -0.1754, -0.1730, -0.1738,
0:         -0.1767], device='cuda:0', grad_fn=<SliceBackward0>)
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: [DEBUG] INPUT BATCH
2: Epoch 33, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-1.3547, -1.4451, -1.5105, -1.5484, -1.5595, -1.5475, -1.5224, -1.5010,
2:         -1.4992, -1.5251, -1.5748, -1.6385, -1.7052, -1.7674, -1.8201, -1.8613,
2:         -1.8913, -1.9126, -1.4237, -1.4913, -1.5366, -1.5619, -1.5702, -1.5652,
2:         -1.5534], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1774, -0.2010, -0.2457, -0.2960, -0.3327, -0.3400, -0.3127, -0.2569,
2:         -0.1870, -0.1176, -0.0575, -0.0095,  0.0264,  0.0519,  0.0684,  0.0771,
2:          0.0788,  0.0730, -0.2910, -0.3064, -0.3329, -0.3609, -0.3786, -0.3767,
2:         -0.3515], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.3054, -0.3357, -0.3523, -0.3668, -0.3731, -0.3843, -0.3924, -0.3982,
2:         -0.4003, -0.4021, -0.3973, -0.3949, -0.3827, -0.3655, -0.3416, -0.3115,
2:         -0.2817, -0.2523, -0.3398, -0.3639, -0.3758, -0.3889, -0.3905, -0.3900,
2:         -0.3877], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.4566,  0.7480,  0.9497,  0.9755,  0.7502,  0.2683, -0.3482, -0.8784,
2:         -1.1261, -1.0398, -0.7282, -0.3662, -0.0770,  0.1102,  0.2201,  0.2806,
2:          0.2985,  0.2806,  0.4622,  0.5933,  0.6572,  0.6426,  0.5126,  0.2358,
2:         -0.1622], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([0.7487, 0.7172, 0.6704, 0.6179, 0.5712, 0.5408, 0.5351, 0.5551, 0.5930,
2:         0.6354, 0.6694, 0.6893, 0.6954, 0.6908, 0.6798, 0.6655, 0.6509, 0.6379,
2:         0.6264, 0.6157, 0.6045, 0.5930, 0.5818, 0.5709, 0.5606],
2:        device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 33, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan, -0.2488,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.2443,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2488,     nan,     nan,     nan,
2:             nan, -0.1805,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan, -0.1919,     nan,
2:             nan,  0.0348,     nan,     nan,     nan,     nan,  0.4985,     nan,
2:             nan,     nan,     nan,     nan,  0.4267,     nan,  0.5235,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3777,
2:             nan, -0.2443,     nan,     nan,     nan, -0.2488,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan, -0.1713,     nan,     nan,     nan,     nan,     nan,
2:         -0.2055,     nan,     nan,  0.0155,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,  0.1430,     nan,     nan, -0.1189,     nan,
2:             nan,     nan,     nan,     nan,  0.3185,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,  0.4574,     nan,     nan,
2:             nan,     nan,  0.2501,     nan,  0.2456,     nan,  0.2365,  0.2205,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,  0.1863,     nan,     nan,     nan, -0.2488,     nan,     nan,
2:             nan,     nan, -0.2146,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1827,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0779,
2:             nan,     nan,  0.0884,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,  0.2182,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,  0.2023,     nan,     nan,
2:             nan,     nan,  0.0884,     nan,     nan,     nan,  0.2501,     nan,
2:             nan,     nan,     nan])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 33, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-2.5841, -2.6273, -2.6652, -2.6978, -2.7173, -2.7319, -2.7382, -2.7434,
2:         -2.7412, -2.7279, -2.7065, -2.6820, -2.6524, -2.6230, -2.5902, -2.5593,
2:         -2.5270, -2.4914, -2.6045, -2.6442, -2.6824, -2.7012, -2.7086, -2.7084,
2:         -2.6999], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.5775, -0.5603, -0.5435, -0.5324, -0.5333, -0.5328, -0.5380, -0.5347,
2:         -0.5200, -0.5011, -0.4747, -0.4506, -0.4279, -0.4031, -0.3772, -0.3522,
2:         -0.3314, -0.3154, -0.6229, -0.6132, -0.5964, -0.5764, -0.5615, -0.5504,
2:         -0.5443], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.0804, -0.0716, -0.0489, -0.0138,  0.0173,  0.0482,  0.0797,  0.1243,
2:          0.1647,  0.2173,  0.2687,  0.3192,  0.3473,  0.3663,  0.3657,  0.3493,
2:          0.3237,  0.2952,  0.0462,  0.0556,  0.0817,  0.1093,  0.1358,  0.1610,
2:          0.1911], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.8723, -1.1743, -1.7383, -2.3899, -2.6765, -2.1985, -0.8904,  0.6988,
2:          2.0522,  2.8943,  3.1188,  2.8946,  2.3651,  1.7465,  1.1667,  0.6642,
2:          0.3205,  0.0547, -1.4461, -1.4714, -1.4559, -1.4248, -1.2628, -0.8160,
2:          0.0589], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.6564, -0.6234, -0.5870, -0.5491, -0.5148, -0.4856, -0.4652, -0.4536,
2:         -0.4504, -0.4539, -0.4656, -0.4814, -0.4969, -0.5094, -0.5161, -0.5157,
2:         -0.5088, -0.4938, -0.4746, -0.4547, -0.4348, -0.4184, -0.4052, -0.3938,
2:         -0.3809], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1730, -0.1738, -0.1820, -0.1799, -0.1800, -0.1799, -0.1811, -0.1735,
2:         -0.1716, -0.1809, -0.1740, -0.1787, -0.1807, -0.1819, -0.1768, -0.1793,
2:         -0.1754, -0.1685, -0.1764, -0.1758, -0.1735, -0.1731, -0.1748, -0.1707,
2:         -0.1727], device='cuda:2', grad_fn=<SliceBackward0>)
1: [DEBUG] INPUT BATCH
1: Epoch 33, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3: [DEBUG] INPUT BATCH
3: Epoch 33, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-1.3547, -1.4451, -1.5105, -1.5484, -1.5595, -1.5475, -1.5224, -1.5010,
1:         -1.4992, -1.5251, -1.5748, -1.6385, -1.7052, -1.7674, -1.8201, -1.8613,
1:         -1.8913, -1.9126, -1.4237, -1.4913, -1.5366, -1.5619, -1.5702, -1.5652,
1:         -1.5534], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1774, -0.2010, -0.2457, -0.2960, -0.3327, -0.3400, -0.3127, -0.2569,
1:         -0.1870, -0.1176, -0.0575, -0.0095,  0.0264,  0.0519,  0.0684,  0.0771,
1:          0.0788,  0.0730, -0.2910, -0.3064, -0.3329, -0.3609, -0.3786, -0.3767,
1:         -0.3515], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.3054, -0.3357, -0.3523, -0.3668, -0.3731, -0.3843, -0.3924, -0.3982,
1:         -0.4003, -0.4021, -0.3973, -0.3949, -0.3827, -0.3655, -0.3416, -0.3115,
1:         -0.2817, -0.2523, -0.3398, -0.3639, -0.3758, -0.3889, -0.3905, -0.3900,
1:         -0.3877], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.4566,  0.7480,  0.9497,  0.9755,  0.7502,  0.2683, -0.3482, -0.8784,
1:         -1.1261, -1.0398, -0.7282, -0.3662, -0.0770,  0.1102,  0.2201,  0.2806,
1:          0.2985,  0.2806,  0.4622,  0.5933,  0.6572,  0.6426,  0.5126,  0.2358,
1:         -0.1622], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.7487, 0.7172, 0.6704, 0.6179, 0.5712, 0.5408, 0.5351, 0.5551, 0.5930,
1:         0.6354, 0.6694, 0.6893, 0.6954, 0.6908, 0.6798, 0.6655, 0.6509, 0.6379,
1:         0.6264, 0.6157, 0.6045, 0.5930, 0.5818, 0.5709, 0.5606],
1:        device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 33, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan, -0.2443,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2397,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2488,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1816,
1:             nan,     nan,     nan,  0.1966,     nan,     nan, -0.1919,     nan,
1:         -0.0654,     nan,     nan,     nan,     nan,  0.4814,  0.4985,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  0.1818,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2488,     nan,     nan,
1:         -0.2215,     nan,     nan,     nan, -0.2465,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0656,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,  0.2433,     nan,  0.2342,     nan,     nan,
1:             nan,     nan,     nan,  0.2684,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2420,     nan, -0.1349,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  0.0200,  0.1408,     nan,     nan,     nan, -0.0779,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,  0.1590,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,  0.0838,     nan,     nan,  0.2410,  0.2615,
1:             nan,     nan,     nan,     nan,     nan,     nan,  0.2501,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 33, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-2.5764, -2.6174, -2.6652, -2.7080, -2.7349, -2.7541, -2.7585, -2.7605,
1:         -2.7540, -2.7404, -2.7185, -2.6943, -2.6694, -2.6411, -2.6093, -2.5746,
1:         -2.5340, -2.4858, -2.5801, -2.6091, -2.6504, -2.6771, -2.6951, -2.7031,
1:         -2.6993], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5097, -0.4926, -0.4726, -0.4548, -0.4465, -0.4398, -0.4445, -0.4405,
1:         -0.4332, -0.4236, -0.4140, -0.4025, -0.3913, -0.3772, -0.3578, -0.3408,
1:         -0.3291, -0.3290, -0.5689, -0.5543, -0.5292, -0.5021, -0.4853, -0.4753,
1:         -0.4712], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.0335, -0.0107,  0.0197,  0.0552,  0.0813,  0.1116,  0.1398,  0.1834,
1:          0.2193,  0.2601,  0.2969,  0.3349,  0.3550,  0.3747,  0.3810,  0.3754,
1:          0.3545,  0.3269,  0.0922,  0.1111,  0.1369,  0.1617,  0.1802,  0.2008,
1:          0.2303], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.1021, -1.4969, -2.0717, -2.5930, -2.6893, -2.1389, -0.9539,  0.5146,
1:          1.8888,  2.7739,  3.0114,  2.8071,  2.3034,  1.7177,  1.1951,  0.7617,
1:          0.4414,  0.1466, -1.5903, -1.6564, -1.6531, -1.5394, -1.2225, -0.6886,
1:          0.0920], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.6504, -0.6268, -0.5924, -0.5484, -0.5052, -0.4716, -0.4524, -0.4502,
1:         -0.4578, -0.4699, -0.4841, -0.4963, -0.5063, -0.5154, -0.5209, -0.5227,
1:         -0.5178, -0.5056, -0.4876, -0.4667, -0.4446, -0.4239, -0.4043, -0.3853,
1:         -0.3647], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1615, -0.1678, -0.1769, -0.1808, -0.1812, -0.1825, -0.1810, -0.1733,
1:         -0.1707, -0.1645, -0.1626, -0.1726, -0.1802, -0.1812, -0.1776, -0.1795,
1:         -0.1742, -0.1668, -0.1607, -0.1659, -0.1670, -0.1709, -0.1746, -0.1730,
1:         -0.1725], device='cuda:1', grad_fn=<SliceBackward0>)
3:      first 25 values: tensor([-1.3547, -1.4451, -1.5105, -1.5484, -1.5595, -1.5475, -1.5224, -1.5010,
3:         -1.4992, -1.5251, -1.5748, -1.6385, -1.7052, -1.7674, -1.8201, -1.8613,
3:         -1.8913, -1.9126, -1.4237, -1.4913, -1.5366, -1.5619, -1.5702, -1.5652,
3:         -1.5534], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1774, -0.2010, -0.2457, -0.2960, -0.3327, -0.3400, -0.3127, -0.2569,
3:         -0.1870, -0.1176, -0.0575, -0.0095,  0.0264,  0.0519,  0.0684,  0.0771,
3:          0.0788,  0.0730, -0.2910, -0.3064, -0.3329, -0.3609, -0.3786, -0.3767,
3:         -0.3515], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.3054, -0.3357, -0.3523, -0.3668, -0.3731, -0.3843, -0.3924, -0.3982,
3:         -0.4003, -0.4021, -0.3973, -0.3949, -0.3827, -0.3655, -0.3416, -0.3115,
3:         -0.2817, -0.2523, -0.3398, -0.3639, -0.3758, -0.3889, -0.3905, -0.3900,
3:         -0.3877], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.4566,  0.7480,  0.9497,  0.9755,  0.7502,  0.2683, -0.3482, -0.8784,
3:         -1.1261, -1.0398, -0.7282, -0.3662, -0.0770,  0.1102,  0.2201,  0.2806,
3:          0.2985,  0.2806,  0.4622,  0.5933,  0.6572,  0.6426,  0.5126,  0.2358,
3:         -0.1622], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([0.7487, 0.7172, 0.6704, 0.6179, 0.5712, 0.5408, 0.5351, 0.5551, 0.5930,
3:         0.6354, 0.6694, 0.6893, 0.6954, 0.6908, 0.6798, 0.6655, 0.6509, 0.6379,
3:         0.6264, 0.6157, 0.6045, 0.5930, 0.5818, 0.5709, 0.5606],
3:        device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 33, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2454,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2477, -0.2363,     nan, -0.2272,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  0.2763,     nan,     nan,     nan,
3:             nan,  0.0348,     nan,     nan,  0.4506,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  0.4267,     nan,  0.5235,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2488,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.1668, -0.0825,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.1098,     nan,     nan,     nan,     nan,     nan, -0.1463,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,  0.3572,     nan,     nan,
3:             nan,     nan,     nan,  0.2615,     nan,     nan,     nan,     nan,
3:             nan,     nan,  0.2501,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan, -0.2146,     nan, -0.2488,     nan,     nan,     nan,
3:             nan,     nan, -0.1349, -0.0529,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:          0.1225,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,  0.0018,     nan,     nan,     nan,
3:             nan,  0.1225,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 33, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-2.6449, -2.6714, -2.6901, -2.6985, -2.6986, -2.7008, -2.7005, -2.7105,
3:         -2.7181, -2.7161, -2.7046, -2.6812, -2.6545, -2.6192, -2.5822, -2.5493,
3:         -2.5167, -2.4851, -2.6406, -2.6587, -2.6731, -2.6730, -2.6681, -2.6626,
3:         -2.6578], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5704, -0.5527, -0.5275, -0.5048, -0.4976, -0.4937, -0.5012, -0.4966,
3:         -0.4809, -0.4589, -0.4328, -0.4068, -0.3845, -0.3612, -0.3404, -0.3295,
3:         -0.3274, -0.3353, -0.6052, -0.5925, -0.5669, -0.5371, -0.5194, -0.5120,
3:         -0.5093], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.0031,  0.0158,  0.0350,  0.0622,  0.0849,  0.1162,  0.1436,  0.1831,
3:          0.2160,  0.2502,  0.2783,  0.3090,  0.3215,  0.3278,  0.3285,  0.3213,
3:          0.3057,  0.2809,  0.1197,  0.1338,  0.1544,  0.1716,  0.1905,  0.2137,
3:          0.2420], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.8006, -1.1887, -1.7829, -2.3866, -2.6948, -2.3147, -1.1366,  0.3142,
3:          1.5699,  2.3922,  2.6712,  2.5446,  2.1261,  1.5753,  1.0095,  0.5512,
3:          0.3073,  0.0988, -1.1628, -1.2265, -1.2709, -1.2921, -1.2306, -0.8858,
3:         -0.1301], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.6824, -0.6454, -0.6001, -0.5491, -0.5026, -0.4666, -0.4442, -0.4363,
3:         -0.4388, -0.4492, -0.4662, -0.4852, -0.5022, -0.5139, -0.5200, -0.5188,
3:         -0.5111, -0.4971, -0.4801, -0.4620, -0.4444, -0.4295, -0.4155, -0.4019,
3:         -0.3883], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1673, -0.1707, -0.1769, -0.1774, -0.1798, -0.1838, -0.1849, -0.1807,
3:         -0.1808, -0.1732, -0.1715, -0.1743, -0.1789, -0.1822, -0.1794, -0.1840,
3:         -0.1828, -0.1763, -0.1714, -0.1739, -0.1716, -0.1723, -0.1735, -0.1730,
3:         -0.1775], device='cuda:3', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 33 [1/5 (20%)]	Loss: nan : nan :: 0.14058 (1.63 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 33 [2/5 (40%)]	Loss: nan : nan :: 0.13839 (10.39 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 33 [3/5 (60%)]	Loss: nan : nan :: 0.13953 (10.52 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 33 [4/5 (80%)]	Loss: nan : nan :: 0.14533 (10.42 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch33.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch33.mod
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 33 : nan
0: validation loss for velocity_u : 0.0383576974272728
0: validation loss for velocity_v : 0.07132308930158615
0: validation loss for specific_humidity : 0.024471014738082886
0: validation loss for velocity_z : 0.47220098972320557
0: validation loss for temperature : 0.07805896550416946
0: validation loss for total_precip : nan
1: 34 : 02:03:41 :: batch_size = 96, lr = 1e-05
2: 34 : 02:03:41 :: batch_size = 96, lr = 1e-05
3: 34 : 02:03:41 :: batch_size = 96, lr = 1e-05
0: 34 : 02:03:41 :: batch_size = 96, lr = 1e-05
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: [DEBUG] INPUT BATCH
3: Epoch 34, first input batch shapes / sample data:
3:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: Created sparse mask for total_precip with 10.0% data retained
3:      first 25 values: tensor([ 0.3098,  0.3138,  0.3152,  0.3123,  0.3042,  0.2899,  0.2688,  0.2412,
3:          0.2073,  0.1674,  0.1219,  0.0707,  0.0129, -0.0526, -0.1256, -0.2034,
3:         -0.2804, -0.3483,  0.1998,  0.1979,  0.1910,  0.1784,  0.1602,  0.1374,
3:          0.1113], device='cuda:3')
3:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.1779, -0.1465, -0.1174, -0.0935, -0.0753, -0.0640, -0.0594, -0.0616,
3:         -0.0711, -0.0882, -0.1134, -0.1457, -0.1832, -0.2223, -0.2590, -0.2893,
3:         -0.3102, -0.3213, -0.1650, -0.1414, -0.1235, -0.1122, -0.1082, -0.1108,
3:         -0.1191], device='cuda:3')
3:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([-0.7839, -0.7829, -0.7817, -0.7805, -0.7793, -0.7783, -0.7773, -0.7766,
3:         -0.7745, -0.7723, -0.7702, -0.7679, -0.7655, -0.7632, -0.7530, -0.7429,
3:         -0.7334, -0.7286, -0.7781, -0.7776, -0.7772, -0.7768, -0.7755, -0.7743,
3:         -0.7730], device='cuda:3')
3:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
3:      first 25 values: tensor([ 0.1641,  0.2062,  0.2840,  0.3661,  0.4124,  0.4019,  0.3472,  0.2819,
3:          0.2609,  0.3135,  0.4355,  0.5786,  0.6628,  0.6144,  0.3998,  0.0526,
3:         -0.3262, -0.6061,  0.1914,  0.2672,  0.3261,  0.3324,  0.2819,  0.2062,
3:          0.1536], device='cuda:3')
3:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
3:      first 25 values: tensor([-0.2947, -0.2989, -0.3035, -0.3102, -0.3201, -0.3340, -0.3514, -0.3716,
3:         -0.3939, -0.4182, -0.4447, -0.4731, -0.5030, -0.5325, -0.5596, -0.5816,
3:         -0.5971, -0.6050, -0.6061, -0.6018, -0.5947, -0.5867, -0.5798, -0.5755,
3:         -0.5747], device='cuda:3')
3:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
3:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
3:         0.], device='cuda:3')
3: [DEBUG] TARGET BATCH
3: Epoch 34, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
3: [DEBUG] First 243 batch values:
3: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2405,
3:             nan,     nan,     nan, -0.2416,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
3:             nan, -0.2438,     nan,     nan,     nan, -0.2427,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2438,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
3:         -0.2427,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan, -0.2405,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
3:             nan,     nan, -0.2427,     nan, -0.2427,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
3:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:         -0.2438,     nan,     nan, -0.2416,     nan, -0.2427,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan, -0.2438, -0.2438, -0.2438,
3:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,
3:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
3:             nan,     nan,     nan])
3: [DEBUG] PREDICTIONS TRAIN BATCH
3: Epoch 34, first predictions sample:
3:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([0.0268, 0.0422, 0.0578, 0.0772, 0.0966, 0.1134, 0.1272, 0.1371, 0.1450,
3:         0.1542, 0.1618, 0.1660, 0.1654, 0.1555, 0.1381, 0.1134, 0.0857, 0.0592,
3:         0.0187, 0.0350, 0.0549, 0.0758, 0.0958, 0.1134, 0.1313],
3:        device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.5424, -0.6010, -0.6672, -0.7394, -0.8130, -0.8873, -0.9587, -1.0233,
3:         -1.0735, -1.1114, -1.1308, -1.1297, -1.1067, -1.0596, -0.9931, -0.9185,
3:         -0.8474, -0.7862, -0.5057, -0.5664, -0.6359, -0.7057, -0.7808, -0.8583,
3:         -0.9361], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([-0.7949, -0.7957, -0.7951, -0.7920, -0.7905, -0.7873, -0.7857, -0.7821,
3:         -0.7805, -0.7782, -0.7744, -0.7705, -0.7672, -0.7628, -0.7580, -0.7534,
3:         -0.7475, -0.7418, -0.7924, -0.7941, -0.7922, -0.7913, -0.7889, -0.7859,
3:         -0.7820], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
3:      first 25 pred values: tensor([ 0.0394,  0.0663,  0.1342,  0.2362,  0.3216,  0.3454,  0.2806,  0.1614,
3:          0.0336, -0.0898, -0.1446, -0.1231, -0.0495, -0.0198, -0.0936, -0.2132,
3:         -0.3712, -0.4885,  0.0573,  0.1123,  0.1891,  0.2703,  0.2870,  0.2232,
3:          0.0830], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
3:      first 25 pred values: tensor([-0.2452, -0.2559, -0.2634, -0.2721, -0.2772, -0.2816, -0.2837, -0.2863,
3:         -0.2866, -0.2830, -0.2748, -0.2604, -0.2407, -0.2183, -0.1973, -0.1771,
3:         -0.1599, -0.1450, -0.1304, -0.1168, -0.1031, -0.0883, -0.0713, -0.0539,
3:         -0.0402], device='cuda:3', grad_fn=<SliceBackward0>)
3:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
3:      first 25 pred values: tensor([-0.1627, -0.1650, -0.1689, -0.1679, -0.1677, -0.1706, -0.1690, -0.1640,
3:         -0.1630, -0.1664, -0.1626, -0.1662, -0.1671, -0.1696, -0.1654, -0.1709,
3:         -0.1654, -0.1602, -0.1639, -0.1639, -0.1627, -0.1625, -0.1616, -0.1619,
3:         -0.1627], device='cuda:3', grad_fn=<SliceBackward0>)
2: [DEBUG] INPUT BATCH
2: Epoch 34, first input batch shapes / sample data:
2:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.3098,  0.3138,  0.3152,  0.3123,  0.3042,  0.2899,  0.2688,  0.2412,
2:          0.2073,  0.1674,  0.1219,  0.0707,  0.0129, -0.0526, -0.1256, -0.2034,
2:         -0.2804, -0.3483,  0.1998,  0.1979,  0.1910,  0.1784,  0.1602,  0.1374,
2:          0.1113], device='cuda:2')
2:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.1779, -0.1465, -0.1174, -0.0935, -0.0753, -0.0640, -0.0594, -0.0616,
2:         -0.0711, -0.0882, -0.1134, -0.1457, -0.1832, -0.2223, -0.2590, -0.2893,
2:         -0.3102, -0.3213, -0.1650, -0.1414, -0.1235, -0.1122, -0.1082, -0.1108,
2:         -0.1191], device='cuda:2')
2:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([-0.7839, -0.7829, -0.7817, -0.7805, -0.7793, -0.7783, -0.7773, -0.7766,
2:         -0.7745, -0.7723, -0.7702, -0.7679, -0.7655, -0.7632, -0.7530, -0.7429,
2:         -0.7334, -0.7286, -0.7781, -0.7776, -0.7772, -0.7768, -0.7755, -0.7743,
2:         -0.7730], device='cuda:2')
2:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
2:      first 25 values: tensor([ 0.1641,  0.2062,  0.2840,  0.3661,  0.4124,  0.4019,  0.3472,  0.2819,
2:          0.2609,  0.3135,  0.4355,  0.5786,  0.6628,  0.6144,  0.3998,  0.0526,
2:         -0.3262, -0.6061,  0.1914,  0.2672,  0.3261,  0.3324,  0.2819,  0.2062,
2:          0.1536], device='cuda:2')
2:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
2:      first 25 values: tensor([-0.2947, -0.2989, -0.3035, -0.3102, -0.3201, -0.3340, -0.3514, -0.3716,
2:         -0.3939, -0.4182, -0.4447, -0.4731, -0.5030, -0.5325, -0.5596, -0.5816,
2:         -0.5971, -0.6050, -0.6061, -0.6018, -0.5947, -0.5867, -0.5798, -0.5755,
2:         -0.5747], device='cuda:2')
2:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
2:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
2:         0.], device='cuda:2')
2: [DEBUG] TARGET BATCH
2: Epoch 34, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
2: [DEBUG] First 243 batch values:
2: tensor([    nan,     nan,     nan,     nan,     nan, -0.2405, -0.2405,     nan,
2:             nan,     nan,     nan,     nan, -0.2416,     nan,     nan,     nan,
2:         -0.2373,     nan,     nan,     nan,     nan,     nan, -0.2405,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
2:             nan, -0.2427,     nan,     nan,     nan, -0.2438,     nan, -0.2438,
2:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2438,     nan,     nan, -0.2427,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan, -0.2438,     nan,     nan,     nan,     nan, -0.2416,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2427,     nan, -0.2438,     nan,
2:             nan,     nan, -0.2427,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2438, -0.2427,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2438, -0.2438,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
2:         -0.2438,     nan,     nan,     nan, -0.2416,     nan,     nan,     nan,
2:             nan,     nan, -0.2427,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
2:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:         -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
2:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
2:             nan,     nan, -0.2427])
2: [DEBUG] PREDICTIONS TRAIN BATCH
2: Epoch 34, first predictions sample:
2:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([0.0038, 0.0225, 0.0431, 0.0647, 0.0842, 0.1015, 0.1157, 0.1253, 0.1356,
2:         0.1469, 0.1587, 0.1669, 0.1700, 0.1648, 0.1463, 0.1199, 0.0862, 0.0555,
2:         0.0123, 0.0313, 0.0515, 0.0699, 0.0884, 0.1049, 0.1222],
2:        device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.5496, -0.6076, -0.6773, -0.7556, -0.8353, -0.9085, -0.9777, -1.0383,
2:         -1.0804, -1.1118, -1.1258, -1.1218, -1.0967, -1.0519, -0.9845, -0.9068,
2:         -0.8293, -0.7616, -0.5254, -0.5808, -0.6475, -0.7196, -0.7944, -0.8735,
2:         -0.9470], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([-0.7864, -0.7852, -0.7837, -0.7802, -0.7792, -0.7768, -0.7767, -0.7746,
2:         -0.7742, -0.7727, -0.7703, -0.7672, -0.7645, -0.7612, -0.7567, -0.7530,
2:         -0.7486, -0.7440, -0.7926, -0.7910, -0.7875, -0.7837, -0.7806, -0.7776,
2:         -0.7742], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
2:      first 25 pred values: tensor([ 0.0730,  0.0802,  0.1345,  0.2324,  0.3206,  0.3542,  0.3170,  0.2097,
2:          0.0352, -0.1271, -0.1635, -0.1081, -0.0414, -0.0192, -0.0562, -0.1752,
2:         -0.3842, -0.5420,  0.0330,  0.0693,  0.1498,  0.2395,  0.2530,  0.1902,
2:          0.0885], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
2:      first 25 pred values: tensor([-0.2316, -0.2513, -0.2650, -0.2737, -0.2748, -0.2745, -0.2747, -0.2777,
2:         -0.2802, -0.2791, -0.2730, -0.2607, -0.2438, -0.2244, -0.2062, -0.1873,
2:         -0.1710, -0.1553, -0.1394, -0.1232, -0.1057, -0.0883, -0.0713, -0.0554,
2:         -0.0438], device='cuda:2', grad_fn=<SliceBackward0>)
2:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
2:      first 25 pred values: tensor([-0.1377, -0.1441, -0.1511, -0.1523, -0.1574, -0.1626, -0.1649, -0.1605,
2:         -0.1618, -0.1483, -0.1474, -0.1507, -0.1604, -0.1612, -0.1627, -0.1669,
2:         -0.1663, -0.1587, -0.1483, -0.1506, -0.1512, -0.1498, -0.1548, -0.1584,
2:         -0.1626], device='cuda:2', grad_fn=<SliceBackward0>)
0: [DEBUG] INPUT BATCH
0: Epoch 34, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3098,  0.3138,  0.3152,  0.3123,  0.3042,  0.2899,  0.2688,  0.2412,
0:          0.2073,  0.1674,  0.1219,  0.0707,  0.0129, -0.0526, -0.1256, -0.2034,
0:         -0.2804, -0.3483,  0.1998,  0.1979,  0.1910,  0.1784,  0.1602,  0.1374,
0:          0.1113], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1779, -0.1465, -0.1174, -0.0935, -0.0753, -0.0640, -0.0594, -0.0616,
0:         -0.0711, -0.0882, -0.1134, -0.1457, -0.1832, -0.2223, -0.2590, -0.2893,
0:         -0.3102, -0.3213, -0.1650, -0.1414, -0.1235, -0.1122, -0.1082, -0.1108,
0:         -0.1191], device='cuda:0')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7839, -0.7829, -0.7817, -0.7805, -0.7793, -0.7783, -0.7773, -0.7766,
0:         -0.7745, -0.7723, -0.7702, -0.7679, -0.7655, -0.7632, -0.7530, -0.7429,
0:         -0.7334, -0.7286, -0.7781, -0.7776, -0.7772, -0.7768, -0.7755, -0.7743,
0:         -0.7730], device='cuda:0')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1641,  0.2062,  0.2840,  0.3661,  0.4124,  0.4019,  0.3472,  0.2819,
0:          0.2609,  0.3135,  0.4355,  0.5786,  0.6628,  0.6144,  0.3998,  0.0526,
0:         -0.3262, -0.6061,  0.1914,  0.2672,  0.3261,  0.3324,  0.2819,  0.2062,
0:          0.1536], device='cuda:0')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2947, -0.2989, -0.3035, -0.3102, -0.3201, -0.3340, -0.3514, -0.3716,
0:         -0.3939, -0.4182, -0.4447, -0.4731, -0.5030, -0.5325, -0.5596, -0.5816,
0:         -0.5971, -0.6050, -0.6061, -0.6018, -0.5947, -0.5867, -0.5798, -0.5755,
0:         -0.5747], device='cuda:0')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
0:         0.], device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 34, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2405,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2416,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2416,     nan, -0.2405,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2427,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2416, -0.2405,     nan,
0:         -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2438, -0.2438,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2427,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,
0:             nan, -0.2438,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 34, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0118, 0.0328, 0.0523, 0.0721, 0.0895, 0.1066, 0.1227, 0.1386, 0.1529,
0:         0.1683, 0.1804, 0.1866, 0.1862, 0.1737, 0.1504, 0.1175, 0.0824, 0.0509,
0:         0.0143, 0.0333, 0.0523, 0.0717, 0.0902, 0.1091, 0.1284],
0:        device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5380, -0.5886, -0.6533, -0.7303, -0.8112, -0.8886, -0.9582, -1.0160,
0:         -1.0586, -1.0913, -1.1089, -1.1085, -1.0839, -1.0376, -0.9701, -0.8896,
0:         -0.8145, -0.7524, -0.5004, -0.5501, -0.6155, -0.6912, -0.7737, -0.8571,
0:         -0.9335], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7855, -0.7850, -0.7833, -0.7795, -0.7772, -0.7746, -0.7747, -0.7733,
0:         -0.7736, -0.7724, -0.7691, -0.7649, -0.7606, -0.7559, -0.7506, -0.7465,
0:         -0.7435, -0.7404, -0.7915, -0.7919, -0.7894, -0.7863, -0.7845, -0.7818,
0:         -0.7796], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1046,  0.1245,  0.1830,  0.2831,  0.3591,  0.3992,  0.3528,  0.2198,
0:          0.0581, -0.1004, -0.1682, -0.1193, -0.0319, -0.0133, -0.0606, -0.1638,
0:         -0.3601, -0.5149,  0.1173,  0.1739,  0.2471,  0.3209,  0.3268,  0.2815,
0:          0.1622], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.2495, -0.2655, -0.2762, -0.2841, -0.2853, -0.2853, -0.2854, -0.2883,
0:         -0.2907, -0.2892, -0.2816, -0.2670, -0.2473, -0.2266, -0.2073, -0.1890,
0:         -0.1729, -0.1588, -0.1458, -0.1331, -0.1193, -0.1030, -0.0848, -0.0670,
0:         -0.0518], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1384, -0.1441, -0.1507, -0.1570, -0.1597, -0.1666, -0.1671, -0.1580,
0:         -0.1546, -0.1428, -0.1411, -0.1495, -0.1573, -0.1585, -0.1567, -0.1583,
0:         -0.1552, -0.1455, -0.1402, -0.1408, -0.1415, -0.1430, -0.1465, -0.1467,
0:         -0.1481], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 34, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.3098,  0.3138,  0.3152,  0.3123,  0.3042,  0.2899,  0.2688,  0.2412,
1:          0.2073,  0.1674,  0.1219,  0.0707,  0.0129, -0.0526, -0.1256, -0.2034,
1:         -0.2804, -0.3483,  0.1998,  0.1979,  0.1910,  0.1784,  0.1602,  0.1374,
1:          0.1113], device='cuda:1')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.1779, -0.1465, -0.1174, -0.0935, -0.0753, -0.0640, -0.0594, -0.0616,
1:         -0.0711, -0.0882, -0.1134, -0.1457, -0.1832, -0.2223, -0.2590, -0.2893,
1:         -0.3102, -0.3213, -0.1650, -0.1414, -0.1235, -0.1122, -0.1082, -0.1108,
1:         -0.1191], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.7839, -0.7829, -0.7817, -0.7805, -0.7793, -0.7783, -0.7773, -0.7766,
1:         -0.7745, -0.7723, -0.7702, -0.7679, -0.7655, -0.7632, -0.7530, -0.7429,
1:         -0.7334, -0.7286, -0.7781, -0.7776, -0.7772, -0.7768, -0.7755, -0.7743,
1:         -0.7730], device='cuda:1')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.1641,  0.2062,  0.2840,  0.3661,  0.4124,  0.4019,  0.3472,  0.2819,
1:          0.2609,  0.3135,  0.4355,  0.5786,  0.6628,  0.6144,  0.3998,  0.0526,
1:         -0.3262, -0.6061,  0.1914,  0.2672,  0.3261,  0.3324,  0.2819,  0.2062,
1:          0.1536], device='cuda:1')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.2947, -0.2989, -0.3035, -0.3102, -0.3201, -0.3340, -0.3514, -0.3716,
1:         -0.3939, -0.4182, -0.4447, -0.4731, -0.5030, -0.5325, -0.5596, -0.5816,
1:         -0.5971, -0.6050, -0.6061, -0.6018, -0.5947, -0.5867, -0.5798, -0.5755,
1:         -0.5747], device='cuda:1')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
1:         0.], device='cuda:1')
1: [DEBUG] TARGET BATCH
1: Epoch 34, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan, -0.2416,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2416,     nan,     nan,
1:             nan,     nan, -0.2405,     nan,     nan, -0.2438,     nan,     nan,
1:             nan, -0.2416, -0.2416, -0.2427,     nan,     nan,     nan, -0.2438,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2438,     nan,     nan,     nan, -0.2438,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2427,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2416,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2405,     nan,
1:         -0.2395, -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2427,     nan,     nan,     nan,
1:         -0.2438,     nan,     nan,     nan,     nan, -0.2416,     nan,     nan,
1:         -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2438,     nan,     nan,     nan,     nan, -0.2438, -0.2438,     nan,
1:             nan,     nan, -0.2416, -0.2416,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan, -0.2416,     nan,     nan,     nan, -0.2438,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan, -0.2438,
1:         -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2438, -0.2438,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 34, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.0261, 0.0368, 0.0510, 0.0680, 0.0858, 0.1028, 0.1187, 0.1295, 0.1407,
1:         0.1517, 0.1606, 0.1643, 0.1646, 0.1560, 0.1366, 0.1098, 0.0799, 0.0520,
1:         0.0217, 0.0323, 0.0487, 0.0653, 0.0849, 0.1042, 0.1229],
1:        device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.5327, -0.5855, -0.6527, -0.7330, -0.8169, -0.8960, -0.9706, -1.0362,
1:         -1.0838, -1.1205, -1.1402, -1.1374, -1.1118, -1.0643, -0.9972, -0.9190,
1:         -0.8412, -0.7740, -0.5060, -0.5577, -0.6228, -0.6972, -0.7757, -0.8588,
1:         -0.9355], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.7983, -0.7980, -0.7962, -0.7924, -0.7907, -0.7873, -0.7856, -0.7818,
1:         -0.7811, -0.7785, -0.7762, -0.7729, -0.7688, -0.7642, -0.7582, -0.7513,
1:         -0.7434, -0.7369, -0.7999, -0.8012, -0.7981, -0.7960, -0.7935, -0.7911,
1:         -0.7869], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([ 0.0637,  0.0806,  0.1706,  0.3025,  0.3804,  0.4036,  0.3746,  0.2562,
1:          0.0728, -0.0792, -0.1289, -0.0965, -0.0327, -0.0151, -0.0781, -0.2161,
1:         -0.4274, -0.5917,  0.0671,  0.1056,  0.2117,  0.3228,  0.3480,  0.2911,
1:          0.1792], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.2534, -0.2659, -0.2769, -0.2866, -0.2901, -0.2901, -0.2876, -0.2849,
1:         -0.2824, -0.2770, -0.2672, -0.2519, -0.2322, -0.2096, -0.1879, -0.1663,
1:         -0.1483, -0.1333, -0.1203, -0.1084, -0.0959, -0.0823, -0.0674, -0.0535,
1:         -0.0426], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1432, -0.1470, -0.1549, -0.1570, -0.1597, -0.1621, -0.1635, -0.1583,
1:         -0.1553, -0.1521, -0.1483, -0.1552, -0.1582, -0.1613, -0.1601, -0.1643,
1:         -0.1607, -0.1540, -0.1562, -0.1561, -0.1546, -0.1552, -0.1578, -0.1575,
1:         -0.1577], device='cuda:1', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 34 [1/5 (20%)]	Loss: nan : nan :: 0.15125 (1.63 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 34 [2/5 (40%)]	Loss: nan : nan :: 0.14039 (10.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 34 [3/5 (60%)]	Loss: nan : nan :: 0.13882 (10.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 34 [4/5 (80%)]	Loss: nan : nan :: 0.14616 (10.41 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_embeds_token_info_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_u_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_v_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_specific_humidity_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_velocity_z_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_temperature_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_encoder_total_precip_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_u_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_v_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_specific_humidity_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_velocity_z_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_temperature_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_decoder_total_precip_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_u_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_v_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_specific_humidity_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_velocity_z_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_temperature_idpjppryaq_epoch34.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idpjppryaq/AtmoRep_tail_total_precip_idpjppryaq_epoch34.mod
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
2: Created sparse mask for total_precip with 10.0% data retained
3: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
