0: Wandb run: atmorep-3xl7ehv0-17443458
0: l50018:398398:398398 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.118<0>
0: l50018:398398:398398 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50018:398398:398398 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50018:398398:398398 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50018:398398:398398 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
0: l50018:398398:398755 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.118<0>
0: l50018:398398:398755 [0] NCCL INFO Using non-device net plugin version 0
0: l50018:398398:398755 [0] NCCL INFO Using network IB
0: l50018:398398:398755 [0] NCCL INFO ncclCommInitRank comm 0x55555f268f10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0xb5cbe795b5954a27 - Init START
0: l50018:398398:398755 [0] NCCL INFO Setting affinity for GPU 0 to 0f0000,00000000,00000000,00000000,000f0000,00000000
0: l50018:398398:398755 [0] NCCL INFO comm 0x55555f268f10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50018:398398:398755 [0] NCCL INFO Channel 00/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 01/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 02/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 03/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 04/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 05/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 06/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 07/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 08/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 09/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 10/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 11/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 12/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 13/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 14/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 15/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 16/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 17/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 18/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 19/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 20/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 21/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 22/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 23/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 24/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 25/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 26/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 27/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 28/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 29/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 30/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Channel 31/32 :    0
0: l50018:398398:398755 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50018:398398:398755 [0] NCCL INFO P2P Chunksize set to 131072
0: l50018:398398:398755 [0] NCCL INFO Connected all rings
0: l50018:398398:398755 [0] NCCL INFO Connected all trees
0: l50018:398398:398755 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50018:398398:398755 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50018:398398:398755 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50018:398398:398755 [0] NCCL INFO ncclCommInitRank comm 0x55555f268f10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0xb5cbe795b5954a27 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 4
0: par_rank : 0
0: par_size : 1
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 1, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 2, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 3, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 3, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17443458
0: wandb_id : 3xl7ehv0
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: years_test : [2021]
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 1
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 2
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded model id = wc5e2i3t at epoch = 0.
0: Loaded run 'wc5e2i3t' at epoch 0.
0: l50018:398398:398784 [1] NCCL INFO Using non-device net plugin version 0
0: l50018:398398:398784 [1] NCCL INFO Using network IB
0: l50018:398398:398784 [1] NCCL INFO ncclCommInitRank comm 0x55556eada540 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0x97c41ce12c4497b8 - Init START
0: l50018:398398:398784 [1] NCCL INFO Setting affinity for GPU 1 to 0f0000,00000000,00000000,00000000,000f0000
0: l50018:398398:398784 [1] NCCL INFO comm 0x55556eada540 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50018:398398:398784 [1] NCCL INFO Channel 00/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 01/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 02/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 03/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 04/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 05/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 06/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 07/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 08/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 09/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 10/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 11/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 12/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 13/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 14/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 15/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 16/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 17/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 18/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 19/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 20/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 21/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 22/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 23/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 24/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 25/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 26/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 27/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 28/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 29/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 30/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Channel 31/32 :    0
0: l50018:398398:398784 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50018:398398:398784 [1] NCCL INFO P2P Chunksize set to 131072
0: l50018:398398:398784 [1] NCCL INFO Connected all rings
0: l50018:398398:398784 [1] NCCL INFO Connected all trees
0: l50018:398398:398784 [1] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50018:398398:398784 [1] NCCL INFO ncclCommInitRank comm 0x55556eada540 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0x97c41ce12c4497b8 - Init COMPLETE
0: l50018:398398:398789 [2] NCCL INFO Using non-device net plugin version 0
0: l50018:398398:398789 [2] NCCL INFO Using network IB
0: l50018:398398:398789 [2] NCCL INFO ncclCommInitRank comm 0x555583260a50 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0xb7322c03accb6df9 - Init START
0: l50018:398398:398789 [2] NCCL INFO Setting affinity for GPU 2 to 1f0000,00000000,00000000,00000000,001f0000,00000000,00000000,00000000
0: l50018:398398:398789 [2] NCCL INFO comm 0x555583260a50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50018:398398:398789 [2] NCCL INFO Channel 00/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 01/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 02/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 03/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 04/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 05/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 06/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 07/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 08/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 09/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 10/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 11/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 12/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 13/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 14/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 15/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 16/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 17/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 18/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 19/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 20/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 21/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 22/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 23/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 24/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 25/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 26/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 27/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 28/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 29/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 30/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Channel 31/32 :    0
0: l50018:398398:398789 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50018:398398:398789 [2] NCCL INFO P2P Chunksize set to 131072
0: l50018:398398:398789 [2] NCCL INFO Connected all rings
0: l50018:398398:398789 [2] NCCL INFO Connected all trees
0: l50018:398398:398789 [2] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50018:398398:398789 [2] NCCL INFO ncclCommInitRank comm 0x555583260a50 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0xb7322c03accb6df9 - Init COMPLETE
0: l50018:398398:398794 [3] NCCL INFO Using non-device net plugin version 0
0: l50018:398398:398794 [3] NCCL INFO Using network IB
0: l50018:398398:398794 [3] NCCL INFO ncclCommInitRank comm 0x555585538570 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0xbb4276f875c415ba - Init START
0: l50018:398398:398794 [3] NCCL INFO Setting affinity for GPU 3 to 0f0000,00000000,00000000,00000000,000f0000,00000000,00000000
0: l50018:398398:398794 [3] NCCL INFO comm 0x555585538570 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50018:398398:398794 [3] NCCL INFO Channel 00/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 01/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 02/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 03/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 04/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 05/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 06/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 07/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 08/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 09/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 10/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 11/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 12/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 13/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 14/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 15/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 16/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 17/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 18/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 19/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 20/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 21/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 22/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 23/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 24/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 25/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 26/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 27/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 28/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 29/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 30/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Channel 31/32 :    0
0: l50018:398398:398794 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50018:398398:398794 [3] NCCL INFO P2P Chunksize set to 131072
0: l50018:398398:398794 [3] NCCL INFO Connected all rings
0: l50018:398398:398794 [3] NCCL INFO Connected all trees
0: l50018:398398:398794 [3] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50018:398398:398794 [3] NCCL INFO ncclCommInitRank comm 0x555585538570 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0xbb4276f875c415ba - Init COMPLETE
0: Number of trainable parameters: 741,136,272
0: 1 : 18:01:41 :: batch_size = 96, lr = 1.5000000000000002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1002, -1.1167, -1.1272, -1.1359, -1.1435, -1.1442, -1.1323, -1.0839, -0.9876, -0.8642, -0.7823, -0.7801,
0:         -0.8788, -1.0005, -1.0298, -1.0323, -1.0227, -0.9878, -1.1223, -1.1310, -1.1316, -1.1287, -1.1272, -1.1208,
0:         -1.1107], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2737, -0.2926, -0.3264, -0.3621, -0.4112, -0.4536, -0.5112, -0.5522, -0.5600, -0.5199, -0.3852, -0.2181,
0:         -0.0968, -0.1465, -0.3453, -0.5108, -0.5783, -0.5932, -0.2616, -0.2865, -0.3305, -0.3808, -0.4439, -0.5057,
0:         -0.5639], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.6168, 2.6867, 2.7579, 2.7261, 2.6501, 2.5815, 2.5246, 2.5458, 2.6285, 2.8483, 3.1751, 3.3085, 3.4101, 3.4181,
0:         3.1643, 2.8762, 2.7100, 2.8543, 2.5010, 2.5987, 2.7167, 2.8350, 2.7958, 2.7563, 2.6985], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2198,  0.4075,  0.3299,  0.3622,  0.2414,  0.1508,  0.3385,  0.1832, -0.0283, -0.7639, -1.5254, -1.6096,
0:         -1.9008, -3.1304, -3.8337, -3.0096, -2.8651, -2.6494,  0.1120,  0.3083,  0.2608,  0.2975,  0.3708,  0.2738,
0:          0.3881], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1926, -0.2362, -0.3000, -0.2820, -0.2104, -0.0108,  0.3387,  0.7593,  1.2331,  1.3194,  1.0506,  0.6350,
0:          0.3786,  0.5923,  0.8664,  0.9590,  1.0176,  1.1367,  0.8312,  0.4027,  0.1834, -0.1348, -0.1233, -0.1216,
0:         -0.0762], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1892, -0.1870, -0.1981, -0.2048, -0.2048, -0.1514, -0.0769, -0.0602, -0.0080, -0.2315, -0.2326, -0.2370,
0:         -0.2381, -0.2370, -0.2348, -0.1870, -0.1770, -0.1447, -0.2370, -0.2381, -0.2381, -0.2381, -0.2337, -0.2315,
0:         -0.2326], device='cuda:0')
0: [DEBUG] Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3690,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.9405,     nan,     nan,     nan,  0.5469,     nan,     nan,
0:          0.5180,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.6121,  1.5921,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  3.0042,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  2.1080,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          1.3274,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  3.9805,     nan,     nan,     nan,     nan,
0:             nan,  2.9842,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  2.9753,     nan,     nan,  3.9671,     nan,
0:             nan,     nan,     nan,  1.4297,     nan,     nan,  4.5920,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.3252,  0.5080, -0.0246,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          6.1731,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.5663, -1.5541, -1.5434, -1.5326, -1.5201, -1.5080, -1.4888, -1.4713, -1.4499, -1.4311, -1.4136, -1.4033,
0:         -1.3970, -1.3888, -1.3835, -1.3726, -1.3593, -1.3457, -1.6192, -1.6152, -1.6110, -1.5998, -1.5851, -1.5665,
0:         -1.5389], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1870, -0.1934, -0.2075, -0.2287, -0.2532, -0.2715, -0.2819, -0.2778, -0.2564, -0.2247, -0.1888, -0.1552,
0:         -0.1322, -0.1215, -0.1244, -0.1327, -0.1437, -0.1489, -0.1690, -0.1599, -0.1560, -0.1509, -0.1462, -0.1481,
0:         -0.1475], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.4965, 2.5515, 2.6303, 2.6983, 2.7461, 2.7499, 2.7055, 2.6363, 2.5458, 2.4533, 2.3782, 2.3332, 2.3028, 2.2830,
0:         2.2767, 2.2770, 2.2837, 2.3048, 2.5719, 2.6130, 2.6792, 2.7402, 2.7793, 2.7715, 2.7419], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3218,  0.2559,  0.1361, -0.0616, -0.1663, -0.2097, -0.3448, -0.4654, -0.5554, -0.6091, -0.5974, -0.5724,
0:         -0.4521, -0.2462, -0.1511, -0.1325, -0.0676, -0.0385,  0.2294,  0.1437,  0.0636, -0.1046, -0.2064, -0.2719,
0:         -0.4635], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.6395, 0.6389, 0.6097, 0.5480, 0.4782, 0.4117, 0.3618, 0.3360, 0.3407, 0.3758, 0.4313, 0.4833, 0.4990, 0.4582,
0:         0.3596, 0.2344, 0.1273, 0.0780, 0.1046, 0.1947, 0.3178, 0.4333, 0.5059, 0.5171, 0.4606], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1678, -0.2044, -0.1503, -0.0629,  0.0547,  0.1542,  0.2210,  0.2310,  0.2531, -0.0866, -0.1148, -0.0914,
0:         -0.0236,  0.0238,  0.0730,  0.0564,  0.0449, -0.0137,  0.0872,  0.1107,  0.1529,  0.1980,  0.1921,  0.1461,
0:          0.0413], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.05517268180847168; velocity_v: 0.0919634997844696; specific_humidity: 0.03228048235177994; velocity_z: 0.5044724345207214; temperature: 0.08244699984788895; total_precip: 0.530627965927124; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05666665732860565; velocity_v: 0.08871853351593018; specific_humidity: 0.037495147436857224; velocity_z: 0.5578513741493225; temperature: 0.09249353408813477; total_precip: 0.8536545038223267; 
0: epoch: 1 [1/5 (20%)]	Loss: 0.69214 : 0.22402 :: 0.13979 (2.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05957360193133354; velocity_v: 0.09339404106140137; specific_humidity: 0.041849538683891296; velocity_z: 0.593787431716919; temperature: 0.10283910483121872; total_precip: 1.1464825868606567; 
0: epoch: 1 [2/5 (40%)]	Loss: 1.14648 : 0.31310 :: 0.14965 (16.43 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06955230236053467; velocity_v: 0.10441901534795761; specific_humidity: 0.03421354293823242; velocity_z: 0.5800766348838806; temperature: 0.09174271672964096; total_precip: 0.6605599522590637; 
0: epoch: 1 [3/5 (60%)]	Loss: 0.66056 : 0.23148 :: 0.14512 (16.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07234028726816177; velocity_v: 0.11004496365785599; specific_humidity: 0.0413091741502285; velocity_z: 0.4506692886352539; temperature: 0.10385990142822266; total_precip: 0.36127087473869324; 
0: epoch: 1 [4/5 (80%)]	Loss: 0.36127 : 0.16199 :: 0.15983 (13.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [9.53674316e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.14440918e-05 3.33786011e-05 3.33786011e-05 3.62396240e-05
0:  4.95910645e-05 4.10079956e-05 5.05447388e-05 4.67300415e-05
0:  6.58035278e-05 7.43865967e-05 7.34329224e-05 8.96453857e-05
0:  9.44137573e-05 1.08718872e-04 7.53402710e-05 4.48226929e-05
0:  2.86102295e-05 1.90734863e-05 1.90734863e-05 1.43051147e-05
0:  9.53674316e-06 4.76837158e-06 3.81469727e-06 4.76837158e-06
0:  6.67572021e-06 2.47955322e-05 1.04904175e-05 4.76837158e-06
0:  8.58306885e-06 1.33514404e-05 1.04904175e-05 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  9.53674316e-06 5.72204590e-06 6.67572021e-06 1.33514404e-05
0:  5.72204590e-06 0.00000000e+00 1.90734863e-06 5.72204590e-06
0:  2.28881836e-05 5.53131104e-05 9.53674316e-05 1.04904175e-04
0:  2.38418579e-05 3.71932983e-05 9.15527344e-05 1.13487244e-04
0:  9.82284546e-05 1.23977661e-04 7.34329224e-05 2.67028809e-05
0:  6.67572021e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-06 6.67572021e-06
0:  4.76837158e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 2.57492065e-05 4.81605530e-04 1.39141083e-03
0:  2.23827362e-03 2.50911713e-03 2.98881531e-03 2.37464905e-03
0:  1.81293488e-03 1.63078308e-03 1.54685974e-03 1.62410736e-03
0:  2.55298615e-03 2.88009644e-03 3.02982330e-03 2.63404846e-03
0:  1.68800354e-03 1.04331959e-03 1.05190277e-03 1.50299072e-03
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.90734863e-06
0:  5.14984131e-05 3.71932983e-05 5.43594360e-05 9.25064087e-05
0:  1.18255615e-04 7.72476196e-05 5.43594360e-05 9.15527344e-05
0:  1.09672546e-04 9.34600830e-05 6.38961792e-05 3.33786011e-05
0:  4.00543213e-05 9.63211060e-05 9.63211060e-05 9.34600830e-05
0:  8.67843628e-05 5.05447388e-05 1.90734863e-05 1.62124634e-05
0:  1.33514404e-05 6.67572021e-06 2.86102295e-06 3.81469727e-06
0:  8.58306885e-06 7.62939453e-06 9.53674316e-06 4.76837158e-06
0:  1.33514404e-05 2.00271606e-05 4.76837158e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 2.86102295e-06 4.76837158e-06 1.33514404e-05
0:  4.76837158e-06 1.90734863e-06 1.90734863e-06 2.86102295e-06
0:  4.76837158e-06 7.62939453e-06 1.04904175e-05 1.62124634e-05
0:  2.00271606e-05 1.71661377e-05 2.76565552e-05 8.96453857e-05
0:  4.29153442e-05 2.86102295e-05 6.19888306e-05 9.72747803e-05
0:  1.27792358e-04 1.70707703e-04 1.23023987e-04 1.55448914e-04
0:  1.32560730e-04 1.00135803e-04 7.43865967e-05 1.90734863e-05
0:  2.86102295e-06 5.72204590e-06 3.81469727e-06 7.62939453e-06
0:  1.04904175e-05 5.72204590e-06 0.00000000e+00 0.00000000e+00
0:  1.90734863e-06 1.27792358e-04 4.60624695e-04 1.19209290e-03]
0: Target values (first 200):
0: [2.76565552e-05 4.95910645e-05 6.38961792e-05 7.15255737e-05
0:  6.10351562e-05 4.67300415e-05 4.29153442e-05 4.14848328e-05
0:  3.19480896e-05 1.95503235e-05 1.57356262e-05 1.95503235e-05
0:  4.86373901e-05 1.22070312e-04 1.81198120e-04 1.45435333e-04
0:  8.72612000e-05 1.45435333e-04 9.15527344e-05 5.76972961e-05
0:  4.05311584e-05 4.29153442e-06 0.00000000e+00 0.00000000e+00
0:  1.90734863e-06 1.85966492e-05 4.52995300e-05 7.82012939e-05
0:  1.15871429e-04 1.09672546e-04 6.86645508e-05 4.19616699e-05
0:  2.38418579e-05 1.09672546e-05 6.67572021e-06 1.23977661e-05
0:  7.62939453e-06 5.10215759e-05 7.67707825e-05 7.34329224e-05
0:  5.48362732e-05 3.52859497e-05 5.38825989e-05 3.05175781e-05
0:  2.38418579e-05 7.62939453e-05 2.19345093e-04 3.86238098e-04
0:  1.52111053e-04 9.20295715e-05 2.90870667e-05 2.86102295e-06
0:  1.33514404e-05 5.10215759e-05 1.25408173e-04 4.14848328e-05
0:  6.77108765e-05 1.09672546e-04 1.57833099e-04 1.76429749e-04
0:  1.12533569e-04 1.08718872e-04 1.41143799e-04 1.58786774e-04
0:  1.09672546e-04 3.86238098e-05 1.76429749e-05 8.58306885e-06
0:  6.15119934e-05 3.52859497e-05 1.14440918e-05 2.86102295e-06
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.90734863e-05
0:  1.21116638e-04 1.65462494e-04 2.77519226e-04 5.75542450e-04
0:  1.90734863e-04 8.53538513e-05 5.48362732e-05 1.85966492e-04
0:  2.44617462e-04 1.05857849e-04 1.43527985e-04 1.24931335e-04
0:  5.35488129e-04 6.40392303e-04 6.48021698e-04 7.28607178e-04
0:  6.94274902e-04 2.23636627e-04 2.85625458e-04 5.44547976e-04
0:  1.01566315e-03 1.37376785e-03 1.38187408e-03 1.60884857e-03
0:  1.70040131e-03 1.26647949e-03 9.59396362e-04 7.28130341e-04
0:  6.91413879e-04 1.42002106e-03 1.98888779e-03 1.38187408e-03
0:  7.62939453e-06 1.14440918e-05 1.76429749e-05 4.43458557e-05
0:  6.91413879e-05 8.58306885e-05 6.96182251e-05 4.52995300e-05
0:  2.76565552e-05 1.52587891e-05 1.38282776e-05 9.05990601e-06
0:  1.66893005e-05 4.81605530e-05 1.22070312e-04 2.49385834e-04
0:  1.58786774e-04 1.25885010e-04 9.58442688e-05 6.34193420e-05
0:  4.86373901e-05 4.38690186e-05 2.38418579e-06 0.00000000e+00
0:  4.76837158e-07 4.76837158e-07 7.15255737e-06 7.15255737e-06
0:  1.19209290e-05 1.14440918e-05 1.14440918e-05 3.33786011e-06
0:  3.33786011e-06 4.76837158e-07 1.43051147e-06 7.15255737e-06
0:  4.24385071e-05 5.29289246e-05 5.43594360e-05 3.71932983e-05
0:  2.33650208e-05 5.10215759e-05 1.00135803e-05 6.67572021e-06
0:  1.85966492e-05 1.24454498e-04 2.08377838e-04 7.05718994e-05
0:  6.48498535e-05 3.14712524e-05 5.72204590e-06 4.76837158e-07
0:  2.19345093e-05 6.24656677e-05 3.19480896e-05 3.05175781e-05
0:  6.00814819e-05 8.63075256e-05 1.01089478e-04 1.14917755e-04
0:  1.58786774e-04 2.05993652e-04 2.10285187e-04 2.19345093e-04
0:  1.65939331e-04 7.82012939e-05 3.76701355e-05 5.38825989e-05
0:  2.57492065e-05 2.09808350e-05 2.71797180e-05 2.38418579e-05
0:  6.19888306e-06 1.90734863e-06 0.00000000e+00 0.00000000e+00
0:  2.00271606e-05 2.62260437e-05 1.01566315e-04 1.99794769e-04
0:  2.30789185e-04 7.58171082e-05 3.52859497e-05 1.21116638e-04
0:  2.29835510e-04 2.52723694e-04 1.51634216e-04 1.26838684e-04
0:  4.72068787e-04 5.42640628e-04 5.25474490e-04 5.36918582e-04]
0: Prediction values (first 20):
0: [2.453875   2.529262   2.5435567  2.477776   2.385374   2.2518082
0:  2.1297479  1.9901528  1.8227305  1.6610918  1.5248308  1.3692417
0:  1.27425    1.1991     1.0971131  0.98185587 0.8550358  0.7325554
0:  0.28012896 0.3060918 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.373, max = 0.672, mean = -0.420
0:          sample (first 20): tensor([-0.3827, -0.3765, -0.3754, -0.3807, -0.3882, -0.3991, -0.4090, -0.4203, -0.4339, -0.4470, -0.4581, -0.4707,
0:         -0.4784, -0.4845, -0.4928, -0.5022, -0.5125, -0.5224, -0.3355, -0.3330])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1646988 3.0173993 2.9128742 2.7872715 2.6958709 2.63562   2.6697907
0:  2.7099328 2.8033261 2.8901715 3.0058994 3.1275864 3.268578  3.4667015
0:  3.6669972 3.8235157 3.904892  3.9042776 3.7774875 3.7853417]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.8295903   0.89501286  0.9968519   1.0584979   1.0781937   1.0515628
0:   0.99759626  0.92214155  0.8085799   0.6796298   0.5666542   0.38869286
0:   0.23579168  0.10090923 -0.06108093 -0.24145699 -0.49977112 -0.8052707
0:  -2.3854465  -2.8197021 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4646196 4.6821465 4.9108276 5.1037264 5.271629  5.420165  5.58894
0:  5.674309  5.7475796 5.8116283 5.861517  5.913316  5.9779215 6.0689664
0:  6.1674457 6.2630606 6.401572  6.562807  6.7391663 7.003846 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.98525   -5.3288293 -5.677322  -6.0994134 -6.5327706 -6.912727
0:  -7.090038  -7.247749  -7.2810197 -7.305035  -7.4046674 -7.505939
0:  -7.6039567 -7.6186566 -7.590696  -7.5221906 -7.4594293 -7.377897
0:  -7.352117  -7.204412 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2953653 5.332449  5.3773184 5.4256153 5.4541388 5.4076724 5.290987
0:  5.080471  4.807586  4.529343  4.2651787 3.9506602 3.701865  3.5039597
0:  3.293275  3.139492  2.996279  2.8249485 1.8584447 1.6954584]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5413737 -5.357471  -5.201095  -5.0522943 -4.8692365 -4.6741138
0:  -4.336797  -3.9778533 -3.5818992 -3.252871  -3.025004  -2.8988872
0:  -2.8910518 -2.9069314 -2.9583259 -3.0361762 -3.1074452 -3.244573
0:  -4.98719   -5.134954 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.455945  10.366308  10.345428  10.261506  10.122025   9.947287
0:   9.704064   9.430093   9.180496   8.941731   8.809394   8.7053175
0:   8.647813   8.631368   8.607835   8.508077   8.400699   8.292573
0:   7.7988095  7.670294 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8010616 2.093093  2.3654585 2.612179  2.830381  3.0103188 3.1892824
0:  3.321256  3.4328396 3.5493736 3.6704047 3.7523024 3.876145  4.0033975
0:  4.136652  4.3259287 4.567133  4.8366203 4.8500595 5.147688 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.2360153 6.092677  6.1805973 6.3274775 6.4234014 6.524627  6.6977215
0:  6.70314   6.8182287 6.8181067 6.7248354 6.6381536 6.6491985 6.717923
0:  6.873925  6.9855413 7.041139  7.1146927 6.4826217 6.343489 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.03     33.74963  33.589542 33.53792  33.656544 33.905357 34.263916
0:  34.4379   34.76632  35.0416   35.278725 35.37502  35.355595 35.212418
0:  35.02059  34.847576 34.787758 34.715534 35.639076 35.663033]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.51718  17.757744 17.984001 18.149628 18.307812 18.46121  18.624388
0:  18.74304  18.857431 18.981434 19.105686 19.144215 19.129143 19.035921
0:  18.834269 18.639565 18.522245 18.506996 18.506264 18.659985]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.041214  4.0526667 4.0738316 4.0577335 4.0084295 3.9104633 3.791148
0:  3.6317198 3.4194746 3.186334  2.9359965 2.6622853 2.4342098 2.215054
0:  2.0195956 1.8313651 1.6495852 1.4604254 0.7875271 0.7206192]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.6159725 8.684677  8.780296  8.856881  8.925093  9.004286  9.119679
0:  9.184399  9.2506485 9.308516  9.342323  9.342797  9.337412  9.299417
0:  9.24931   9.217065  9.224422  9.277623  8.930117  9.044496 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.188862  11.211339  11.2217655 11.228979  11.26321   11.338112
0:  11.4793    11.531457  11.589453  11.626532  11.656158  11.69368
0:  11.738778  11.786747  11.819565  11.839504  11.933238  12.11381
0:  12.556711  12.785379 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.83221  33.88202  33.988533 34.16368  34.325546 34.519787 34.872818
0:  34.998894 35.28644  35.689724 36.17376  36.86455  37.796974 38.907936
0:  40.186855 41.561256 42.845646 43.896236 45.099106 46.0703  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.660309  5.7831793 5.9554753 6.0571823 6.125063  6.148059  6.138159
0:  6.089646  6.03953   6.0236506 6.0338955 5.995661  5.957735  5.895865
0:  5.7527676 5.6281586 5.5280704 5.435912  4.8646183 4.838976 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.853204  -9.871861  -9.890067  -9.878192  -9.892734  -9.964597
0:  -10.076261 -10.246343 -10.425988 -10.554423 -10.603726 -10.688573
0:  -10.670535 -10.615268 -10.567748 -10.457818 -10.336869 -10.195397
0:  -11.096591 -10.755113]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.4714966  5.3372555  5.218521   5.131651   5.051092   4.951838
0:   4.822728   4.627573   4.335267   3.997254   3.6495097  3.2536812
0:   2.9280462  2.6388783  2.2975607  1.9638329  1.5941029  1.1915116
0:  -0.5571027 -1.1409097]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.100407  5.435874  5.8249674 6.2378583 6.6285577 6.9761014 7.3218575
0:  7.5979724 7.82792   8.032976  8.20966   8.321135  8.435421  8.538661
0:  8.619512  8.707156  8.800201  8.897485  8.050318  8.157478 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2922025 5.45185   5.562173  5.6135035 5.640003  5.6782103 5.761561
0:  5.851892  5.9358425 5.9763627 6.015693  5.949521  5.8988867 5.8296747
0:  5.6945953 5.5737543 5.4417715 5.315188  4.301616  4.421731 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.244424   5.655145   6.1509023  6.644231   7.12011    7.5628796
0:   8.022427   8.397343   8.770668   9.147764   9.551498   9.958931
0:  10.38426   10.784239  11.154249  11.502419  11.854501  12.195795
0:  13.381769  13.820545 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.24494  38.026844 37.81105  37.59348  37.43405  37.32723  37.226818
0:  36.97575  36.799503 36.55289  36.173    35.75575  35.269035 34.81493
0:  34.41984  34.10301  33.900078 33.69617  35.503487 35.564854]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.1930895  -9.283866   -9.318517   -9.298499   -9.211713   -9.151404
0:   -9.06107    -9.089634   -9.225461   -9.409011   -9.624536   -9.838473
0:   -9.962417   -9.991216   -9.973054   -9.956002   -9.992459  -10.104146
0:  -11.179957  -11.414445 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.347029 11.303124 11.331657 11.4343   11.580948 11.745756 11.882982
0:  11.929583 11.90854  11.821297 11.714645 11.545437 11.389788 11.223127
0:  11.053847 10.931107 10.915533 10.978186 10.604946 10.519446]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.20471525 -0.18674278 -0.07111931  0.19068098  0.6198468   1.1070461
0:   1.5825357   1.8850722   2.1105933   2.1479707   2.171209    2.185738
0:   2.259138    2.3537986   2.4992898   2.603108    2.7884753   3.0251372
0:   3.1584566   3.279589  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.225301 15.93115  16.822058 17.806576 18.899086 19.944891 20.951632
0:  21.56937  22.107946 22.5471   23.035927 23.689522 24.420246 25.186012
0:  25.901737 26.541412 27.17572  27.865597 30.92397  31.539093]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5484    6.6300883 6.7770805 6.9738283 7.181157  7.381614  7.5783296
0:  7.752542  7.9012737 8.055986  8.201298  8.264142  8.32427   8.3479395
0:  8.3125105 8.293928  8.293921  8.296856  7.506284  7.4179044]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8670692 -6.7258196 -6.561493  -6.451048  -6.4184303 -6.487403
0:  -6.6333647 -6.892645  -7.152089  -7.396921  -7.5466623 -7.6255007
0:  -7.541477  -7.3398123 -7.1498203 -7.0091133 -6.9933167 -7.1181087
0:  -8.690834  -8.844237 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8512125 6.902887  7.0148926 7.1242123 7.191335  7.217604  7.2348766
0:  7.198743  7.151438  7.126066  7.1017685 7.041247  6.9857407 6.925193
0:  6.839989  6.7553205 6.684032  6.609772  6.401594  6.3450274]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.973743 27.06461  27.15218  27.263298 27.442778 27.746975 28.162195
0:  28.43754  28.69567  28.895586 29.016306 29.141037 29.297426 29.476557
0:  29.71977  29.974314 30.300035 30.640942 30.628178 30.875027]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [41.928448 41.911293 41.824696 41.686794 41.595814 41.58659  41.668674
0:  41.58807  41.49167  41.300022 41.031033 40.768124 40.49664  40.294815
0:  40.11199  39.951786 39.822475 39.660362 39.564213 39.400032]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.801727 -13.03393  -13.291063 -13.507906 -13.794191 -14.133537
0:  -14.542042 -15.056808 -15.604788 -16.145454 -16.657227 -17.180521
0:  -17.52715  -17.765608 -17.808538 -17.767454 -17.696188 -17.5675
0:  -15.556917 -15.344255]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.915447 30.930363 30.874153 30.65503  30.43996  30.250933 30.116886
0:  29.749529 29.464499 28.992514 28.479193 27.927948 27.424702 27.005983
0:  26.626451 26.332367 26.142462 25.965101 26.218279 26.074568]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.6748402 3.6675773 3.6942694 3.705177  3.6661189 3.5642753 3.4376965
0:  3.2531145 3.0627384 2.8915558 2.744583  2.5944073 2.489218  2.3927846
0:  2.294418  2.206173  2.1206193 2.0511436 1.5370936 1.5428348]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.923693 13.021346 13.225476 13.463854 13.769464 14.11002  14.452131
0:  14.752476 15.030104 15.289713 15.53475  15.728566 15.905667 16.04285
0:  16.129604 16.197853 16.308466 16.428009 16.12405  16.039331]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.699025  -9.542848  -9.291284  -9.022845  -8.760605  -8.552328
0:  -8.340678  -8.188986  -8.049761  -7.8888817 -7.7144413 -7.561036
0:  -7.3393736 -7.0701694 -6.7778063 -6.469739  -6.1880875 -5.9784994
0:  -6.243082  -6.034069 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5676303  -0.5533767  -0.3019104   0.08976412  0.48349857  0.71049213
0:   0.7115183   0.31863594 -0.2951045  -1.1015887  -1.9447799  -2.7559152
0:  -3.3891644  -3.840177   -4.1786523  -4.380549   -4.496987   -4.4620433
0:  -4.9227567  -4.7854095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.387516 18.15815  18.05222  17.960148 17.972048 18.040724 18.27636
0:  18.447369 18.697956 18.917385 19.00027  18.888039 18.622066 18.220407
0:  17.622505 16.94733  16.244938 15.563464 13.784991 13.285842]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.717524 20.590767 20.553673 20.528244 20.602842 20.773098 20.927473
0:  20.969885 20.97113  20.891865 20.829247 20.749071 20.647587 20.542519
0:  20.401096 20.217064 20.081638 20.032541 20.33308  20.2305  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.28716  18.485516 18.77923  19.151028 19.574076 20.028961 20.493353
0:  20.829556 21.13131  21.368042 21.570518 21.729746 21.898745 22.017944
0:  22.06056  22.065847 22.094326 22.175976 21.978485 22.089441]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.965708  10.316901  10.643964  10.942215  11.2709675 11.649254
0:  12.095337  12.491299  12.882427  13.268488  13.624397  13.94855
0:  14.21035   14.423346  14.580788  14.751371  15.043776  15.469944
0:  15.58008   16.024378 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.841099 31.422596 30.985424 30.600922 30.344463 30.250572 30.225193
0:  30.216915 30.228428 30.24942  30.272583 30.200977 30.064484 29.764805
0:  29.263344 28.69408  28.187141 27.79329  27.22113  26.7462  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.04878   -8.943602  -8.725908  -8.47677   -8.225424  -8.006224
0:  -7.7859445 -7.610973  -7.4835887 -7.386454  -7.305592  -7.269008
0:  -7.191524  -7.068543  -6.91386   -6.728268  -6.5528617 -6.413757
0:  -6.7992215 -6.6288495]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.584471 -10.501589 -10.42417  -10.321905 -10.220808 -10.190792
0:  -10.133524 -10.142405 -10.183838 -10.161509 -10.139153 -10.17523
0:  -10.167889 -10.174858 -10.194647 -10.189463 -10.215786 -10.233648
0:  -11.361919 -11.426249]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8136158  -0.626245   -0.47872257 -0.39809227 -0.38764286 -0.46949053
0:  -0.5980339  -0.768054   -0.97633886 -1.1472898  -1.2526035  -1.3829842
0:  -1.4030128  -1.380578   -1.3625126  -1.3048282  -1.2587466  -1.2633944
0:  -2.137464   -2.1284175 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.559381   -1.5617356  -1.5732179  -1.5755992  -1.5718198  -1.5709605
0:  -1.5249367  -1.46802    -1.3910522  -1.2698889  -1.1155558  -0.9754348
0:  -0.793715   -0.6147747  -0.47501326 -0.35386324 -0.27475262 -0.25851917
0:  -0.88359165 -0.96471596]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.274874  -11.119131  -10.956364  -10.815384  -10.699427  -10.611118
0:  -10.462357  -10.230919   -9.838331   -9.259777   -8.473707   -7.6591563
0:   -6.731787   -5.8442473  -5.157745   -4.6347804  -4.336786   -4.245213
0:   -6.014606   -6.342321 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.0952125 6.2769656 6.4860797 6.71028   6.9428997 7.172311  7.3913674
0:  7.527877  7.612927  7.6148276 7.551981  7.3493094 7.1160693 6.833374
0:  6.4832044 6.1497107 5.8698926 5.643892  5.707878  5.7510195]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.518289  -8.298985  -8.082738  -7.862378  -7.66603   -7.51031
0:  -7.336934  -7.1804714 -7.010871  -6.782361  -6.517461  -6.289687
0:  -6.00191   -5.690529  -5.382126  -5.03656   -4.713524  -4.4673014
0:  -4.651862  -4.2984123]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.4211235  9.23561    8.9683075  8.599784   8.287619   8.084484
0:   8.065058   8.183628   8.47982    8.96299    9.592225  10.464059
0:  11.553717  12.759823  13.944197  15.059286  15.994911  16.662964
0:  16.096144  16.371368 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.573101  8.604867  8.624765  8.636488  8.660255  8.695223  8.798916
0:  8.8253765 8.885413  8.937856  9.001871  9.084014  9.202031  9.3359785
0:  9.463598  9.57146   9.691605  9.822605  9.854698  9.811585 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.89655   8.068492  8.2560005 8.456015  8.677651  8.9136715 9.133465
0:  9.217735  9.251786  9.175743  9.107187  9.013034  8.972845  8.975679
0:  8.973799  8.94812   8.933915  9.036915  9.078456  9.071632 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0998468  -0.61540556  0.11458254  0.96830416  1.8225002   2.5971656
0:   3.2219608   3.7549636   4.2122765   4.568058    4.8822145   5.0126677
0:   5.039051    5.041708    5.01661     5.0616503   5.1609983   5.2740307
0:   5.698353    5.332041  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.0247345 11.156366  11.268296  11.301549  11.296675  11.279636
0:  11.290016  11.217879  11.126883  10.997255  10.828983  10.622297
0:  10.435655  10.252642  10.079111   9.920042   9.817285   9.766348
0:   9.642836   9.59207  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5271063 -5.448574  -5.34956   -5.275738  -5.2634363 -5.3472786
0:  -5.4616423 -5.645775  -5.8644695 -6.058049  -6.2199793 -6.4364424
0:  -6.5741625 -6.6460476 -6.7044554 -6.6614494 -6.599024  -6.5488873
0:  -7.0920515 -7.0040455]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.555925  -4.6410365 -4.6611934 -4.5959506 -4.498302  -4.4090056
0:  -4.31125   -4.2518954 -4.248631  -4.285622  -4.3329015 -4.499679
0:  -4.6206365 -4.749618  -4.9083514 -5.040608  -5.2093883 -5.4303317
0:  -6.8547015 -7.231919 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.474139   7.553476   7.7735715  8.117979   8.611748   9.193519
0:   9.789709  10.266737  10.621664  10.819487  10.907507  10.942558
0:  10.95528   10.953825  10.972647  10.948837  10.981818  11.01598
0:  10.582306  10.466409 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1564817  -1.0017571  -0.85015917 -0.7072573  -0.5955677  -0.53448343
0:  -0.4507289  -0.41212416 -0.36657238 -0.3071556  -0.2514248  -0.2613635
0:  -0.25707436 -0.24969721 -0.28788042 -0.2896266  -0.28925753 -0.29522943
0:  -1.2566509  -1.3103123 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.006994  -6.9968104 -6.960338  -6.904592  -6.8716645 -6.8874364
0:  -6.9332566 -7.0291705 -7.165796  -7.2848716 -7.368603  -7.4983573
0:  -7.5348563 -7.5308757 -7.511852  -7.436746  -7.3441186 -7.273585
0:  -7.8189282 -7.834908 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.5982504 4.6477165 4.607256  4.519182  4.4258795 4.3118525 4.2381535
0:  4.100997  3.9998798 3.887498  3.7566507 3.6189356 3.475596  3.3375196
0:  3.190852  3.1112018 3.1599922 3.3293123 3.0594983 3.1774788]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.703784   -2.4200573  -1.9578638  -1.360857   -0.5820565   0.34380388
0:   1.481255    2.6310802   3.819827    5.0169487   6.1749926   7.312605
0:   8.431074    9.481482   10.474256   11.327362   12.036518   12.609397
0:  12.614227   13.088818  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.689493  -11.79661   -11.8618965 -11.889584  -11.927029  -12.032858
0:  -12.190325  -12.3933525 -12.558281  -12.593792  -12.478027  -12.297178
0:  -11.967632  -11.584002  -11.2275505 -10.880978  -10.605186  -10.434446
0:  -10.544859  -10.373262 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.32736  11.126982 11.064886 11.035219 11.165091 11.340658 11.633369
0:  11.836763 12.07309  12.22948  12.384798 12.561413 12.725496 12.977251
0:  13.174303 13.357856 13.491373 13.602961 13.60814  13.629268]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.486823 8.443331 8.497427 8.557749 8.648679 8.714166 8.822428 8.86275
0:  8.901713 8.960089 9.013547 9.113778 9.246796 9.438331 9.591349 9.70592
0:  9.786016 9.818319 8.772578 8.631126]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.745585  13.821516  13.954715  14.0773735 14.206512  14.335447
0:  14.48058   14.529369  14.570983  14.557102  14.525156  14.466577
0:  14.415016  14.3719635 14.319336  14.27627   14.265835  14.316851
0:  14.185958  14.266559 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8130412  -0.6987939  -0.6019826  -0.4730773  -0.31434727 -0.16348553
0:   0.01560545  0.16846895  0.31783867  0.4715705   0.6399112   0.7422004
0:   0.8783922   1.0026655   1.0898442   1.1946826   1.3213964   1.4380317
0:   1.0824442   1.4706984 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.636314  -7.5329347 -7.4110503 -7.2109056 -6.99009   -6.751387
0:  -6.409474  -6.1311946 -5.845058  -5.5252023 -5.1663632 -4.8259625
0:  -4.426268  -4.01728   -3.6269364 -3.2338138 -2.850288  -2.4902225
0:  -2.3705173 -1.9335952]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.0465565 5.8669205 5.699251  5.470563  5.220497  4.932887  4.6900845
0:  4.4169397 4.120487  3.8473096 3.521641  3.1885564 2.8364704 2.5236063
0:  2.2535872 2.006916  1.7544212 1.4996157 1.381609  1.2262325]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.74563   -11.872789  -12.152386  -12.537821  -13.016523  -13.584028
0:  -14.064144  -14.5433655 -14.877277  -15.035013  -14.991913  -14.850332
0:  -14.52083   -14.009283  -13.465641  -12.834805  -12.296139  -11.883377
0:  -12.83346   -12.509455 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [41.441414 41.223198 40.966354 40.77489  40.718803 40.792282 40.993874
0:  40.9915   41.040154 40.96617  40.855614 40.75731  40.613094 40.53346
0:  40.314117 40.10248  39.92003  39.70564  39.62203  39.531345]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7717714  -4.137226   -3.2511673  -2.071167   -0.92148876  0.01127195
0:   0.62183046  0.8185234   0.72626543  0.45361662  0.10458326 -0.33654547
0:  -0.69241905 -0.97334385 -1.1746616  -1.1984844  -1.1673775  -1.1745672
0:  -1.6916723  -1.5572157 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.044757  -2.072525  -1.9892683 -1.9303932 -1.8617    -1.8023653
0:  -1.7243118 -1.6899757 -1.6599197 -1.6617012 -1.6731424 -1.7316389
0:  -1.7506189 -1.7720127 -1.7938032 -1.8713408 -1.9516869 -2.0005836
0:  -2.1792116 -2.2125912]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.3870311  1.3965664  1.4019747  1.3768492  1.343823   1.2754898
0:  1.2235379  1.139492   1.0551171  0.97461176 0.9049864  0.8293624
0:  0.79870796 0.7945199  0.7955775  0.8220515  0.85405445 0.87552834
0:  0.56267166 0.6105814 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.90938   5.014134  5.1357956 5.2585573 5.353762  5.405126  5.448485
0:  5.4743147 5.4953537 5.5303583 5.5815997 5.5878825 5.6108136 5.608871
0:  5.5909715 5.58191   5.5622907 5.527255  4.7453775 4.755133 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.127408   8.205782   8.399036   8.640836   8.9382     9.267753
0:   9.601114   9.833166  10.025099  10.142157  10.246769  10.338825
0:  10.479677  10.62358   10.778934  10.902918  11.032305  11.15524
0:  11.113766  11.2638855]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.74265  26.672947 26.738918 26.989449 27.539768 28.27502  29.066254
0:  29.630173 29.986732 29.965488 29.62994  29.046314 28.307907 27.514286
0:  26.763134 26.162724 25.829395 25.803001 27.087086 27.215878]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.363287 21.505453 21.970047 22.647434 23.505432 24.39684  25.420753
0:  26.202747 26.920639 27.577976 28.095165 28.708748 29.299335 29.899979
0:  30.471931 30.952656 31.422466 31.864109 31.501724 31.5223  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.103653  9.07863   9.097416  9.118986  9.157863  9.191364  9.231186
0:  9.196349  9.166756  9.115308  9.050072  8.963066  8.874782  8.760239
0:  8.656532  8.560447  8.535442  8.5709915 8.336264  8.3596   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.414114  9.398602  9.395561  9.320003  9.163784  8.900852  8.535751
0:  8.066992  7.594094  7.1597133 6.8492475 6.626778  6.5624676 6.626631
0:  6.765523  6.9891996 7.294778  7.6458316 8.14599   8.499772 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.64979124 -0.44409847 -0.19913292  0.04429483  0.24836874  0.39156103
0:   0.5323796   0.63932085  0.7488289   0.8872633   1.0465255   1.1629405
0:   1.3114033   1.4258728   1.4858909   1.5099726   1.4689012   1.3746114
0:   0.24887705  0.02483368]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3665571 -2.5454473 -2.561984  -2.513898  -2.4660492 -2.4332027
0:  -2.3492584 -2.362112  -2.3842802 -2.4269676 -2.513095  -2.570897
0:  -2.554636  -2.4364424 -2.2366352 -2.0606565 -1.9011593 -1.7598696
0:  -1.1953273 -1.0472836]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.392523 10.638859 10.934522 11.19301  11.431332 11.644066 11.830212
0:  11.972013 12.084394 12.173454 12.269958 12.294029 12.330449 12.350681
0:  12.332689 12.338501 12.371878 12.39576  12.013529 12.079686]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1579409 2.341217  2.6216908 2.8962898 3.1325376 3.3454363 3.462242
0:  3.555255  3.6578183 3.804079  3.981479  4.1235704 4.3437634 4.5363913
0:  4.737994  4.9266033 5.100495  5.2777762 4.9577227 5.2524624]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1125321   0.89767027  0.6369357   0.27935028 -0.19039631 -0.7432518
0:  -1.2712917  -1.7732358  -2.1298862  -2.3577838  -2.4457164  -2.5869155
0:  -2.721766   -2.9248128  -3.250896   -3.5432477  -3.797789   -3.952661
0:  -4.1580157  -3.8795304 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.876667  11.28067   10.643848   9.985024   9.411936   8.999509
0:   8.835921   8.851879   9.112214   9.57873   10.169343  10.787548
0:  11.336163  11.702742  11.8480015 11.76712   11.551181  11.224764
0:  10.022947   9.623008 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.599041 16.720928 16.860744 16.934837 17.012749 17.100697 17.230335
0:  17.287949 17.381931 17.462927 17.5748   17.688175 17.8763   18.031559
0:  18.156628 18.241856 18.315075 18.338242 17.737118 17.713026]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.207844   -7.9412107  -7.652769   -7.406149   -7.260516   -7.266611
0:   -7.468937   -7.876485   -8.422821   -8.996064   -9.486619   -9.936781
0:  -10.178884  -10.2752695 -10.296863  -10.21095   -10.102906   -9.992989
0:  -10.639407  -10.015136 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.796771   -5.648836   -5.4438267  -5.233984   -5.061863   -4.974743
0:  -4.864214   -4.77182    -4.401206   -3.7558737  -2.7344952  -1.5511107
0:  -0.24940729  0.9550266   1.7985353   2.348516    2.6366649   2.6915858
0:   2.725877    3.2594826 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.554472   -3.113965   -2.6329393  -2.1810102  -1.771904   -1.3986359
0:  -1.0180569  -0.69482327 -0.38890314 -0.07050133  0.25488663  0.5323229
0:   0.88028     1.2517667   1.6860371   2.1304946   2.526781    2.789443
0:   2.4810586   2.8799381 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.220314 34.360252 34.441525 34.41217  34.364117 34.29957  34.221577
0:  33.98102  33.6965   33.31811  32.839725 32.286625 31.727064 31.136665
0:  30.492561 29.881983 29.389742 28.985191 29.029411 28.872099]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.77588844 -0.70050764 -0.619226   -0.5673957  -0.57775784 -0.67952347
0:  -0.8345041  -1.0601544  -1.3172708  -1.5665402  -1.7789946  -1.9953465
0:  -2.13943    -2.2322235  -2.2942204  -2.3025193  -2.2804618  -2.2575793
0:  -2.5908828  -2.461657  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.026209 18.17804  18.146534 17.966948 17.837511 17.800484 17.924076
0:  17.954372 17.924767 17.82214  17.630743 17.502686 17.458168 17.570673
0:  17.765701 18.037853 18.417025 18.854118 19.725027 20.084846]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7640529  -2.4453368  -2.0857282  -1.7239995  -1.4047809  -1.1598163
0:  -0.92265606 -0.7522416  -0.5959015  -0.44248343 -0.28970098 -0.21696901
0:  -0.15698338 -0.13666344 -0.19728994 -0.2560215  -0.32915497 -0.41149044
0:  -1.0976024  -1.0852017 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.959904 31.142862 31.278559 31.361267 31.53303  31.816944 32.241512
0:  32.566593 32.879604 33.113064 33.268494 33.399082 33.508118 33.63476
0:  33.747765 33.88591  34.100407 34.295437 34.317272 34.558517]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.7269187  -0.50431776 -0.23259735  0.01437855  0.23635054  0.42121744
0:   0.6109419   0.734354    0.8263488   0.8835149   0.9253144   0.906837
0:   0.9458394   1.0528846   1.1529245   1.2923336   1.379713    1.3690977
0:   1.0345421   0.7917676 ]
0: validation loss for strategy=forecast at epoch 1 : 4.345968723297119
0: validation loss for velocity_u : 0.030481046065688133
0: validation loss for velocity_v : 0.05960020795464516
0: validation loss for specific_humidity : 0.02332594059407711
0: validation loss for velocity_z : 0.4667727053165436
0: validation loss for temperature : 0.08461612462997437
0: validation loss for total_precip : 25.41102409362793
0: 2 : 18:05:50 :: batch_size = 96, lr = 2e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 2, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5594,  0.4995,  0.4695,  0.4495,  0.4223,  0.3831,  0.3352,  0.2817,  0.2276,  0.1747,  0.1229,  0.0735,
0:          0.0254, -0.0226, -0.0685, -0.1117, -0.1511, -0.1839,  0.4712,  0.4380,  0.4185,  0.3921,  0.3499,  0.2965,
0:          0.2387], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.7732, 1.6634, 1.5360, 1.4161, 1.3219, 1.2476, 1.1909, 1.1530, 1.1281, 1.1106, 1.0959, 1.0790, 1.0572, 1.0290,
0:         0.9944, 0.9540, 0.9091, 0.8618, 1.6573, 1.5331, 1.4153, 1.3176, 1.2415, 1.1807, 1.1385], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0284, -0.0680, -0.1172, -0.1647, -0.2192, -0.2760, -0.3297, -0.3647, -0.3861, -0.3946, -0.3956, -0.3990,
0:         -0.4045, -0.4123, -0.4242, -0.4395, -0.4544, -0.4678, -0.0499, -0.1136, -0.1558, -0.2059, -0.2587, -0.3078,
0:         -0.3509], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2110,  0.0926, -0.1333, -0.3914, -0.5165, -0.5619, -0.6793, -0.7967, -0.8387, -0.8133, -0.6948, -0.5519,
0:         -0.4788, -0.4622, -0.5076, -0.5984, -0.6582, -0.6892,  0.1169, -0.0957, -0.3282, -0.4833, -0.5818, -0.6660,
0:         -0.7402], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.1149, 0.1754, 0.2650, 0.3786, 0.5090, 0.6429, 0.7542, 0.8318, 0.8782, 0.9025, 0.9215, 0.9450, 0.9700, 0.9990,
0:         1.0284, 1.0502, 1.0665, 1.0764, 1.0736, 1.0614, 1.0406, 1.0140, 0.9873, 0.9657, 0.9522], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1843, -0.2353, -0.2424, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2270, -0.2436, -0.2436,
0:         -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436,
0:         -0.2436], device='cuda:0')
0: [DEBUG] Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2009,     nan,     nan,     nan, -0.1523,     nan,
0:             nan, -0.1926,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2211,
0:             nan,     nan, -0.1724,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2424,     nan,     nan, -0.2412,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1914,     nan, -0.1973,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2401,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2412,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2033,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2401,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2412,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2424,     nan, -0.2436,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2436,     nan,     nan, -0.2424,     nan,     nan, -0.2282,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 2, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0286, -0.0208, -0.0116,  0.0004,  0.0087,  0.0140,  0.0177,  0.0166,  0.0168,  0.0137,  0.0068, -0.0095,
0:         -0.0306, -0.0560, -0.0862, -0.1185, -0.1506, -0.1792, -0.0741, -0.0666, -0.0568, -0.0464, -0.0404, -0.0393,
0:         -0.0408], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.5708, 1.5487, 1.5166, 1.4694, 1.4129, 1.3484, 1.2835, 1.2139, 1.1450, 1.0714, 0.9917, 0.9135, 0.8494, 0.8028,
0:         0.7755, 0.7617, 0.7509, 0.7321, 1.5343, 1.5143, 1.4832, 1.4472, 1.4008, 1.3494, 1.2909], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0334,  0.0187, -0.0007, -0.0172, -0.0327, -0.0447, -0.0561, -0.0660, -0.0803, -0.0958, -0.1141, -0.1345,
0:         -0.1592, -0.1859, -0.2122, -0.2347, -0.2548, -0.2668,  0.0008, -0.0144, -0.0310, -0.0456, -0.0554, -0.0645,
0:         -0.0707], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5062, -0.4781, -0.4281, -0.4080, -0.4161, -0.4215, -0.4235, -0.4795, -0.4986, -0.4626, -0.4234, -0.3499,
0:         -0.3592, -0.4508, -0.5116, -0.5474, -0.5952, -0.7679, -0.6060, -0.5565, -0.4261, -0.3490, -0.3611, -0.4095,
0:         -0.4370], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0932, -0.0775, -0.0536, -0.0285,  0.0028,  0.0375,  0.0756,  0.1142,  0.1485,  0.1786,  0.2060,  0.2310,
0:          0.2541,  0.2738,  0.2886,  0.3038,  0.3220,  0.3489,  0.3823,  0.4135,  0.4325,  0.4321,  0.4113,  0.3743,
0:          0.3281], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([2.6507, 2.5409, 2.3658, 2.2145, 2.0288, 1.8196, 1.6932, 1.4965, 1.3292, 2.6877, 2.6017, 2.4197, 2.1865, 1.9816,
0:         1.7593, 1.5844, 1.4554, 1.2427, 2.4743, 2.3774, 2.2174, 1.9786, 1.7206, 1.5302, 1.3181], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.06054362654685974; velocity_v: 0.09863059222698212; specific_humidity: 0.03347184881567955; velocity_z: 0.6155635118484497; temperature: 0.09550558775663376; total_precip: 22.43996238708496; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06255751848220825; velocity_v: 0.10038279742002487; specific_humidity: 0.04097561910748482; velocity_z: 0.4817650616168976; temperature: 0.09971823543310165; total_precip: 22.333797454833984; 
0: epoch: 2 [1/5 (20%)]	Loss: 22.38688 : 3.84683 :: 0.15235 (2.53 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.059477318078279495; velocity_v: 0.09530393034219742; specific_humidity: 0.037954263389110565; velocity_z: 0.49751946330070496; temperature: 0.09693963825702667; total_precip: 23.07343292236328; 
0: epoch: 2 [2/5 (40%)]	Loss: 23.07343 : 3.95096 :: 0.15270 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05231844261288643; velocity_v: 0.07904865592718124; specific_humidity: 0.03715558722615242; velocity_z: 0.46263203024864197; temperature: 0.10268538445234299; total_precip: 23.902278900146484; 
0: epoch: 2 [3/5 (60%)]	Loss: 23.90228 : 4.08136 :: 0.14596 (16.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05599464103579521; velocity_v: 0.08743954449892044; specific_humidity: 0.03743765503168106; velocity_z: 0.41970691084861755; temperature: 0.09530150145292282; total_precip: 23.01691246032715; 
0: epoch: 2 [4/5 (80%)]	Loss: 23.01691 : 3.92699 :: 0.14636 (16.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9073486e-06
0:  0.0000000e+00 9.5367432e-07 1.4305042e-06 4.7684443e-07 4.4393539e-04
0:  7.2145462e-04 1.1444092e-05 7.0095062e-05 2.3841858e-06 1.4305042e-06
0:  1.4305042e-06 0.0000000e+00 0.0000000e+00 4.7684443e-07 4.7684443e-07
0:  1.4305042e-06 2.7942657e-04 2.2597313e-03 7.3575974e-04 2.2077560e-04
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9182129e-05
0:  2.5177002e-04 2.5081635e-04 1.4305115e-05 4.6253204e-05 2.3841858e-06
0:  2.3841858e-06 3.2424927e-05 9.0599060e-05 5.1498417e-05 2.1457672e-05
0:  2.8133392e-05 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  2.3841858e-06 2.7179718e-05 1.5258789e-05 3.3378601e-06 4.7684443e-07
0:  3.8146973e-06 3.8146973e-06 2.8610229e-06 7.1525574e-06 6.8664551e-05
0:  1.3256073e-04 1.7757417e-03 4.2915344e-04 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 2.0027161e-05 8.5830688e-06 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 7.4386597e-05 9.2506409e-05 9.4413757e-05 7.9154968e-05
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  1.9073486e-06 2.8610229e-06 2.8610229e-06 1.9073486e-06 6.6757202e-06
0:  4.9591064e-05 4.3869022e-05 2.9563904e-05 2.5749207e-05 2.9563904e-05
0:  3.8146973e-05 4.7683716e-05 3.0517578e-05 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.5367432e-07
0:  9.5367432e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 4.9591064e-05 2.6702881e-05
0:  3.9672852e-04 3.9291382e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 1.9073486e-06 4.7683716e-06 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [4.101677   3.8783004  3.5142047  3.0103626  2.4863     2.0566354
0:  1.8179531  1.7715163  1.9001708  2.1122174  2.3546944  2.470467
0:  2.4857159  2.315712   1.9577193  1.4891367  0.98711205 0.5631137
0:  0.79151726 1.0399017 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.968, max = 1.992, mean = -0.147
0:          sample (first 20): tensor([-0.2217, -0.2406, -0.2714, -0.3140, -0.3584, -0.3947, -0.4149, -0.4189, -0.4080, -0.3900, -0.3695, -0.3597,
0:         -0.3584, -0.3728, -0.4031, -0.4427, -0.4852, -0.5211, -0.1911, -0.2061])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4886308 -3.4809804 -3.538342  -3.6542544 -3.7531247 -3.8713183
0:  -3.863052  -3.8386188 -3.749845  -3.6511388 -3.5498767 -3.4590707
0:  -3.304955  -3.0604968 -2.815308  -2.5600057 -2.365272  -2.2595096
0:  -1.4072042 -1.2405057]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.6304655 0.8758559 1.2445498 1.6281161 1.9962816 2.3191671 2.5654364
0:  2.718165  2.85855   3.0275428 3.2975843 3.605194  3.9542105 4.2823706
0:  4.479592  4.5818105 4.571712  4.471802  3.7388625 3.9640944]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.7871304 6.847794  6.942566  6.996117  7.0016575 6.941888  6.890606
0:  6.795728  6.697191  6.6188664 6.5188823 6.3952856 6.2813854 6.196
0:  6.1246414 6.0779963 6.0239253 5.9484005 5.396584  5.3644943]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.056297 35.93027  35.759247 35.542454 35.38236  35.240005 35.19053
0:  34.96956  34.75737  34.485924 34.15952  33.8379   33.547676 33.28165
0:  33.001022 32.732624 32.485435 32.240967 32.861034 32.745632]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.872328 15.935207 16.015182 16.067562 16.134073 16.20673  16.271994
0:  16.286621 16.30219  16.309    16.317719 16.305605 16.301846 16.295567
0:  16.278677 16.26487  16.298372 16.355267 15.996523 16.081392]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.718712  19.625027  19.493166  19.295277  19.082016  18.881065
0:  18.74298   18.48058   18.201649  17.873322  17.471586  17.044292
0:  16.59739   16.139797  15.6964445 15.287233  14.951142  14.745444
0:  14.1765175 13.957161 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.38903  24.446857 24.66438  24.920948 25.322323 25.765934 26.30474
0:  26.709576 27.083767 27.407421 27.674675 27.933327 28.125042 28.342
0:  28.494015 28.633224 28.769814 28.93137  28.657318 28.62185 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.262375 18.380713 18.4658   18.51364  18.720417 19.041306 19.457602
0:  19.813755 20.188293 20.489813 20.767738 21.018705 21.262064 21.51566
0:  21.717848 21.934965 22.171644 22.39817  22.851254 22.779757]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.922213  9.116869  9.293768  9.440117  9.5612545 9.648701  9.699453
0:  9.719742  9.7142515 9.722604  9.755968  9.73573   9.746663  9.75122
0:  9.726846  9.746385  9.816736  9.928263  9.368462  9.489021 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.157943 14.144215 14.262571 14.400304 14.687155 15.097319 15.647703
0:  16.14217  16.643732 17.082146 17.463242 17.853443 18.214588 18.516758
0:  18.716408 18.716448 18.605524 18.403038 16.13312  16.002048]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.704214    9.622757    9.535412    9.376977    9.144806    8.785815
0:   8.269466    7.5908895   6.7952843   5.9382496   5.127807    4.312435
0:   3.604158    2.9842563   2.376464    1.8506508   1.3979502   1.014607
0:  -0.02287054 -0.40675783]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.726762  9.538412  9.33229   9.1017685 8.855663  8.610629  8.386056
0:  8.158732  7.9496202 7.765491  7.617206  7.4245434 7.2781763 7.149751
0:  6.981362  6.8286567 6.6643853 6.482541  5.6743917 5.5402308]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.268145 27.248222 27.290312 27.333292 27.489035 27.739912 27.99122
0:  28.112915 28.21818  28.256874 28.319492 28.37662  28.404366 28.415977
0:  28.289951 28.088589 27.898481 27.762186 27.04535  26.732334]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-19.882483 -19.860077 -19.826725 -19.793793 -19.796656 -19.863628
0:  -19.931633 -20.037046 -20.181402 -20.299765 -20.398838 -20.526491
0:  -20.545622 -20.52846  -20.427717 -20.309454 -20.217619 -20.161097
0:  -21.466253 -21.559866]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.02896   -10.908614  -10.776784  -10.633126  -10.479566  -10.32776
0:  -10.17578   -10.023076   -9.889552   -9.713826   -9.477017   -9.23689
0:   -8.875378   -8.519163   -8.13078    -7.8213177  -7.581624   -7.380726
0:   -7.540613   -7.431053 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.027009  10.938946  10.922942  10.888844  10.838086  10.756662
0:  10.660438  10.49091   10.283881  10.046715   9.810404   9.523343
0:   9.264529   8.993065   8.664203   8.318385   7.9671817  7.6417947
0:   6.5088696  6.2866006]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.9487867  3.786625   3.6856759  3.4742303  3.247488   2.92381
0:   2.5897813  2.2296653  1.8504825  1.4943728  1.1299744  0.7015953
0:   0.2433486 -0.2284522 -0.7653718 -1.2933364 -1.8196664 -2.327582
0:  -3.4883647 -4.0633826]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8544197 6.8295803 6.828006  6.843663  6.8583174 6.8787465 6.918087
0:  6.8946877 6.858467  6.759582  6.662553  6.5480137 6.4869356 6.4667625
0:  6.475693  6.483912  6.475993  6.4357047 6.2157917 6.1870756]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9305105 -5.199376  -5.4290686 -5.636401  -5.7956038 -5.982051
0:  -6.163986  -6.4038677 -6.6680183 -6.9577475 -7.226117  -7.4976616
0:  -7.727639  -7.903058  -8.009166  -8.099493  -8.230762  -8.378463
0:  -8.691455  -9.075358 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3827438 -3.3210545 -3.2392588 -3.1346655 -3.0375013 -2.945589
0:  -2.8013315 -2.714551  -2.6175556 -2.5592127 -2.5047789 -2.4665241
0:  -2.3949137 -2.2969732 -2.18639   -2.0872755 -2.0053358 -1.8983574
0:  -2.0022202 -1.9329453]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.998259  -5.8059506 -5.653112  -5.6503415 -5.78562   -6.0567603
0:  -6.3912873 -6.757673  -7.0851407 -7.3059325 -7.3688912 -7.3750234
0:  -7.2120194 -6.9176555 -6.5976157 -6.24233   -5.937859  -5.7502046
0:  -5.728662  -5.551063 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1778026 -1.3188081 -1.4770322 -1.6161313 -1.7668443 -1.9350562
0:  -2.083343  -2.3067822 -2.5929418 -2.946756  -3.3526216 -3.8642745
0:  -4.344686  -4.7962823 -5.2273526 -5.5473523 -5.7945733 -5.9786124
0:  -6.7769294 -7.060736 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7389283 -5.8027587 -5.8753057 -5.9207892 -5.99101   -6.117393
0:  -6.2545667 -6.4470162 -6.676642  -6.8905005 -7.0828977 -7.335731
0:  -7.5102334 -7.635485  -7.7482915 -7.818154  -7.895511  -8.021215
0:  -9.18586   -9.235104 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.9438705 5.985137  6.0541725 6.10932   6.146566  6.1193113 6.0162625
0:  5.818414  5.532233  5.2239113 4.970639  4.7259045 4.6173925 4.5874424
0:  4.5538273 4.5565434 4.546818  4.4935017 3.8318877 3.6281931]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2083888 -2.2015634 -2.1972318 -2.1122937 -1.9307446 -1.7071576
0:  -1.4847713 -1.3696008 -1.3985515 -1.5519795 -1.7696471 -2.1048245
0:  -2.3697782 -2.5167823 -2.586516  -2.5097585 -2.3574967 -2.2255025
0:  -3.008473  -2.9878392]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3933282 -3.272612  -3.2547212 -3.2863994 -3.2648025 -3.250235
0:  -3.158341  -3.0474448 -2.9432569 -2.802938  -2.6425776 -2.5506873
0:  -2.4004636 -2.247889  -2.0412192 -1.7735438 -1.4870152 -1.2771745
0:  -1.7334523 -1.6408038]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.029844 26.981766 26.81999  26.432142 25.969006 25.448223 24.988922
0:  24.448488 23.999609 23.633974 23.326597 23.115261 22.949266 22.80096
0:  22.610014 22.392942 22.182537 21.964613 21.285421 20.939024]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2047148 2.3385677 2.489695  2.6146502 2.724506  2.8264744 2.8903
0:  2.937913  2.9813218 3.022871  3.108328  3.148769  3.2076454 3.2608
0:  3.2545695 3.2429245 3.2150226 3.1660235 2.4561005 2.536008 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.13796616 0.29098177 0.53732395 0.77534056 1.0621223  1.4795494
0:  1.9687123  2.490707   2.9802454  3.246771   3.3030186  3.0282638
0:  2.5990133  2.1735306  1.7692366  1.4624267  1.2028823  0.9331403
0:  0.39882708 0.2142725 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.0448856   2.1421819   2.1507764   2.0861402   2.0190349   1.938468
0:   1.9003963   1.7689047   1.539669    1.1809502   0.7354355   0.27865028
0:  -0.07669783 -0.24653006 -0.27334976 -0.18979263 -0.10426331 -0.06963873
0:   0.8262296   0.6744633 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.4898806e+00  5.6006436e+00  4.8160982e+00  4.0567923e+00
0:   3.3533885e+00  2.7695367e+00  2.3344598e+00  1.9331961e+00
0:   1.6302686e+00  1.3513799e+00  1.0733199e+00  7.6372290e-01
0:   4.8679924e-01  2.5824165e-01  6.4417362e-02 -1.0444260e-01
0:  -2.4844217e-01 -3.3828640e-01 -4.3692589e-03  1.4101028e-01]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.3879075  7.535382   7.7604904  8.02527    8.316335   8.631557
0:   9.00432    9.283665   9.585789   9.87735   10.171063  10.514748
0:  10.876486  11.21917   11.532099  11.72397   11.872778  12.022102
0:  12.048454  12.344135 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0657873  3.0958986  3.1447966  3.179054   3.1994402  3.1962216
0:  3.2024407  3.1810668  3.149721   3.1093202  3.0584185  2.9538062
0:  2.838613   2.7007756  2.494318   2.2607017  1.9948807  1.720191
0:  0.97703695 1.0706191 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.17392  18.045366 18.8499   19.38723  19.67881  19.706463 19.498684
0:  19.047096 18.477362 17.863482 17.323399 16.912506 16.79343  16.99209
0:  17.496313 18.235962 19.087296 19.880589 19.736557 19.188864]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2426224 -2.1672344 -2.0630827 -2.0096664 -2.028473  -2.126072
0:  -2.2511988 -2.4520864 -2.6494637 -2.8447762 -3.0036564 -3.1664743
0:  -3.2718391 -3.3342276 -3.4063644 -3.4402857 -3.474904  -3.4957151
0:  -3.3582726 -3.2101192]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5882564 -3.4292035 -3.2720532 -3.172687  -3.1864753 -3.334002
0:  -3.601622  -3.972139  -4.3768735 -4.7435365 -4.998568  -5.2116838
0:  -5.267835  -5.215483  -5.1457915 -5.0247483 -4.9136796 -4.831365
0:  -5.2212358 -4.991095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.049543   -9.879965   -9.732109   -9.614777   -9.547659   -9.558516
0:   -9.577946   -9.617584   -9.6340275  -9.577955   -9.438093   -9.33247
0:   -9.14864    -8.9464245  -8.78117    -8.57058    -8.359701   -8.180706
0:   -8.4923     -8.213284 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.678291  14.270636  15.100126  15.982952  16.988564  17.968761
0:  18.738617  19.186752  19.212172  18.808279  18.072817  17.13763
0:  16.175608  15.269365  14.450209  13.732064  13.122347  12.573561
0:  11.648603  11.5250435]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.1497254  -6.7866855  -6.363128   -5.8960633  -5.4471946  -5.019447
0:  -4.541947   -4.061972   -3.5670695  -3.0525985  -2.495707   -1.940372
0:  -1.3024707  -0.6364908   0.02734709  0.6674204   1.2109065   1.6272526
0:   2.0806074   2.4588094 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.161655 15.802414 15.483415 15.274247 15.202388 15.298939 15.464024
0:  15.695218 15.968939 16.285324 16.696552 17.067259 17.431847 17.65612
0:  17.620045 17.330473 16.869932 16.332005 14.415645 13.898682]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.296499  12.38173   12.437682  12.429522  12.363209  12.248239
0:  12.104038  11.9101715 11.704958  11.510817  11.336472  11.133449
0:  10.976317  10.820257  10.659679  10.528255  10.4009075 10.296454
0:   9.578882   9.582442 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.458845   -9.9485855  -9.359901   -8.811291   -8.334082   -7.9843173
0:   -7.725574   -7.623304   -7.6471124  -7.792338   -8.010759   -8.35709
0:   -8.633177   -8.859955   -9.028673   -9.174116   -9.330036   -9.4563675
0:  -10.901548  -11.202517 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.303577 18.370272 18.622353 19.002865 19.48856  20.091013 20.755732
0:  21.367687 21.999722 22.603397 23.161575 23.65978  24.136356 24.572248
0:  25.01386  25.453074 25.974745 26.504658 27.085712 27.406693]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2511759 -1.4282141 -1.6050472 -1.8205624 -2.0729847 -2.3642626
0:  -2.6185875 -2.8900032 -3.1394606 -3.3519478 -3.5251503 -3.6764412
0:  -3.720193  -3.6902246 -3.6674414 -3.6554537 -3.721179  -3.8406706
0:  -4.5272093 -4.674617 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.719009  -3.6741185 -3.6068864 -3.6437964 -3.7987437 -4.100817
0:  -4.4750237 -4.9697948 -5.5084453 -6.0578704 -6.534864  -7.027758
0:  -7.3087773 -7.500957  -7.656813  -7.7339816 -7.881993  -7.989799
0:  -8.421835  -8.453239 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.908202  5.0341063 5.287324  5.5694776 5.8399954 6.072973  6.250157
0:  6.3210697 6.3290257 6.303657  6.235146  6.0994363 5.949138  5.7771015
0:  5.592376  5.431452  5.3232083 5.3040533 4.5777617 4.629247 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.971777   9.215078   9.605732  10.007764  10.446465  10.834953
0:  11.147198  11.331126  11.4681225 11.582375  11.8829    12.292028
0:  12.880011  13.533982  13.956922  14.122686  13.946833  13.4539385
0:  10.30286    9.942137 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9187813 3.9252315 3.9101667 3.8861732 3.8155365 3.6925762 3.6012623
0:  3.4897494 3.4168527 3.379572  3.3701406 3.3046637 3.2407238 3.1348255
0:  2.9821324 2.8619633 2.7825723 2.739285  1.9541225 1.8955479]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.05163288  0.06714392  0.22318506  0.44452     0.7117624   0.97343254
0:   1.2806864   1.5510216   1.8241963   2.1296506   2.4924378   2.8745923
0:   3.3363767   3.8156068   4.26976     4.722238    5.169585    5.5856957
0:   6.0749044   6.4245734 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.793562  -10.983213  -11.174103  -11.2968235 -11.359119  -11.453512
0:  -11.561731  -11.801859  -12.135258  -12.485487  -12.7750225 -13.068281
0:  -13.159651  -13.168779  -13.084702  -12.956259  -12.849073  -12.770811
0:  -13.95105   -14.195517 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.537007  -5.3184743 -5.103129  -4.911473  -4.8092065 -4.7966905
0:  -4.843515  -4.9615493 -5.1499925 -5.3533263 -5.554174  -5.8264747
0:  -6.0142226 -6.146256  -6.2375827 -6.2312512 -6.204326  -6.191509
0:  -6.946359  -6.8542247]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.757618   4.777179   4.7838187  4.7300806  4.576665   4.368187
0:  4.163683   4.003471   3.931217   3.9157877  3.942841   3.8547366
0:  3.7393649  3.5426576  3.2227767  2.8741696  2.4950123  2.0818048
0:  0.99258137 0.70435524]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.174843   7.2567797  7.4277363  7.612749   7.801987   7.9774055
0:   8.216787   8.384838   8.565179   8.751178   8.924395   9.143829
0:   9.426196   9.7539625 10.144047  10.540575  10.927791  11.306414
0:  11.2766285 11.540522 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7199588 2.8843303 3.0539246 3.191285  3.2563403 3.2641673 3.259851
0:  3.225849  3.2015564 3.1904457 3.1878786 3.1345277 3.1112752 3.0878778
0:  3.0607538 3.0676498 3.0628905 3.055163  2.5098567 2.6160107]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7877316 3.632771  3.520915  3.4028149 3.3134563 3.2249439 3.1748548
0:  3.0848658 2.9829965 2.880346  2.7494113 2.600366  2.4255028 2.2670636
0:  2.0814395 1.9000654 1.7184501 1.5519032 0.9005027 0.5737901]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.340785  8.402929  8.515732  8.622956  8.675097  8.643578  8.559189
0:  8.374842  8.143556  7.873627  7.5877056 7.235704  6.920838  6.6696925
0:  6.4407864 6.307388  6.200901  6.119455  5.976363  5.9149346]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.894005 16.237228 16.646917 17.121733 17.691013 18.427656 19.400408
0:  20.28181  21.274164 22.231339 23.180435 24.14294  25.07548  25.932829
0:  26.659845 27.174625 27.530758 27.771921 26.994286 27.479158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.2744193 6.45996   6.655599  6.824627  6.956033  7.040092  7.0690227
0:  7.0386314 6.9466496 6.8407207 6.7584295 6.62526   6.585212  6.5717077
0:  6.5546627 6.5682545 6.562067  6.523389  6.0542073 6.12889  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.225424 17.156448 17.173458 17.20773  17.321007 17.476812 17.702444
0:  17.761366 17.744337 17.628654 17.399126 17.181553 16.93839  16.7378
0:  16.598602 16.475147 16.386017 16.365313 16.70026  16.785376]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.264343  19.006916  18.70051   18.232788  17.732811  17.263168
0:  16.902084  16.523598  16.200092  15.893042  15.537567  15.145836
0:  14.765579  14.411486  14.09332   13.841048  13.681719  13.602999
0:  13.2576685 13.211178 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.001315   -9.859628   -9.730676   -9.643682   -9.624747   -9.69493
0:   -9.735824   -9.793809   -9.846261   -9.858075   -9.8623495  -9.906976
0:   -9.891595   -9.847768   -9.777825   -9.67077    -9.6071825  -9.5833
0:  -10.290912  -10.214763 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.393699 14.225999 14.050323 13.86932  13.690134 13.51157  13.346081
0:  13.130524 12.900268 12.658968 12.40649  12.13303  11.858669 11.56103
0:  11.248305 10.932611 10.702812 10.563156 10.256516 10.056429]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.909431 24.580643 24.275421 24.056229 24.021976 24.173134 24.44078
0:  24.525753 24.648262 24.746838 24.900658 25.195633 25.538116 25.960678
0:  26.330032 26.614151 26.86588  27.11731  27.283813 27.036327]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.552649  -5.461163  -5.3790374 -5.335006  -5.3219066 -5.344443
0:  -5.376861  -5.412745  -5.3908334 -5.2851434 -5.057483  -4.8473964
0:  -4.566561  -4.2897935 -4.094748  -3.9071393 -3.760983  -3.662581
0:  -4.305557  -4.41592  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8232946  -2.5705605  -2.3117075  -2.088192   -1.9040384  -1.7639904
0:  -1.5804925  -1.4188452  -1.2473903  -1.0712967  -0.87044525 -0.7277069
0:  -0.5477419  -0.38529873 -0.28423977 -0.19467306 -0.15938568 -0.1604042
0:  -0.67117167 -0.65049696]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6628447 2.684423  2.6803021 2.6604414 2.6591492 2.644987  2.6629477
0:  2.6668367 2.6780314 2.6839604 2.6923542 2.6091952 2.53438   2.4279156
0:  2.2622309 2.1216516 2.0151577 1.928493  1.1245322 1.0951099]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.013784 27.761482 28.391445 28.76967  28.980772 28.947609 28.794321
0:  28.243824 27.550108 26.673512 25.714016 24.735447 23.87728  23.149664
0:  22.58275  22.20886  22.03412  22.042833 23.036585 23.218815]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2829995 2.332231  2.3637471 2.3615632 2.3441935 2.2914119 2.273559
0:  2.2448773 2.2319508 2.2457037 2.2616897 2.2265816 2.202609  2.1585608
0:  2.1277523 2.1334853 2.1663718 2.1977425 1.7946491 1.8070579]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2527766 3.5589595 3.915062  4.2197223 4.412391  4.4661684 4.350022
0:  4.0486536 3.6213431 3.1246321 2.6511962 2.1850858 1.8539286 1.6797342
0:  1.5967116 1.629333  1.7323508 1.8466153 1.6542072 1.7458639]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.7054253 1.8985958 2.1863737 2.4495714 2.7475655 3.0297716 3.4046154
0:  3.7547255 4.1122932 4.4266024 4.6714263 4.8670516 5.040396  5.2280297
0:  5.4299507 5.634474  5.820571  5.9793377 6.2972054 6.660541 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.70014   10.718141  10.746931  10.775786  10.8139925 10.917463
0:  11.091038  11.242326  11.433926  11.638317  11.831837  11.9315
0:  11.972315  11.922001  11.778011  11.621441  11.536427  11.546066
0:  11.547836  11.6779785]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.228832 27.160332 27.272137 27.570194 28.05412  28.702328 29.334099
0:  29.764381 30.111053 30.221764 30.197784 30.059566 29.861523 29.672
0:  29.40343  29.148712 28.910015 28.725025 28.12936  28.057266]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.036381  4.159648  4.2895203 4.402807  4.4865103 4.5584793 4.6071134
0:  4.6228704 4.626597  4.609025  4.6083803 4.5556355 4.553625  4.5565166
0:  4.553138  4.5654926 4.5692472 4.5641828 4.1241007 4.2391405]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.729125  6.894448  7.089763  7.3188562 7.529704  7.7128735 7.8614607
0:  7.902602  7.9320707 7.944854  7.9939747 8.066695  8.198675  8.336428
0:  8.470796  8.571153  8.686951  8.835547  8.535054  8.679148 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.940508    5.7095575   5.5082164   5.325796    5.169279    4.971345
0:   4.688802    4.282105    3.7912393   3.2393446   2.7071192   2.0857925
0:   1.5496812   1.1442261   0.80500937  0.62564325  0.56060505  0.509562
0:  -0.68228245 -0.839355  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.083577  4.626001  4.0990753 3.5450459 3.084034  2.7567825 2.6067443
0:  2.5787263 2.668151  2.8543735 3.0887542 3.302697  3.522855  3.7111287
0:  3.8766937 4.041154  4.220332  4.4022865 4.441025  4.358042 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.3609502 3.4034884 3.3939774 3.3417115 3.3076792 3.2714577 3.2686884
0:  3.2668898 3.3065164 3.3705273 3.4794679 3.5911129 3.750185  3.9508681
0:  4.168848  4.423886  4.7268324 5.053998  5.119237  5.440625 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.213136  -8.049346  -7.888672  -7.7920237 -7.7337723 -7.813463
0:  -7.912836  -8.132415  -8.334748  -8.490398  -8.560687  -8.525946
0:  -8.354879  -8.080657  -7.722439  -7.3297396 -6.9566946 -6.6037045
0:  -6.1491613 -5.7567472]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.363182  -12.875847  -12.25392   -11.55533   -10.851801  -10.196215
0:   -9.528889   -9.007488   -8.490446   -8.033985   -7.6512475  -7.2461658
0:   -6.7869983  -6.3461127  -5.8373322  -5.456618   -5.1857305  -4.996554
0:   -4.286287   -3.826159 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.6909943  7.964196   8.244532   8.511129   8.757899   8.980785
0:   9.222442   9.414583   9.595674   9.735962   9.860815   9.9408455
0:  10.066017  10.192023  10.325098  10.439278  10.549571  10.639786
0:   9.991317  10.1564   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.24879122 0.46312332 0.7590256  1.120657   1.5383172  1.9606695
0:  2.4450288  2.8362403  3.1938145  3.4361362  3.5708077  3.5675664
0:  3.5330374  3.4918249  3.4083645  3.360609   3.2903104  3.1404855
0:  2.4727955  2.3922317 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.818913 34.8176   34.78479  34.769703 34.865036 35.059383 35.44139
0:  35.600388 35.8081   35.939587 35.959557 35.9919   35.978424 35.9775
0:  35.977074 36.013664 36.066097 36.128483 36.668888 36.993813]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.132752 10.966126 10.796578 10.603262 10.445009 10.357913 10.363367
0:  10.358585 10.359184 10.354465 10.320056 10.189198 10.010956  9.790772
0:   9.507428  9.264286  9.109701  9.052539  8.994361  8.979692]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.996325  14.135422  14.312817  14.498886  14.702175  14.904722
0:  15.071409  15.077381  14.967373  14.704279  14.348938  13.943945
0:  13.5540085 13.1766815 12.804608  12.407296  12.050978  11.766432
0:  11.015385  11.028437 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.214211  -5.849357  -5.496141  -5.15482   -4.9009166 -4.739247
0:  -4.569513  -4.461024  -4.335645  -4.1882844 -4.0162244 -3.8860135
0:  -3.7105937 -3.5299754 -3.3515048 -3.1591449 -3.0079331 -2.85187
0:  -3.4375806 -3.230044 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.24909  43.3561   43.540173 43.84598  44.292877 44.873474 45.514
0:  46.056824 46.636677 47.064167 47.383823 47.64698  47.87289  48.026226
0:  48.02205  47.916508 47.689243 47.227413 46.07704  46.20978 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3657198 2.6496096 2.9578595 3.2708402 3.577753  3.8326836 4.083849
0:  4.277692  4.486285  4.6987596 4.9127836 5.073466  5.239223  5.3968463
0:  5.5351396 5.7130365 5.903351  6.0956235 5.6156087 5.847584 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.441736  -8.43412   -8.364788  -8.220045  -7.9577713 -7.7047186
0:  -7.35577   -7.075365  -6.7792826 -6.444764  -6.0733147 -5.7094927
0:  -5.3373766 -4.936352  -4.5155153 -4.0423594 -3.544343  -3.0728378
0:  -3.019268  -2.7644796]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.310432 14.129505 13.960455 13.804722 13.729441 13.691317 13.731733
0:  13.634584 13.564066 13.411244 13.24065  13.134619 13.096192 13.104637
0:  13.157772 13.142828 13.109198 13.076183 12.762478 12.521494]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.506493 39.10369  38.793587 38.56815  38.58688  38.71972  39.05656
0:  39.097683 39.147263 38.94499  38.416653 37.671726 36.679203 35.638443
0:  34.60768  33.817806 33.270824 32.98443  33.45877  33.086754]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8558097  -0.7138977  -0.5188508  -0.3495903  -0.1918559  -0.05092955
0:   0.10617352  0.23123407  0.3397727   0.41650295  0.46771526  0.47486782
0:   0.5198145   0.6234827   0.76441     0.94216156  1.1430693   1.3378625
0:   0.96201134  1.0412121 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.99467754 -0.8275962  -0.66182566 -0.51566553 -0.42927122 -0.41524696
0:  -0.38588905 -0.4662156  -0.58520555 -0.74702406 -0.93479156 -1.107862
0:  -1.2162309  -1.2658286  -1.3353963  -1.4100165  -1.5526147  -1.7168441
0:  -2.7957997  -2.8544507 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.47462   6.3237095 6.213999  6.086473  5.958527  5.794976  5.6923985
0:  5.5620704 5.4468613 5.353733  5.265404  5.1762414 5.1402807 5.1386313
0:  5.1261563 5.1112413 5.0278397 4.885564  4.9089966 4.878911 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.50536  39.658234 39.730824 39.74423  39.805283 39.95854  40.21474
0:  40.30328  40.40435  40.4208   40.365967 40.31573  40.225906 40.16711
0:  40.09767  40.032265 40.00097  39.954185 40.917717 41.062256]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.1339083 -7.235712  -7.3171206 -7.3154097 -7.1824236 -6.934782
0:  -6.5432224 -6.1142426 -5.650425  -5.2172346 -4.795272  -4.4814806
0:  -4.1596212 -3.8465462 -3.5652199 -3.224658  -2.8838615 -2.6035137
0:  -3.086975  -2.7850752]
0: validation loss for strategy=forecast at epoch 2 : 4.199079513549805
0: validation loss for velocity_u : 0.0358089841902256
0: validation loss for velocity_v : 0.06017136946320534
0: validation loss for specific_humidity : 0.02450951747596264
0: validation loss for velocity_z : 0.5758445858955383
0: validation loss for temperature : 0.08776503801345825
0: validation loss for total_precip : 24.410375595092773
0: 3 : 18:09:53 :: batch_size = 96, lr = 1.9512195121951222e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 3, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4026, -0.3919, -0.3815, -0.3714, -0.3618, -0.3523, -0.3434, -0.3346, -0.3262, -0.3184, -0.3109, -0.3038,
0:         -0.2971, -0.2910, -0.2854, -0.2803, -0.2758, -0.2718, -0.4417, -0.4295, -0.4175, -0.4057, -0.3941, -0.3826,
0:         -0.3716], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2677, -0.2893, -0.3113, -0.3342, -0.3575, -0.3816, -0.4065, -0.4321, -0.4582, -0.4851, -0.5125, -0.5401,
0:         -0.5684, -0.5964, -0.6247, -0.6525, -0.6798, -0.7064, -0.2243, -0.2465, -0.2687, -0.2914, -0.3142, -0.3373,
0:         -0.3608], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6495, -0.6490, -0.6483, -0.6480, -0.6479, -0.6477, -0.6472, -0.6467, -0.6463, -0.6458, -0.6454, -0.6461,
0:         -0.6469, -0.6477, -0.6484, -0.6492, -0.6506, -0.6526, -0.6497, -0.6492, -0.6488, -0.6485, -0.6483, -0.6481,
0:         -0.6480], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1830,  0.1765,  0.1656,  0.1481,  0.1241,  0.1002,  0.0696,  0.0413,  0.0108, -0.0176, -0.0415, -0.0633,
0:         -0.0764, -0.0873, -0.0895, -0.0851, -0.0742, -0.0590,  0.4577,  0.4642,  0.4621,  0.4555,  0.4424,  0.4272,
0:          0.4054], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.1791, -1.1719, -1.1643, -1.1567, -1.1493, -1.1414, -1.1338, -1.1259, -1.1177, -1.1092, -1.1004, -1.0912,
0:         -1.0819, -1.0723, -1.0625, -1.0521, -1.0421, -1.0316, -1.0215, -1.0116, -1.0019, -0.9927, -0.9841, -0.9764,
0:         -0.9692], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2331, -0.2226, -0.2050, -0.2004, -0.2086, -0.2097, -0.2027, -0.1945, -0.1863, -0.2378, -0.2378, -0.2367,
0:         -0.2378, -0.2378, -0.2378, -0.2378, -0.2331, -0.2249, -0.2191, -0.2214, -0.2261, -0.2320, -0.2367, -0.2378,
0:         -0.2390], device='cuda:0')
0: [DEBUG] Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1886,     nan,     nan,     nan, -0.1945,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1289,     nan,     nan,     nan,     nan,     nan,     nan,  0.0385,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2050,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1886,     nan,     nan,     nan,     nan,     nan, -0.1840,
0:             nan,     nan,     nan,     nan,     nan, -0.1289,     nan,     nan,     nan, -0.1687,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0013, -0.0224,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.1521,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0270,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2027,     nan,     nan,
0:         -0.1968,     nan,     nan, -0.1980,     nan,     nan,     nan,     nan, -0.1875,     nan,     nan,     nan,
0:             nan, -0.1851,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.2154,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.1744,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.0104,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 3, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1957, -0.1832, -0.1714, -0.1668, -0.1622, -0.1587, -0.1550, -0.1607, -0.1634, -0.1637, -0.1615, -0.1609,
0:         -0.1551, -0.1491, -0.1465, -0.1415, -0.1392, -0.1396, -0.1871, -0.1759, -0.1703, -0.1650, -0.1578, -0.1475,
0:         -0.1376], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8373, -0.8521, -0.8689, -0.8937, -0.9206, -0.9465, -0.9708, -0.9882, -0.9993, -1.0082, -1.0191, -1.0298,
0:         -1.0445, -1.0616, -1.0824, -1.0986, -1.1057, -1.1032, -0.7703, -0.7667, -0.7680, -0.7761, -0.7905, -0.8076,
0:         -0.8284], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7057, -0.7021, -0.7008, -0.6979, -0.6977, -0.6956, -0.6963, -0.6950, -0.6940, -0.6928, -0.6904, -0.6893,
0:         -0.6894, -0.6902, -0.6905, -0.6931, -0.6941, -0.6937, -0.7030, -0.6996, -0.6972, -0.6944, -0.6928, -0.6922,
0:         -0.6913], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0506,  0.0855,  0.0966,  0.1666,  0.2548,  0.2804,  0.2775,  0.2022,  0.2277,  0.3132,  0.3020,  0.3488,
0:          0.3460,  0.2617,  0.1912,  0.0842,  0.0379,  0.0034, -0.0046,  0.1064,  0.1654,  0.2934,  0.3871,  0.4071,
0:          0.3710], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.3886, -1.3985, -1.4000, -1.3948, -1.3841, -1.3738, -1.3672, -1.3625, -1.3608, -1.3600, -1.3579, -1.3541,
0:         -1.3458, -1.3337, -1.3195, -1.3055, -1.2945, -1.2878, -1.2829, -1.2784, -1.2749, -1.2715, -1.2710, -1.2735,
0:         -1.2753], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.7051, 0.5246, 0.4000, 0.4078, 0.3074, 0.2175, 0.1814, 0.1525, 0.0944, 0.8514, 0.7388, 0.6440, 0.6063, 0.6135,
0:         0.5641, 0.5047, 0.4721, 0.3834, 1.0161, 0.9329, 0.9376, 0.9473, 0.9851, 0.9889, 0.8835], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.05991058796644211; velocity_v: 0.09617624431848526; specific_humidity: 0.035909537225961685; velocity_z: 0.5399692058563232; temperature: 0.08135242760181427; total_precip: 23.114360809326172; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07529129087924957; velocity_v: 0.11068034172058105; specific_humidity: 0.040690138936042786; velocity_z: 0.4852752089500427; temperature: 0.13648104667663574; total_precip: 22.173141479492188; 
0: epoch: 3 [1/5 (20%)]	Loss: 22.64375 : 3.88555 :: 0.17157 (2.56 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06181422993540764; velocity_v: 0.10200002044439316; specific_humidity: 0.04874539375305176; velocity_z: 0.536339282989502; temperature: 0.12143053114414215; total_precip: 28.302892684936523; 
0: epoch: 3 [2/5 (40%)]	Loss: 28.30289 : 4.83566 :: 0.15358 (16.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.059244122356176376; velocity_v: 0.09326405823230743; specific_humidity: 0.03128507733345032; velocity_z: 0.45691582560539246; temperature: 0.08467261493206024; total_precip: 22.06122589111328; 
0: epoch: 3 [3/5 (60%)]	Loss: 22.06123 : 3.77330 :: 0.15286 (15.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05320453643798828; velocity_v: 0.09296229481697083; specific_humidity: 0.0395800955593586; velocity_z: 0.49607956409454346; temperature: 0.10679248720407486; total_precip: 0.761948823928833; 
0: epoch: 3 [4/5 (80%)]	Loss: 0.76195 : 0.23203 :: 0.14372 (16.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  4.7683716e-07 4.7683716e-07 4.7683716e-07 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 9.5367432e-07 1.4305115e-06 1.4305115e-06
0:  1.9073486e-06 3.3378601e-06 1.9073486e-06 1.4305115e-06 1.4305115e-06
0:  2.3841858e-06 3.3378601e-06 4.2915344e-06 4.7683716e-06 5.2452087e-06
0:  8.5830688e-06 1.0013580e-05 1.2397766e-05 1.5735626e-05 2.2411346e-05
0:  2.6702881e-05 3.0994415e-05 3.5762787e-05 4.4822693e-05 4.8637390e-05
0:  3.7670135e-05 1.5735626e-05 1.0013580e-05 6.1988831e-06 3.8146973e-06
0:  1.9073486e-06 1.4305115e-06 4.7683716e-07 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 4.7683716e-07 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07
0:  4.7683716e-07 0.0000000e+00 4.7683716e-07 9.5367432e-07 1.9073486e-06
0:  2.8610229e-06 4.2915344e-06 5.2452087e-06 6.6757202e-06 8.1062317e-06
0:  9.0599060e-06 9.5367432e-06 1.1920929e-05 1.3828278e-05 1.6689301e-05
0:  1.7166138e-05 1.7642975e-05 2.0027161e-05 2.4318695e-05 2.9563904e-05]
0: Target values (first 200):
0: [1.4305115e-06 1.4305115e-06 1.4305115e-06 9.5367432e-07 1.4305115e-06
0:  1.9073486e-06 1.9073486e-06 1.9073486e-06 2.3841858e-06 2.3841858e-06
0:  3.3378601e-06 5.2452087e-06 2.3841858e-06 1.9073486e-06 9.5367432e-07
0:  9.5367432e-07 4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 4.7683716e-07 9.5367432e-07 9.5367432e-07
0:  1.4305115e-06 1.4305115e-06 9.5367432e-07 1.9073486e-06 2.3841858e-06
0:  1.4305115e-06 9.5367432e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [6.0106697 6.1328373 6.2858067 6.449008  6.615953  6.721513  6.810802
0:  6.7964683 6.6857853 6.546851  6.3817782 6.238103  6.20323   6.240449
0:  6.3836594 6.619939  6.9384413 7.316516  7.190387  7.3688493]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.112, max = 2.116, mean = 0.163
0:          sample (first 20): tensor([-0.0911, -0.0810, -0.0684, -0.0549, -0.0411, -0.0324, -0.0250, -0.0262, -0.0353, -0.0468, -0.0604, -0.0723,
0:         -0.0752, -0.0721, -0.0603, -0.0407, -0.0144,  0.0168, -0.1609, -0.1546])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.485086 18.539595 18.66337  18.756838 18.847767 18.909306 19.000177
0:  19.02361  19.056234 19.07339  19.050735 18.991535 18.984787 18.993008
0:  19.041142 19.14526  19.252605 19.331991 18.888683 18.950775]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5444703 1.5223594 1.6451144 1.8043928 1.9667892 2.0404696 2.2887774
0:  2.3938136 2.5409908 2.6607077 2.6374462 2.6324825 2.6184514 2.7186298
0:  2.9065483 3.1296594 3.2965553 3.3806372 3.0933986 2.959076 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.319662 15.427448 15.582113 15.75115  15.962399 16.16193  16.44226
0:  16.653996 16.877956 17.072844 17.22183  17.36667  17.514584 17.687862
0:  17.886097 18.07368  18.281948 18.48446  17.454912 17.679277]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.311724 15.508966 15.783159 16.075317 16.395746 16.71629  17.255836
0:  17.606014 17.920574 18.03129  17.88277  17.622911 17.289904 17.058746
0:  16.86531  16.725225 16.604744 16.474731 15.084439 14.906326]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.699283  -6.8900676 -6.976275  -7.0185065 -7.091146  -7.2793746
0:  -7.5425725 -7.862183  -8.248196  -8.555706  -8.8053665 -9.066185
0:  -9.236231  -9.335121  -9.422024  -9.405455  -9.361275  -9.343943
0:  -9.552153  -9.633085 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.39059  23.263885 22.907158 22.148224 21.063358 19.792294 18.531542
0:  17.467012 16.638046 16.061867 15.641603 15.246518 14.876385 14.458326
0:  13.994095 13.563023 13.176717 12.845411 11.43669  11.221149]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.833513  -7.973884  -8.093863  -8.153997  -8.192516  -8.236158
0:  -8.318935  -8.3509655 -8.378479  -8.282143  -8.029106  -7.806356
0:  -7.4295096 -7.062818  -6.6985345 -6.2735057 -5.823051  -5.4509425
0:  -5.9290767 -5.7730737]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5934463  -1.501081   -1.3426833  -1.1616321  -0.96502924 -0.8376899
0:  -0.64834166 -0.50620365 -0.33600903 -0.17892122 -0.07674217  0.02745533
0:   0.17326546  0.33303118  0.5226555   0.66709757  0.75971794  0.80857134
0:   0.58327293  0.6706333 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.483111  12.453327  12.5105295 12.593931  12.72735   12.879372
0:  13.179865  13.4047    13.716595  14.060755  14.4005995 14.793629
0:  15.277519  15.844064  16.527996  17.22163   17.926918  18.520393
0:  18.562563  18.918    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.383614 37.492565 37.561802 37.514637 37.457745 37.37051  37.44536
0:  37.360947 37.331493 37.219112 36.985718 36.78128  36.605305 36.50296
0:  36.46002  36.423973 36.38515  36.268513 36.137276 36.019676]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.6310573 1.8469243 2.170589  2.490571  2.804216  3.0604796 3.2527556
0:  3.4426768 3.5978873 3.815478  4.078644  4.236911  4.4238944 4.561517
0:  4.681183  4.874152  5.11154   5.3068943 5.0279136 5.349243 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.548574 8.453697 8.430203 8.371706 8.349604 8.337003 8.455999 8.50045
0:  8.551084 8.558728 8.465789 8.338725 8.236632 8.210957 8.25703  8.36356
0:  8.436798 8.468325 8.516918 8.552645]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.10129   -5.1572766 -5.1335177 -5.05109   -4.919001  -4.8554683
0:  -4.82202   -4.824577  -4.897672  -4.8921266 -4.8492265 -4.9260125
0:  -4.945827  -4.998729  -5.0295196 -4.931996  -4.7887607 -4.6411223
0:  -5.3980126 -5.4614625]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.100268  13.8923435 13.742073  13.596788  13.365274  13.0161495
0:  12.598846  12.10054   11.574455  11.067278  10.583086  10.071869
0:   9.662828   9.255541   8.914773   8.665347   8.553302   8.562265
0:   7.0317993  6.81433  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.451215  5.6918907 6.0230613 6.3719697 6.6603355 6.878453  7.075665
0:  7.1930127 7.2412577 7.2497187 7.183625  7.008343  6.8751316 6.7467628
0:  6.678502  6.6736784 6.733631  6.8014216 6.149023  6.1728587]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6750364 -5.495373  -5.1755886 -4.7249117 -4.2695684 -3.888218
0:  -3.6526012 -3.513606  -3.5793276 -3.7391472 -3.9159489 -4.2543926
0:  -4.4927273 -4.71574   -4.8772535 -4.8951654 -4.856922  -4.8303337
0:  -6.162719  -6.4388256]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -6.126683   -6.3092995  -6.4297953  -6.5328665  -6.711931   -7.0335827
0:   -7.4330673  -7.8894796  -8.375166   -8.767472   -9.096815   -9.470053
0:   -9.776568  -10.044967  -10.297866  -10.452783  -10.603646  -10.7400675
0:  -11.721267  -11.929776 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3613405 -5.547842  -5.562375  -5.3108873 -4.8627605 -4.3462133
0:  -3.8380814 -3.3978224 -3.100812  -2.9150715 -2.7708626 -2.8259077
0:  -2.819584  -2.8236675 -2.8081532 -2.6369772 -2.4215631 -2.208599
0:  -2.7048788 -2.6985064]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.63214684 -0.49323654 -0.22586918  0.03829527  0.3384514   0.61183834
0:   0.87627506  1.0804253   1.227715    1.3665161   1.5182366   1.6261563
0:   1.7706599   1.9407516   2.11725     2.3375273   2.5260553   2.7119155
0:   2.3029685   2.3861132 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.3806314 -7.231758  -7.020234  -6.770585  -6.5601196 -6.411998
0:  -6.280116  -6.189495  -6.14876   -6.0730505 -5.9877615 -5.9381475
0:  -5.8217516 -5.699484  -5.549986  -5.3873644 -5.2498093 -5.126129
0:  -5.5053387 -5.363943 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.894463 36.02181  36.142906 36.21708  36.3214   36.45416  36.726524
0:  36.82103  36.886562 36.77444  36.476227 36.15073  35.85904  35.626305
0:  35.497227 35.344296 35.163067 34.941513 34.296326 34.237774]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.313816  8.296416  8.326875  8.316277  8.266601  8.126137  7.938654
0:  7.705493  7.405442  7.125624  6.845194  6.491299  6.1599326 5.835577
0:  5.537371  5.336735  5.177848  5.0106854 3.8189926 3.644248 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.80924   -6.7923064 -6.609876  -6.3423257 -6.0268292 -5.7770495
0:  -5.619019  -5.558582  -5.6533074 -5.7914453 -5.9909587 -6.297569
0:  -6.518617  -6.7200274 -6.83067   -6.8329396 -6.805679  -6.7844634
0:  -7.3170485 -7.2580028]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.952342  3.035446  3.1821532 3.3267744 3.4549358 3.540656  3.5973938
0:  3.616439  3.5990565 3.5839155 3.5821443 3.5247023 3.5483348 3.6015542
0:  3.6924307 3.8311574 3.9747622 4.0783124 3.4192781 3.5267138]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1558528 -1.4895883 -1.9501767 -2.6003852 -3.381764  -4.2197137
0:  -5.020884  -5.660361  -6.2138104 -6.578075  -6.792128  -7.0438094
0:  -7.2036595 -7.335483  -7.454148  -7.471699  -7.4202514 -7.338202
0:  -6.996079  -6.636029 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-23.215752 -23.022402 -22.719532 -22.359    -22.071218 -21.887348
0:  -21.85489  -21.868614 -21.994783 -22.087751 -22.182968 -22.365366
0:  -22.366224 -22.384794 -22.305502 -22.206799 -22.188354 -22.226793
0:  -23.158781 -23.09346 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.081383  -10.877977  -10.488077   -9.9778595  -9.392946   -8.861422
0:   -8.394539   -8.00679    -7.738501   -7.4677978  -7.174027   -6.9890027
0:   -6.7493834  -6.5462346  -6.3512015  -6.0958514  -5.773931   -5.44342
0:   -5.621531   -5.318501 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.683578  -3.426745  -3.0701861 -2.641088  -2.2645106 -1.9673486
0:  -1.764586  -1.6117444 -1.5663123 -1.5438962 -1.5125585 -1.605638
0:  -1.6387553 -1.7221408 -1.8286843 -1.8791423 -1.9122386 -1.9583373
0:  -4.0294785 -4.1787753]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5204787 -3.5455546 -3.3782535 -3.0861497 -2.7692657 -2.4880786
0:  -2.2537885 -2.1357093 -2.205895  -2.413176  -2.731833  -3.1862254
0:  -3.5412335 -3.7776484 -3.836944  -3.6758676 -3.4047275 -3.1178775
0:  -3.494721  -3.3885226]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.337841 15.421156 15.550661 15.699427 15.926411 16.16775  16.640053
0:  17.013775 17.425152 17.78474  17.984762 18.135048 18.232548 18.422102
0:  18.672178 18.973858 19.28652  19.507395 19.11698  19.095453]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.455476  12.690271  13.049456  13.429365  13.794725  14.122746
0:  14.4439125 14.7140465 14.951617  15.199989  15.451881  15.652431
0:  15.882933  16.120962  16.383018  16.655706  16.941292  17.199306
0:  17.020292  17.232311 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.518079  11.495253  11.4927845 11.499453  11.468854  11.367689
0:  11.15145   10.904734  10.598938  10.304692  10.052979   9.634428
0:   9.230053   8.813852   8.461549   8.304202   8.312085   8.36594
0:   7.412058   7.1353407]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.760873  14.723071  14.815266  14.960638  15.107817  15.203888
0:  15.360693  15.4144335 15.459419  15.503184  15.482138  15.425257
0:  15.399664  15.416989  15.495002  15.634888  15.802702  15.958072
0:  15.311002  15.262177 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.8640733 -5.8404403 -5.693064  -5.3233953 -4.7887416 -4.1756463
0:  -3.6316752 -3.174705  -2.9057336 -2.7445836 -2.5775628 -2.554401
0:  -2.4489465 -2.371552  -2.299914  -2.1181889 -1.8123703 -1.5346913
0:  -1.8833308 -1.7483654]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.3641186 5.472747  5.4915886 5.445455  5.40459   5.413881  5.3844576
0:  5.172064  4.587719  3.697173  2.6863756 1.7325406 1.1662331 1.0140638
0:  1.1969552 1.5793133 1.9442387 2.1937103 2.0251317 2.172113 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.383775  -6.417956  -6.295337  -6.007836  -5.6641355 -5.3595157
0:  -5.1416793 -5.0185113 -5.0224967 -5.0766892 -5.084172  -5.1487527
0:  -5.0587363 -4.9066234 -4.6936803 -4.3833594 -4.058499  -3.8246217
0:  -4.5109305 -4.6569963]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.472956 13.924648 14.443118 14.955168 15.464626 15.929829 16.407738
0:  16.8577   17.280115 17.681194 18.041473 18.245195 18.409004 18.468576
0:  18.52106  18.634712 18.776978 18.834558 17.428925 17.30467 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.99097  25.76514  25.526943 25.28233  25.102348 24.950886 25.14502
0:  25.161507 25.299809 25.322308 25.13602  24.986263 24.846012 24.879055
0:  25.07249  25.330109 25.608505 25.83036  26.006493 26.096458]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.75558  18.061846 18.45349  18.907501 19.40242  19.891481 20.497894
0:  20.928988 21.347883 21.653263 21.84078  22.038546 22.278282 22.626738
0:  23.043219 23.50919  24.039762 24.59694  24.126099 24.65421 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.16671   11.297211  11.517864  11.749307  11.939032  11.9871855
0:  12.009805  11.860344  11.646837  11.400863  11.120084  10.823832
0:  10.572926  10.327955  10.052738   9.740338   9.390155   9.013098
0:   7.344062   6.992485 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.1015244 4.9687243 4.944499  4.9175467 4.9115324 4.917899  4.9257774
0:  4.9358935 4.8954496 4.810843  4.6851406 4.4936657 4.3977027 4.323044
0:  4.368964  4.462372  4.5963516 4.7446175 4.8412986 4.7440777]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.478532  9.450275  9.45061   9.43649   9.415977  9.35155   9.270908
0:   9.17433   9.061129  9.01494   9.032017  9.085173  9.236253  9.452916
0:   9.745544 10.077522 10.449768 10.802909 10.265263 10.430028]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.5515475 5.9004617 6.285253  6.578326  6.746609  6.753676  6.644869
0:  6.467341  6.249689  6.119417  6.124017  6.197512  6.46274   6.831486
0:  7.303851  7.871055  8.440707  8.893801  8.179431  8.117533 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.028933  10.84289   10.7500105 10.651474  10.523655  10.362237
0:  10.240814  10.089606   9.954455   9.785562   9.585828   9.329521
0:   9.173945   9.108177   9.17457    9.323069   9.518219   9.697
0:   9.028819   8.912836 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.138599  -5.242766  -5.122087  -4.8727612 -4.5584474 -4.3060813
0:  -4.109591  -4.0592694 -4.1395116 -4.242579  -4.295172  -4.35249
0:  -4.227288  -3.9529157 -3.6251092 -3.194512  -2.8051915 -2.5310206
0:  -2.9292798 -2.9002337]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.83673334  0.7597618   0.81333065  0.8824401   0.9229951   0.8804803
0:   0.80079746  0.63563395  0.41293907  0.18123913 -0.06896496 -0.3646865
0:  -0.5601692  -0.66157055 -0.6732197  -0.59133005 -0.52967453 -0.5196295
0:  -1.2291656  -1.2108812 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.971584 21.99247  21.959272 21.825624 21.657188 21.464048 21.2821
0:  21.111929 20.924564 20.747353 20.603159 20.421312 20.347532 20.306086
0:  20.319979 20.413563 20.562025 20.728745 20.488493 20.614767]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1261353  3.002941   2.8969688  2.8071876  2.7116318  2.5636597
0:  2.4607906  2.3256073  2.165576   2.0216637  1.8541193  1.647057
0:  1.4776187  1.3462524  1.2354078  1.1919823  1.1640625  1.0884695
0:  0.19799376 0.04976988]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.706245 8.746553 8.841287 8.938864 9.060504 9.183754 9.316811 9.451409
0:  9.540827 9.617957 9.65938  9.5937   9.528069 9.454124 9.407317 9.441252
0:  9.555019 9.640747 9.433192 9.506704]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.17481  21.274445 21.396072 21.501017 21.647873 21.79367  22.07004
0:  22.213554 22.341728 22.356483 22.223557 22.078882 21.965864 21.897465
0:  21.901245 21.89066  21.830194 21.768333 20.698782 20.674286]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.9718275  9.2654     9.653684  10.04476   10.373386  10.621627
0:  10.783897  10.865378  10.861845  10.845628  10.813649  10.651532
0:  10.523012  10.411928  10.327942  10.346994  10.387442  10.407431
0:   9.334536   9.318571 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.10215  -13.2484   -13.168901 -12.805115 -12.325113 -11.878496
0:  -11.596588 -11.433088 -11.404213 -11.349218 -11.220264 -11.167017
0:  -10.988821 -10.819645 -10.66246  -10.419548 -10.142998  -9.905056
0:  -10.706066 -10.659693]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.92623  37.799503 37.611122 37.381763 37.26112  37.17033  37.455666
0:  37.517944 37.65778  37.61723  37.32489  37.062767 36.825962 36.77598
0:  36.88197  37.054832 37.148853 36.98937  35.99814  35.669273]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.3754783 -6.5799823 -6.666327  -6.6408105 -6.575011  -6.5591273
0:  -6.584871  -6.6181846 -6.6995215 -6.681071  -6.5740666 -6.5129457
0:  -6.374539  -6.255607  -6.182997  -6.0681653 -5.9615645 -5.916323
0:  -6.8222375 -6.809708 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.023582 17.04836  17.089912 17.051395 16.955044 16.785189 16.61277
0:  16.413092 16.216839 16.019524 15.803127 15.500454 15.185434 14.857279
0:  14.560274 14.353207 14.246584 14.168566 13.03674  12.851034]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.601015  13.50729   13.498632  13.460907  13.312454  13.045343
0:  12.8289795 12.472086  12.138777  11.805826  11.3434725 10.888271
0:  10.448549  10.0329     9.72143    9.382844   9.086271   8.86348
0:   8.105827   7.6524196]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3850865 6.2022166 6.137477  6.0909743 6.0213194 5.9223714 5.888117
0:  5.832796  5.799117  5.7584    5.6594644 5.45816   5.3016233 5.175486
0:  5.163735  5.2441463 5.4078183 5.591585  5.424445  5.4608154]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.5196891   0.9818306   1.9327178   3.2201943   4.697493    6.1934423
0:   7.5671024   8.55088     8.888053    8.514463    7.4784346   5.8042536
0:   4.011241    2.4073853   1.1688657   0.56622076  0.4471798   0.6016903
0:  -0.53151846 -0.3150916 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.503119 20.654806 20.914564 21.154785 21.316818 21.246914 21.083452
0:  20.71037  20.231432 19.76631  19.267614 18.874023 18.610462 18.47385
0:  18.460926 18.518816 18.627682 18.745333 18.565952 18.59401 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.8304   23.004633 23.221483 23.435186 23.756165 24.059755 24.588346
0:  24.905165 25.268549 25.497501 25.533443 25.592909 25.655775 25.759136
0:  25.883884 25.992874 26.074543 26.104523 25.467178 25.564941]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3878374 -5.2765455 -4.9933434 -4.6692    -4.3572826 -4.139224
0:  -4.02334   -4.002022  -4.0708    -4.1281996 -4.1678734 -4.264392
0:  -4.2897263 -4.275093  -4.247616  -4.140648  -4.0176954 -3.9500494
0:  -4.770117  -4.7569256]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.671049  4.5288787 4.4204164 4.3058825 4.153951  3.944535  3.692373
0:  3.453613  3.1985095 2.9957864 2.8471248 2.6623156 2.5832212 2.4974794
0:  2.456679  2.4870641 2.557653  2.63289   1.4745975 1.3017497]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.3153694   3.0722497   2.9160948   2.7848806   2.6179533   2.3754041
0:   2.170572    1.9105158   1.682559    1.5151439   1.3364964   1.11237
0:   0.8951349   0.7221284   0.564765    0.41748     0.23459911  0.00922585
0:  -1.1257677  -1.395195  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.370819 10.565618 10.939865 11.430292 12.088797 12.852419 13.869942
0:  14.818902 15.773579 16.614574 17.23714  17.722057 18.052938 18.348457
0:  18.598866 18.776459 18.868391 18.83589  17.73346  17.572296]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.732784 22.609262 22.533506 22.461124 22.43954  22.399485 22.557276
0:  22.522018 22.491302 22.369724 22.066118 21.832523 21.607103 21.509226
0:  21.47541  21.412363 21.295486 21.128113 19.848997 19.653948]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.3999996 6.9993763 6.6787887 6.507652  6.4593306 6.4616776 6.4396224
0:  6.388635  6.1958823 5.9175615 5.6011    5.1031284 4.5996227 4.1202803
0:  3.7300124 3.530475  3.465854  3.4519782 2.4306974 2.254644 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.752318  -5.5307984 -5.1055408 -4.5698686 -4.063129  -3.6827216
0:  -3.4285398 -3.2884374 -3.281629  -3.2898145 -3.3254266 -3.4841428
0:  -3.5859294 -3.6533036 -3.6544347 -3.4967332 -3.2745214 -3.0897002
0:  -3.8010945 -3.9755464]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.271341 19.540892 19.844814 20.126858 20.432554 20.75788  21.170801
0:  21.518003 21.89102  22.174524 22.383787 22.567863 22.78564  23.025354
0:  23.34837  23.665138 23.969156 24.255674 24.494888 24.765108]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.708475  -7.732709  -7.6810884 -7.558223  -7.4484625 -7.416629
0:  -7.430995  -7.5035973 -7.661648  -7.7997255 -7.9140997 -8.09977
0:  -8.199101  -8.268829  -8.280742  -8.204156  -8.110968  -8.038059
0:  -8.6911335 -8.72066  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9063382  -1.7576041  -1.5836549  -1.3781934  -1.1842599  -1.0006242
0:  -0.8093581  -0.6534648  -0.5875468  -0.5701637  -0.6049824  -0.777616
0:  -0.8993764  -0.9901147  -1.0008111  -0.8700776  -0.6482725  -0.43204355
0:  -0.7163129  -0.4286585 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.944778  -5.054748  -5.0457845 -4.9872527 -4.929191  -4.919784
0:  -4.947226  -4.940601  -4.9255424 -4.8287215 -4.668713  -4.578954
0:  -4.459293  -4.395222  -4.4048915 -4.450537  -4.5819583 -4.83497
0:  -7.0886683 -7.3423724]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.019357  -6.8256106 -6.635323  -6.483277  -6.3946767 -6.3687415
0:  -6.362639  -6.324287  -6.3257823 -6.293513  -6.2169366 -6.2377048
0:  -6.149077  -6.0079303 -5.790049  -5.435783  -5.03303   -4.687823
0:  -5.123861  -5.0579743]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3147    4.4234753 4.587119  4.756402  4.9209595 5.027072  5.100953
0:  5.1327543 5.123001  5.1025834 5.105512  5.046872  5.0532084 5.0358515
0:  5.0180864 5.037918  5.0762105 5.089961  4.4102945 4.4983487]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.341595 29.319832 29.40432  29.559017 29.831532 30.124321 30.665363
0:  31.01144  31.437023 31.72744  31.842464 31.938873 32.056725 32.24173
0:  32.4701   32.667244 32.827965 32.840157 32.696903 32.65613 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.03591  21.4781   20.793392 20.111393 19.446213 18.75534  18.32647
0:  17.905863 17.799728 17.890995 18.099043 18.43986  18.823479 19.125721
0:  19.221859 19.027994 18.584839 17.99545  14.132643 13.921068]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.2617083   3.4489403   3.6104193   3.51287     3.0973704   2.3384213
0:   1.2724648   0.06402063 -1.1471095  -2.1212668  -2.7483058  -3.1494756
0:  -3.2372146  -3.172587   -2.9997811  -2.690597   -2.3421397  -2.001883
0:  -2.1366067  -1.9344182 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.036503  -13.053957  -12.913038  -12.685835  -12.455744  -12.318972
0:  -12.273113  -12.289309  -12.420883  -12.502869  -12.575321  -12.753143
0:  -12.845736  -12.940292  -12.975834  -12.89954   -12.779507  -12.664541
0:  -13.440136  -13.4192295]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.0492263 4.2005835 4.4254436 4.608537  4.800234  4.942479  5.1393075
0:  5.298976  5.4616036 5.610289  5.705209  5.746102  5.8050523 5.909387
0:  6.0573072 6.2485127 6.4054575 6.497246  5.9475756 6.0901275]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.6769862e+00  2.0576248e+00  2.4288659e+00  2.6329041e+00
0:   2.6536136e+00  2.4710417e+00  2.1040716e+00  1.6225495e+00
0:   1.0409880e+00  4.6425486e-01 -1.7976761e-03 -4.2980766e-01
0:  -5.9849215e-01 -5.7684946e-01 -3.5044909e-01  4.3374538e-02
0:   5.4871511e-01  9.9541235e-01  3.9039135e-01  7.4148464e-01]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4993834 -3.2597823 -2.8803153 -2.439589  -2.0592074 -1.8182006
0:  -1.7836299 -1.8725553 -2.1404042 -2.4356618 -2.711429  -3.1747603
0:  -3.5708213 -3.9731097 -4.3602366 -4.600864  -4.7374616 -4.882007
0:  -6.6088033 -6.9277844]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -6.0846086  -6.4854045  -6.8055005  -7.139858   -7.4774365  -7.9026017
0:   -8.224764   -8.5917225  -9.017845   -9.49184   -10.106075  -10.819481
0:  -11.5313225 -12.133211  -12.60149   -12.880335  -13.096001  -13.211699
0:  -15.43701   -15.394663 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.0212083 5.0294867 5.1405296 5.2136083 5.275069  5.2853003 5.3108892
0:  5.289757  5.2534895 5.213328  5.14718   5.030014  4.9346685 4.851501
0:  4.782675  4.823926  4.931988  5.0746317 4.5851536 4.685927 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.753188  5.7312093 5.8876896 6.126986  6.4084263 6.6575212 6.9165115
0:  7.088753  7.1783776 7.252245  7.2435284 7.1072836 6.925387  6.7827926
0:  6.647839  6.5903416 6.5449133 6.5055027 5.436906  5.3543897]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.879147 20.069397 20.171673 20.070393 19.842798 19.54088  19.140438
0:  18.676561 18.138046 17.623123 17.132502 16.543596 16.030924 15.529616
0:  15.058505 14.701283 14.500273 14.407633 13.325083 13.347715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.039427  8.937506  8.93598   8.976904  9.029594  9.048641  9.048621
0:  9.016188  8.954248  8.876266  8.774822  8.593238  8.443689  8.299267
0:  8.206923  8.172862  8.170778  8.123532  7.38212   7.2841845]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.60611   18.408749  18.271751  18.130424  17.958658  17.719582
0:  17.568329  17.264786  16.958818  16.615423  16.168102  15.76667
0:  15.412023  15.159156  15.0156145 14.888688  14.742571  14.572454
0:  13.655563  13.649151 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.7365203   1.6591787   1.6575785   1.7414193   1.8398609   1.894444
0:   1.9381309   1.9552579   1.8896246   1.8602061   1.8811841   1.7772002
0:   1.7379918   1.687974    1.6026416   1.5966964   1.5744858   1.5084367
0:  -0.22638083 -0.44937325]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.518134   8.606825   8.841518   9.116759   9.427746   9.741111
0:  10.060932  10.323889  10.536562  10.7122555 10.826343  10.877588
0:  10.987505  11.090883  11.251478  11.4154625 11.573084  11.671286
0:  11.298737  11.278957 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.805019  -9.783018  -9.601381  -9.310747  -8.968622  -8.668032
0:  -8.450943  -8.315685  -8.302143  -8.320766  -8.319387  -8.393608
0:  -8.381647  -8.3567295 -8.324179  -8.23399   -8.122253  -8.044201
0:  -9.10906   -9.103394 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.36687803 -0.44209433 -0.4744625  -0.48329878 -0.50934553 -0.60693836
0:  -0.72232676 -0.8569989  -1.0285892  -1.1655583  -1.2880993  -1.4853177
0:  -1.6224623  -1.7673683  -1.8777399  -1.9172058  -1.8902187  -1.8812227
0:  -2.8546762  -2.8251743 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.083815 12.240926 12.393791 12.432599 12.376888 12.153404 11.935762
0:  11.559376 11.1129   10.66954  10.21747   9.882197  9.698379  9.714857
0:   9.91294  10.240852 10.607201 10.92329  11.019027 11.23064 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.45082092 -0.30237198 -0.09131622  0.12249088  0.27572775  0.32889843
0:   0.34093666  0.3093629   0.23308134  0.19057989  0.1631074   0.05220699
0:  -0.00276947 -0.05568123 -0.08827877 -0.02538252  0.09158945  0.20964432
0:  -0.66396713 -0.56760836]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.649973  -9.89764   -9.882532  -9.746353  -9.494585  -9.27429
0:  -9.174194  -9.259815  -9.481308  -9.652269  -9.639408  -9.461304
0:  -9.0151825 -8.369106  -7.720219  -7.077748  -6.6095414 -6.36933
0:  -6.982921  -7.2222533]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.538276  12.29059   12.046244  11.746954  11.387289  10.936521
0:  10.571721  10.03025    9.344565   8.484472   7.3192954  5.869746
0:   4.2540255  2.6750135  1.1970763 -0.0927577 -1.1744962 -2.0294547
0:  -3.118629  -3.3397746]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.05270004 -0.00648689 -0.24382448 -0.83475256 -1.6318202  -2.5191426
0:  -3.3068023  -3.8765264  -4.252749   -4.3132157  -4.1106057  -3.794251
0:  -3.3039708  -2.6707854  -2.075945   -1.5108061  -1.1193595  -0.9298935
0:  -1.8554864  -2.175551  ]
0: validation loss for strategy=forecast at epoch 3 : 0.2243073284626007
0: validation loss for velocity_u : 0.03609670326113701
0: validation loss for velocity_v : 0.06896542757749557
0: validation loss for specific_humidity : 0.03469834849238396
0: validation loss for velocity_z : 0.4185841381549835
0: validation loss for temperature : 0.07197684794664383
0: validation loss for total_precip : 0.7155224680900574
0: 4 : 18:13:56 :: batch_size = 96, lr = 1.903628792385485e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 4, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0850, 1.0735, 1.0810, 1.1022, 1.1236, 1.1343, 1.1328, 1.1229, 1.1073, 1.0845, 1.0517, 1.0107, 0.9660, 0.9223,
0:         0.8868, 0.8639, 0.8502, 0.8352, 1.1394, 1.1227, 1.1229, 1.1399, 1.1628, 1.1789, 1.1830], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3012, -0.3094, -0.3133, -0.3096, -0.2990, -0.2869, -0.2808, -0.2849, -0.3039, -0.3391, -0.3818, -0.4182,
0:         -0.4389, -0.4414, -0.4254, -0.3897, -0.3361, -0.2724, -0.3105, -0.3176, -0.3264, -0.3316, -0.3301, -0.3248,
0:         -0.3205], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4052, -0.4360, -0.4630, -0.4855, -0.4937, -0.4903, -0.4938, -0.5044, -0.5202, -0.5290, -0.5159, -0.4839,
0:         -0.4336, -0.3742, -0.3180, -0.2549, -0.1774, -0.0779, -0.3372, -0.3719, -0.4163, -0.4498, -0.4846, -0.4979,
0:         -0.5040], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4890, -0.7086, -0.7935, -0.5782, -0.1541,  0.2525,  0.4613,  0.4178,  0.1699, -0.0845, -0.1498, -0.0606,
0:          0.0655,  0.1742,  0.2090,  0.1416,  0.0503, -0.0171, -0.4042, -0.6195, -0.8369, -0.8304, -0.4977, -0.0106,
0:          0.3352], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.0179, 1.0122, 1.0094, 1.0123, 1.0214, 1.0361, 1.0506, 1.0565, 1.0396, 0.9795, 0.8866, 0.7956, 0.7333, 0.7064,
0:         0.7037, 0.7052, 0.6949, 0.6694, 0.6327, 0.5968, 0.5742, 0.5690, 0.5730, 0.5746, 0.5657], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2430, -0.2430, -0.2430, -0.2430, -0.2430, -0.2430, -0.2269, -0.2178, -0.2338, -0.2430, -0.2430, -0.2430,
0:         -0.2430, -0.2430, -0.2384, -0.2292, -0.2155, -0.2064, -0.2430, -0.2430, -0.2430, -0.2430, -0.2430, -0.2407,
0:         -0.2407], device='cuda:0')
0: [DEBUG] Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2430,     nan,     nan, -0.2430,     nan, -0.2430, -0.2430,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2430,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan, -0.2430,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,
0:         -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2430,     nan, -0.2430,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan, -0.2430,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan, -0.2430,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2430,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 4, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2534, 0.2526, 0.2554, 0.2542, 0.2599, 0.2668, 0.2931, 0.3064, 0.3251, 0.3379, 0.3428, 0.3524, 0.3662, 0.3992,
0:         0.4378, 0.4809, 0.5126, 0.5302, 0.2939, 0.3216, 0.3443, 0.3486, 0.3441, 0.3465, 0.3599], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5535, 0.5615, 0.5653, 0.5741, 0.5845, 0.6006, 0.6230, 0.6539, 0.6894, 0.7335, 0.7787, 0.8257, 0.8759, 0.9269,
0:         0.9884, 1.0708, 1.1649, 1.2590, 0.5457, 0.5594, 0.5675, 0.5802, 0.5994, 0.6259, 0.6550], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3621, -0.3554, -0.3589, -0.3603, -0.3694, -0.3770, -0.3887, -0.3974, -0.4060, -0.4165, -0.4298, -0.4417,
0:         -0.4528, -0.4617, -0.4704, -0.4786, -0.4845, -0.4896, -0.3726, -0.3687, -0.3660, -0.3655, -0.3733, -0.3846,
0:         -0.3943], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4891, 0.4148, 0.3480, 0.3061, 0.2975, 0.2890, 0.2634, 0.2299, 0.2365, 0.2690, 0.2833, 0.3482, 0.4169, 0.3911,
0:         0.3769, 0.4578, 0.5369, 0.5316, 0.4704, 0.4097, 0.3661, 0.3073, 0.2715, 0.2526, 0.2207], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.6551, 0.6657, 0.6745, 0.6794, 0.6909, 0.7121, 0.7412, 0.7726, 0.7986, 0.8145, 0.8205, 0.8187, 0.8134, 0.8050,
0:         0.7941, 0.7796, 0.7629, 0.7440, 0.7251, 0.7080, 0.6908, 0.6730, 0.6539, 0.6357, 0.6165], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2417, -0.2442, -0.2409, -0.2426, -0.2473, -0.2518, -0.2505, -0.2489, -0.2478, -0.2413, -0.2396, -0.2458,
0:         -0.2445, -0.2459, -0.2446, -0.2489, -0.2470, -0.2465, -0.2408, -0.2455, -0.2417, -0.2427, -0.2439, -0.2412,
0:         -0.2420], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.0692344456911087; velocity_v: 0.10820010304450989; specific_humidity: 0.04725603014230728; velocity_z: 0.5643941760063171; temperature: 0.11020198464393616; total_precip: 1.1174061298370361; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07551977783441544; velocity_v: 0.11268197745084763; specific_humidity: 0.06402859091758728; velocity_z: 0.514776885509491; temperature: 0.10699040442705154; total_precip: 1.0363179445266724; 
0: epoch: 4 [1/5 (20%)]	Loss: 1.07686 : 0.29983 :: 0.16326 (2.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.062222134321928024; velocity_v: 0.10586394369602203; specific_humidity: 0.06662546098232269; velocity_z: 0.4504728615283966; temperature: 0.09907854348421097; total_precip: 0.5354986190795898; 
0: epoch: 4 [2/5 (40%)]	Loss: 0.53550 : 0.19371 :: 0.15765 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06825000047683716; velocity_v: 0.10872843116521835; specific_humidity: 0.07839912176132202; velocity_z: 0.4391637146472931; temperature: 0.10406836867332458; total_precip: 0.6235451698303223; 
0: epoch: 4 [3/5 (60%)]	Loss: 0.62355 : 0.21002 :: 0.16306 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.10868275165557861; velocity_v: 0.13330814242362976; specific_humidity: 0.11658450961112976; velocity_z: 0.48196184635162354; temperature: 0.14304758608341217; total_precip: 0.7991088032722473; 
0: epoch: 4 [4/5 (80%)]	Loss: 0.79911 : 0.26680 :: 0.17753 (16.24 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [3.62873077e-04 1.99317932e-04 1.96456909e-04 1.66416168e-04
0:  7.10487366e-05 6.91413879e-05 6.10351562e-05 7.62939453e-05
0:  1.32083893e-04 7.96318054e-05 3.76701319e-05 1.57356262e-05
0:  1.76429749e-05 3.76701319e-05 4.52995337e-05 4.52995337e-05
0:  3.00407410e-05 2.71797180e-05 9.10758972e-05 1.63555145e-04
0:  2.00271606e-04 1.97887421e-04 2.28404999e-04 3.43799591e-04
0:  2.81333923e-04 1.92642212e-04 8.48770142e-05 1.84535980e-04
0:  2.41756439e-04 3.07083130e-04 1.86443329e-04 2.32696533e-04
0:  2.96592712e-04 2.55107880e-04 4.06742096e-04 6.62803650e-04
0:  7.34329224e-04 6.53743744e-04 6.17980957e-04 4.99725342e-04
0:  3.96728516e-04 3.08036804e-04 1.85012817e-04 1.20162964e-04
0:  7.00950623e-05 1.09672546e-04 3.52382660e-04 5.92231750e-04
0:  5.22136688e-04 3.58104706e-04 2.37464905e-04 1.27792358e-04
0:  3.52859497e-05 2.81333923e-05 6.48498535e-05 5.24520874e-05
0:  1.28746033e-05 9.05990601e-06 3.33786011e-06 6.19888306e-06
0:  2.47478485e-04 2.49385834e-04 9.53674316e-06 3.81469727e-06
0:  2.38418579e-06 1.19209290e-05 1.23977661e-05 1.71661377e-05
0:  2.00271606e-05 7.62939453e-06 1.95503235e-05 1.95503235e-05
0:  1.71661377e-05 4.76837158e-07 4.76837158e-07 1.43051147e-06
0:  6.19888306e-06 1.47819519e-05 1.91211700e-04 3.15189362e-04
0:  5.76496066e-04 7.17639923e-04 9.81807709e-04 5.52654208e-04
0:  4.32968140e-04 6.15119934e-05 3.52859497e-05 3.00407410e-05
0:  1.23977661e-05 1.19209290e-05 1.57356262e-05 1.66893005e-05
0:  2.62260437e-05 1.81198120e-05 1.43051147e-05 1.19209290e-05
0:  5.72204590e-06 5.24520874e-06 1.90734863e-06 9.53674316e-07
0:  1.90734863e-06 1.43051147e-06 4.76837158e-06 1.00135803e-05
0:  2.14576721e-05 1.76429749e-05 2.19345093e-05 2.81333923e-05
0:  4.14848328e-05 4.24385071e-05 5.00679016e-05 3.52859497e-05
0:  3.24249268e-05 3.09944153e-05 2.86102295e-05 4.29153406e-05
0:  8.86917114e-05 5.00679016e-05 3.52859497e-05 1.23977661e-05
0:  1.52587891e-05 2.28881836e-05 2.95639038e-05 3.38554382e-05
0:  7.67707825e-05 9.58442688e-05 1.95026398e-04 2.76088715e-04
0:  2.91347504e-04 2.54631042e-04 1.84059143e-04 2.67028809e-04
0:  2.05516815e-04 1.51157379e-04 6.00814819e-05 2.40325928e-04
0:  4.28199768e-04 4.71115112e-04 3.11851501e-04 1.85012817e-04
0:  2.26974487e-04 1.95980072e-04 2.85625458e-04 5.16414642e-04
0:  6.25610352e-04 6.86168671e-04 6.46114349e-04 5.34057559e-04
0:  3.99589539e-04 2.88486481e-04 1.71184540e-04 1.11103058e-04
0:  6.81877136e-05 1.10149384e-04 2.03609467e-04 4.04834747e-04
0:  3.27587128e-04 2.13623047e-04 9.72747803e-05 3.05175781e-05
0:  3.71932983e-05 2.57492065e-05 6.19888306e-05 7.91549683e-05
0:  5.05447388e-05 3.05175781e-05 8.53538513e-05 1.33991241e-04
0:  5.42163791e-04 4.83512878e-04 4.48226929e-05 3.95774841e-05
0:  2.62260437e-05 2.81333923e-05 2.24113464e-05 1.38282776e-05
0:  1.66893005e-05 6.19888306e-06 4.76837158e-06 3.81469727e-06
0:  2.86102295e-06 9.53674316e-07 1.43051147e-06 1.90734863e-06
0:  5.24520874e-06 1.28746033e-05 1.58309937e-04 2.97069550e-04
0:  5.55038452e-04 6.40392303e-04 5.26428164e-04 1.29699707e-04
0:  6.53266907e-05 4.24385071e-05 1.76429749e-05 1.95503235e-05
0:  2.76565552e-05 2.76565552e-05 4.62532080e-05 2.73227692e-04]
0: Target values (first 200):
0: [2.4890900e-04 1.6403198e-04 9.8705292e-05 7.9154968e-05 3.7670132e-05
0:  7.5340271e-05 5.3405762e-05 3.7193298e-05 3.2424927e-05 8.1062317e-06
0:  8.1062317e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 4.2915344e-06 1.6689301e-05
0:  1.5735626e-05 1.1920929e-05 9.0599060e-06 5.5313110e-05 5.6743622e-05
0:  4.0054325e-05 4.3869019e-05 4.6253208e-05 5.7220459e-06 1.4543533e-04
0:  1.6736984e-04 2.7370453e-04 6.2036514e-04 6.1464310e-04 1.9073486e-04
0:  1.0118484e-03 1.8849372e-03 1.6055107e-03 1.5511513e-03 1.3632774e-03
0:  9.3173987e-04 8.2015991e-04 2.3937225e-04 2.3508072e-04 1.8644333e-04
0:  8.8691711e-05 3.8623806e-05 3.6716461e-05 1.8119812e-05 1.0967255e-05
0:  1.0013580e-05 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5830994e-04 1.2779236e-04
0:  4.8637390e-05 4.2915341e-05 5.5789948e-05 8.7261200e-05 8.6307526e-05
0:  1.0108948e-04 9.0599060e-05 6.1988831e-05 2.3365021e-05 5.7220459e-06
0:  5.7220459e-06 1.9073486e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 9.0599060e-06 8.5830688e-06 2.2888184e-05
0:  2.8133392e-05 7.0571899e-05 6.0558319e-05 3.5762787e-05 4.4822693e-05
0:  4.1484833e-05 1.9073486e-06 1.1014938e-04 1.3065338e-04 2.2268295e-04
0:  5.1641464e-04 6.0701370e-04 3.0231476e-04 1.1482239e-03 1.3132095e-03
0:  8.8977820e-04 9.4890594e-04 7.1716309e-04 1.7404556e-04 3.5285950e-04
0:  3.4379959e-04 1.2731552e-04 9.0599060e-05 9.0599060e-05 6.0558319e-05
0:  4.1961666e-05 6.1988831e-06 0.0000000e+00 0.0000000e+00 2.3841858e-06
0:  3.8146973e-06 2.3841858e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [ 0.1930685  -0.41866636 -0.81990576 -1.0939975  -1.3300338  -1.6071668
0:  -1.8881068  -2.1504607  -2.481647   -2.7432075  -3.0466304  -3.427464
0:  -3.6874518  -3.7440162  -3.6098256  -3.2637248  -2.8923593  -2.5896544
0:  -3.4889264  -3.3808408 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.102, max = 2.715, mean = 0.284
0:          sample (first 20): tensor([-0.5690, -0.6202, -0.6538, -0.6767, -0.6964, -0.7196, -0.7431, -0.7651, -0.7928, -0.8146, -0.8400, -0.8719,
0:         -0.8936, -0.8984, -0.8871, -0.8582, -0.8271, -0.8018, -0.4538, -0.5175])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.4010067   6.9108276   6.503995    6.0894394   5.659035    5.1887045
0:   4.7649374   4.4014797   4.0317163   3.749575    3.4431481   3.0520015
0:   2.7085652   2.4134607   2.2015958   2.1261396   2.1037388   2.0152988
0:  -0.08942556 -0.37083483]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.561235  9.785625 10.132645 10.449372 10.66749  10.778846 10.927121
0:  11.041033 11.140211 11.233213 11.239195 11.136868 11.110786 11.15559
0:  11.362116 11.70596  12.099794 12.387035 11.310649 11.532269]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5739827 -3.6982093 -3.5945563 -3.451242  -3.3795485 -3.484159
0:  -3.7179189 -4.0467772 -4.5101857 -4.8512244 -5.1477733 -5.4832683
0:  -5.7392077 -5.8815618 -5.901364  -5.714228  -5.444528  -5.1422734
0:  -5.0745273 -4.9279056]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.4995    26.319725  26.163221  25.812218  25.264362  24.420969
0:  23.647686  22.710688  21.737328  20.729523  19.490795  18.251116
0:  17.080872  16.115398  15.393507  14.747005  14.022578  13.2448225
0:  10.647366  10.317385 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.795144  5.0831347 5.5270925 5.9015737 6.2898407 6.706402  7.146959
0:  7.653434  7.950775  8.090828  7.9280376 7.3137693 6.5264964 5.746596
0:  5.2080927 5.085853  5.348707  5.6715765 4.91711   4.6722794]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7496514 3.9184322 4.213098  4.537263  4.8143644 4.9873166 5.136669
0:  5.2227716 5.253545  5.2630415 5.228321  5.112055  5.0870333 5.1460495
0:  5.35411   5.70162   6.1033783 6.4277086 6.010031  6.2774773]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.777963 11.99268  12.259296 12.517712 12.762554 12.987581 13.356551
0:  13.726011 14.068008 14.357341 14.527565 14.633047 14.81615  15.090956
0:  15.567669 16.17064  16.835283 17.437906 17.123013 17.329798]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8308578 -7.074286  -7.1026444 -6.8767276 -6.582813  -6.3433337
0:  -6.4245687 -6.5573363 -6.973909  -7.372728  -7.734446  -8.335556
0:  -8.877415  -9.454327  -9.882971  -9.99222   -9.730818  -9.26989
0:  -9.679449  -9.86179  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7036664 3.5373445 3.487862  3.5877743 3.653433  3.6832225 3.549804
0:  3.5569842 3.4397461 3.4401312 3.4601824 3.195187  2.9066281 2.4775991
0:  2.1665359 2.1450496 2.4538293 2.8323846 0.8824811 0.6230545]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.125002 14.983715 14.924015 14.795996 14.583567 14.218245 13.992163
0:  13.671135 13.374319 13.059792 12.61622  12.147147 11.76897  11.601763
0:  11.675226 11.916191 12.208502 12.415425 10.75952  10.682834]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.0739155 12.387574  12.724913  12.978545  13.147673  13.253456
0:  13.541676  13.856615  14.221212  14.464331  14.532354  14.484407
0:  14.510162  14.673386  15.0791    15.65843   16.248022  16.716702
0:  14.077724  14.279106 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1602616  -1.596663   -1.8324313  -1.7343278  -1.3923631  -0.89076567
0:  -0.50906706 -0.03848505  0.18790436  0.36368704  0.42373753  0.04827499
0:  -0.49719    -1.3184361  -2.1666512  -2.7635427  -2.9586492  -2.909697
0:  -5.287597   -5.3954997 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.863545 13.972858 14.073326 14.021158 14.014671 13.923168 14.390705
0:  14.743782 15.268095 15.714445 15.778009 15.826506 15.681995 15.857466
0:  16.24419  16.77173  17.173923 17.290897 16.283705 16.195534]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.346836  -4.391918  -4.2789345 -4.0511847 -3.8098164 -3.5943098
0:  -3.5093994 -3.4935398 -3.6932287 -3.8828773 -4.125742  -4.564193
0:  -4.924653  -5.259138  -5.426011  -5.3132315 -5.021244  -4.636674
0:  -5.081581  -5.2693615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.964916  5.0918946 5.2220435 5.2559857 5.3342037 5.390735  5.6828485
0:  5.946893  6.2405715 6.4563336 6.495292  6.439783  6.4085608 6.5744042
0:  6.8967147 7.4275374 8.005777  8.541461  8.243107  8.552076 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.673537 15.845486 16.083887 16.299232 16.489777 16.627567 16.898335
0:  17.13887  17.401138 17.605352 17.71425  17.780174 17.949343 18.218098
0:  18.696049 19.28653  19.875656 20.314903 19.52055  19.65966 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.431311  6.546663  6.793435  7.0642767 7.271553  7.3376136 7.27166
0:  7.1530166 6.8709373 6.5980897 6.2905784 5.841542  5.43482   5.011637
0:  4.70889   4.5927444 4.5986333 4.5905213 3.008372  2.8893619]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3636055 4.4508877 4.7338696 5.127547  5.4924273 5.74549   5.8212843
0:  5.9040704 5.8459415 5.852731  5.8720007 5.6996503 5.546881  5.31885
0:  5.2062263 5.337054  5.698307  6.06879   5.000821  4.9892163]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.498314 26.721205 26.926111 26.99001  26.923801 26.761042 26.605461
0:  26.479485 26.29481  26.125774 25.86237  25.459127 25.154507 24.826773
0:  24.620436 24.524992 24.451889 24.358137 23.930246 23.685974]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.359325 10.554552 10.826601 11.115897 11.33283  11.449453 11.664786
0:  11.839832 12.020879 12.174078 12.234194 12.208837 12.280115 12.447923
0:  12.798941 13.309685 13.842513 14.277384 13.165997 13.251262]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.522089  14.603317  14.727161  14.723167  14.660065  14.514351
0:  14.507048  14.496807  14.510851  14.4911995 14.340742  14.118488
0:  13.962784  13.893467  14.00075   14.272291  14.609825  14.879208
0:  13.920533  13.986259 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.271769 17.279406 17.272257 17.173258 17.0565   16.93708  16.964418
0:  17.000551 17.041359 17.003645 16.780668 16.373695 15.952576 15.634832
0:  15.562729 15.713846 15.992603 16.22898  14.885035 15.098429]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.5003543 -4.5819983 -4.451444  -4.203283  -3.8975883 -3.625566
0:  -3.3819833 -3.1571069 -3.0441728 -2.8974824 -2.760203  -2.7708888
0:  -2.7427506 -2.738915  -2.7097573 -2.53048   -2.2700458 -2.0008922
0:  -1.6131625 -1.0621748]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.873961   -2.8845496  -2.658564   -2.175961   -1.6220508  -1.1162038
0:  -0.8448987  -0.6375699  -0.6764741  -0.7380748  -0.8052983  -1.0941672
0:  -1.3262563  -1.5948167  -1.7199111  -1.5467534  -1.0596375  -0.48304987
0:  -0.9109025  -0.87976694]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.17340136  0.21204996  0.42653847  0.6320987   0.78776884  0.8253541
0:   0.9284415   0.98774385  0.9893236   0.9805026   0.84174395  0.5778427
0:   0.29875422  0.14635181  0.08639908  0.19159698  0.2912035   0.3594308
0:  -0.47548342 -0.46759748]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.991224  -9.062671  -8.778895  -8.1855755 -7.4133406 -6.673004
0:  -6.158678  -5.847286  -5.9419594 -6.1864824 -6.5428267 -7.117278
0:  -7.567872  -7.9179163 -8.069244  -7.923586  -7.580044  -7.233527
0:  -7.863059  -7.923581 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.04371834 0.344553   0.9976802  1.7675295  2.4533608  2.9464357
0:  3.2081466  3.3360722  3.2671044  3.1610382  3.0126264  2.6543474
0:  2.3598623  2.1039648  2.0629945  2.2958374  2.6820953  3.0508814
0:  1.207346   1.1052451 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.5427303  4.424467   4.6219153  5.01725    5.4559736  5.860761
0:   6.1771116  6.509316   6.7737393  7.167076   7.699261   8.199198
0:   8.727925   8.998316   8.854948   8.16994    6.8765297  5.1787643
0:  -2.4509902 -4.544497 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.86258507 -0.6307173  -0.20206118  0.3658657   0.9342532   1.372541
0:   1.6609628   1.8255882   1.798183    1.7222877   1.6116414   1.4025455
0:   1.2782755   1.2177124   1.2781944   1.5094223   1.7858958   1.976928
0:   0.42383385  0.3486638 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.0952015 10.150286  10.128088   9.804396   9.331024   8.76889
0:   8.656623   8.597103   8.787035   9.006252   9.055811   9.192834
0:   9.330214   9.848734  10.612178  11.481265  12.167238  12.565979
0:  12.71027   12.949003 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.2264533   2.1045809   2.1204443   2.1649103   2.1448336   2.0093508
0:   1.8129358   1.5888238   1.2616434   0.9617801   0.61935616  0.17038774
0:  -0.2198062  -0.60541725 -0.8571272  -0.90006113 -0.82864237 -0.7177925
0:  -1.5775089  -1.6965113 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.2991157 -4.2720675 -3.9783683 -3.557651  -3.1669536 -2.8754144
0:  -2.647406  -2.4627833 -2.4297404 -2.3961787 -2.4333138 -2.6835656
0:  -2.9378133 -3.1904626 -3.363669  -3.3318439 -3.2113862 -3.0904298
0:  -4.336411  -4.448961 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.264727 18.276098 18.256912 17.969093 17.593744 17.085514 17.004454
0:  16.905346 17.039434 17.161657 17.031103 16.920074 16.736473 16.806623
0:  17.08222  17.531487 17.866705 17.98272  17.101246 17.120184]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1091561e+00 -9.6139956e-01 -5.7020855e-01  2.9878616e-03
0:   5.9910059e-01  1.0984325e+00  1.4456749e+00  1.7353034e+00
0:   1.8228698e+00  1.8594522e+00  1.8636436e+00  1.7086473e+00
0:   1.6717582e+00  1.6788988e+00  1.9039340e+00  2.4115353e+00
0:   3.1121819e+00  3.7561889e+00  3.4721768e+00  3.5867441e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.366287 21.259663 21.301085 21.347301 21.31986  21.102448 20.62059
0:  20.174068 19.57328  19.089657 18.678223 18.108799 17.558794 16.935946
0:  16.457262 16.204754 16.159754 16.080814 14.210397 13.822249]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.507346  -5.9853325 -6.168871  -5.9970274 -5.581912  -5.0662975
0:  -4.6580725 -4.2576065 -4.1369905 -4.0730124 -4.077592  -4.3550253
0:  -4.6009083 -4.9117517 -5.1041784 -4.985303  -4.5367355 -3.9688797
0:  -4.4325867 -4.4428587]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.938263 29.856167 29.802456 29.707687 29.64772  29.527405 29.625938
0:  29.697895 29.778225 29.82566  29.699223 29.56015  29.445942 29.406586
0:  29.523907 29.690754 29.844837 29.92862  28.633015 28.471123]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4441082 3.4124448 3.5670664 3.7251616 3.8950548 3.9504657 4.0383453
0:  4.096502  4.0974607 4.0987973 4.0220733 3.841454  3.680793  3.5381985
0:  3.4591305 3.5331647 3.6629035 3.7840993 2.8929744 2.8162303]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.9148207 -6.3340387 -6.5652122 -6.7851753 -7.0894785 -7.520995
0:  -7.875948  -8.126632  -8.294474  -8.034685  -7.5934258 -7.096965
0:  -6.503313  -5.923828  -5.465307  -5.164427  -5.1249866 -5.4432764
0:  -4.88729   -4.7676888]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.51626   -13.463264  -13.15452   -12.656279  -12.016941  -11.42584
0:  -10.844589  -10.339285  -10.007824   -9.688028   -9.376255   -9.183979
0:   -8.926325   -8.667576   -8.429903   -8.070181   -7.706514   -7.3493476
0:   -8.819092   -8.753265 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.687313 39.859814 39.66353  39.167942 38.530914 37.83748  37.65237
0:  37.55409  37.692142 37.82878  37.673714 37.483112 37.28165  37.263657
0:  37.47665  37.905293 38.377342 38.650116 38.988567 39.564312]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.7723856  7.2201576  7.7215896  8.261392   8.720703   9.080377
0:   9.326095   9.558464   9.630893   9.6435585  9.597919   9.415809
0:   9.328079   9.270571   9.394079   9.666109   9.992819  10.280308
0:   9.494385   9.606581 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.928179  10.818203  10.847554  11.0454235 11.272489  11.4873295
0:  11.597477  11.853278  12.034622  12.260606  12.45664   12.3714905
0:  12.233048  12.0069895 11.934578  12.123038  12.5633    13.044256
0:  11.948862  12.091027 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.999666 13.215751 13.510169 13.77788  13.986695 14.091791 14.188372
0:  14.312159 14.377147 14.478815 14.537172 14.459372 14.386923 14.257944
0:  14.227847 14.34761  14.549114 14.684082 13.219041 13.317915]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.73513  19.612637 19.572084 19.491806 19.396847 19.242432 19.331436
0:  19.355042 19.420027 19.336939 19.008238 18.592188 18.2944   18.183647
0:  18.33996  18.674969 18.965418 19.066416 17.145615 16.660397]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.465637 32.580986 32.61617  32.49401  32.22139  31.836292 31.539959
0:  31.170418 30.826614 30.39273  29.834007 29.239563 28.8354   28.546577
0:  28.554226 28.697437 28.864048 28.920345 27.168964 27.090992]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.4944434 -7.5771146 -7.3754907 -6.879685  -6.294396  -5.7495093
0:  -5.444834  -5.209583  -5.231647  -5.250563  -5.2654014 -5.4996934
0:  -5.6854835 -5.977674  -6.204931  -6.2269263 -6.010677  -5.703918
0:  -6.790088  -6.917298 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.1169066 -7.2289624 -7.104811  -6.7413764 -6.301989  -5.9295974
0:  -5.840034  -5.8051887 -6.0589147 -6.286279  -6.477446  -6.906902
0:  -7.2432    -7.625296  -7.891338  -7.851563  -7.518253  -7.0621066
0:  -7.8217816 -7.91575  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.202506 -13.120444 -12.82525  -12.390685 -11.979674 -11.730475
0:  -11.702716 -11.794818 -12.130479 -12.430289 -12.753282 -13.195849
0:  -13.520503 -13.797009 -13.914985 -13.845193 -13.715465 -13.602437
0:  -14.162093 -14.084424]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.570715  1.594646  1.8013582 2.020752  2.228819  2.3257568 2.4307985
0:  2.5305235 2.5573342 2.6031368 2.5726748 2.4528759 2.3852432 2.360929
0:  2.4546726 2.6839724 2.9341936 3.096769  2.3980541 2.458031 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.3093   17.60626  17.863892 17.969267 18.027945 17.9876   18.179892
0:  18.278738 18.409441 18.534101 18.508661 18.512175 18.562918 18.821192
0:  19.262611 19.793379 20.300173 20.640194 20.506472 20.726194]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.590843  -12.633816  -12.441099  -12.000345  -11.469824  -11.005909
0:  -10.786844  -10.672119  -10.82398   -10.96064   -11.045351  -11.232237
0:  -11.308378  -11.3419895 -11.292339  -11.051199  -10.657454  -10.274385
0:  -10.808283  -10.580635 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.12124395 -0.01102018  0.3053627   0.85198545  1.4444427   2.0053568
0:   2.3866944   2.76686     2.9209592   3.0642014   3.1726327   3.0202622
0:   2.8785074   2.6175847   2.4451022   2.4925418   2.761683    3.0818763
0:   2.256971    2.1206937 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.470991  -15.4266205 -15.127863  -14.567795  -13.909496  -13.322372
0:  -12.915264  -12.69445   -12.779003  -12.934912  -13.180338  -13.583553
0:  -13.875439  -14.136879  -14.225908  -14.125574  -13.897972  -13.630728
0:  -14.59577   -14.61746  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.770248 11.943634 12.212382 12.435972 12.56569  12.5717   12.575974
0:  12.623748 12.642078 12.667608 12.67524  12.51582  12.434757 12.379181
0:  12.529771 12.912678 13.4515   13.942135 13.122103 13.320639]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.2641294   3.136444    3.1483579   3.1856804   3.1250484   2.9439163
0:   2.7288148   2.4545221   2.1125956   1.7447839   1.3214746   0.78875875
0:   0.3620491   0.06386948 -0.04330254  0.06191397  0.25452137  0.40277195
0:  -0.98191833 -0.98780346]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.661879  -10.79907   -10.625551  -10.1329     -9.555595   -9.043997
0:   -8.735928   -8.493303   -8.466675   -8.471647   -8.482046   -8.71336
0:   -8.852898   -8.964102   -8.885052   -8.528654   -7.9812493  -7.3741574
0:   -7.894524   -7.92808  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.813891 -10.951924 -10.878806 -10.652949 -10.359637 -10.192348
0:  -10.079457 -10.110194 -10.353719 -10.620454 -10.990436 -11.521457
0:  -11.964646 -12.355649 -12.575547 -12.595012 -12.52576  -12.421126
0:  -13.206723 -13.371123]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1122518 -3.5373197 -3.7788892 -3.8091378 -3.6747565 -3.5353332
0:  -3.2670588 -3.0504117 -2.9385123 -2.7808518 -2.6985822 -2.6616569
0:  -2.5910907 -2.4215713 -2.235211  -2.100039  -2.0989556 -2.3288627
0:  -2.5826688 -2.6057115]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.0645561   0.87080145  0.82117033  0.8340144   0.7871041   0.6337404
0:   0.4335823   0.2726512   0.02647495 -0.18440771 -0.4257083  -0.8839855
0:  -1.3477368  -1.8476815  -2.2557335  -2.4214544  -2.3980713  -2.3001595
0:  -2.7475939  -2.6535902 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.469871  10.476938  10.547335  10.556082  10.516659  10.411861
0:  10.416693  10.466549  10.508263  10.496389  10.376007  10.1236515
0:   9.89215    9.713473   9.688223   9.844857  10.145094  10.500841
0:   8.80012    8.711899 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.1530657 -6.260618  -6.0984516 -5.8065157 -5.5807333 -5.5741167
0:  -5.860465  -6.283764  -6.94176   -7.5111017 -7.970915  -8.4711
0:  -8.808353  -8.978111  -8.859361  -8.413075  -7.723893  -6.969656
0:  -6.2802353 -6.082576 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.109396 24.061583 24.024643 23.862873 23.664513 23.371181 23.360136
0:  23.244093 23.205452 23.087757 22.749607 22.381859 22.090937 21.960793
0:  22.087645 22.395931 22.74146  22.976393 21.934177 21.863157]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.175675   -2.093121   -1.8306155  -1.5338035  -1.2613711  -1.1127744
0:  -0.9518709  -0.8723216  -0.8567109  -0.81946087 -0.85646486 -0.9389725
0:  -0.9639106  -0.80503273 -0.48325157 -0.03846073  0.4293065   0.8173032
0:   0.2933669   0.8373742 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.1814604  7.355528   7.7800784  8.314547   8.899451   9.330181
0:   9.731393  10.042115  10.315184  10.665684  11.024982  11.335703
0:  11.565279  11.724997  11.732609  11.631823  11.375272  11.004927
0:   7.5566826  7.34968  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.284874 22.328112 22.396334 22.36653  22.29208  22.090937 22.119568
0:  22.028133 21.944092 21.758413 21.332884 20.831924 20.375748 20.039747
0:  19.93322  19.99726  20.075483 20.10132  18.031013 17.746151]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2099133 3.570091  4.111386  4.6537743 5.132377  5.5025673 5.9656606
0:  6.387831  6.7894273 7.1550837 7.3404875 7.3841205 7.4088535 7.543373
0:  7.846268  8.282297  8.723386  9.056488  8.287169  8.528516 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.505061  15.346185  15.31895   15.340741  15.338376  15.250937
0:  15.13677   14.954901  14.646894  14.329595  13.917476  13.345044
0:  12.790094  12.329801  12.071264  12.044127  12.188064  12.3393955
0:  11.120775  10.852277 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4345098 4.2196717 4.0656857 3.9139762 3.76677   3.5591502 3.5409777
0:  3.491389  3.4479518 3.372491  3.1470382 2.8923185 2.680131  2.6726873
0:  2.8592815 3.1965435 3.5290773 3.6956263 2.2555482 2.1904132]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.64296  27.34156  28.245256 29.007664 29.6828   30.12906  30.690437
0:  31.00993  31.24215  31.232382 30.951906 30.40344  29.936417 29.60295
0:  29.537458 29.68767  29.910467 29.991905 27.634623 27.806084]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.502563   -0.86828136 -1.0645165  -1.0997019  -1.0698242  -1.0658069
0:  -1.139935   -1.2535973  -1.6203651  -2.1116967  -2.7021747  -3.5030236
0:  -4.179585   -4.7254577  -5.046637   -5.045818   -4.8627524  -4.7013316
0:  -5.143212   -5.5427747 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [46.101772 46.551376 46.788742 46.75613  46.634956 46.348785 46.590923
0:  46.63683  46.80568  46.77823  46.35218  46.053032 45.78607  45.838913
0:  46.160282 46.468636 46.571674 46.27491  44.229496 44.265327]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.262529 12.410196 12.62244  12.795392 12.910855 12.911112 12.924427
0:  12.946991 12.9175   12.914797 12.860214 12.699908 12.57223  12.479761
0:  12.53878  12.805847 13.193218 13.527075 12.270493 12.360608]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.720479  -12.835864  -12.626388  -12.193576  -11.7196665 -11.431745
0:  -11.294963  -11.282649  -11.507133  -11.707828  -11.959561  -12.408107
0:  -12.781074  -13.119584  -13.283065  -13.189557  -12.932551  -12.648955
0:  -13.101268  -13.105237 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.1492    -6.1874757 -5.969729  -5.5653663 -5.149444  -4.8649793
0:  -4.77403   -4.8151402 -5.096211  -5.358607  -5.6294885 -6.071836
0:  -6.4321027 -6.786117  -7.0306907 -7.067042  -6.9846234 -6.857144
0:  -7.3358655 -7.365044 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.568877  6.6067405 7.697754  8.557648  9.135281  9.3894825 9.62938
0:  9.709565  9.831974  9.866923  9.744688  9.484688  9.148808  8.87623
0:  8.72323   8.713947  8.8552685 9.006253  7.687937  7.9743834]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.609098   4.6124535  4.766439   4.999629   5.2363405  5.3771114
0:   5.3234196  5.218466   4.8285427  4.3812265  3.8270705  2.9990425
0:   2.1907225  1.365447   0.7112689  0.3970766  0.3878646  0.4883728
0:  -1.210134  -1.6086936]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7908702 3.9387555 4.202881  4.4180636 4.6098757 4.769203  4.9238377
0:  5.0715885 5.1532793 5.180888  5.146981  5.0530863 5.0830193 5.1711698
0:  5.4329615 5.7823076 6.13173   6.424479  5.026555  5.3920903]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.578901 25.611946 25.792671 25.906128 25.955393 25.80194  25.61418
0:  25.147148 24.402973 23.411522 22.083895 20.60799  19.235443 18.252275
0:  17.770313 17.889309 18.432518 19.134388 21.101501 21.361298]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.93994856 -0.6077657  -0.01841307  0.63976955  1.2530642   1.6608529
0:   1.8385067   1.7946754   1.4887342   1.1827674   0.9161091   0.6199689
0:   0.523901    0.50325966  0.6198683   0.9186816   1.2938824   1.6163974
0:   1.6414323   2.0526218 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.335028  13.861227  13.525585  13.148493  12.877708  12.571419
0:  12.62311   12.57819   12.633196  12.584787  12.300219  11.989153
0:  11.700754  11.600046  11.67387   11.858044  11.9801855 12.021843
0:  11.50918   11.233536 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.58634   11.556427  11.584778  11.619604  11.633057  11.57201
0:  11.724982  11.856359  12.018031  12.105324  11.987204  11.793148
0:  11.606222  11.586739  11.769802  12.117765  12.476606  12.717713
0:  11.7127495 11.792959 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.0238647 6.988403  7.0252104 7.072564  7.0157175 6.802819  6.4412107
0:  6.0999327 5.637218  5.2315865 4.8308735 4.2728224 3.7540843 3.251767
0:  2.9108157 2.838985  2.9682708 3.1483068 1.5855637 1.372127 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.731434 16.900776 17.190027 17.416456 17.558264 17.519476 17.54392
0:  17.516594 17.55068  17.598318 17.561241 17.483486 17.440825 17.48627
0:  17.690083 18.005718 18.342596 18.518373 17.849884 18.03007 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.002771   -9.827436  -10.50526   -10.979536  -11.290836  -11.534424
0:  -11.84119   -12.04154   -12.338848  -12.468975  -12.479891  -12.661814
0:  -12.789629  -12.939022  -12.999628  -12.800941  -12.374676  -11.833305
0:  -11.162994  -11.0692215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.6566577 5.6377993 5.796816  5.980963  6.060876  6.027527  5.8816166
0:  5.6753    5.324032  4.987728  4.591572  4.078046  3.6978652 3.3780491
0:  3.3051372 3.4884756 3.8227136 4.1858053 4.3856325 4.4366827]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.051186  4.875049  4.8610086 4.874986  4.8331947 4.6452284 4.4774685
0:  4.2170897 3.8791227 3.5179338 3.096905  2.5981011 2.2635598 2.1214437
0:  2.2359886 2.6347487 3.1437685 3.587064  2.1321235 2.107943 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.05187  38.497    37.648785 36.34739  34.449997 31.902802 29.389317
0:  26.813648 24.752857 22.968826 21.290607 19.834892 18.48241  17.349796
0:  16.589622 16.126354 16.086409 16.209946 13.843141 13.501033]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.720581   9.10159    9.599712  10.002656  10.252354  10.365076
0:  10.391354  10.469166  10.432715  10.279363   9.979275   9.388498
0:   8.887709   8.578657   8.582395   8.99896    9.551297  10.0567465
0:   8.499078   7.307613 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.432087 25.440605 25.517048 25.600206 25.700308 25.672476 25.981857
0:  26.089146 26.374428 26.421106 26.218662 26.030266 25.960594 26.089727
0:  26.489073 27.01908  27.52311  27.90722  25.826363 25.957813]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2261558 -2.3681388 -2.325727  -2.086825  -1.7544713 -1.4608836
0:  -1.3158264 -1.2785769 -1.4539771 -1.6332331 -1.8026519 -2.0942044
0:  -2.2843795 -2.4477677 -2.451301  -2.1696858 -1.6428804 -1.008626
0:  -1.0748339 -0.7839289]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.5867662 -2.8306189 -2.817151  -2.6428804 -2.4354334 -2.3328447
0:  -2.323347  -2.4255486 -2.6826792 -2.872119  -3.0927505 -3.3917856
0:  -3.5988426 -3.732355  -3.70533   -3.544137  -3.3643265 -3.2608328
0:  -2.9637337 -2.7061024]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.17958   6.2449813 6.417004  6.6213584 6.7665563 6.813288  6.765251
0:  6.746238  6.595773  6.4303493 6.1776953 5.6731443 5.1461935 4.607454
0:  4.2716045 4.2516775 4.535489  4.9218903 4.3680944 4.365834 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.9496036 7.7039022 7.446521  7.1466994 6.853648  6.5776596 6.5841107
0:  6.682327  6.878092  7.1458797 7.30939   7.464617  7.676869  8.09729
0:  8.637701  9.250995  9.697917  9.896769  9.642052  9.765223 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.625996  6.742836  6.9884577 7.1795115 7.2908244 7.266319  7.4074097
0:  7.5078    7.6221848 7.711988  7.6058116 7.4075084 7.152112  7.0831327
0:  7.129338  7.2823133 7.3470187 7.2547398 5.9179792 5.8244576]
0: validation loss for strategy=forecast at epoch 4 : 0.2999112606048584
0: validation loss for velocity_u : 0.05153374746441841
0: validation loss for velocity_v : 0.08248920738697052
0: validation loss for specific_humidity : 0.09981054812669754
0: validation loss for velocity_z : 0.5418700575828552
0: validation loss for temperature : 0.09270329028367996
0: validation loss for total_precip : 0.9310605525970459
0: 5 : 18:17:57 :: batch_size = 96, lr = 1.857198821839498e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 5, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0719, -0.0647, -0.0577, -0.0510, -0.0445, -0.0381, -0.0319, -0.0257, -0.0198, -0.0141, -0.0084, -0.0030,
0:          0.0026,  0.0078,  0.0130,  0.0183,  0.0235,  0.0285, -0.0410, -0.0338, -0.0271, -0.0204, -0.0142, -0.0080,
0:         -0.0022], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.8701, 1.8490, 1.8279, 1.8070, 1.7859, 1.7650, 1.7443, 1.7236, 1.7031, 1.6826, 1.6623, 1.6420, 1.6219, 1.6019,
0:         1.5820, 1.5623, 1.5426, 1.5232, 1.9537, 1.9305, 1.9074, 1.8842, 1.8613, 1.8383, 1.8156], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5636, -0.5630, -0.5626, -0.5620, -0.5615, -0.5610, -0.5604, -0.5600, -0.5594, -0.5589, -0.5584, -0.5579,
0:         -0.5574, -0.5570, -0.5565, -0.5561, -0.5555, -0.5551, -0.5628, -0.5622, -0.5617, -0.5611, -0.5606, -0.5600,
0:         -0.5594], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4757, -0.4647, -0.4536, -0.4447, -0.4337, -0.4248, -0.4160, -0.4093, -0.4005, -0.3916, -0.3850, -0.3783,
0:         -0.3695, -0.3606, -0.3540, -0.3451, -0.3363, -0.3274, -0.7413, -0.7148, -0.6904, -0.6661, -0.6417, -0.6174,
0:         -0.5953], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.7943, 0.8075, 0.8210, 0.8342, 0.8474, 0.8601, 0.8733, 0.8859, 0.8990, 0.9112, 0.9238, 0.9364, 0.9486, 0.9607,
0:         0.9723, 0.9839, 0.9955, 1.0071, 1.0182, 1.0290, 1.0401, 1.0507, 1.0609, 1.0712, 1.0814], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1764, -0.1824, -0.1883, -0.1942, -0.1966, -0.2002, -0.2026, -0.2049, -0.2085, -0.1098, -0.1158, -0.1217,
0:         -0.1289, -0.1348, -0.1407, -0.1479, -0.1538, -0.1598, -0.0801, -0.0813, -0.0813, -0.0825, -0.0884, -0.0956,
0:         -0.1027], device='cuda:0')
0: [DEBUG] Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0385,     nan,     nan,     nan, -0.0682,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1883,
0:             nan,     nan,     nan,     nan,     nan, -0.2073,     nan,     nan,     nan, -0.2133,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2252,     nan, -0.2263, -0.2275,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0230,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0837,     nan,     nan, -0.1003,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1717,     nan,     nan,     nan,     nan,     nan,     nan, -0.1871,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2168,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2240,     nan,     nan,     nan,     nan, -0.0409,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0302,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1538,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1800,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2180,     nan, -0.2204])
0: [DEBUG] Epoch 5, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8595, -0.8870, -0.8967, -0.8964, -0.8918, -0.8879, -0.8798, -0.8769, -0.8801, -0.8827, -0.8937, -0.9155,
0:         -0.9372, -0.9518, -0.9598, -0.9574, -0.9515, -0.9552, -0.8729, -0.9017, -0.9196, -0.9210, -0.9226, -0.9207,
0:         -0.9137], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1486, -1.1204, -1.1187, -1.1324, -1.1543, -1.1817, -1.2066, -1.2229, -1.2294, -1.2264, -1.2165, -1.2001,
0:         -1.1847, -1.1784, -1.1683, -1.1387, -1.0863, -1.0097, -1.2600, -1.2063, -1.1862, -1.2033, -1.2258, -1.2594,
0:         -1.2795], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6712, -0.6703, -0.6718, -0.6680, -0.6652, -0.6622, -0.6661, -0.6676, -0.6698, -0.6723, -0.6740, -0.6702,
0:         -0.6630, -0.6569, -0.6576, -0.6604, -0.6718, -0.6741, -0.6727, -0.6695, -0.6680, -0.6650, -0.6622, -0.6631,
0:         -0.6614], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0017,  0.0632,  0.1261,  0.1206,  0.0630,  0.0372,  0.0481,  0.0407,  0.0460,  0.0586,  0.0675,  0.1016,
0:          0.1027,  0.0697,  0.0536,  0.0616,  0.0820,  0.0471,  0.0060, -0.0059,  0.0035,  0.0146, -0.0194, -0.0372,
0:         -0.0084], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.1653, 0.1261, 0.1138, 0.1238, 0.1440, 0.1542, 0.1531, 0.1421, 0.1332, 0.1273, 0.1216, 0.1179, 0.1169, 0.1182,
0:         0.1143, 0.0999, 0.0815, 0.0751, 0.0894, 0.1229, 0.1513, 0.1589, 0.1420, 0.1161, 0.1011], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2106, -0.2108, -0.2115, -0.2205, -0.2296, -0.2307, -0.2305, -0.2275, -0.2245, -0.2095, -0.2080, -0.2214,
0:         -0.2268, -0.2327, -0.2318, -0.2356, -0.2288, -0.2265, -0.2178, -0.2202, -0.2236, -0.2293, -0.2363, -0.2341,
0:         -0.2382], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.09881385415792465; velocity_v: 0.15927964448928833; specific_humidity: 0.09859280288219452; velocity_z: 0.563754677772522; temperature: 0.12192050367593765; total_precip: 0.5858391523361206; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1336750090122223; velocity_v: 0.14582616090774536; specific_humidity: 0.17406989634037018; velocity_z: 0.5269728899002075; temperature: 0.17073304951190948; total_precip: 1.1076703071594238; 
0: epoch: 5 [1/5 (20%)]	Loss: 0.84675 : 0.29324 :: 0.18143 (2.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11768936365842819; velocity_v: 0.1755504459142685; specific_humidity: 0.12526848912239075; velocity_z: 0.6308576464653015; temperature: 0.1188943162560463; total_precip: 0.7659129500389099; 
0: epoch: 5 [2/5 (40%)]	Loss: 0.76591 : 0.29266 :: 0.18040 (16.03 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1063363328576088; velocity_v: 0.14129719138145447; specific_humidity: 0.14994730055332184; velocity_z: 0.5840311050415039; temperature: 0.12241771072149277; total_precip: 0.7663594484329224; 
0: epoch: 5 [3/5 (60%)]	Loss: 0.76636 : 0.28169 :: 0.17545 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11866267025470734; velocity_v: 0.18431490659713745; specific_humidity: 0.1837788075208664; velocity_z: 0.5549761652946472; temperature: 0.1407494843006134; total_precip: 1.0405352115631104; 
0: epoch: 5 [4/5 (80%)]	Loss: 1.04054 : 0.33886 :: 0.17974 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.43051147e-06 2.86102295e-06
0:  6.19888306e-06 4.29153442e-06 0.00000000e+00 7.62939453e-06
0:  3.33786011e-06 5.86509705e-05 1.09672546e-04 1.83582306e-04
0:  2.88963318e-04 2.54631042e-04 1.61647797e-04 7.53402710e-05
0:  7.62939453e-05 8.53538513e-05 1.02043152e-04 1.00135803e-04
0:  6.58035278e-05 3.48091125e-05 1.69754028e-04 7.82012939e-05
0:  4.86373901e-05 2.29835510e-04 2.87055969e-04 2.37464905e-04
0:  2.65598297e-04 2.47478485e-04 1.53064728e-04 1.05857849e-04
0:  8.86917114e-05 1.00612640e-04 3.86238098e-05 1.71661377e-05
0:  7.15255737e-06 1.90734863e-06 5.72204590e-06 1.19209290e-05
0:  1.04904175e-05 9.53674316e-06 4.81605530e-05 5.29289246e-05
0:  6.38961792e-05 6.15119934e-05 1.71661377e-05 4.00543213e-05
0:  7.72476196e-05 1.05381012e-04 1.54018402e-04 1.15394592e-04
0:  9.01222229e-05 9.87052917e-05 1.20639801e-04 1.50203705e-04
0:  1.74999237e-04 2.26497650e-04 1.71661377e-04 2.33173370e-04
0:  2.71320343e-04 1.99317932e-04 6.48498535e-05 4.76837158e-07
0:  9.53674316e-07 1.43051147e-06 7.62939453e-06 8.10623169e-06
0:  1.90734863e-05 2.38418579e-05 3.14712524e-05 3.00407410e-05
0:  2.71797180e-05 1.62124634e-05 9.05990601e-06 1.43051147e-06
0:  0.00000000e+00 0.00000000e+00 1.90734863e-06 5.72204590e-06
0:  2.19345093e-05 1.43051147e-05 7.62939453e-06 1.19209290e-05
0:  9.05990601e-06 2.52723694e-05 3.38554382e-05 8.10623169e-06
0:  1.76429749e-05 2.24113464e-05 2.33650208e-05 1.19209290e-05
0:  2.86102295e-06 0.00000000e+00 1.95503235e-05 3.86238098e-05
0:  6.05583191e-05 5.72204590e-06 2.09808350e-05 1.95503235e-05
0:  1.85966492e-05 2.43186951e-05 1.71661377e-05 1.38282776e-05
0:  6.67572021e-06 6.67572021e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.71661377e-05 0.00000000e+00 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 3.91006470e-05 1.36375427e-04
0:  2.36988068e-04 2.24590302e-04 3.19957733e-04 1.10626221e-04
0:  1.09195709e-04 9.15527344e-05 9.96589661e-05 1.00612640e-04
0:  6.43730164e-05 3.67164612e-05 6.72340393e-05 2.19345093e-05
0:  9.05990601e-06 7.67707825e-05 9.63211060e-05 2.24590302e-04
0:  3.34739685e-04 2.77042389e-04 1.22070312e-04 8.82148743e-05
0:  8.24928284e-05 7.48634338e-05 6.43730164e-05 4.48226929e-05
0:  2.62260437e-05 1.19209290e-05 2.38418579e-06 1.28746033e-05
0:  4.00543213e-05 3.76701391e-05 1.46865845e-04 3.05652618e-04
0:  2.59399414e-04 1.19209290e-04 3.19480896e-05 2.43186951e-05
0:  3.57627869e-05 5.67436218e-05 1.13010406e-04 1.75476074e-04
0:  1.56879425e-04 1.02043152e-04 8.34465027e-05 1.36852264e-04
0:  1.90258026e-04 2.34127045e-04 2.91347504e-04 2.59876251e-04
0:  1.99794769e-04 6.91413879e-05 1.04904175e-05 1.00135803e-05
0:  1.09672546e-05 3.81469727e-06 7.15255737e-06 2.57492065e-05
0:  4.86373901e-05 7.05718994e-05 5.96046448e-05 5.14984131e-05
0:  3.86238098e-05 2.62260437e-05 1.52587891e-05 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-06
0:  2.52723694e-05 2.14576721e-05 1.66893005e-05 1.95503235e-05
0:  2.00271606e-05 5.24520874e-06 1.23977661e-05 1.57356262e-05]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 4.7683716e-07 4.7683716e-07 1.7642975e-05
0:  4.2915344e-06 4.2915344e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  9.0599060e-06 1.3065338e-04 2.5606155e-04 2.7465820e-04 2.6178360e-04
0:  1.3113022e-04 9.9182129e-05 4.3392185e-05 2.0027161e-05 3.6239624e-05
0:  4.1961670e-05 6.1511993e-05 8.3923340e-05 6.9141388e-05 6.4849854e-05
0:  1.2397766e-05 3.8146973e-06 2.8610229e-06 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 6.1988831e-06 2.9563904e-05
0:  7.7724457e-05 1.0681152e-04 1.1587143e-04 1.4305115e-04 1.7309189e-04
0:  1.4448166e-04 3.0517578e-05 8.5830688e-06 1.4305115e-06 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  5.7220459e-06 1.4305115e-05 2.2411346e-05 2.5749207e-05 2.3841858e-05
0:  2.2888184e-05 0.0000000e+00 0.0000000e+00 3.8146973e-06 1.6689301e-05
0:  1.9073486e-05 8.5830688e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07 7.1525574e-06
0:  6.6757202e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 9.5367432e-07 9.5367432e-07 1.1920929e-05
0:  1.2874603e-05 7.6293945e-06 7.5340271e-05 2.3841858e-05 1.3351440e-05
0:  8.5830688e-06 3.8146973e-06 3.8146973e-06 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.5367432e-07
0:  9.5367432e-07 9.5367432e-07 1.4305115e-06 1.6689301e-05 1.4209747e-04
0:  2.7179718e-04 2.8038025e-04 2.8419495e-04 3.3712387e-04 3.1375885e-04
0:  7.2002411e-05 4.0054321e-05 4.5299534e-05 4.8160553e-05 6.7234039e-05
0:  9.2029572e-05 7.6770782e-05 7.9154968e-05 4.1007996e-05 2.1457672e-05
0:  5.2452087e-06 1.9073486e-06 4.2915344e-06 4.2915344e-06 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4305115e-06
0:  2.3841858e-06 3.8146973e-06 2.3841858e-06 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07
0:  4.7683716e-07 4.7683716e-07 8.1062317e-06 2.1457672e-05 1.4781952e-05
0:  2.3841858e-05 3.5285950e-05 2.5272369e-05 1.8119812e-05 0.0000000e+00
0:  0.0000000e+00 9.5367432e-06 2.0503998e-05 1.9550323e-05 8.1062317e-06
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [-4.5066323 -4.991709  -5.14525   -4.963841  -4.704433  -4.560921
0:  -4.784304  -5.0772424 -5.678635  -6.1929803 -6.6285815 -7.2478604
0:  -7.7089057 -8.143013  -8.261286  -7.964256  -7.2861447 -6.546224
0:  -6.3760486 -6.2222466]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.545, max = 1.110, mean = -0.549
0:          sample (first 20): tensor([-0.8905, -0.9282, -0.9401, -0.9260, -0.9059, -0.8947, -0.9121, -0.9348, -0.9815, -1.0215, -1.0553, -1.1034,
0:         -1.1392, -1.1729, -1.1821, -1.1590, -1.1063, -1.0489, -0.8434, -0.9334])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7968254 -5.200979  -5.265621  -4.9853725 -4.59999   -4.2959566
0:  -4.339308  -4.4120283 -4.802782  -5.1205196 -5.39511   -5.9194565
0:  -6.364352  -6.8606234 -7.1475353 -7.0475035 -6.567133  -5.955168
0:  -5.5662327 -5.829316 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.530678 14.732774 14.989646 15.108758 15.200434 15.226167 15.701304
0:  16.214317 16.84884  17.374058 17.506256 17.483139 17.401203 17.540386
0:  17.949848 18.54642  19.065111 19.313427 16.780903 16.770924]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8613448  -1.6830268  -1.1413655  -0.3187194   0.5168276   1.2348938
0:   1.651329    1.9711552   1.9624753   1.8910656   1.6926026   1.1593876
0:   0.62864685  0.05425692 -0.22344637 -0.05717039  0.5104852   1.1877308
0:   1.0627604   1.2563243 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4161663 4.438367  4.633268  5.0152383 5.441571  5.7543836 5.880322
0:  5.982666  5.8886523 5.8465066 5.7785463 5.640171  5.551637  5.4196253
0:  5.412299  5.5392976 5.7896986 6.048826  5.6285906 5.713487 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.262587 17.477926 17.67958  17.728222 17.687502 17.576855 17.750034
0:  17.917427 18.114735 18.266468 18.141056 17.9728   17.856071 17.984179
0:  18.451744 19.152822 19.896378 20.51504  19.38311  19.604744]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.763927 18.115736 18.47686  18.779606 19.133032 19.373178 19.960426
0:  20.37944  20.818615 21.089966 21.072107 21.045893 21.106472 21.389679
0:  21.977596 22.741459 23.543983 24.204464 23.371323 23.434895]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.01262712 -0.05995655  0.15805006  0.49994707  0.8464298   1.0549273
0:   1.226717    1.310369    1.2264652   1.1543512   0.9635587   0.5819254
0:   0.22531939 -0.09979773 -0.27823496 -0.2584176  -0.14658594 -0.08052015
0:  -0.70282507 -0.6784992 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.38031  21.568188 21.80616  22.031784 22.23526  22.365335 22.813705
0:  23.207462 23.67386  24.025742 24.0916   24.120634 24.22517  24.483547
0:  25.026632 25.693125 26.304708 26.688751 24.697031 24.734789]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.141639  4.103302  4.2420845 4.352167  4.4138756 4.374386  4.5230284
0:  4.6374955 4.7509193 4.819041  4.677723  4.3963604 4.14731   4.124483
0:  4.325666  4.767113  5.1809087 5.419524  4.1992035 4.11567  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.05905676  0.06161594  0.40769625  0.91842604  1.4475055   1.8549838
0:   2.3271213   2.7583094   3.1267471   3.5301971   3.8735218   4.1214705
0:   4.441821    4.8582783   5.484377    6.292915    7.167471    7.898601
0:   7.5707726   7.9446526 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.526338    6.0105057   5.641811    5.464897    5.331607    5.1827006
0:   4.896621    4.6933756   4.2991495   3.9722366   3.5868113   2.8783455
0:   2.117412    1.2720923   0.6030016   0.30938435  0.4290347   0.7389393
0:  -1.6856132  -2.302628  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.434082 33.458546 33.349392 33.086765 32.86024  32.6107   33.022575
0:  33.398    33.964237 34.374763 34.32461  34.408287 34.522915 35.040844
0:  36.007687 37.045944 37.931366 38.38298  38.61872  38.950935]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.189983 27.62238  27.977486 28.03933  28.043488 27.912354 28.357296
0:  28.708748 29.176128 29.457802 29.33662  29.14278  28.943165 29.04774
0:  29.488216 30.141174 30.692696 30.949127 28.83522  28.969685]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.9478846  6.1723123  6.5719137  7.002633   7.344858   7.554579
0:   7.7796     8.01639    8.214837   8.4104595  8.539595   8.5362015
0:   8.638214   8.835448   9.230755   9.813299  10.402504  10.835015
0:   9.979292  10.270605 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.066855  14.3883095 14.7995205 15.213556  15.60327   15.993719
0:  16.806156  17.648537  18.577158  19.3573    19.753326  19.88089
0:  19.885334  20.049175  20.416548  20.905226  21.32402   21.438229
0:  19.22289   19.261772 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.855749  9.157976  9.48246   9.743809  9.922937  9.994214 10.131629
0:  10.322235 10.46392  10.570612 10.559539 10.337385 10.179527 10.138163
0:  10.308851 10.683626 11.101226 11.411379  9.703348  9.797026]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2267282 3.2475364 3.53022   3.8994951 4.235796  4.453543  4.6470857
0:  4.740513  4.7065554 4.657011  4.501941  4.297802  4.1741524 4.1281853
0:  4.277598  4.5961175 4.905891  5.106992  4.471545  4.5859084]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.337303  -8.61405   -8.483301  -7.916308  -7.1172633 -6.2754016
0:  -5.647262  -5.059191  -4.823919  -4.61457   -4.529742  -4.7564206
0:  -5.0052733 -5.347958  -5.5703015 -5.509475  -5.1479325 -4.6445546
0:  -5.8390737 -6.211354 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5542355 6.5463214 6.599791  6.519225  6.3275995 5.9854736 5.956435
0:  5.95856   6.067609  6.155968  5.9532247 5.664223  5.27316   5.1562805
0:  5.288421  5.634573  5.939459  6.0475883 4.9585204 4.897729 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.0082815   2.8953376   2.9387975   2.9741466   2.8498664   2.5762563
0:   2.3043196   2.1102235   1.8940382   1.7932262   1.6349883   1.2442555
0:   0.83965254  0.46747303  0.27648687  0.35066366  0.56505394  0.74243736
0:  -0.6206789  -0.8952141 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.738155  6.895287  7.1886625 7.495318  7.7652597 7.925374  8.27986
0:  8.578046  8.852579  9.067573  9.020287  8.821705  8.578928  8.534517
0:  8.6868725 9.041174  9.373942  9.526545  8.500212  8.639782 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1657276 6.3131704 6.6225343 6.938383  7.1918607 7.3209934 7.5736794
0:  7.7707114 7.9330654 8.030145  7.8931317 7.6576786 7.444773  7.398313
0:  7.5750914 7.966634  8.369564  8.644192  6.783187  6.8630915]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.368735   -4.040713   -3.5457082  -2.9876704  -2.3909516  -1.8879409
0:  -1.2599225  -0.694098   -0.19911385  0.31460142  0.7479811   1.1412401
0:   1.6539078   2.4098072   3.3443322   4.4283333   5.4059973   6.133331
0:   5.512639    6.0376196 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.4574084  -4.2937055  -3.825046   -3.235476   -2.6162553  -2.128202
0:  -1.5676703  -1.1403346  -0.84903955 -0.57299185 -0.4733219  -0.58788776
0:  -0.7112365  -0.8210907  -0.7906394  -0.5630765  -0.17584515  0.1827321
0:  -1.3731961  -1.2437005 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.044755 13.137472 13.430385 13.762001 13.978114 14.005151 13.891919
0:  13.73465  13.417094 13.104104 12.711382 12.133198 11.630379 11.141591
0:  10.859817 10.843864 10.98357  11.033291  9.28453   9.166248]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.108919   8.329784   8.813722   9.317807   9.755021   9.987499
0:  10.41366   10.700048  11.073629  11.29118   11.289503  11.185438
0:  11.1864805 11.343914  11.787743  12.392731  12.985374  13.431425
0:  11.540379  11.2100935]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4655652 -2.6744084 -2.6021695 -2.265839  -1.8716626 -1.5861106
0:  -1.6193995 -1.7199922 -2.1001296 -2.4156356 -2.7260356 -3.270018
0:  -3.7920594 -4.353391  -4.772885  -4.8085637 -4.454005  -3.9420276
0:  -5.0118394 -5.1789937]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.80081  41.005512 41.108006 41.116695 41.064884 40.828377 41.024162
0:  41.120293 41.32105  41.36635  41.029842 40.70626  40.503822 40.55113
0:  40.93394  41.45552  41.876343 41.994545 40.75501  40.95293 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -3.111342   -3.6476064  -4.006911   -4.232389   -4.4844613  -4.796372
0:   -5.3956265  -6.006393   -6.96271    -7.901195   -8.911747  -10.081713
0:  -11.206875  -12.250278  -13.069365  -13.636671  -13.996986  -14.278358
0:  -16.388912  -16.886951 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.803507  -7.7819386 -7.447131  -7.030279  -6.6654005 -6.4709415
0:  -6.261389  -6.1344404 -6.116025  -6.045595  -6.0830183 -6.268656
0:  -6.361661  -6.300624  -6.0909796 -5.7415724 -5.4507732 -5.3178616
0:  -6.231594  -5.7510576]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.52436  29.520042 29.564362 29.624195 29.772514 29.921928 30.590416
0:  31.162086 31.855083 32.384434 32.494415 32.73496  33.115147 33.86438
0:  35.04532  36.31616  37.41959  38.11896  36.578888 36.792004]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.973577  7.1232743 7.4382625 7.718484  7.91668   8.002647  8.04196
0:  8.118596  8.082327  8.0742    8.008792  7.753341  7.564141  7.3923044
0:  7.426453  7.7377996 8.194496  8.56448   7.557689  7.7338123]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.6851425 15.860336  16.119865  16.302828  16.391533  16.370722
0:  16.234167  16.193565  16.027725  15.906305  15.7372875 15.364103
0:  15.032746  14.664415  14.480152  14.539291  14.74718   14.889345
0:  12.626492  12.529597 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.335789 18.535175 18.762945 18.948463 19.107    19.13798  19.342113
0:  19.333048 19.251326 18.92224  18.309996 17.637707 17.070086 16.743212
0:  16.78382  17.080318 17.569496 18.084513 17.120548 17.586416]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.256239  -13.457701  -13.381203  -13.01832   -12.597605  -12.335705
0:  -12.214277  -12.1979065 -12.5553    -12.933628  -13.568609  -14.566353
0:  -15.530954  -16.358202  -16.804548  -16.841288  -16.626562  -16.34563
0:  -15.6819    -15.824554 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.6816916 -5.088823  -5.235828  -5.1404634 -4.94313   -4.728008
0:  -4.6712193 -4.5819955 -4.7587285 -4.9651017 -5.272226  -5.9377055
0:  -6.5581365 -7.2246394 -7.7019944 -7.7678657 -7.4409394 -6.909431
0:  -6.7356963 -6.6873155]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.586306  15.654705  15.789734  15.883747  15.945023  15.929541
0:  16.149076  16.322706  16.470387  16.531862  16.326809  16.025946
0:  15.696087  15.532278  15.566456  15.7427025 15.921139  15.978739
0:  14.592442  14.842541 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3365021e-05 -4.0794134e-01 -6.1740780e-01 -7.2310019e-01
0:  -8.2937622e-01 -1.0067806e+00 -1.2092991e+00 -1.4782381e+00
0:  -1.9031048e+00 -2.3189011e+00 -2.8117218e+00 -3.4193077e+00
0:  -3.9173298e+00 -4.3400106e+00 -4.6363621e+00 -4.7578716e+00
0:  -4.8203468e+00 -4.9002829e+00 -6.7384491e+00 -6.9304790e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.579838  6.4805136 6.5769    6.7788153 6.944321  6.9718513 6.809816
0:  6.5795016 6.124707  5.65335   5.1021757 4.349643  3.6459332 2.9601378
0:  2.5091393 2.3538277 2.435163  2.5483909 1.5234079 1.2959003]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.900019 26.004414 26.063328 25.962809 25.825253 25.58807  25.72797
0:  25.855011 26.021753 26.064384 25.754673 25.38527  25.039988 24.897072
0:  25.109533 25.496313 25.91612  26.144226 24.191217 24.095604]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.08912277  0.10887098  0.42041826  0.61007404  0.42916822 -0.17129421
0:  -1.1489139  -2.193098   -3.276595   -4.085101   -4.660845   -5.204465
0:  -5.5740156  -5.7989016  -5.7836466  -5.3862486  -4.750575   -4.058178
0:  -4.217601   -3.9790869 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.100368 17.372717 17.741394 17.949816 18.034782 18.008842 18.136337
0:  18.266296 18.40387  18.51299  18.439175 18.25845  18.179955 18.20539
0:  18.491354 18.960665 19.438463 19.749332 17.742039 18.001587]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3298564 -2.6409707 -2.6673093 -2.4132733 -2.066176  -1.7500644
0:  -1.5349002 -1.3102708 -1.2942462 -1.2988572 -1.4139352 -1.8453755
0:  -2.3123202 -2.8670897 -3.3159823 -3.4821677 -3.4173446 -3.26047
0:  -5.4509053 -5.867747 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.789919  4.9833703 5.411127  5.9651036 6.5189276 6.905061  7.1496596
0:  7.248885  7.1215796 6.947707  6.6483727 6.0813274 5.4651046 4.7997637
0:  4.247806  3.878709  3.6764052 3.4578052 2.2178993 1.9760404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.04515   -15.837441  -16.211979  -16.229939  -16.034958  -15.783643
0:  -15.397206  -15.039567  -14.770726  -14.248424  -13.652581  -13.032594
0:  -12.127608  -11.074178   -9.837083   -8.534565   -7.3009186  -6.374956
0:   -6.1057014  -5.1682644]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.4163065 7.175757  7.0723977 6.9390664 6.7753057 6.5308247 6.581611
0:  6.6266394 6.6990204 6.68851   6.384078  5.9546976 5.5355206 5.44763
0:  5.6769695 6.1894913 6.6606393 6.908848  5.1587534 5.022135 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.343603  -5.3888464 -5.2345505 -4.971695  -4.7643795 -4.6935253
0:  -4.6175547 -4.596633  -4.686365  -4.7897167 -5.026414  -5.399391
0:  -5.7187195 -5.88772   -5.8682623 -5.6376476 -5.3885717 -5.263133
0:  -6.6190643 -6.7127037]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.125361   7.982386   7.914689   7.790492   7.7417545  7.7553015
0:   8.1729355  8.693892   9.27061    9.792663  10.010042  10.022293
0:  10.135318  10.555969  11.34864   12.47835   13.638512  14.512253
0:  13.695242  14.452463 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.786848  12.710061  12.54101   11.995938  11.316679  10.5009365
0:  10.222574   9.982023   9.921343   9.744626   9.141447   8.551695
0:   7.8804226  7.723424   7.966899   8.489115   8.843581   8.863871
0:   8.36866    8.435624 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.550087 20.783722 21.034798 21.195656 21.286434 21.201231 21.428223
0:  21.594488 21.815483 21.932587 21.760548 21.521511 21.328812 21.335554
0:  21.660982 22.232378 22.87317  23.356232 21.652075 21.717453]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.773914 17.556992 17.505295 17.479553 17.418144 17.201551 17.215263
0:  17.0873   16.940239 16.678995 16.070553 15.341845 14.645098 14.207607
0:  14.16062  14.378206 14.629242 14.760911 12.573696 12.255389]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.442922   -2.6715922  -2.7478642  -2.5762     -2.2158875  -1.7479215
0:  -1.2917285  -0.84789896 -0.66044855 -0.5541711  -0.53876305 -0.7309551
0:  -0.87831116 -1.0354972  -1.070725   -0.8878093  -0.5076051  -0.08418274
0:  -0.6339202  -0.5914979 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.948782   9.940603  10.094351  10.220186  10.264355  10.139263
0:  10.2745695 10.418224  10.599413  10.750643  10.589258  10.298376
0:   9.981548   9.924677  10.163824  10.692602  11.194813  11.488165
0:  10.364456  10.534972 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.9294195 -3.0660276 -2.9137774 -2.5300436 -2.141241  -1.9309902
0:  -1.9262252 -2.0576525 -2.47931   -2.8675146 -3.3355064 -3.9816723
0:  -4.5634155 -5.1130447 -5.4157906 -5.4627905 -5.3768883 -5.28487
0:  -5.183902  -5.3796415]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.588253  16.737627  16.90107   16.859024  16.637691  16.206955
0:  16.052227  15.882954  15.806154  15.697225  15.349415  14.964561
0:  14.573452  14.4422455 14.532392  14.7933035 14.945318  14.891547
0:  12.662846  12.647158 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.069096   -1.9455371  -1.5936561  -1.0625906  -0.5694685  -0.16995144
0:   0.05981731  0.34288168  0.42756033  0.5117326   0.45957232  0.00402927
0:  -0.5591674  -1.2489886  -1.7504339  -1.8572903  -1.5692616  -1.0895171
0:  -1.4315906  -1.4663601 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.7755923  1.6768641  1.8571119  2.2649393  2.6725554  2.959299
0:  2.9536247  2.9574237  2.729138   2.5962677  2.4805388  2.0700507
0:  1.629693   1.0841303  0.71545696 0.7696638  1.2859335  1.9643435
0:  1.4950666  1.353425  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.554846   -9.941572  -10.001314   -9.856983   -9.727509   -9.749195
0:   -9.951849  -10.242796  -10.717535  -11.105427  -11.4923115 -12.004166
0:  -12.35062   -12.545319  -12.525068  -12.234932  -11.897778  -11.639645
0:  -12.507019  -12.656958 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.646151  6.646731  6.834253  6.967841  7.029565  6.868024  6.7765265
0:  6.5586047 6.2834673 6.0071673 5.6316123 5.203296  4.9505787 4.971114
0:  5.2694235 5.8521376 6.487507  6.971289  6.1666374 6.414448 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.4392047 -6.6982436 -6.68562   -6.4624305 -6.188273  -6.025623
0:  -6.1707654 -6.387854  -6.8614    -7.2686973 -7.6019473 -8.155226
0:  -8.603985  -9.037886  -9.237879  -9.01799   -8.358383  -7.4909415
0:  -7.489466  -7.474095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4569106  -2.4360652  -2.076439   -1.4748988  -0.76751566 -0.13682842
0:   0.31903076  0.68529177  0.7363391   0.742496    0.67195845  0.34427547
0:   0.12731886 -0.11968565 -0.1871872  -0.01349258  0.3610854   0.7213135
0:  -0.45472288 -0.28389025]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.548254   0.81322813 1.2950497  1.9172735  2.5357184  3.058294
0:  3.5745642  4.0913258  4.434319   4.7382793  4.9036503  4.8034344
0:  4.7585564  4.655339   4.7061844  4.9925604  5.381631   5.7148395
0:  4.2140026  4.1955833 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.706115  7.6054573 7.6928782 7.8385973 7.9361587 7.882634  7.793547
0:  7.6284323 7.3104105 6.939639  6.434811  5.737358  5.03477   4.399433
0:  3.8909762 3.5764785 3.3056736 2.9963067 1.0190458 0.7739067]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.619501 20.150286 20.637978 20.863953 21.01975  21.087467 21.615768
0:  22.13619  22.765932 23.30558  23.51446  23.688093 23.865873 24.300028
0:  24.992853 25.797663 26.427427 26.76127  25.476238 25.71074 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.207653  10.021961   9.910965   9.678984   9.404275   9.035312
0:   9.020507   9.032051   9.049744   8.954958   8.487528   7.861732
0:   7.1914244  6.795569   6.7366276  6.864722   7.0108747  6.9759483
0:   6.41477    6.296018 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.013685  12.700157  12.543886  12.313791  11.961801  11.450201
0:  10.989707  10.504808   9.941137   9.428631   8.790383   8.060645
0:   7.371294   6.842704   6.5066056  6.3908634  6.3365035  6.221698
0:   4.8670063  4.5999193]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.694984  9.49374   9.44699   9.468059  9.539098  9.588409  9.674763
0:   9.780552  9.871637 10.127138 10.442108 10.783087 11.284117 11.853441
0:  12.66109  13.596952 14.663628 15.64566  17.437042 17.790966]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.3040004 -7.148991  -6.673474  -6.048552  -5.4532237 -5.014113
0:  -4.700834  -4.422913  -4.331754  -4.2404447 -4.283796  -4.565228
0:  -4.8444266 -5.1094537 -5.168305  -4.9351745 -4.5777698 -4.2431083
0:  -4.4864573 -4.264931 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.7084494 6.8268657 7.203521  7.5436044 7.822646  7.9369965 7.954233
0:  7.9246593 7.8813486 7.8515563 7.8168693 7.5827446 7.4118485 7.224193
0:  7.193524  7.4791017 8.00669   8.578638  7.7455144 7.82314  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.926373  -4.8879323 -4.570287  -4.096825  -3.5486884 -3.1143079
0:  -2.79075   -2.5796084 -2.6236424 -2.7635808 -3.0390496 -3.4971213
0:  -3.921678  -4.274553  -4.4418797 -4.3901258 -4.188048  -4.0061507
0:  -5.4005866 -5.616034 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.284609  2.340504  2.5728338 2.9384751 3.3082888 3.6244798 3.8398845
0:  4.0691843 4.1285777 4.1832967 4.186368  4.0151362 3.9048524 3.796863
0:  3.9016106 4.255597  4.8031406 5.33986   4.722041  4.9014893]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.025936 16.139149 16.366072 16.554134 16.54803  16.321016 16.080383
0:  15.856852 15.572319 15.297756 14.869589 14.14626  13.387067 12.622807
0:  12.144818 12.063974 12.270523 12.567251 10.297854 10.223799]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.578352 11.50342  11.587142 11.593096 11.561837 11.405362 11.492924
0:  11.529675 11.669676 11.769877 11.658533 11.466947 11.352589 11.431606
0:  11.867609 12.589315 13.408911 14.144144 12.817451 12.862771]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8978696  -2.8606238  -2.591435   -2.174169   -1.6044698  -1.046751
0:  -0.49014854 -0.05486584  0.16537762  0.3822918   0.49631596  0.52516174
0:   0.58186865  0.6730342   0.8488703   1.0941854   1.3482852   1.557169
0:   0.29126072  0.32092285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1998277 -3.3340516 -3.2027383 -2.9358068 -2.6054807 -2.3593717
0:  -2.0445485 -1.803895  -1.6552453 -1.5551186 -1.6505051 -1.896565
0:  -2.151443  -2.2982135 -2.2703424 -2.0422683 -1.8005109 -1.6347575
0:  -2.4858584 -2.5803142]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.823868 21.301964 21.896383 22.454472 22.896828 23.165272 23.62164
0:  24.057365 24.523808 24.932537 25.089851 25.083195 25.088474 25.174316
0:  25.50272  25.996674 26.495098 26.798584 24.82211  24.861252]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7471828  -2.3037062  -1.458703   -0.5453105   0.27047777  0.8775492
0:   1.4331932   1.8449936   2.1301107   2.4421024   2.6287484   2.673195
0:   2.7539473   2.933598    3.2490735   3.7543092   4.252286    4.624939
0:   3.9523258   4.302699  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.439613 36.85007  37.231113 37.496548 37.67517  37.672955 38.0109
0:  38.191853 38.380424 38.39088  38.03056  37.653404 37.396057 37.35358
0:  37.652752 38.103325 38.480705 38.60335  36.66409  36.96763 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.808403 14.763636 14.713381 14.550245 14.303099 13.953307 13.829077
0:  13.683963 13.581934 13.457073 13.135773 12.76778  12.421217 12.338142
0:  12.510541 12.890543 13.282715 13.546388 12.076912 12.117052]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.8481297 -7.9508843 -7.834376  -7.4415708 -6.946092  -6.4956956
0:  -6.306375  -6.196201  -6.3437066 -6.4788766 -6.571593  -6.8440413
0:  -7.051246  -7.2561307 -7.290426  -6.972298  -6.282434  -5.4589896
0:  -5.8681445 -5.724788 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.746155 33.675728 33.33212  32.71889  32.103035 31.389536 31.543982
0:  31.657082 31.991728 32.057297 31.514633 31.115118 30.700027 30.823635
0:  31.416187 32.175694 32.685944 32.6236   32.49755  32.528786]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.953427  15.790142  15.664694  15.444438  15.16954   14.784278
0:  14.621386  14.466584  14.398824  14.343353  14.112406  13.750022
0:  13.39322   13.116604  13.0106125 13.085584  13.181223  13.192568
0:  11.168968  11.317444 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.67834  29.821358 30.032236 30.206808 30.306707 30.308613 30.441565
0:  30.517859 30.628984 30.648647 30.494831 30.241816 30.133406 30.150862
0:  30.498463 31.000278 31.497423 31.85111  29.404146 29.266174]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.774304  4.662139  4.736541  4.9380493 5.2782145 5.6407623 6.029643
0:  6.445409  6.6418486 6.849683  6.9801207 6.945007  6.9160028 6.861717
0:  6.942936  7.1322656 7.4499526 7.7170496 7.8184037 7.9647746]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.289167 18.677525 18.971466 18.960855 18.913162 18.746899 19.088797
0:  19.389343 19.843702 20.21512  20.300808 20.368279 20.50942  20.949736
0:  21.669157 22.638756 23.612822 24.393345 24.05801  24.63723 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.073206 21.899813 21.605547 21.114388 20.642977 20.030046 20.05901
0:  19.974665 20.057764 19.933664 19.312986 18.85188  18.360846 18.364754
0:  18.73192  19.3165   19.762861 19.850214 20.094835 20.010618]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.609528 13.821819 14.346197 15.032223 15.77668  16.405127 17.142426
0:  17.782883 18.306652 18.670605 18.742302 18.585579 18.42003  18.395401
0:  18.636354 19.128586 19.6492   19.957518 17.919495 17.710846]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.887817   9.155948   9.54755    9.8928175 10.138664  10.273078
0:  10.519277  10.743044  10.903959  10.929439  10.687348  10.292929
0:   9.959539   9.818959   9.979139  10.376677  10.770376  10.978755
0:   9.257477   9.40927  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.878891   12.101639   12.314186   12.330032   12.100288   11.52273
0:  10.65871     9.61223     8.296927    6.9753523   5.677158    4.3596044
0:   3.2887611   2.4760907   2.006772    1.9049325   2.0040655   2.1410937
0:   1.0376363   0.95070934]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0302873  -3.8994012  -3.4040275  -2.6367097  -1.8151822  -1.1502419
0:  -0.7837157  -0.60913134 -0.81331396 -1.1007571  -1.491149   -2.117474
0:  -2.7041707  -3.2722745  -3.6263995  -3.6399703  -3.3594394  -2.9866004
0:  -3.946282   -3.8028016 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.302839 14.957762 15.721617 16.325497 16.768518 17.055033 17.374651
0:  17.697996 17.950256 18.14188  18.143887 17.933361 17.763838 17.67615
0:  17.855198 18.263657 18.804892 19.257055 17.301697 17.186756]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.17627954 -0.44562435 -0.5170517  -0.50825214 -0.5033603  -0.5507102
0:  -0.46904135 -0.37864876 -0.2718315  -0.17029715 -0.18094158 -0.29144955
0:  -0.34034157 -0.2044859   0.14383936  0.657773    1.1542201   1.5089183
0:   0.7765231   0.8244424 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.155214  6.2032967 6.4110837 6.703493  6.9477034 7.101532  7.109897
0:  7.1382017 6.9917707 6.870878  6.710816  6.3843226 6.084528  5.7849574
0:  5.6915474 5.880056  6.2676415 6.6296406 5.414791  5.419144 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.042543 19.01182  18.998835 18.906286 18.823914 18.591787 18.718668
0:  18.719942 18.844273 18.875834 18.648136 18.456753 18.311922 18.393803
0:  18.759289 19.354279 19.983212 20.462757 19.85915  19.91634 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.246141    3.067585    3.0529299   3.1473746   3.2083557   3.1498241
0:   3.1264725   3.0800009   2.9047756   2.641356    2.1821876   1.485714
0:   0.8111286   0.20858192 -0.15134287 -0.19608831 -0.07913923  0.02091837
0:  -0.8916259  -0.8280034 ]
0: validation loss for strategy=forecast at epoch 5 : 0.2775728404521942
0: validation loss for velocity_u : 0.09384246915578842
0: validation loss for velocity_v : 0.12255945801734924
0: validation loss for specific_humidity : 0.15120843052864075
0: validation loss for velocity_z : 0.5048273801803589
0: validation loss for temperature : 0.0963149294257164
0: validation loss for total_precip : 0.6966843008995056
0: 6 : 18:21:58 :: batch_size = 96, lr = 1.8119012895995104e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 6, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5568, -0.5781, -0.5973, -0.6154, -0.6335, -0.6527, -0.6740, -0.6978, -0.7240, -0.7519, -0.7801, -0.8070,
0:         -0.8315, -0.8528, -0.8705, -0.8842, -0.8943, -0.9007, -0.5586, -0.5779, -0.5959, -0.6130, -0.6298, -0.6468,
0:         -0.6643], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1865, -1.1827, -1.1735, -1.1631, -1.1541, -1.1473, -1.1416, -1.1337, -1.1201, -1.0974, -1.0630, -1.0154,
0:         -0.9539, -0.8798, -0.7955, -0.7042, -0.6084, -0.5110, -1.2080, -1.2088, -1.2055, -1.2009, -1.1971, -1.1948,
0:         -1.1919], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3443, -0.3459, -0.3806, -0.4171, -0.4513, -0.4693, -0.4875, -0.5047, -0.5188, -0.5333, -0.5442, -0.5516,
0:         -0.5619, -0.5671, -0.5646, -0.5609, -0.5586, -0.5566, -0.3479, -0.3492, -0.3836, -0.4189, -0.4521, -0.4699,
0:         -0.4879], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2202, -1.1931, -1.2021, -1.2494, -1.3192, -1.3845, -1.4273, -1.4273, -1.3845, -1.3057, -1.2044, -1.1031,
0:         -1.0130, -0.9432, -0.8914, -0.8599, -0.8464, -0.8577, -1.2224, -1.2269, -1.2517, -1.2945, -1.3372, -1.3688,
0:         -1.3733], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.5854, -0.5919, -0.5832, -0.5590, -0.5234, -0.4827, -0.4429, -0.4067, -0.3749, -0.3474, -0.3248, -0.3072,
0:         -0.2947, -0.2842, -0.2728, -0.2578, -0.2386, -0.2151, -0.1876, -0.1564, -0.1217, -0.0846, -0.0472, -0.0134,
0:          0.0137], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2305, -0.2448, -0.2460, -0.2472, -0.2472, -0.2472, -0.2484, -0.2484, -0.2484, -0.2329, -0.2448, -0.2472,
0:         -0.2472, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2329, -0.2460, -0.2484, -0.2484, -0.2484, -0.2484,
0:         -0.2484], device='cuda:0')
0: [DEBUG] Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1935,     nan,     nan, -0.2424,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2412,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2460,     nan, -0.2460,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2042,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2377,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2448,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2448,     nan,     nan,     nan,     nan, -0.2245,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2460,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2233,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2424,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2424,     nan,     nan, -0.2401,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2401,     nan, -0.2460])
0: [DEBUG] Epoch 6, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.6574, -1.6733, -1.6668, -1.6456, -1.6224, -1.6106, -1.6049, -1.6043, -1.6134, -1.6221, -1.6320, -1.6545,
0:         -1.6672, -1.6721, -1.6656, -1.6325, -1.5910, -1.5529, -1.6625, -1.7046, -1.7147, -1.6993, -1.6741, -1.6486,
0:         -1.6213], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1782,  0.1710,  0.1369,  0.0835,  0.0228, -0.0327, -0.0910, -0.1423, -0.1762, -0.2088, -0.2305, -0.2535,
0:         -0.2939, -0.3485, -0.4011, -0.4163, -0.3859, -0.3139,  0.1422,  0.1506,  0.1256,  0.0764,  0.0151, -0.0549,
0:         -0.1162], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5323, -0.5354, -0.5416, -0.5383, -0.5328, -0.5243, -0.5232, -0.5202, -0.5219, -0.5249, -0.5329, -0.5371,
0:         -0.5383, -0.5360, -0.5406, -0.5427, -0.5539, -0.5576, -0.5251, -0.5251, -0.5231, -0.5155, -0.5118, -0.5083,
0:         -0.5050], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0870, -0.0504,  0.0445, -0.0164, -0.1148, -0.0861, -0.0876, -0.1186, -0.1034, -0.1321, -0.1982, -0.1590,
0:         -0.0988, -0.1790, -0.2445, -0.1937, -0.1579, -0.2057, -0.0771, -0.1198, -0.0729, -0.1345, -0.2246, -0.1891,
0:         -0.1809], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.2488, -1.2682, -1.2538, -1.2183, -1.1730, -1.1393, -1.1193, -1.1092, -1.1002, -1.0883, -1.0750, -1.0595,
0:         -1.0395, -1.0103, -0.9766, -0.9412, -0.9120, -0.8850, -0.8572, -0.8268, -0.7998, -0.7831, -0.7724, -0.7604,
0:         -0.7386], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1918, -0.1941, -0.1953, -0.2093, -0.2187, -0.2189, -0.2209, -0.2124, -0.2078, -0.1841, -0.1845, -0.2005,
0:         -0.2062, -0.2157, -0.2180, -0.2187, -0.2108, -0.1988, -0.1811, -0.1877, -0.1900, -0.2014, -0.2049, -0.2047,
0:         -0.2059], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.11711455136537552; velocity_v: 0.15249387919902802; specific_humidity: 0.18789207935333252; velocity_z: 0.5444409847259521; temperature: 0.12533022463321686; total_precip: 1.4722464084625244; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1497569978237152; velocity_v: 0.19926810264587402; specific_humidity: 0.19273994863033295; velocity_z: 0.47050511837005615; temperature: 0.17345286905765533; total_precip: 0.8724654316902161; 
0: epoch: 6 [1/5 (20%)]	Loss: 1.17236 : 0.35673 :: 0.19032 (2.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1572110950946808; velocity_v: 0.1928938776254654; specific_humidity: 0.20986758172512054; velocity_z: 0.6136360764503479; temperature: 0.1712716668844223; total_precip: 1.0556080341339111; 
0: epoch: 6 [2/5 (40%)]	Loss: 1.05561 : 0.36718 :: 0.18817 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1436263620853424; velocity_v: 0.1999777853488922; specific_humidity: 0.21629424393177032; velocity_z: 0.6028363704681396; temperature: 0.1481848806142807; total_precip: 0.8156518340110779; 
0: epoch: 6 [3/5 (60%)]	Loss: 0.81565 : 0.32097 :: 0.18404 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14216488599777222; velocity_v: 0.21006034314632416; specific_humidity: 0.20043689012527466; velocity_z: 0.5752171874046326; temperature: 0.13752636313438416; total_precip: 0.8540069460868835; 
0: epoch: 6 [4/5 (80%)]	Loss: 0.85401 : 0.32077 :: 0.18301 (16.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [1.04427338e-04 2.09808350e-04 3.75270844e-04 4.00543213e-04
0:  2.95639038e-04 2.92778015e-04 4.86850739e-04 5.52177429e-04
0:  4.56809998e-04 4.63008881e-04 3.81469727e-04 3.10897827e-04
0:  2.27928162e-04 1.33037567e-04 6.00814819e-05 4.24385071e-05
0:  1.00135803e-04 2.83718109e-04 1.23023987e-04 8.86917114e-05
0:  1.55448914e-04 1.48296356e-04 7.05718994e-05 2.90870667e-05
0:  4.48226929e-05 2.19345093e-05 5.38825989e-05 1.24454498e-04
0:  1.06334686e-04 4.10079956e-05 5.72204590e-06 1.19209290e-05
0:  1.76429749e-05 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 1.43051147e-06 1.43051147e-06 4.76837158e-07
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  1.43051147e-06 1.90734863e-06 3.33786011e-06 1.38282776e-05
0:  3.43322754e-05 4.05311584e-05 4.67300415e-05 4.77313995e-04
0:  7.75337219e-04 1.45244598e-03 1.24835968e-03 4.98771667e-04
0:  6.86645508e-05 9.48905945e-05 1.71661377e-04 4.38690186e-05
0:  2.76565552e-05 1.19209290e-05 4.29153442e-06 4.29153442e-06
0:  6.77108765e-05 1.23023987e-04 6.58035278e-05 2.05039978e-05
0:  1.90734863e-06 1.90734863e-06 4.76837158e-06 7.15255737e-06
0:  8.10623169e-06 1.04904175e-05 1.52587891e-05 1.76429749e-05
0:  6.19888306e-06 1.90734863e-06 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  5.81741333e-05 1.38282776e-04 1.90734863e-05 8.77380371e-05
0:  1.69277191e-04 4.72545624e-04 3.72886658e-04 2.85625458e-04
0:  2.18391418e-04 2.16007233e-04 4.03404236e-04 7.79628754e-04
0:  4.31537628e-04 9.02652740e-04 6.27040863e-04 3.09944153e-04
0:  3.84330750e-04 3.42369080e-04 1.01089478e-04 8.05854797e-05
0:  1.61170959e-04 5.07831573e-04 8.05377960e-04 7.23838806e-04
0:  5.43594302e-04 3.82423401e-04 1.99794769e-04 1.27792358e-04
0:  1.28746033e-04 6.43730164e-05 2.51770020e-04 4.02450562e-04
0:  3.19004059e-04 1.34944916e-04 4.24385071e-05 1.76429749e-05
0:  1.66893005e-05 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 2.14576721e-04 5.48362732e-05 4.76837158e-07
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 1.10149384e-04
0:  3.66687775e-04 1.91688538e-04 3.67164612e-05 9.82284546e-05
0:  4.41074371e-04 9.67502536e-04 8.16822052e-04 3.32832336e-04
0:  5.91278076e-05 5.48362732e-05 5.29289246e-05 2.52723694e-05
0:  4.81605566e-05 2.24113464e-05 7.62939453e-06 3.81469727e-06
0:  4.76837158e-06 1.47819519e-05 1.47819519e-05 5.24520874e-06
0:  9.53681592e-07 3.81469727e-06 1.43051147e-06 1.43051147e-06
0:  2.86102295e-06 4.76837158e-06 6.67572021e-06 6.67572021e-06
0:  1.28746033e-05 3.81469727e-06 7.27595761e-12 7.27595761e-12]
0: Target values (first 200):
0: [9.39845981e-04 8.86440277e-04 1.10626221e-03 1.01613998e-03
0:  9.07421112e-04 1.36995316e-03 2.16770172e-03 2.16436386e-03
0:  1.83343876e-03 1.00660324e-03 5.95092773e-04 9.41753446e-04
0:  2.02798843e-03 2.06422806e-03 1.11722946e-03 7.72476196e-04
0:  6.49929047e-04 2.08950043e-03 4.60147858e-03 1.42860413e-03
0:  2.72274017e-04 2.85148621e-04 2.44140625e-04 2.18391418e-04
0:  4.05311584e-04 2.10762024e-04 7.62939453e-05 2.22682953e-04
0:  1.21116638e-04 3.67164612e-05 6.19888306e-06 4.76837158e-07
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 1.34944916e-04 5.75542450e-04
0:  3.05652618e-04 8.62121582e-04 9.59873258e-04 3.22341919e-04
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 1.90734863e-06
0:  3.52859497e-05 3.71932983e-05 1.62124634e-05 8.15391541e-05
0:  2.43663788e-04 2.99453735e-04 5.10692596e-04 8.86917114e-05
0:  3.24249268e-05 1.85966492e-05 1.47819519e-05 3.19480896e-05
0:  4.48226929e-05 2.90870667e-05 1.47819519e-05 1.52587891e-05
0:  1.28746033e-05 3.67164612e-05 3.43322754e-05 1.57356262e-05
0:  6.19888306e-06 6.67572021e-06 4.29153442e-06 4.76837158e-06
0:  4.76837158e-07 9.53681592e-07 4.76837158e-07 1.43051147e-06
0:  2.38418579e-06 3.71932983e-05 1.33037567e-04 2.28881836e-04
0:  1.95026398e-04 1.50203705e-04 4.67300415e-05 4.31537628e-04
0:  9.15050507e-04 5.25951327e-04 5.86509705e-05 4.05311584e-05
0:  2.47955322e-05 2.09808350e-05 3.24249268e-05 2.26497650e-04
0:  8.96453857e-05 5.96046448e-05 1.19209290e-04 2.10762024e-04
0:  2.81810760e-04 2.77519226e-04 1.82151794e-04 2.86102295e-05
0:  2.11954117e-03 1.54352188e-03 1.31082535e-03 1.06382370e-03
0:  9.40322818e-04 1.26838684e-03 1.67274475e-03 2.09903717e-03
0:  1.73187256e-03 1.38759613e-03 9.88006592e-04 3.94630432e-03
0:  4.70876694e-03 3.21292877e-03 1.67655945e-03 6.14166260e-04
0:  5.42163849e-04 3.61156487e-03 4.12654877e-03 1.40666962e-03
0:  9.43660736e-04 9.30786075e-04 7.22408295e-04 6.16550446e-04
0:  4.66823578e-04 3.34262848e-04 2.55060196e-03 8.77857208e-04
0:  5.67436160e-04 1.55925751e-04 2.38418579e-05 2.86102295e-06
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 4.52995337e-05 2.15530396e-04
0:  3.44276428e-04 1.33228302e-03 6.44207001e-04 1.79290771e-04
0:  1.90734863e-06 4.76837158e-07 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 9.53681592e-07 3.33786011e-06 1.04904175e-05
0:  4.72068787e-05 1.38282776e-04 2.22206116e-04 2.01225281e-04
0:  5.29289246e-05 8.91685486e-05 8.82148743e-05 7.77244568e-05
0:  1.66893005e-05 2.05039978e-05 1.90734863e-05 2.52723694e-05
0:  3.19480896e-05 2.00271606e-05 1.47819519e-05 8.58306885e-06
0:  3.33786011e-06 1.19209290e-05 9.05990601e-06 6.67572021e-06
0:  9.05990601e-06 7.15255737e-06 2.38418579e-06 3.33786011e-06
0:  9.53681592e-07 7.27595761e-12 1.43051147e-06 9.53681592e-07
0:  4.76837158e-07 1.43051147e-06 5.86509705e-05 1.74522400e-04
0:  2.37941742e-04 1.61647797e-04 5.05447388e-05 1.71184540e-04]
0: Prediction values (first 20):
0: [-1.4244862  -0.9335141  -0.26668453  0.41639614  0.87051296  1.0281177
0:   0.8865118   0.5948458   0.11131382 -0.24897957 -0.44296503 -0.60286236
0:  -0.4438944  -0.1612196   0.3286867   1.0421877   1.822938    2.4996893
0:   1.4627256   1.4028873 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.621, max = 1.428, mean = -0.292
0:          sample (first 20): tensor([-0.6251, -0.5835, -0.5270, -0.4691, -0.4307, -0.4173, -0.4293, -0.4540, -0.4950, -0.5255, -0.5419, -0.5555,
0:         -0.5420, -0.5181, -0.4766, -0.4161, -0.3500, -0.2927, -0.5784, -0.5673])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.582335 23.000324 23.30106  23.31568  23.203812 22.981089 23.136646
0:  23.326479 23.60146  23.828178 23.759329 23.62916  23.572735 23.807343
0:  24.390245 25.143728 25.88872  26.386253 24.721918 25.019667]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.40389   11.343091  11.327965  11.30418   11.2416    11.064377
0:  11.150709  11.235201  11.3295765 11.352804  11.132192  10.82292
0:  10.600846  10.648136  11.003129  11.621119  12.244853  12.665865
0:  11.410654  11.420931 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.973736  4.0052395 4.2757673 4.7077923 5.1048    5.383285  5.480329
0:  5.574395  5.473156  5.450383  5.400229  5.0840364 4.754365  4.334747
0:  4.1062436 4.217662  4.6873293 5.2968507 4.0110235 4.095808 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.008876   8.324785   8.806804   9.319332   9.752798  10.055628
0:  10.358869  10.599936  10.710612  10.740498  10.625229  10.392587
0:  10.2733555 10.277261  10.549887  11.050785  11.6043825 12.079964
0:  10.273865  10.361446 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.07325  12.568204 12.989948 13.248623 13.452838 13.612478 14.133657
0:  14.696848 15.286835 15.807259 16.060844 16.273039 16.54095  17.092134
0:  17.909327 18.872896 19.747335 20.345383 18.938334 19.450808]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.014174 30.014135 29.85141  29.47243  29.035831 28.488901 28.535563
0:  28.570398 28.795197 28.812626 28.315033 27.883427 27.497082 27.575092
0:  28.15862  28.940742 29.584538 29.81218  28.683058 28.730875]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.9659078 2.8949382 3.0763426 3.4244893 3.8061402 4.142572  4.534506
0:  4.9558077 5.2236786 5.543873  5.7421465 5.731042  5.7351446 5.863822
0:  6.1130953 6.519773  6.9387617 7.1163297 7.848715  7.415309 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.746124 21.952913 22.068699 21.90496  21.598427 21.118055 21.078014
0:  21.021854 21.055504 20.99311  20.52665  19.990448 19.465462 19.279007
0:  19.457645 19.879509 20.239857 20.381142 19.492754 19.662598]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.29058  19.424992 19.637825 19.718056 19.532358 19.079344 18.465366
0:  17.916986 17.331785 16.85507  16.40371  15.806865 15.297461 14.831112
0:  14.647898 14.79895  15.259803 15.745645 13.281679 13.224574]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8784394 6.910003  6.994397  6.9994726 6.912704  6.6668935 6.692322
0:  6.713526  6.8211856 6.8777704 6.7203016 6.509622  6.3643355 6.576033
0:  7.1606817 8.052602  8.939121  9.656736  9.542244  9.877186 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.792169  -9.1016    -9.11276   -8.889956  -8.678518  -8.71763
0:   -8.799392  -9.09113   -9.530051 -10.027366 -10.687568 -11.463023
0:  -12.211598 -12.724984 -13.023257 -13.080813 -13.035274 -13.059649
0:  -12.288316 -12.815761]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.7260885   4.6774783   4.8381853   5.239691    5.6838436   5.9941173
0:   6.0256357   6.001065    5.591315    5.2380843   4.7725015   3.9896595
0:   3.1066859   2.0699081   1.2024856   0.7499838   0.8288379   1.1848154
0:  -0.32559395 -0.6673784 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.300292  10.63607   11.123575  11.719894  12.196697  12.340391
0:  12.533116  12.375484  11.945801  11.217699  10.140846   8.893861
0:   7.6793084  6.864416   6.5581527  6.6716194  6.915928   6.9922915
0:   4.040435   4.054537 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.23553133  0.22577143  0.45045948  0.8384495   1.1629982   1.3281374
0:   1.3119788   1.2834458   1.0082507   0.7221632   0.30208158 -0.40761232
0:  -1.078968   -1.7419796  -2.1286898  -2.133625   -1.8212271  -1.4480214
0:  -2.999936   -3.2435036 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.437794  5.4044476 5.4964986 5.575045  5.5077605 5.278509  4.9964685
0:  4.752306  4.4006987 4.07498   3.6590786 3.0816877 2.6205697 2.295926
0:  2.3320637 2.7511785 3.3925111 4.0108232 2.7335393 2.7023115]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.230282 10.396521 10.633283 10.747749 10.741112 10.537383 10.674864
0:  10.806793 11.091127 11.345246 11.267388 11.138964 10.957855 11.076104
0:  11.506913 12.195118 12.861385 13.236731 12.21622  12.376699]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8915787  -0.9516449  -0.7682357  -0.45458174 -0.16999102 -0.01390553
0:   0.08479357  0.08557367 -0.07344294 -0.2942462  -0.64139414 -1.1175613
0:  -1.4134717  -1.4593039  -1.1537828  -0.55114985  0.11551285  0.6796546
0:   0.01411772  0.27424145]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.900762  -2.9160933 -2.8238015 -2.538054  -2.1680913 -1.8765035
0:  -1.5244508 -1.1326985 -1.0037518 -0.9233422 -1.0763187 -1.487247
0:  -1.8902459 -2.207354  -2.2612844 -2.045136  -1.7579465 -1.5114932
0:  -2.213902  -2.4092708]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.755571   9.77344   10.034156  10.40199   10.69809   10.854631
0:  10.915031  11.0121565 11.014553  11.0617485 11.043314  10.788115
0:  10.5281925 10.271464  10.268584  10.585432  11.18652   11.79102
0:  10.558891  10.567957 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.871121  15.085088  15.492163  15.868673  15.984856  15.784438
0:  15.394848  14.97002   14.426096  13.939417  13.305435  12.312795
0:  11.184586   9.937071   8.878846   8.234974   7.9326487  7.787301
0:   5.37712    5.3623343]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6559196  -3.4015088  -3.9121099  -4.3497815  -4.761867   -5.213345
0:  -5.371884   -5.3817134  -5.207537   -4.7337747  -4.2425065  -3.5548453
0:  -2.6593795  -1.3440518   0.28425932  2.0927982   3.6348674   4.7471905
0:   6.5949726   7.8510513 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.287021  5.3909974 5.6674857 5.9472504 6.161067  6.26368   6.4246163
0:  6.5448184 6.5747833 6.530891  6.3123403 5.943207  5.6554513 5.5098653
0:  5.590079  5.86923   6.1126404 6.1920743 4.7767115 4.828272 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.514626 40.55922  40.30112  39.97195  39.57857  39.10097  39.373882
0:  39.464546 39.909824 39.947277 39.473812 39.040577 38.94313  39.189877
0:  40.155247 41.132828 41.880745 42.153328 40.82562  41.07342 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.67267275 -0.77868605 -0.7780156  -0.8349433  -1.0952125  -1.488687
0:  -1.6960549  -1.8591352  -2.0017934  -2.2146382  -2.6877637  -3.4363294
0:  -4.068458   -4.3615603  -4.2623744  -3.884069   -3.553309   -3.3785653
0:  -3.2909026  -3.058599  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3638663  -1.0747075  -0.5190277   0.08399439  0.57950354  0.87227917
0:   1.0656996   1.1182499   0.9475193   0.7566967   0.446064    0.00917292
0:  -0.25609064 -0.25330544  0.0570755   0.6129117   1.152317    1.4986439
0:  -0.58169556 -0.5540519 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.876999  10.118656  10.57141   11.069836  11.344959  11.371813
0:  11.177626  11.13511   11.001543  11.060019  11.111801  10.7554
0:  10.299679   9.646813   9.262852   9.435149  10.219129  11.235773
0:   8.946131   9.0273075]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.6192203 -4.62668   -4.3523707 -4.04143   -3.7778573 -3.7382035
0:  -3.5894237 -3.493857  -3.1898227 -2.8964028 -2.7466884 -2.756987
0:  -2.7194695 -2.5443187 -2.1016736 -1.445497  -0.8544774 -0.4626708
0:  -2.0109591 -2.1378198]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.319971  9.345324  9.636069  9.979908 10.178348 10.18157  10.058539
0:   9.97586   9.779114  9.654253  9.470266  9.036601  8.659106  8.302641
0:   8.263392  8.652072  9.365128 10.056329  8.892145  9.1878  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.776677  -4.802334  -4.5303936 -4.107147  -3.7172804 -3.506566
0:  -3.469555  -3.4379292 -3.6124678 -3.7557154 -3.9979172 -4.5526276
0:  -5.0543976 -5.5784407 -5.882745  -5.772258  -5.349476  -4.7996817
0:  -6.2642345 -6.3155737]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.40458632 0.5836425  1.0320706  1.6584396  2.2293625  2.6389768
0:  2.8904092  2.9993196  2.8031101  2.539626   2.1107926  1.4403114
0:  0.8794856  0.4293723  0.30564642 0.5947633  1.1267543  1.7280269
0:  1.5017958  1.7403283 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.28323  34.430286 34.242634 33.672802 33.066704 32.331944 32.55625
0:  32.678444 32.90892  32.774345 31.868214 31.176353 30.624401 30.793015
0:  31.679245 32.847576 33.779522 34.16751  35.048306 35.259525]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.55192  17.624931 17.652212 17.558077 17.498817 17.37795  17.987146
0:  18.530584 19.21115  19.623251 19.50616  19.395033 19.307747 19.791517
0:  20.74169  21.957472 23.014679 23.588987 23.400267 23.75713 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1691117  -3.319859   -3.1348062  -2.719051   -2.2192302  -1.8412576
0:  -1.1945138  -0.6501069  -0.13797903  0.2242732   0.1952405  -0.06374359
0:  -0.43497562 -0.5961499  -0.58693933 -0.39047146 -0.33630133 -0.44847012
0:  -3.6707616  -4.5390773 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5338254  -0.44472837 -0.06604195  0.53123665  1.0918779   1.5453091
0:   1.781313    2.0860949   2.183309    2.3274555   2.3696666   2.0310383
0:   1.5954609   1.0511279   0.77279997  1.0331306   1.8678732   2.951239
0:   2.3352127   2.3618646 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.029013  14.052348  14.302061  14.577382  14.702498  14.636279
0:  14.6598015 14.687531  14.746461  14.747828  14.558313  14.121691
0:  13.791277  13.531225  13.648329  14.087981  14.660041  15.090367
0:  13.370984  13.30246  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.790745 14.876416 15.071112 15.202032 15.208319 15.042936 15.014466
0:  14.935232 14.901041 14.810815 14.525225 14.117015 13.758949 13.559076
0:  13.667362 14.029443 14.431468 14.69778  12.769856 12.744365]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.1498923 -5.2912917 -5.035153  -4.433125  -3.7626061 -3.2613654
0:  -3.1310563 -3.1979384 -3.6826024 -4.167833  -4.7519336 -5.5788293
0:  -6.3274665 -7.0843396 -7.57472   -7.697807  -7.5317063 -7.275639
0:  -8.288193  -8.473148 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.761747 16.035124 16.263    16.341549 16.36753  16.292706 16.698294
0:  17.105516 17.607794 18.085138 18.268787 18.480877 18.783537 19.529894
0:  20.639986 21.994663 23.174772 24.003855 23.571325 24.047739]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3610797  -2.354569   -2.0247827  -1.4291911  -0.75143385 -0.1583004
0:   0.19695807  0.48860645  0.48388958  0.49990177  0.46806574  0.18404198
0:  -0.11374664 -0.4720583  -0.67461157 -0.546556   -0.07214117  0.51334953
0:  -0.4513502  -0.5325451 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.0277629   0.48363972  0.23559904  0.28507853  0.4703288   0.64155054
0:   0.647223    0.72349405  0.53693485  0.4648447   0.44532204  0.2137351
0:   0.07389927 -0.09093523 -0.08332348  0.26959133  0.9682784   1.7795291
0:   1.0742259   1.070641  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.216484 17.928831 17.721302 17.60929  17.489086 17.271072 17.315199
0:  17.34705  17.353342 17.239983 16.843006 16.37041  15.979807 15.840191
0:  16.140144 16.721628 17.36352  17.824284 16.393772 16.25056 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.667812 16.734556 16.940859 17.141663 17.254578 17.283691 17.43929
0:  17.63936  17.817133 17.91337  17.793255 17.4455   17.147902 16.96999
0:  17.170553 17.727448 18.51139  19.25881  17.8736   17.951319]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.0953693 6.1252303 6.333498  6.609078  6.8937726 7.0324054 7.3289084
0:  7.578125  7.844966  8.032519  8.017161  7.8659544 7.7407694 7.80755
0:  8.141588  8.695333  9.331225  9.854799  9.029511  9.237042 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.497433 39.314625 38.875454 38.15445  37.473705 36.651535 36.64518
0:  36.55826  36.584507 36.348827 35.485474 34.796715 34.124676 33.980934
0:  34.313984 34.775665 34.99287  34.718933 33.967518 33.83024 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8304443 6.722212  6.8651953 7.135874  7.384516  7.5009274 7.537524
0:  7.460268  7.2105613 6.985789  6.6979685 6.232144  5.8423924 5.5103927
0:  5.3983464 5.543645  5.8305807 6.079777  4.2122445 4.1705165]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.462615   -1.2756319  -0.8515358  -0.32449007  0.09095716  0.33303452
0:   0.43348503  0.4998536   0.33962917  0.16072321 -0.14711094 -0.71054983
0:  -1.1985836  -1.6128774  -1.7380471  -1.4916301  -1.0173383  -0.5236306
0:  -2.1005569  -2.1088862 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.46305  21.998022 22.478472 22.834846 23.06868  23.241512 23.645061
0:  24.0191   24.3836   24.625895 24.594852 24.500393 24.517632 24.807981
0:  25.431517 26.145184 26.709732 26.98159  24.956541 25.449816]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.575937 16.917955 17.32722  17.525932 17.392519 16.882402 16.29232
0:  15.611652 14.930496 14.338886 13.716845 13.11277  12.697079 12.543322
0:  12.660833 12.92559  13.089741 13.022689 10.211832 10.08539 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.69175  29.777107 29.817307 29.800148 29.80013  29.665924 29.995693
0:  30.193611 30.51449  30.631588 30.358534 30.141102 30.17545  30.512547
0:  31.31758  32.27352  33.108498 33.62094  32.483242 32.441517]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.480618  8.660211  8.988712  9.333049  9.531863  9.593857  9.656002
0:   9.735984  9.750661  9.793961  9.743925  9.578375  9.466369  9.422514
0:   9.630977 10.074415 10.651206 11.15378  10.510413 10.678852]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.4389    -11.252047  -10.623922   -9.698538   -8.753784   -7.984344
0:   -7.5781198  -7.335316   -7.514422   -7.7411084  -8.091963   -8.734262
0:   -9.306053   -9.892773  -10.274225  -10.302479  -10.022421   -9.566853
0:  -10.470999  -10.583244 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.77096  18.149414 18.428387 18.365252 18.125603 17.773352 17.83204
0:  17.919714 18.074942 18.194368 18.015015 17.76014  17.534346 17.649668
0:  18.082382 18.738129 19.324093 19.674227 18.026453 18.437542]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3501716  -1.1848555  -0.8281183  -0.36555862  0.11754274  0.5248928
0:   0.9547558   1.3724079   1.665463    1.9354172   2.1109977   2.1106443
0:   2.1859658   2.335825    2.6877694   3.2940106   4.006958    4.7144823
0:   3.8343794   4.220276  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9246387 -4.9900827 -4.770607  -4.432268  -4.0518723 -3.7816734
0:  -3.4734716 -3.2989168 -3.314519  -3.377758  -3.6162767 -3.972396
0:  -4.189023  -4.2364674 -4.0895023 -3.7819538 -3.544857  -3.4648418
0:  -5.5179806 -5.5846605]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.018705  12.372143  12.531466  12.374676  12.119901  11.897438
0:  12.460206  13.142319  14.087949  14.925698  15.350596  15.7969475
0:  16.265465  17.342785  18.898323  20.696348  22.304745  23.41042
0:  26.100243  26.944897 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.014031   -2.2999616  -2.1891036  -1.7143726  -1.1568356  -0.72708416
0:  -0.7072687  -0.85074854 -1.4745936  -2.231627   -3.2018065  -4.642092
0:  -6.138634   -7.6645684  -8.8677635  -9.348181   -9.068386   -8.371601
0:  -8.759764   -8.982546  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.016315 16.272884 16.482296 16.327385 15.941391 15.361782 15.064959
0:  14.756453 14.45896  14.086433 13.39804  12.734242 12.190863 12.179305
0:  12.658575 13.510449 14.327854 14.967794 16.222164 16.608582]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.85617  30.530024 30.12485  29.568716 29.105223 28.48326  28.567766
0:  28.494307 28.531826 28.327467 27.542969 26.930754 26.412506 26.443604
0:  26.951849 27.681164 28.25503  28.404984 28.125977 27.916431]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.220398  -4.3159676 -4.0589514 -3.4918137 -2.8976345 -2.4302464
0:  -2.3435402 -2.28049   -2.5789728 -2.808219  -3.0628319 -3.6259394
0:  -4.102112  -4.6066637 -4.808379  -4.516123  -3.7763243 -2.8863602
0:  -3.8888597 -4.0290174]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1870675  -4.1371427  -3.703692   -2.999971   -2.2956166  -1.7110324
0:  -1.3799424  -1.1132636  -1.1236243  -1.1390262  -1.205338   -1.5379024
0:  -1.7317367  -1.900003   -1.7789559  -1.2596202  -0.48938608  0.29776716
0:  -0.33346462 -0.11946392]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.12092113  0.04536724  0.17881775  0.4785118   0.79923344  1.0940433
0:   1.4163923   1.7608032   1.9127197   2.0898566   2.0929065   1.8761444
0:   1.6461267   1.3720961   1.2130866   1.1987352   1.2117171   1.1931667
0:  -0.82187414 -0.7765775 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.8468075 13.955292  14.119371  14.2753935 14.319035  14.186738
0:  14.368509  14.492044  14.660488  14.711824  14.429179  13.999418
0:  13.576025  13.438317  13.659374  14.18689   14.760067  15.104677
0:  12.547953  12.408314 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6439033  -2.5405746  -2.137218   -1.5326152  -0.9421406  -0.5013466
0:  -0.25631    -0.07160473 -0.1560173  -0.2547183  -0.4624791  -0.9521637
0:  -1.3697419  -1.7926755  -1.9496331  -1.7016006  -1.1487675  -0.5450344
0:  -1.1808581  -1.0482898 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.1538119  1.1612864  1.372097   1.688262   1.9385262  2.0575008
0:  2.1249962  2.2139773  2.1531563  2.0794263  1.8769641  1.4360447
0:  1.0121274  0.633749   0.4960909  0.71073055 1.154634   1.6347256
0:  0.7645898  0.75257826]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.6546707  -0.63262653 -0.32069445  0.1764493   0.6232896   0.93164635
0:   1.1200614   1.2794223   1.2507882   1.2965965   1.2550821   0.95255613
0:   0.6243386   0.27520323  0.04315138  0.09096527  0.3565488   0.6892328
0:   0.0465436  -0.30054235]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.314813  10.450763  10.710657  10.93302   10.985792  10.857412
0:  10.847645  10.829328  10.841621  10.844555  10.641983  10.313812
0:  10.016806   9.880877  10.043692  10.51469   11.044386  11.470451
0:   9.88979    9.9981785]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.54385  31.720057 31.755001 31.580976 31.378105 31.060177 31.388466
0:  31.67383  32.130352 32.41206  32.194176 32.113163 32.090343 32.522163
0:  33.407806 34.414642 35.266804 35.691887 34.04598  34.37257 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8643031  -0.8806839  -0.6858945  -0.55321026 -0.51228046 -0.62727547
0:  -0.51391363 -0.4314499  -0.33944702 -0.29583073 -0.51186085 -0.9057002
0:  -1.2612567  -1.2839208  -1.046535   -0.49656153 -0.02133846  0.26714802
0:  -0.5181303  -0.3760209 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.1112723 -6.006286  -5.498957  -4.711759  -3.9520478 -3.3866372
0:  -3.1274009 -3.0140738 -3.217403  -3.4595227 -3.8109355 -4.478744
0:  -5.0444083 -5.558558  -5.795672  -5.5690484 -5.0367045 -4.4656196
0:  -5.8501678 -6.0159144]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.301187 -15.273515 -14.711261 -13.770415 -12.820634 -12.148639
0:  -11.908376 -12.009404 -12.529874 -13.123666 -13.86852  -14.804456
0:  -15.599814 -16.293385 -16.56884  -16.474499 -16.109169 -15.716886
0:  -13.401206 -13.155755]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.5728164 5.694555  5.980494  6.31072   6.571549  6.7232943 6.8414583
0:  6.948442  6.900509  6.866047  6.717581  6.3849955 6.1364794 5.928057
0:  5.959464  6.2162366 6.5580583 6.8473773 5.896836  6.1064982]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.5777855   9.339911    8.035907    6.535727    4.821165    2.919527
0:   1.4525371   0.26789856 -0.3553195  -0.5715647  -0.60278654 -0.33525705
0:   0.36651087  1.5256457   3.187925    5.17095     7.1090465   8.702063
0:   7.655498    6.990757  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.19681597 0.5369477  1.0737762  1.7132459  2.2757783  2.7147925
0:  3.0112207  3.3869147  3.582496   3.8363624  4.013218   3.848542
0:  3.6420965  3.3015158  3.1264443  3.3282087  3.873456   4.520097
0:  3.3170729  3.37099   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.926465  12.033545  12.352583  12.735769  12.998642  13.122568
0:  13.148844  13.200428  13.157997  13.167946  13.114517  12.845774
0:  12.611307  12.3611145 12.375528  12.708769  13.265109  13.795706
0:  11.941864  11.959496 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.299643 18.406395 18.450708 18.2924   18.110077 17.734077 18.08852
0:  18.347675 18.873882 19.251919 19.086227 18.97071  18.786118 19.153763
0:  19.893265 20.918318 21.696087 21.960823 20.660286 20.911999]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.7394376 -7.7111764 -7.331487  -6.6524844 -5.8689494 -5.1450114
0:  -4.6704574 -4.2698913 -4.228024  -4.1666408 -4.17578   -4.4685903
0:  -4.734037  -5.007796  -5.082602  -4.768646  -4.0624747 -3.2195191
0:  -3.9119    -4.026237 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.157188  -10.323555  -10.020767   -9.329151   -8.538092   -7.889563
0:   -7.4940534  -7.352094   -7.655562   -7.991388   -8.498154   -9.224524
0:   -9.796268  -10.228685  -10.352764  -10.085157   -9.619724   -9.128439
0:  -10.529713  -10.509689 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.937706    1.3152738   1.9838753   2.7528977   3.3476279   3.6309907
0:   3.516971    3.227974    2.5932474   1.981545    1.323719    0.47100544
0:  -0.3063159  -1.0433764  -1.5255666  -1.6010766  -1.3041387  -0.853909
0:  -1.563951   -1.5570025 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.387753  1.6782112 2.1820474 2.75423   3.3006942 3.6958756 3.9988348
0:  4.2103815 4.245394  4.320446  4.3411164 4.301443  4.3429737 4.4121385
0:  4.5724483 4.8323035 5.0829153 5.228046  1.6650949 1.2970366]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.26003265 -0.32546234 -0.1701746   0.10358191  0.35097456  0.49814415
0:   0.710691    0.92211294  0.99837303  0.99991894  0.78763485  0.43119574
0:   0.1330862  -0.00433207  0.14228868  0.56184673  0.9960284   1.3156548
0:  -0.11195326 -0.06216526]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.609498  4.590726  4.7825317 5.117223  5.5085163 5.723714  5.8518147
0:  5.819223  5.563073  5.3176436 4.990206  4.5226746 4.0330486 3.5479493
0:  3.2459826 3.1344213 3.271701  3.3715582 3.2818503 3.2091289]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.9841237 -7.9456134 -7.494188  -6.6460195 -5.690514  -4.810265
0:  -4.342466  -4.0377574 -4.251346  -4.521009  -4.9398627 -5.7194295
0:  -6.4271617 -7.1445394 -7.5368876 -7.448791  -6.7951884 -5.923775
0:  -6.6177506 -6.529676 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.572039  -4.6025558 -4.2820745 -3.6741357 -3.0211816 -2.4965014
0:  -2.2805333 -2.0713105 -2.211886  -2.3095288 -2.4847856 -3.035359
0:  -3.5658832 -4.212953  -4.6178136 -4.5345244 -3.9764676 -3.2076936
0:  -3.7240014 -3.721211 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1297317  -3.0001054  -2.5472884  -1.8761344  -1.2434888  -0.8068743
0:  -0.4984231  -0.29214907 -0.317863   -0.38911772 -0.62026024 -1.0644851
0:  -1.4320283  -1.6808524  -1.6343169  -1.2818971  -0.7974477  -0.41549873
0:  -1.4510131  -1.4005961 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.516083 21.843832 22.259674 22.772243 23.113192 23.350845 23.348255
0:  23.435472 23.426552 23.583794 23.823723 23.83231  23.880695 23.842714
0:  24.053312 24.63844  25.607977 26.62704  24.39736  24.369144]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5638428  1.8041348  2.2233973  2.813614   3.3917587  3.9891646
0:  4.5237646  5.0371103  5.2214565  5.1863675  4.796506   3.9978027
0:  3.1091676  2.202918   1.6055007  1.4165215  1.5748401  1.9476542
0:  0.6712971  0.37759304]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.024075  6.093383  6.350997  6.604299  6.739488  6.685775  6.609581
0:  6.534203  6.4200983 6.300161  6.10211   5.742337  5.4960065 5.3734784
0:  5.5405693 6.0429955 6.7219305 7.3190403 5.914094  6.03602  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.8047636 2.8426967 3.0688517 3.4309669 3.7430418 3.9579563 3.9820712
0:  4.0832357 3.9627373 3.8974762 3.7640417 3.3598692 2.951512  2.474238
0:  2.1966233 2.242293  2.6109471 3.082759  2.3713255 2.3805604]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.4830594  -0.35747337  0.05376196  0.5782461   0.92763376  0.9696832
0:   0.68054247  0.25289917 -0.41122818 -1.0517554  -1.7909017  -2.7284312
0:  -3.6177135  -4.392792   -4.7960706  -4.64718    -4.068134   -3.3119607
0:  -3.6671548  -3.457498  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.6979938  6.0615954  6.547208   6.9463296  7.208215   7.3414803
0:   7.6072445  7.8351555  8.066737   8.2745075  8.325343   8.316832
0:   8.420262   8.7994585  9.4595375 10.344668  11.211378  11.914146
0:  12.373516  12.8782215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.818466 22.121391 22.419521 22.58622  22.671959 22.603527 23.08841
0:  23.497734 24.091919 24.550293 24.58434  24.577976 24.623444 25.053825
0:  25.950367 27.119099 28.227325 28.94263  26.646008 26.90129 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.219549  -7.1153803 -6.7003965 -6.0955353 -5.559103  -5.222823
0:  -5.102541  -5.055132  -5.2931347 -5.5005646 -5.873149  -6.5290337
0:  -7.152789  -7.698497  -7.9185553 -7.751747  -7.3696938 -6.91881
0:  -6.9741764 -6.879779 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2111421  -1.1621494  -0.8994813  -0.5918431  -0.30079937 -0.13941717
0:  -0.0693841  -0.0144186  -0.13584185 -0.10684729 -0.08851147 -0.2220583
0:  -0.2263236  -0.20985031 -0.03062105  0.42300367  1.050302    1.7085717
0:   1.8012569   2.1954966 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.5261874 -6.771711  -6.574385  -5.961642  -5.2330585 -4.593039
0:  -4.321731  -4.135123  -4.325951  -4.4279838 -4.5367885 -4.951388
0:  -5.3390384 -5.8184733 -6.1167006 -5.9911036 -5.396462  -4.606422
0:  -5.1287093 -5.2833495]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6179466 -5.5893517 -5.257106  -4.7423067 -4.2328224 -3.8675833
0:  -3.6460676 -3.5090427 -3.6580229 -3.7786674 -4.0106134 -4.4908686
0:  -4.9229164 -5.272355  -5.421626  -5.2241917 -4.852166  -4.4634676
0:  -5.859252  -5.820284 ]
0: validation loss for strategy=forecast at epoch 6 : 0.37830284237861633
0: validation loss for velocity_u : 0.107345812022686
0: validation loss for velocity_v : 0.1601019650697708
0: validation loss for specific_humidity : 0.1936764121055603
0: validation loss for velocity_z : 0.6078381538391113
0: validation loss for temperature : 0.10855083912611008
0: validation loss for total_precip : 1.0923038721084595
0: 7 : 18:25:59 :: batch_size = 96, lr = 1.7677085752190346e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 7, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0820, -1.0996, -1.1319, -1.1704, -1.2038, -1.2346, -1.2599, -1.2860, -1.3154, -1.3457, -1.3797, -1.4095,
0:         -1.4347, -1.4518, -1.4616, -1.4656, -1.4683, -1.4712, -1.1062, -1.1364, -1.1765, -1.2185, -1.2489, -1.2694,
0:         -1.2848], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3132, -0.3022, -0.2861, -0.2731, -0.2669, -0.2725, -0.2891, -0.3145, -0.3477, -0.3815, -0.4126, -0.4408,
0:         -0.4664, -0.4931, -0.5203, -0.5446, -0.5636, -0.5740, -0.2589, -0.2542, -0.2492, -0.2490, -0.2555, -0.2719,
0:         -0.2968], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-9.6240e-02,  6.4293e-03,  4.6350e-02,  7.3676e-02,  5.0286e-02,  2.0598e-02, -9.8762e-03, -3.7764e-02,
0:         -7.2962e-02, -1.0164e-01, -1.2491e-01, -1.3391e-01, -1.3908e-01, -1.3560e-01, -1.3785e-01, -1.4100e-01,
0:         -1.4133e-01, -1.4774e-01,  8.0198e-02,  1.3035e-01,  1.4632e-01,  1.1708e-01,  7.5812e-02,  3.3305e-02,
0:          1.9547e-05], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2588,  0.2656,  0.1549,  0.1063,  0.1164,  0.1978,  0.3425,  0.3154,  0.2927,  0.1560,  0.0351, -0.0192,
0:         -0.0926, -0.0780,  0.0079,  0.1006,  0.1797,  0.2261,  0.2645,  0.2261,  0.1605,  0.0803,  0.1424,  0.2351,
0:          0.3640], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.8795, 0.8306, 0.8115, 0.8512, 0.9414, 1.0466, 1.1354, 1.1847, 1.2114, 1.2134, 1.1894, 1.1547, 1.1165, 1.0915,
0:         1.0704, 1.0568, 1.0478, 1.0428, 1.0469, 1.0754, 1.1375, 1.2322, 1.3642, 1.5154, 1.6606], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2480, -0.2480, -0.2480, -0.2480, -0.2480, -0.2480, -0.2480, -0.2468, -0.2433, -0.2480, -0.2480, -0.2480,
0:         -0.2480, -0.2480, -0.2480, -0.2480, -0.2480, -0.2433, -0.2480, -0.2480, -0.2480, -0.2480, -0.2480, -0.2480,
0:         -0.2480], device='cuda:0')
0: [DEBUG] Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2468,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2150,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1986, -0.2245,     nan,     nan,     nan,     nan, -0.1173,     nan,     nan,     nan,     nan,
0:         -0.1220,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1844,     nan,     nan,     nan,     nan,     nan,
0:         -0.1762,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2092,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2457,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2433,     nan,     nan,     nan,     nan,     nan,     nan, -0.1703,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1750,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2127,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2068,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2386,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2480, -0.2433,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2362,     nan, -0.2339,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 7, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2364, -1.2051, -1.1375, -1.0519, -0.9789, -0.9369, -0.9319, -0.9498, -0.9848, -1.0166, -1.0455, -1.0822,
0:         -1.1157, -1.1492, -1.1731, -1.1775, -1.1689, -1.1510, -1.1994, -1.2211, -1.1986, -1.1371, -1.0740, -1.0216,
0:         -0.9946], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3000, -0.2848, -0.2925, -0.3062, -0.3182, -0.3195, -0.3241, -0.3240, -0.3179, -0.3287, -0.3321, -0.3351,
0:         -0.3580, -0.4138, -0.4751, -0.5018, -0.4723, -0.3978, -0.3559, -0.3155, -0.3027, -0.3173, -0.3403, -0.3678,
0:         -0.3790], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3062, -0.3143, -0.3249, -0.3232, -0.3305, -0.3504, -0.3686, -0.4040, -0.4317, -0.4647, -0.4794, -0.4808,
0:         -0.4689, -0.4287, -0.3981, -0.3523, -0.3003, -0.2610, -0.3146, -0.3198, -0.3157, -0.3124, -0.3125, -0.3370,
0:         -0.3682], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1320, 0.2446, 0.3277, 0.3771, 0.2974, 0.3465, 0.4389, 0.4628, 0.5022, 0.4983, 0.4777, 0.5112, 0.5755, 0.4254,
0:         0.2662, 0.3983, 0.4842, 0.4536, 0.3130, 0.2756, 0.2912, 0.3617, 0.2900, 0.3403, 0.3893], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.6324, -0.7349, -0.8028, -0.8456, -0.8641, -0.8949, -0.9419, -0.9925, -1.0135, -0.9867, -0.9225, -0.8458,
0:         -0.7762, -0.7491, -0.7908, -0.9165, -1.1051, -1.2983, -1.4072, -1.3872, -1.2444, -1.0272, -0.7995, -0.5958,
0:         -0.4289], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0633, -0.0679, -0.0580, -0.0900, -0.1046, -0.1173, -0.1101, -0.0968, -0.0920, -0.0519, -0.0631, -0.0812,
0:         -0.0948, -0.1091, -0.1163, -0.1120, -0.0990, -0.0775, -0.0329, -0.0528, -0.0648, -0.0842, -0.1061, -0.0985,
0:         -0.0926], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.16749732196331024; velocity_v: 0.22797028720378876; specific_humidity: 0.22143900394439697; velocity_z: 0.5650011301040649; temperature: 0.16772739589214325; total_precip: 0.9073233604431152; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15573330223560333; velocity_v: 0.19286644458770752; specific_humidity: 0.2257646918296814; velocity_z: 0.4989657402038574; temperature: 0.16431865096092224; total_precip: 0.8833442330360413; 
0: epoch: 7 [1/5 (20%)]	Loss: 0.89533 : 0.33135 :: 0.18742 (2.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17250147461891174; velocity_v: 0.23011697828769684; specific_humidity: 0.1831396371126175; velocity_z: 0.5827093720436096; temperature: 0.1726691573858261; total_precip: 0.8186779618263245; 
0: epoch: 7 [2/5 (40%)]	Loss: 0.81868 : 0.32659 :: 0.19142 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15914124250411987; velocity_v: 0.1918679028749466; specific_humidity: 0.18903061747550964; velocity_z: 0.5348960161209106; temperature: 0.10897155106067657; total_precip: 0.6934168934822083; 
0: epoch: 7 [3/5 (60%)]	Loss: 0.69342 : 0.28193 :: 0.19404 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16384895145893097; velocity_v: 0.23275846242904663; specific_humidity: 0.20700716972351074; velocity_z: 0.49191105365753174; temperature: 0.16837084293365479; total_precip: 0.623765766620636; 
0: epoch: 7 [4/5 (80%)]	Loss: 0.62377 : 0.28134 :: 0.19079 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [8.91685486e-05 8.96453857e-05 9.01222229e-05 9.10758972e-05
0:  9.15527344e-05 9.20295715e-05 9.25064087e-05 9.34600830e-05
0:  9.39369202e-05 9.44137573e-05 9.48905945e-05 9.53674316e-05
0:  9.63211060e-05 9.67979431e-05 9.72747803e-05 9.77516174e-05
0:  9.82284546e-05 9.87052917e-05 9.91821289e-05 9.91821289e-05
0:  9.96589661e-05 1.00135803e-04 1.00612640e-04 1.01089478e-04
0:  1.01566315e-04 1.02519989e-04 1.02996826e-04 1.03473663e-04
0:  1.03950500e-04 1.04427338e-04 1.05381012e-04 1.05857849e-04
0:  1.06334686e-04 1.06811523e-04 1.07288361e-04 1.07765198e-04
0:  1.08242035e-04 1.09195709e-04 1.09672546e-04 1.10149384e-04
0:  1.10626221e-04 1.11103058e-04 1.11579895e-04 1.12533569e-04
0:  1.13010406e-04 1.13487244e-04 1.13964081e-04 1.14440918e-04
0:  1.14917755e-04 1.15394592e-04 1.15871429e-04 1.16825104e-04
0:  1.17301941e-04 1.17301941e-04 1.17778778e-04 1.17778778e-04
0:  1.17778778e-04 1.18255615e-04 1.18255615e-04 1.18732452e-04
0:  1.18732452e-04 1.18732452e-04 1.19209290e-04 1.19209290e-04
0:  1.19686127e-04 1.19686127e-04 1.19686127e-04 1.20162964e-04
0:  1.20162964e-04 1.20162964e-04 1.20639801e-04 1.20639801e-04
0:  1.20639801e-04 1.21116638e-04 1.21116638e-04 1.21593475e-04
0:  1.21593475e-04 1.21593475e-04 1.22070312e-04 1.22070312e-04
0:  1.22070312e-04 1.22547150e-04 1.22547150e-04 1.22547150e-04
0:  1.22547150e-04 1.22547150e-04 1.22547150e-04 1.22547150e-04
0:  1.22547150e-04 1.22070312e-04 1.22070312e-04 1.21593475e-04
0:  1.21593475e-04 1.21593475e-04 1.21116638e-04 1.21116638e-04
0:  1.20639801e-04 1.20639801e-04 1.20162964e-04 1.20162964e-04
0:  1.19686127e-04 1.19686127e-04 1.19209290e-04 1.18732452e-04
0:  1.18732452e-04 1.18255615e-04 1.18255615e-04 1.17778778e-04
0:  8.58306885e-05 8.63075256e-05 8.72612000e-05 8.82148743e-05
0:  8.91685486e-05 9.01222229e-05 9.10758972e-05 9.20295715e-05
0:  9.29832458e-05 9.39369202e-05 9.48905945e-05 9.58442688e-05
0:  9.67979431e-05 9.72747803e-05 9.82284546e-05 9.91821289e-05
0:  1.00135803e-04 1.01089478e-04 1.02043152e-04 1.02996826e-04
0:  1.03950500e-04 1.04904175e-04 1.05857849e-04 1.06811523e-04
0:  1.07765198e-04 1.08718872e-04 1.09195709e-04 1.10149384e-04
0:  1.10626221e-04 1.11103058e-04 1.12056732e-04 1.12533569e-04
0:  1.13487244e-04 1.13964081e-04 1.14917755e-04 1.15871429e-04
0:  1.16825104e-04 1.17778778e-04 1.18732452e-04 1.19686127e-04
0:  1.20639801e-04 1.21593475e-04 1.22547150e-04 1.23500824e-04
0:  1.24454498e-04 1.25408173e-04 1.26361847e-04 1.27315521e-04
0:  1.28269196e-04 1.29222870e-04 1.30176544e-04 1.31130219e-04
0:  1.32083893e-04 1.32560730e-04 1.33514404e-04 1.34468079e-04
0:  1.35421753e-04 1.36375427e-04 1.37329102e-04 1.37805939e-04
0:  1.37805939e-04 1.38282776e-04 1.38282776e-04 1.38759613e-04
0:  1.38759613e-04 1.39236450e-04 1.39236450e-04 1.39713287e-04
0:  1.39713287e-04 1.39713287e-04 1.40190125e-04 1.40190125e-04
0:  1.40666962e-04 1.40666962e-04 1.40666962e-04 1.41143799e-04
0:  1.41143799e-04 1.41143799e-04 1.41620636e-04 1.41620636e-04
0:  1.41620636e-04 1.42097473e-04 1.42097473e-04 1.42097473e-04
0:  1.42574310e-04 1.42574310e-04 1.42574310e-04 1.43051147e-04
0:  1.43051147e-04 1.43051147e-04 1.43051147e-04 1.43527985e-04]
0: Target values (first 200):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0.]
0: Prediction values (first 20):
0: [1.5786004 1.6945763 2.0291648 2.4857903 2.8535223 3.075385  3.1318314
0:  3.171084  2.9571986 2.8154688 2.5558877 2.0804195 1.6099367 1.125843
0:  0.9403615 1.1129308 1.5716872 2.1269674 1.8210049 2.043304 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.504, max = 2.383, mean = -0.182
0:          sample (first 20): tensor([-0.4573, -0.4477, -0.4201, -0.3824, -0.3520, -0.3336, -0.3290, -0.3257, -0.3434, -0.3551, -0.3766, -0.4159,
0:         -0.4547, -0.4947, -0.5101, -0.4958, -0.4579, -0.4120, -0.4076, -0.4441])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.154686   -1.0142965  -0.61223507 -0.08047771  0.37545252  0.6736908
0:   0.7940211   0.93940973  0.86383295  0.8312783   0.677371    0.18282652
0:  -0.32269    -0.85326195 -1.1141586  -0.8804264  -0.22134113  0.55412436
0:   0.13998556  0.40499735]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.569508  -13.610569  -13.25683   -12.57024   -11.794695  -11.157335
0:  -10.7993145 -10.620209  -10.876715  -11.254639  -11.803196  -12.644899
0:  -13.335894  -13.844038  -14.013449  -13.761299  -13.306398  -12.8022
0:  -12.861742  -12.903366 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.56736   -13.612751  -13.216183  -12.533163  -11.8678665 -11.489529
0:  -11.576254  -11.918898  -12.686562  -13.409126  -14.161848  -15.062328
0:  -15.815853  -16.442364  -16.74105   -16.581778  -16.012924  -15.238333
0:  -14.995989  -14.85195  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.0577025 -4.992748  -4.5757585 -3.892344  -3.1837192 -2.5575004
0:  -2.2554402 -1.9291177 -1.9237113 -1.8486094 -1.8310971 -2.185555
0:  -2.5298152 -2.9849858 -3.167972  -2.8396096 -1.9752641 -0.893322
0:  -1.0362458 -0.8260493]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.644824  7.5751233 7.5579376 7.380198  7.15119   6.836094  6.9785147
0:  7.21643   7.532828  7.745865  7.556015  7.2338076 6.8329363 6.8194227
0:  7.1285934 7.6652365 8.0088415 8.020394  6.4634957 6.410132 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.0572381  -0.07724619  0.20125246  0.70171785  1.1817975   1.4826541
0:   1.5273366   1.4291892   0.9888196   0.59972095  0.13487244 -0.5175104
0:  -1.0636072  -1.5695972  -1.7643719  -1.6224279  -1.2595158  -0.83111715
0:  -1.4645662  -1.4638557 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.372095 18.681953 18.926207 18.932377 18.832867 18.5975   18.834623
0:  19.049074 19.418653 19.73186  19.719475 19.68201  19.728498 20.122864
0:  20.874664 21.863462 22.82851  23.552383 22.159132 22.579613]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [46.394073 46.332787 45.998924 45.57233  45.062195 44.469242 44.693756
0:  44.91709  45.311966 45.3802   44.73631  44.232468 43.96361  44.140495
0:  45.00525  45.949177 46.648846 46.792408 46.21169  46.036488]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.8468885 2.8347387 2.9420495 3.0110435 2.9973526 2.8685842 3.0557723
0:  3.2558339 3.3927965 3.4733255 3.2017305 2.7749639 2.3634381 2.3040342
0:  2.5230231 2.9387164 3.161775  3.066875  2.3101864 2.259365 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.259162  -10.44472   -10.270703   -9.683074   -8.941688   -8.259893
0:   -7.910903   -7.6341114  -7.797947   -7.940936   -8.201764   -8.894309
0:   -9.558421  -10.310624  -10.769431  -10.690045   -9.992398   -8.963559
0:   -9.485044   -9.681592 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.89225  16.91237  16.91087  16.841713 16.722788 16.52324  16.581854
0:  16.642872 16.678808 16.676004 16.452282 16.177618 15.999065 16.102943
0:  16.592117 17.342226 18.19092  18.880892 18.87041  18.847433]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.97268  15.925268 15.977392 16.085949 16.223152 16.184738 16.705227
0:  17.11868  17.75433  18.183514 18.211267 18.22557  18.322758 18.718369
0:  19.587414 20.71624  21.85852  22.718824 21.371693 21.36819 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.3766503  4.750391   5.460434   6.298264   7.0287037  7.613867
0:   8.167437   8.6815195  9.008656   9.161638   9.033575   8.572613
0:   8.189041   7.947239   8.182852   8.895761   9.850414  10.695259
0:   9.532574   9.534009 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.237377  11.44536   11.562674  11.525024  11.383593  11.122323
0:  11.340293  11.5790825 11.877689  12.077097  11.899443  11.596556
0:  11.326345  11.50285   12.087719  12.94532   13.720247  14.181736
0:  13.906113  14.102184 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.734632 -12.794618 -12.410148 -11.664226 -10.804593 -10.078443
0:   -9.714063  -9.456342  -9.596251  -9.714737  -9.886275 -10.409312
0:  -10.870883 -11.386617 -11.710469 -11.580846 -10.982066 -10.17022
0:  -10.685727 -10.751605]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.5172415 -6.8099976 -6.7860985 -6.7293844 -6.767521  -6.9622107
0:  -7.002384  -7.08354   -7.2051277 -7.308875  -7.667321  -8.154124
0:  -8.694797  -8.90775   -8.842424  -8.56446   -8.435114  -8.530659
0:  -8.794078  -8.918404 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.610295  9.556427  9.682732  9.767174  9.727546  9.543029  9.313147
0:  9.189263  8.972207  8.78771   8.505198  8.007529  7.589195  7.283025
0:  7.358618  7.861235  8.642861  9.4338875 8.329496  8.397079 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.290204  2.2400694 2.515891  3.0735354 3.6986554 4.2087564 4.542548
0:  4.7914257 4.7291985 4.684473  4.565818  4.1961565 3.8804543 3.5590966
0:  3.4392533 3.6701272 4.2062006 4.7882752 3.8454626 3.7653687]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.804268 33.707264 33.20533  32.416996 31.642641 30.803114 31.131588
0:  31.538315 32.230915 32.64139  32.258648 32.03844  31.797333 32.1792
0:  33.18958  34.353546 35.16692  35.230038 36.282246 36.39609 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.7255411   0.5583801   0.6659651   1.0724502   1.4646993   1.6921608
0:   1.4451122   1.0989556   0.24991798 -0.5994296  -1.5123053  -2.8138413
0:  -4.0877643  -5.4345975  -6.375258   -6.618193   -5.9936223  -4.9363627
0:  -5.5087075  -5.8825765 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-28.083893 -28.186825 -27.763916 -26.718891 -25.560207 -24.454254
0:  -23.674397 -23.154823 -23.193928 -23.295742 -23.705173 -24.438332
0:  -25.11641  -25.741318 -25.885529 -25.626839 -24.9049   -24.107677
0:  -23.130062 -23.087254]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.0816836   1.1012397   1.3274784   1.6006813   1.7699447   1.7590227
0:   1.8556838   1.8729887   1.7829843   1.6252179   1.2015033   0.61248255
0:   0.07804823 -0.1892848  -0.16755629  0.10893393  0.30452585  0.30782652
0:  -1.5800948  -1.496264  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.610214   8.870211   9.262184   9.654594   9.981936  10.232917
0:  10.696468  11.157061  11.564917  11.914968  12.027916  11.960499
0:  12.009144  12.240727  12.76738   13.51675   14.237371  14.695318
0:  13.5655575 13.872383 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.3504124 -4.883683  -4.9792023 -4.6706696 -4.1998186 -3.7602015
0:  -3.5441332 -3.4783993 -3.7981977 -4.1077356 -4.5113325 -5.1652946
0:  -5.6926813 -6.2179213 -6.5122194 -6.425438  -6.0435543 -5.530464
0:  -6.585333  -6.6997433]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.581102   9.246534   9.987831  10.645737  11.077879  11.284933
0:  11.667267  11.963232  12.105734  12.05585   11.610481  10.886234
0:  10.139547   9.55636    9.247042   9.272509   9.386502   9.431873
0:   5.0207233  4.618    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.6764965  2.662497   2.8530297  3.1172848  3.481849   3.8630273
0:   4.7131743  5.648285   6.686079   7.6472354  8.286597   8.737688
0:   9.115063   9.714243  10.503323  11.44097   12.161185  12.499858
0:  10.302841  10.505589 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.365525  -10.4081335 -10.071235   -9.418073   -8.671376   -8.044173
0:   -7.644209   -7.315296   -7.339945   -7.333544   -7.4321356  -7.833109
0:   -8.175823   -8.538199   -8.703218   -8.531066   -8.06519    -7.530632
0:   -7.8444557  -7.7213326]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1474953 -3.3851366 -3.3986454 -3.2322884 -3.1130085 -3.0901299
0:  -3.2879028 -3.4994507 -3.9621344 -4.4315147 -4.9649076 -5.71661
0:  -6.3461475 -6.8570194 -7.075452  -6.9318743 -6.5294065 -6.063218
0:  -7.4146557 -7.7527094]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.93408   -5.1075    -4.909175  -4.40024   -3.8356867 -3.3919535
0:  -3.2760897 -3.2169895 -3.5292702 -3.7421627 -4.0140133 -4.584134
0:  -5.09276   -5.646597  -5.928834  -5.6934366 -4.952773  -3.961339
0:  -4.5318108 -4.7035637]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.1017685 9.040701  9.15288   9.282842  9.258287  9.063728  8.906429
0:  8.725056  8.432859  8.080698  7.5027523 6.7403803 6.0967164 5.645889
0:  5.5917797 5.905077  6.3463025 6.619791  5.02097   5.0522137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.896431 18.186605 18.589094 18.931368 19.160727 19.228132 19.421997
0:  19.495417 19.47957  19.397665 19.096498 18.73312  18.59287  18.815014
0:  19.517654 20.556639 21.677559 22.619745 22.059122 22.488678]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.408672 21.48919  21.456293 21.159958 20.839527 20.389744 20.49834
0:  20.5474   20.723007 20.678679 20.183182 19.641165 19.150175 19.131613
0:  19.6516   20.513208 21.387772 22.054405 19.851397 19.86432 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.187202 24.541632 24.895575 25.107506 25.296467 25.36455  25.797203
0:  26.186512 26.662365 27.014889 27.063833 27.104404 27.3155   27.807808
0:  28.685066 29.68719  30.560547 31.140856 29.575499 29.780897]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.4282384 -6.787459  -6.749573  -6.3164353 -5.755694  -5.2622914
0:  -4.981709  -4.819054  -5.01329   -5.2714696 -5.6799397 -6.372352
0:  -6.974402  -7.5342946 -7.817511  -7.6638274 -7.2070756 -6.6401267
0:  -7.955545  -8.085402 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9192739  -4.0577636  -3.923078   -3.5914655  -3.2458043  -2.9774804
0:  -2.8049636  -2.5505385  -2.47619    -2.3635917  -2.3602262  -2.7031884
0:  -3.0517058  -3.43403    -3.5802379  -3.2765145  -2.560925   -1.6796355
0:  -0.22861862 -0.02602005]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.60866  16.575924 16.41549  16.002201 15.457497 14.754555 14.652753
0:  14.526577 14.556866 14.44338  13.854542 13.335821 12.840471 12.898944
0:  13.465539 14.298714 15.067186 15.485405 15.391614 15.491708]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.006823 25.56269  24.897404 23.986189 23.046635 21.862307 21.39327
0:  20.871378 20.594725 20.163597 19.164537 18.322653 17.499815 17.154549
0:  17.31971  17.645195 17.797962 17.570322 16.519144 16.178621]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.456738 22.04468  22.779865 23.44791  23.881226 24.086895 24.134205
0:  24.074137 23.768696 23.31161  22.645973 21.74497  21.008272 20.439346
0:  20.31577  20.533821 20.96508  21.37838  18.396654 18.364532]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.071619  -14.084219  -13.715279  -12.98914   -12.229321  -11.671888
0:  -11.457958  -11.4242325 -11.779839  -12.159903  -12.671377  -13.470123
0:  -14.104292  -14.59416   -14.732916  -14.431784  -13.865736  -13.263678
0:  -14.182465  -14.363596 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.09188  38.123894 37.62298  36.687412 35.69925  34.60282  34.62881
0:  34.787502 35.266678 35.42298  34.749313 34.25717  33.772743 33.95675
0:  34.830288 35.84437  36.528545 36.561413 37.708847 38.054436]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.261863   3.8035455  4.5684977  5.274597   5.836635   6.2679315
0:   6.874271   7.504128   8.106559   8.647158   8.909849   9.049556
0:   9.214938   9.625549  10.339189  11.250227  12.036308  12.52523
0:  10.237481  10.716814 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.38311   11.380549  11.394935  11.425688  11.49626   11.494923
0:  11.950413  12.352568  12.834375  13.173097  13.126988  13.061661
0:  13.009489  13.314663  13.993218  14.9178705 15.838286  16.595879
0:  16.01043   16.269081 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.498037 15.674143 16.023186 16.39132  16.577585 16.481913 16.39167
0:  16.30368  16.184307 16.086716 15.826284 15.374628 14.932312 14.547426
0:  14.460199 14.716061 15.174866 15.579309 13.296119 13.074961]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.24811  15.469961 15.75425  16.10572  16.378006 16.558174 17.042885
0:  17.4317   17.709377 17.758497 17.409916 16.966265 16.700558 16.813837
0:  17.428425 18.38861  19.407333 20.229378 20.647026 21.218952]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.407732   9.147255   9.047753   8.982355   8.9188     8.679862
0:   8.716961   8.680686   8.71373    8.593252   8.174187   7.601223
0:   7.1394415  6.9810147  7.33122    8.152381   9.183639  10.182094
0:  10.291396  10.485753 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.07667828 -0.35152435 -0.31632376  0.08875418  0.6260848   1.1377058
0:   1.5970402   1.9991765   2.115621    2.0957942   1.8430109   1.2603073
0:   0.6977439   0.19242716 -0.02239895  0.12626219  0.53469515  0.9442134
0:  -0.58013725 -0.75384045]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.5955215 -4.7201943 -4.539601  -4.0915666 -3.6508918 -3.3862824
0:  -3.386045  -3.468326  -3.8730493 -4.2438655 -4.6823297 -5.3648543
0:  -5.938071  -6.471555  -6.739845  -6.6599555 -6.3625646 -6.0074334
0:  -6.8107896 -6.9711013]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.027832 31.324461 31.598095 31.753815 31.815153 31.64053  31.915834
0:  32.054363 32.33938  32.3686   31.921741 31.512089 31.287886 31.37627
0:  31.917028 32.605206 33.16445  33.420338 32.38499  32.63569 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7617826  -1.737793   -1.4563732  -0.9704056  -0.49123812 -0.11691427
0:   0.13216448  0.3880806   0.38797808  0.3489189   0.12663412 -0.36022425
0:  -0.8017278  -1.2133503  -1.34338    -1.1578054  -0.7664585  -0.3486414
0:  -1.3480396  -1.3073244 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.089885   5.4339514  5.8707905  6.181725   6.3156114  6.304448
0:   6.4185724  6.5589533  6.776141   6.956018   6.9895678  6.867497
0:   6.868372   7.059761   7.595289   8.428056   9.320177  10.096595
0:   7.5951934  7.989905 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0381846 -3.168491  -2.9927678 -2.6800466 -2.499432  -2.539453
0:  -2.7827754 -3.0172563 -3.4678283 -3.8350701 -4.3023334 -5.0285134
0:  -5.714394  -6.3096623 -6.586168  -6.4232507 -6.000175  -5.5244384
0:  -7.56704   -7.6707463]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.926489  3.18364   3.5648637 3.9572322 4.246429  4.423416  4.4366803
0:  4.3878884 4.1430407 3.947717  3.74519   3.4558864 3.2558858 3.2177522
0:  3.4639776 4.031934  4.8108535 5.621544  4.5216637 4.975054 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.739372 18.901163 18.94292  18.738789 18.488037 18.108042 18.340712
0:  18.54418  18.822157 18.924833 18.538761 18.151785 17.727674 17.810825
0:  18.397669 19.303354 20.150202 20.646076 21.100163 21.371557]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-17.599113 -18.29166  -18.503052 -18.380795 -18.195919 -18.040466
0:  -17.781986 -17.509327 -17.292816 -16.996277 -16.829521 -16.790012
0:  -16.560406 -16.037315 -15.19551  -14.10311  -13.158311 -12.464572
0:  -13.882047 -13.77386 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.7317796  7.0629554  7.639176   8.284894   8.8818035  9.286049
0:   9.625175   9.918504  10.006069  10.047961   9.937759   9.616009
0:   9.298979   9.054874   9.052729   9.265323   9.634953   9.922723
0:   8.241818   8.603217 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.510252 12.674444 12.93367  13.083625 13.099562 12.971842 12.964656
0:  12.921494 12.865091 12.771993 12.481428 12.069065 11.745871 11.580518
0:  11.713405 12.07836  12.475996 12.743238 10.738813 10.856832]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.771384  -6.286159  -6.406836  -6.0978484 -5.630793  -5.175344
0:  -5.0358205 -4.8996587 -5.0657945 -5.1076083 -5.1178803 -5.40519
0:  -5.698981  -6.099589  -6.3336864 -6.1741433 -5.5051484 -4.6292653
0:  -4.3380055 -4.1726403]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.953018  -9.807072  -9.203199  -8.331654  -7.480079  -6.8784194
0:  -6.5104833 -6.3648877 -6.5596786 -6.7805285 -7.16445   -7.7034516
0:  -8.114489  -8.379406  -8.336761  -8.028803  -7.628857  -7.3394485
0:  -7.5507464 -7.38724  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.770321  14.038555  14.481409  14.967592  15.3035965 15.423008
0:  15.311003  15.134277  14.762957  14.448063  14.092595  13.538458
0:  13.02298   12.504398  12.274799  12.373009  12.759177  13.149006
0:  11.147494  11.337919 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7518148 2.9961064 3.4008887 4.017641  4.596484  5.1631613 5.5294757
0:  5.9551854 6.1600733 6.4050913 6.578362  6.4784083 6.3781013 6.2029595
0:  6.250454  6.6446877 7.3842044 8.132289  6.8570447 6.593608 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.530617 10.510778 10.698624 10.829073 10.830869 10.67299  10.506103
0:  10.359871 10.137587  9.951469  9.703126  9.217838  8.9022    8.686878
0:   8.841395  9.417294 10.253468 10.998533  9.13751   9.208137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.109114 -16.358088 -16.29062  -15.938488 -15.600355 -15.368526
0:  -15.411085 -15.531504 -15.913288 -16.35698  -16.92492  -17.7987
0:  -18.544495 -19.112476 -19.27282  -18.858952 -17.995632 -16.885845
0:  -17.179045 -17.292929]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.64607  33.67567  33.496853 33.03644  32.484055 31.827114 31.849506
0:  31.81203  31.901001 31.742172 31.04935  30.4141   29.934246 29.92979
0:  30.489353 31.229187 31.815544 32.068222 32.52077  32.48015 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.0808277 5.043771  5.134865  5.2591047 5.2929697 5.198354  5.0390406
0:  5.0427547 4.876506  4.804673  4.6426334 4.127655  3.619675  3.1508777
0:  2.98818   3.3261786 4.0711427 4.8669667 3.284789  3.3996832]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.961959  5.9613004 6.1753364 6.5281014 6.8766503 7.103319  7.3181725
0:  7.475783  7.45551   7.4638667 7.3100257 6.9751787 6.6538687 6.3910923
0:  6.304941  6.410016  6.5753384 6.6220446 5.270369  5.1191254]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.5154011 2.4021995 2.5339823 2.9431283 3.392118  3.7848566 3.942632
0:  4.140623  4.041005  3.99676   3.886127  3.482168  3.0513322 2.5111945
0:  2.1615088 2.1972678 2.6852188 3.3469884 2.2548437 2.1678057]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1698918  -3.0445104  -2.5806298  -1.8504505  -1.1279774  -0.56402063
0:  -0.29109573 -0.11559772 -0.29887772 -0.50860214 -0.82245016 -1.4112711
0:  -1.9339094  -2.4510226  -2.7324853  -2.620544   -2.1580677  -1.5913062
0:  -2.2937965  -2.608151  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.652336 34.680187 34.306633 33.45012  32.440018 31.235405 31.018852
0:  30.778622 30.720951 30.330318 29.169731 28.128735 27.163559 27.001862
0:  27.619213 28.579819 29.285233 29.239334 30.380909 30.626503]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.265488  5.340576  5.643353  6.0145903 6.3390865 6.5163302 6.722602
0:  6.8593416 6.871837  6.939771  6.900834  6.716793  6.603857  6.5229897
0:  6.637935  6.8630986 7.1465516 7.3049407 7.27044   7.4961514]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.334202  12.637047  12.867325  12.735254  12.409405  11.972555
0:  12.098586  12.36788   12.800566  13.138174  13.015677  12.787909
0:  12.451265  12.517723  12.923031  13.565237  14.02      14.15934
0:  13.7216625 13.953041 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.055895  -7.2232523 -7.0254784 -6.473427  -5.7948065 -5.1791954
0:  -4.830967  -4.518957  -4.566736  -4.600995  -4.7209854 -5.165681
0:  -5.576782  -6.05987   -6.3771    -6.3160744 -5.8662663 -5.276659
0:  -6.852813  -7.1116304]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.242408  7.3412175 7.6624956 8.140812  8.514051  8.714081  8.682694
0:  8.674237  8.425637  8.204006  7.8757143 7.1977253 6.509436  5.8503556
0:  5.588174  5.8968277 6.7638807 7.7604146 6.5277877 6.6170735]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.3196945 15.515158  15.81765   16.00908   16.02792   15.896654
0:  16.00168   16.105717  16.303574  16.449389  16.32351   16.059427
0:  15.892622  15.952038  16.396446  17.12297   17.856388  18.382652
0:  16.318909  16.44439  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.25586  29.342703 29.273989 28.993416 28.756626 28.40285  28.659414
0:  28.810102 29.038822 28.999718 28.47348  27.987087 27.633694 27.708588
0:  28.26382  29.023773 29.734926 30.123611 28.699259 28.860592]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.8180437 -4.862733  -4.520849  -3.8795671 -3.2603064 -2.7946005
0:  -2.6003237 -2.524249  -2.7624798 -2.992631  -3.320077  -3.9443893
0:  -4.485709  -5.0447445 -5.326251  -5.137823  -4.572281  -3.9019327
0:  -5.540696  -5.6029587]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.661343 21.477495 21.047775 20.3361   19.640244 18.816437 18.796772
0:  18.798597 19.035513 19.095621 18.587194 18.17014  17.704977 17.7043
0:  18.087893 18.661734 19.005259 18.915577 18.223932 18.23196 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.249035  11.2730465 11.374561  11.428239  11.405925  11.240443
0:  11.344348  11.37809   11.470172  11.48476   11.241591  10.905429
0:  10.66288   10.680622  11.05654   11.763678  12.531994  13.171328
0:  12.304941  12.340913 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.80001974  0.7259636   0.9015789   1.2217808   1.520289    1.702075
0:   1.8547211   1.9078321   1.758389    1.6176105   1.337893    0.8646312
0:   0.4821453   0.20933962  0.17426586  0.32391787  0.48231268  0.5354152
0:  -0.71393204 -0.6509018 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.53061867  0.70717096  1.1590905   1.8854213   2.6595945   3.2779353
0:   3.6815422   3.9039173   3.7082562   3.4112904   2.8914833   2.0406766
0:   1.24682     0.48547935  0.04825449 -0.04382038  0.06072474  0.18679094
0:  -1.6586308  -1.566917  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.819595 17.018526 17.125404 16.939308 16.55359  16.012455 15.832663
0:  15.699656 15.631937 15.579449 15.257477 14.907162 14.594532 14.701655
0:  15.163942 15.882771 16.465687 16.745766 16.079271 16.283066]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.52043  14.602652 14.727194 14.816767 14.788815 14.634563 14.528332
0:  14.494006 14.431637 14.390195 14.304255 14.069765 13.964544 14.024706
0:  14.473324 15.23075  16.168608 16.988245 16.563412 16.69925 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.166373   8.165337   8.267071   8.40187    8.519221   8.675905
0:   8.9739065  9.50419   10.025393  10.589269  10.970356  10.999287
0:  10.89938   10.622175  10.391165  10.243603  10.140049   9.980136
0:   6.6026077  6.4962864]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.697479 15.902401 16.137415 16.335234 16.456818 16.461838 16.865738
0:  17.21815  17.59927  17.810999 17.637728 17.389702 17.23852  17.443167
0:  18.106535 19.09072  20.091604 20.859257 19.533539 19.75664 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.385569 16.733192 17.087921 17.328407 17.435886 17.358372 17.56242
0:  17.739613 17.973053 18.123774 18.00034  17.756992 17.607956 17.722836
0:  18.186346 18.929504 19.707022 20.31794  19.341427 19.49582 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.437622  9.621879  9.973661 10.209286 10.312997 10.225071 10.441744
0:  10.682035 10.949646 11.250985 11.247445 11.087707 10.87406  10.967289
0:  11.352282 12.041475 12.655222 13.006138 11.845901 12.143191]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.573715 20.344524 20.144482 19.851545 19.633896 19.342155 19.639923
0:  19.913158 20.291515 20.551674 20.411648 20.24751  20.185276 20.560486
0:  21.363995 22.501862 23.64303  24.487274 24.114754 24.24436 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.7756743 6.8734927 7.1766205 7.514416  7.733555  7.8046207 7.884418
0:  8.023651  8.05216   8.046751  7.8170533 7.238392  6.6552854 6.1620026
0:  6.0118246 6.2807956 6.838758  7.388201  5.6942987 5.865971 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.053087  5.0645256 5.3279376 5.696086  6.002877  6.133287  6.176703
0:  6.125506  5.877572  5.5987864 5.182518  4.592687  4.110755  3.8222368
0:  3.8527477 4.2384534 4.804387  5.300676  4.3748875 4.611268 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.0189705 4.1138716 4.3986754 4.7548404 5.0089493 5.1319847 5.1338344
0:  5.1956425 5.0422373 4.8896    4.592226  4.060111  3.5284865 3.0646439
0:  2.9005585 3.1458273 3.6985626 4.3696012 2.3235402 2.2095542]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1217551 3.088255  3.336912  3.6963146 3.9476192 4.0586796 4.2715807
0:  4.4772696 4.691788  4.8593454 4.8247113 4.610477  4.46963   4.5061216
0:  4.914997  5.6719217 6.517847  7.259036  6.241927  6.3722124]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.60822296 0.9544916  1.3131604  1.5168447  1.6352787  1.718523
0:  2.432075   3.2236671  4.0637865  4.670863   4.631402   4.3721857
0:  4.036028   4.260867   5.0172787  6.0311337  6.8189635  7.096655
0:  6.305193   6.683543  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.114735  10.112004  10.271636  10.358106  10.159917   9.740234
0:   9.234668   8.9582205  8.707398   8.622101   8.501131   7.97468
0:   7.4284763  6.9234905  6.9021173  7.5373034  8.777457  10.16156
0:   9.428486   9.756538 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.716267   5.1554565  5.45116    5.4761033  5.314378   5.131612
0:   5.768043   6.5874186  7.598272   8.443927   8.692965   8.899229
0:   8.985805   9.680948  10.775362  12.006888  12.82742   12.98229
0:  14.874128  15.551546 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.675518 13.971746 14.328678 14.649597 14.873769 14.95065  15.201067
0:  15.420515 15.618071 15.734515 15.605141 15.426134 15.323608 15.528679
0:  16.164532 17.095406 18.064774 18.86663  17.512205 17.833153]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0266361  -2.9801235  -2.6001177  -2.011516   -1.4568     -1.0529342
0:  -0.8199215  -0.711421   -0.92902136 -1.1735392  -1.5381365  -2.123396
0:  -2.5622263  -2.9411745  -3.0986729  -3.0176258  -2.849      -2.7143106
0:  -4.067237   -4.1804724 ]
0: validation loss for strategy=forecast at epoch 7 : 0.2841348350048065
0: validation loss for velocity_u : 0.12216398119926453
0: validation loss for velocity_v : 0.17956821620464325
0: validation loss for specific_humidity : 0.18005220592021942
0: validation loss for velocity_z : 0.5035669207572937
0: validation loss for temperature : 0.09182413667440414
0: validation loss for total_precip : 0.6276335120201111
0: 8 : 18:30:01 :: batch_size = 96, lr = 1.7245937319210094e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 8, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9096, 0.8964, 0.8764, 0.8497, 0.8156, 0.7740, 0.7276, 0.6802, 0.6361, 0.5967, 0.5621, 0.5326, 0.5101, 0.4966,
0:         0.4912, 0.4911, 0.4937, 0.4966, 0.8395, 0.8231, 0.8006, 0.7744, 0.7440, 0.7076, 0.6655], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6880, -0.6855, -0.6819, -0.6792, -0.6778, -0.6741, -0.6655, -0.6507, -0.6301, -0.6046, -0.5754, -0.5473,
0:         -0.5269, -0.5197, -0.5254, -0.5375, -0.5483, -0.5517, -0.7365, -0.7459, -0.7528, -0.7577, -0.7596, -0.7553,
0:         -0.7408], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6379, -0.6396, -0.6430, -0.6432, -0.6414, -0.6388, -0.6384, -0.6368, -0.6356, -0.6376, -0.6363, -0.6385,
0:         -0.6432, -0.6404, -0.6335, -0.6157, -0.6015, -0.5817, -0.6308, -0.6325, -0.6330, -0.6307, -0.6290, -0.6264,
0:         -0.6235], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5958,  0.6822,  0.7885,  0.9258,  1.0664,  1.1351,  1.0254,  0.6799,  0.1717, -0.3167, -0.6223, -0.7275,
0:         -0.7486, -0.8050, -0.9213, -1.0254, -1.0453, -0.9623,  0.9081,  1.0254,  1.1329,  1.2093,  1.2137,  1.0886,
0:          0.7874], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.7171, -1.7285, -1.7373, -1.7423, -1.7403, -1.7313, -1.7169, -1.6995, -1.6798, -1.6593, -1.6401, -1.6223,
0:         -1.6052, -1.5856, -1.5626, -1.5387, -1.5170, -1.4980, -1.4815, -1.4661, -1.4520, -1.4382, -1.4233, -1.4072,
0:         -1.3899], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2329, -0.2317, -0.2223, -0.2163, -0.2092, -0.1914, -0.1855, -0.1902, -0.2045, -0.2306, -0.2294, -0.2294,
0:         -0.2187, -0.2116, -0.1938, -0.1689, -0.1772, -0.1914, -0.2377, -0.2341, -0.2341, -0.2234, -0.2175, -0.2009,
0:         -0.1736], device='cuda:0')
0: [DEBUG] Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,  0.0565,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0625,     nan,     nan,     nan,     nan,  0.1752,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.2725,     nan,     nan,     nan,  0.5050,     nan,     nan,     nan,  0.8621,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.5346,  0.7458,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.4279,     nan,     nan,     nan,     nan,  0.7577,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.9261,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0657,     nan, -0.0182, -0.0087,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0909,     nan,     nan,     nan,     nan,  0.4397,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.5655,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.8360,     nan,
0:             nan,     nan,     nan,  0.7624,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.8550,     nan,     nan,     nan,     nan,     nan, -0.1606,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.6343,     nan,     nan,     nan,     nan,     nan,  0.5489,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.4730,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.5774,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 8, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.6804, 1.6842, 1.6896, 1.6896, 1.6775, 1.6342, 1.6173, 1.5796, 1.5522, 1.5133, 1.4484, 1.3859, 1.3467, 1.3411,
0:         1.3661, 1.4218, 1.4972, 1.5566, 1.6815, 1.7229, 1.7565, 1.7599, 1.7021, 1.6382, 1.5777], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1370, 0.1437, 0.1690, 0.1982, 0.2241, 0.2184, 0.1838, 0.1503, 0.1513, 0.1722, 0.2111, 0.2517, 0.2705, 0.2383,
0:         0.1894, 0.1769, 0.2028, 0.2669, 0.0756, 0.0896, 0.1132, 0.1418, 0.1579, 0.1368, 0.1008], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6436, -0.6441, -0.6490, -0.6459, -0.6385, -0.6268, -0.6234, -0.6142, -0.6115, -0.6167, -0.6222, -0.6269,
0:         -0.6263, -0.6266, -0.6338, -0.6388, -0.6559, -0.6639, -0.6606, -0.6542, -0.6540, -0.6472, -0.6387, -0.6305,
0:         -0.6187], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2837, -0.1361,  0.1427,  0.1914,  0.0976,  0.1189, -0.0133, -0.3028, -0.5206, -0.6943, -0.7867, -0.7694,
0:         -0.7396, -0.8810, -1.1466, -1.2361, -1.1759, -1.1134, -0.2221, -0.1566,  0.0170,  0.0220, -0.0848, -0.1082,
0:         -0.2355], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3309, -0.3709, -0.4043, -0.4348, -0.4584, -0.4895, -0.5400, -0.6176, -0.7064, -0.7882, -0.8554, -0.8996,
0:         -0.9337, -0.9685, -1.0107, -1.0585, -1.1074, -1.1503, -1.1867, -1.2188, -1.2447, -1.2630, -1.2705, -1.2686,
0:         -1.2671], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.4444, 0.4147, 0.4438, 0.3719, 0.3774, 0.3696, 0.3942, 0.4413, 0.4704, 0.5642, 0.5384, 0.4770, 0.4407, 0.4208,
0:         0.4024, 0.4556, 0.4957, 0.5433, 0.6848, 0.6388, 0.6226, 0.5625, 0.5470, 0.5439, 0.5826], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.1762257218360901; velocity_v: 0.217579185962677; specific_humidity: 0.19205176830291748; velocity_z: 0.561577320098877; temperature: 0.11890048533678055; total_precip: 0.6109967231750488; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18229971826076508; velocity_v: 0.2440953254699707; specific_humidity: 0.23971205949783325; velocity_z: 0.6350300908088684; temperature: 0.14731746912002563; total_precip: 0.7809917330741882; 
0: epoch: 8 [1/5 (20%)]	Loss: 0.69599 : 0.30792 :: 0.19725 (2.44 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17728373408317566; velocity_v: 0.23870499432086945; specific_humidity: 0.19243845343589783; velocity_z: 0.5754308104515076; temperature: 0.1413656771183014; total_precip: 0.6558161377906799; 
0: epoch: 8 [2/5 (40%)]	Loss: 0.65582 : 0.29625 :: 0.19683 (16.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19965432584285736; velocity_v: 0.2531502842903137; specific_humidity: 0.21530947089195251; velocity_z: 0.6136674880981445; temperature: 0.14790035784244537; total_precip: 0.884844958782196; 
0: epoch: 8 [3/5 (60%)]	Loss: 0.88484 : 0.35148 :: 0.19848 (16.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16659820079803467; velocity_v: 0.25122588872909546; specific_humidity: 0.24051916599273682; velocity_z: 0.5364195108413696; temperature: 0.16567620635032654; total_precip: 0.843112587928772; 
0: epoch: 8 [4/5 (80%)]	Loss: 0.84311 : 0.33296 :: 0.19200 (16.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [8.58306885e-06 1.00135803e-05 6.67572021e-06 1.33514404e-05
0:  2.14576721e-05 2.76565552e-05 7.15255737e-06 6.19888306e-06
0:  4.76837158e-07 1.43051147e-06 9.53674316e-07 3.33786011e-06
0:  3.33786011e-06 4.29153442e-06 4.29153442e-06 4.76837158e-07
0:  9.53674316e-07 1.43051147e-06 7.15255737e-06 1.62124634e-05
0:  1.52587891e-05 7.62939453e-06 8.10623169e-06 5.24520874e-06
0:  8.10623169e-06 1.28746033e-05 1.90734863e-06 2.38418579e-06
0:  5.24520874e-06 1.28746033e-05 1.76429749e-05 7.15255737e-06
0:  1.43051147e-05 1.43051147e-05 1.71661377e-05 5.05447388e-05
0:  4.00543213e-05 3.19480896e-05 4.67300451e-05 5.91278076e-05
0:  4.52995300e-05 3.52859497e-05 4.91142309e-05 2.90870667e-05
0:  1.90734863e-05 6.19888306e-06 4.29153442e-06 4.76837158e-06
0:  1.14440918e-05 9.53674316e-06 6.19888306e-06 5.24520874e-06
0:  1.43051147e-06 1.90734863e-06 2.38418579e-06 1.90734863e-06
0:  9.53674316e-07 5.24520874e-06 8.10623169e-06 1.76429749e-05
0:  2.47955322e-05 6.19888306e-06 4.76837158e-07 1.90734863e-06
0:  1.90734863e-05 2.95639038e-05 2.33650208e-05 2.86102295e-06
0:  1.43051147e-06 9.53674316e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 9.53674316e-07 3.33786011e-06 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 1.90734863e-06
0:  7.15255737e-06 9.53674316e-06 1.81198120e-05 2.33650208e-05
0:  1.38282776e-05 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.90734863e-06 2.38418579e-06 7.62939453e-06
0:  3.29017639e-05 5.91278076e-05 3.38554382e-05 9.53674316e-06
0:  1.33514404e-05 6.19888306e-06 4.29153442e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 4.76837158e-07 2.52723694e-05 2.67028809e-05
0:  1.52587891e-05 1.19209290e-05 3.76701355e-05 3.91006470e-05
0:  1.52587891e-05 2.52723694e-05 1.23977661e-05 9.05990601e-06
0:  1.33514404e-05 1.23977661e-05 9.53674316e-06 4.81605530e-05
0:  3.19480896e-05 5.81741333e-05 5.62667847e-05 4.91142309e-05
0:  1.30176544e-04 9.72747803e-05 4.29153442e-05 4.14848328e-05
0:  1.62124634e-05 1.04904175e-05 1.38282776e-05 8.10623169e-06
0:  2.38418579e-06 6.19888306e-06 2.86102295e-06 4.76837158e-07
0:  3.33786011e-06 3.81469727e-06 2.38418579e-06 5.24520874e-06
0:  1.90734863e-06 3.33786011e-06 2.38418579e-06 1.90734863e-06
0:  2.86102295e-06 6.19888306e-06 1.00135803e-05 1.33514404e-05
0:  1.28746033e-05 1.66893005e-05 9.53674316e-06 9.05990601e-06
0:  1.62124634e-05 1.52587891e-05 8.58306885e-06 1.33514404e-05
0:  3.81469727e-06 1.90734863e-06 4.76837158e-07 1.43051147e-06
0:  2.09808350e-05 2.19345093e-05 2.67028809e-05 2.95639038e-05
0:  2.57492065e-05 2.52723694e-05 1.71661377e-05 2.05039978e-05
0:  1.19209290e-05 2.38418579e-06 7.62939453e-06 2.24113464e-05
0:  2.81333923e-05 1.04904175e-05 1.47819519e-05 1.14440918e-05
0:  6.19888306e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Target values (first 200):
0: [1.33514404e-05 9.53674316e-06 4.29153442e-06 3.81469727e-06
0:  4.29153442e-06 8.10623169e-06 5.24520874e-06 9.05990601e-06
0:  4.29153442e-06 4.29153442e-06 2.38418579e-06 1.43051147e-06
0:  9.53674316e-07 1.43051147e-06 1.90734863e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 1.43051147e-06
0:  1.43051147e-06 9.53674316e-07 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 9.53674316e-07
0:  9.53674316e-07 4.76837158e-07 4.29153442e-06 5.72204590e-06
0:  1.04904175e-05 1.04904175e-05 1.00135803e-05 8.10623169e-06
0:  4.76837158e-06 1.90734863e-06 4.29153442e-06 2.38418579e-06
0:  6.19888306e-06 3.33786011e-06 3.81469727e-06 2.38418579e-06
0:  4.29153442e-06 6.67572021e-06 2.38418579e-06 6.19888306e-06
0:  3.33786011e-06 9.53674316e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  1.43051147e-06 1.90734863e-06 4.29153442e-06 5.24520874e-06
0:  3.81469727e-06 1.90734863e-06 9.53674316e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 9.53674316e-07 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 4.76837158e-07 5.72204590e-06 1.47819519e-05
0:  7.67707825e-05 1.84535980e-04 3.84330750e-04 4.12940979e-04
0:  2.86102295e-06 2.86102295e-06 1.90734863e-06 1.43051147e-06
0:  3.81469727e-06 7.15255737e-06 1.38282776e-05 8.10623169e-06
0:  5.72204590e-06 4.76837158e-06 5.72204590e-06 6.67572021e-06
0:  5.24520874e-06 3.33786011e-06 3.81469727e-06 1.90734863e-06
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 9.53674316e-07
0:  9.53674316e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 1.43051147e-06 3.81469727e-06
0:  7.62939453e-06 6.19888306e-06 5.24520874e-06 5.24520874e-06
0:  2.86102295e-06 1.90734863e-06 2.86102295e-06 1.90734863e-06
0:  1.43051147e-06 9.53674316e-07 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 1.90734863e-06
0:  2.86102295e-06 4.76837158e-07 4.76837158e-07 1.23977661e-05
0:  8.58306885e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 4.29153442e-06
0:  5.72204590e-06 7.15255737e-06 7.62939453e-06 6.67572021e-06
0:  4.76837158e-06 1.90734863e-06 4.76837158e-06 4.29153442e-06
0:  2.38418579e-06 9.53674316e-07 1.43051147e-06 1.43051147e-06
0:  9.53674316e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [-13.320314 -13.143442 -12.502808 -11.525972 -10.522099  -9.705593
0:   -9.236765  -8.918541  -8.990675  -9.060453  -9.252592  -9.748472
0:  -10.189568 -10.641661 -10.889822 -10.722296 -10.219597  -9.543362
0:   -9.812523  -9.493526]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.040, max = 1.788, mean = -0.311
0:          sample (first 20): tensor([-1.6155, -1.6005, -1.5459, -1.4628, -1.3773, -1.3078, -1.2679, -1.2408, -1.2470, -1.2529, -1.2693, -1.3115,
0:         -1.3490, -1.3875, -1.4086, -1.3944, -1.3516, -1.2940, -1.5297, -1.5813])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.281026 13.59951  14.152174 14.620133 14.752623 14.539192 14.217241
0:  13.975639 13.693937 13.454962 13.119564 12.506575 12.018581 11.654789
0:  11.670113 12.112768 12.785642 13.369787 11.084234 11.334517]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.157349 22.843441 22.501236 22.102766 21.735394 21.258263 21.318077
0:  21.332565 21.501472 21.44151  20.952026 20.581871 20.438688 20.834383
0:  21.841797 23.097652 24.236202 25.025795 24.984646 24.921268]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.6025696 -6.8274136 -6.7107525 -6.21039   -5.5252647 -4.9235363
0:  -4.625533  -4.41461   -4.7074633 -5.009919  -5.498231  -6.307987
0:  -7.1092854 -7.9454393 -8.466923  -8.581963  -8.179705  -7.6033764
0:  -7.5036745 -7.5533476]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3714294 -3.4318447 -3.0902433 -2.4744148 -1.9043093 -1.5448809
0:  -1.3946619 -1.3942609 -1.7740068 -2.2265644 -2.94553   -4.0000553
0:  -4.988735  -5.8639    -6.3463173 -6.3280134 -5.9959707 -5.582097
0:  -5.9857635 -5.9383326]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.6359    14.668318  14.804951  14.919234  14.820676  14.34811
0:  13.927469  13.2744055 12.5608635 11.810891  10.950705  10.090457
0:   9.520849   9.403263   9.83824   10.768582  11.89806   12.899038
0:  12.05271   12.305929 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.8541436 -7.898084  -7.6592774 -7.251144  -6.845081  -6.5688167
0:  -6.429609  -6.366321  -6.634049  -6.9013267 -7.346873  -7.97428
0:  -8.447605  -8.775747  -8.8010235 -8.511362  -8.15181   -7.8186374
0:  -7.8868527 -7.812103 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4550471  -1.3264356  -0.85024786 -0.17212343  0.4358387   0.8464365
0:   1.1325507   1.2723613   1.1299834   0.9158201   0.50015306 -0.21040201
0:  -0.8915725  -1.4819198  -1.8341393  -1.8755469  -1.767777   -1.6696048
0:  -3.3377576  -3.2876601 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.279224 18.490896 18.625143 18.599693 18.508942 18.216642 18.447418
0:  18.61138  18.91897  19.093243 18.864286 18.59235  18.35734  18.48705
0:  18.992958 19.794647 20.592716 21.122847 20.412409 20.631582]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.0064945 11.13918   11.354608  11.56099   11.655721  11.601084
0:  11.674461  11.75523   11.762676  11.7507715 11.598511  11.2842655
0:  11.107012  11.131342  11.515446  12.226011  13.038446  13.685373
0:  12.565498  12.720423 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.677137 17.891768 17.986776 17.736523 17.074432 16.100986 15.396774
0:  14.808155 14.378668 13.97765  13.344631 12.525694 11.779281 11.367817
0:  11.470087 12.089104 12.888056 13.528704 12.065573 12.15973 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.762127 21.004797 21.324617 21.486294 21.51855  21.389221 21.398472
0:  21.415617 21.36749  21.228577 20.845913 20.249308 19.754805 19.371944
0:  19.345556 19.615583 20.034595 20.436548 17.749924 17.653969]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.053772  2.1292868 2.423912  2.7925572 3.1285653 3.338944  3.4816418
0:  3.5526435 3.4410539 3.3366342 3.1660223 2.9361324 2.8147922 2.9382272
0:  3.2742252 3.7336264 4.1563406 4.3349824 3.0625753 3.046966 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.32886314 -0.620265   -0.58017683 -0.250072    0.10865307  0.3468895
0:   0.3571067   0.36368132  0.05386353 -0.24164486 -0.68061924 -1.4958787
0:  -2.2962785  -3.16816    -3.7837253  -3.901547   -3.5872235  -3.0460448
0:  -5.603538   -6.108356  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.9968638 -3.2174602 -3.1867633 -2.9219317 -2.6273875 -2.4472318
0:  -2.314261  -2.2502651 -2.4041305 -2.5669532 -2.8605733 -3.2919774
0:  -3.62785   -3.8692813 -3.821941  -3.4488702 -2.9159856 -2.3195963
0:  -1.8771944 -1.8409367]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.0609107 -6.9229226 -6.3576875 -5.50162   -4.6016803 -3.8968368
0:  -3.5310826 -3.3021483 -3.425551  -3.5644956 -3.8130593 -4.369971
0:  -4.9448094 -5.560224  -5.976422  -6.049249  -5.7071033 -5.2389855
0:  -6.047273  -6.1050105]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1267056 -3.040135  -2.6292205 -1.9646497 -1.3341689 -0.8868427
0:  -0.8231983 -0.8576484 -1.3112087 -1.7587762 -2.309216  -3.2018476
0:  -4.01123   -4.8599095 -5.3971705 -5.400186  -4.940806  -4.255134
0:  -4.935449  -5.009085 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.022644  19.862     19.537657  19.020351  18.399057  17.59058
0:  17.431223  17.256495  17.215025  17.054924  16.383007  15.781736
0:  15.204515  15.279518  15.8723755 16.784466  17.544487  17.838121
0:  17.63219   17.619474 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.900306 13.347498 13.612164 13.660401 13.651953 13.520121 14.012717
0:  14.491171 15.05621  15.519621 15.515087 15.526354 15.518604 15.94257
0:  16.778824 17.842941 18.845703 19.574038 19.550262 20.025059]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.1373777 1.1296954 1.3484445 1.69028   1.9675493 2.119924  2.252304
0:  2.354404  2.2738986 2.1882439 1.9201441 1.4454527 1.0534472 0.7835736
0:  0.8406477 1.212636  1.7127595 2.154818  0.9121094 0.9306407]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.380184 -16.834564 -16.91537  -16.573933 -16.011417 -15.466986
0:  -14.768505 -14.167311 -13.8169   -13.432404 -13.331127 -13.374668
0:  -13.326418 -13.083794 -12.578339 -11.91029  -11.392057 -11.132208
0:  -13.126951 -13.363188]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.935421  -9.12001   -8.858255  -8.160831  -7.3322425 -6.591002
0:  -6.2789655 -6.056009  -6.318187  -6.6038566 -6.995609  -7.8279586
0:  -8.616743  -9.443203  -9.959486  -9.842199  -9.095043  -8.049162
0:  -7.922219  -7.996138 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9560485  -3.9430299  -3.5504422  -2.905127   -2.2666678  -1.7763743
0:  -1.3622956  -1.0298357  -0.8974509  -0.74231434 -0.7325568  -0.96115065
0:  -1.1392426  -1.1575122  -0.89921284 -0.32100868  0.3379736   0.85530376
0:   0.33547878  0.49296522]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2967181  -3.2777095  -2.8808284  -2.202579   -1.5197434  -1.0144553
0:  -0.8711357  -0.82358885 -1.1733518  -1.4866796  -1.9006295  -2.6313806
0:  -3.317346   -4.054318   -4.538811   -4.5407176  -4.0467734  -3.3106055
0:  -4.3912253  -4.538852  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7965484  -1.7705531  -1.4444733  -0.85688305 -0.30974293  0.09467793
0:   0.1957407   0.32190895  0.16846275  0.121418    0.04932451 -0.3595233
0:  -0.75237274 -1.2634249  -1.5319815  -1.3456769  -0.6734514   0.20330429
0:  -0.24971914 -0.2925644 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.866344 27.896473 27.753494 27.42683  27.158619 26.67794  27.021645
0:  27.206009 27.567276 27.643578 27.160118 26.791832 26.533493 26.804031
0:  27.619694 28.693527 29.60838  30.01915  29.45668  29.673748]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.34133   -3.3621984 -3.0645595 -2.5596051 -2.1247516 -1.8544021
0:  -1.6852217 -1.5674448 -1.6439524 -1.6801882 -1.8568549 -2.289453
0:  -2.6955686 -3.0540452 -3.1731372 -2.968124  -2.566648  -2.204884
0:  -3.1228127 -3.0377374]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4442997  -1.728621   -1.6337409  -1.1897087  -0.71944237 -0.39701128
0:  -0.48130417 -0.56609344 -1.0171723  -1.3296032  -1.6883492  -2.4366012
0:  -3.217585   -4.173449   -4.915953   -5.125094   -4.732575   -4.029495
0:  -5.1275125  -5.578299  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.651165   9.6812725  9.84149   10.070455  10.187975  10.090126
0:  10.067091   9.963961   9.7738495  9.57081    9.191555   8.654634
0:   8.136677   7.7967     7.7377415  7.972622   8.311611   8.541744
0:   6.923909   6.9324713]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.4232454   1.4318533   1.662188    2.0619786   2.262464    2.1571536
0:   1.6160173   1.0049801   0.08834743 -0.6748233  -1.4254017  -2.4565125
0:  -3.443427   -4.4698324  -5.1470366  -5.1624355  -4.465329   -3.4198751
0:  -4.714469   -5.0050087 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.558704 18.90036  19.268085 19.478909 19.546202 19.450266 19.770376
0:  20.169672 20.675142 21.17911  21.430511 21.529886 21.70203  22.17543
0:  23.05239  24.181551 25.29096  26.02579  24.537474 25.022062]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.062008  10.123011  10.423887  10.764417  10.937809  10.856596
0:  10.780611  10.710705  10.506233  10.272177   9.853656   9.206192
0:   8.656927   8.2779665  8.312231   8.8326845  9.64976   10.448156
0:   9.603233   9.7168865]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.476145 19.595894 19.738308 19.762533 19.662682 19.329697 19.457987
0:  19.57395  19.756208 19.825897 19.456207 19.0008   18.528522 18.421038
0:  18.746819 19.405117 20.048483 20.51154  18.032778 17.970366]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.273033 24.601038 24.974667 25.25601  25.277517 25.04424  25.412933
0:  25.537909 25.810585 25.878399 25.35685  24.762682 24.26148  24.152748
0:  24.516693 25.00059  25.233553 25.06662  23.96839  24.046055]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0946646 -4.0416365 -3.8467855 -3.633542  -3.531808  -3.6078777
0:  -3.401104  -3.2513113 -2.9948053 -2.8430052 -3.040595  -3.3755903
0:  -3.6940765 -3.7091846 -3.401679  -2.7942724 -2.1986923 -1.8138361
0:  -3.8483248 -3.7671537]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.54214  18.638344 18.666986 18.474106 18.271418 17.938633 18.293924
0:  18.648882 19.127056 19.42656  19.188738 18.963612 18.726173 18.971256
0:  19.679976 20.631977 21.400368 21.740105 20.69886  20.835016]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.1025     5.155402   5.297955   5.430744   5.594295   5.6388664
0:   6.341176   6.9642057  7.745508   8.326282   8.361542   8.336672
0:   8.294419   8.77353    9.731234  11.032776  12.216618  13.028591
0:  13.111235  13.293549 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.776243 24.634254 24.247799 23.581165 22.946106 22.14963  22.268091
0:  22.30571  22.562246 22.542652 21.860365 21.262796 20.658674 20.595997
0:  21.044285 21.787792 22.349997 22.419813 23.10276  23.172897]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.060326 25.26612  25.47609  25.532444 25.434315 24.991932 24.888805
0:  24.563183 24.169151 23.549675 22.496584 21.38515  20.42375  19.897343
0:  19.916687 20.279367 20.724922 20.984406 18.06528  17.732891]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.753338 15.553318 15.52809  15.649994 15.717352 15.592606 15.638086
0:  15.587658 15.458355 15.12163  14.42083  13.493535 12.592632 11.868086
0:  11.539617 11.557045 11.675105 11.767607  8.767062  8.424796]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.3547673 -6.6819644 -6.529985  -6.0488257 -5.487388  -5.0352397
0:  -4.6879334 -4.5092998 -4.6330075 -4.782435  -5.1171117 -5.632157
0:  -6.050492  -6.2594323 -6.193295  -5.7699466 -5.2025666 -4.73261
0:  -6.5547566 -6.659824 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4875975 1.5037799 1.7885222 2.3351893 2.8490653 3.263287  3.6079636
0:  4.006647  4.1648397 4.325432  4.270343  3.873707  3.4176526 2.9494262
0:  2.776958  3.0414467 3.6248376 4.251977  3.5351539 3.4268517]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.26961  22.396915 22.548601 22.648457 22.658371 22.562565 22.756546
0:  22.97975  23.226477 23.425135 23.428772 23.302238 23.34851  23.664232
0:  24.385252 25.425768 26.566132 27.507805 26.213503 26.369997]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.777685 25.724627 25.400982 24.735258 24.058353 23.269554 23.478798
0:  23.693523 24.144844 24.312859 23.761524 23.396284 23.035475 23.355316
0:  24.297663 25.4688   26.353783 26.549545 26.970963 27.052588]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.014494  -4.1967793 -4.1295614 -3.8389359 -3.5111237 -3.299429
0:  -3.3983974 -3.498886  -3.9127264 -4.25801   -4.64303   -5.3293867
0:  -5.9879894 -6.674946  -7.1483736 -7.1183596 -6.5578823 -5.726405
0:  -7.2523413 -7.7263947]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.487213 25.936535 26.24731  26.329845 26.33332  26.194626 26.770306
0:  27.34917  28.062191 28.5257   28.398788 28.352428 28.327518 28.837318
0:  29.75148  30.733624 31.374084 31.392021 29.383308 29.620003]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.745259    7.189871    6.73034     6.401108    6.0646343   5.692669
0:   5.3211365   5.0393906   4.556654    4.005252    3.1923497   1.9538989
0:   0.70317125 -0.4711895  -1.2313938  -1.3393736  -0.9262743  -0.29888391
0:  -1.6239595  -1.8425798 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.237738  9.333995  9.490785  9.464768  9.272775  8.939668  8.963689
0:  9.056078  9.152275  9.186674  8.8430605 8.281935  7.6963015 7.478875
0:  7.590379  8.048371  8.447193  8.647658  7.2124023 7.3661094]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.075568  7.935402  7.9164667 7.893383  7.817004  7.6399975 7.513149
0:  7.39583   7.172358  6.972269  6.670586  6.2543    5.97969   5.9040356
0:  6.118105  6.6461525 7.2643175 7.8658857 6.377749  6.388715 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.361029 12.616043 13.101336 13.681566 14.116407 14.459331 15.00943
0:  15.639486 16.243492 16.686037 16.850739 16.56795  16.42841  16.36997
0:  16.724077 17.518219 18.457508 19.186047 15.91501  15.932331]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3478456  -1.5531192  -1.3930411  -0.95431185 -0.5155759  -0.23019028
0:  -0.2821374  -0.3770194  -0.8022418  -1.1655116  -1.5508475  -2.2382312
0:  -2.8887095  -3.5606751  -4.004086   -4.0050454  -3.5858035  -3.0080447
0:  -2.4224267  -2.3352304 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.73279  36.333855 36.93986  37.51175  38.10759  38.593544 39.297173
0:  39.43305  39.0292   38.128796 36.90205  35.995945 35.61075  35.400307
0:  35.159904 34.503983 33.283985 31.742401 26.464302 25.196312]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-17.644619 -17.682076 -17.323284 -16.632435 -15.899222 -15.42467
0:  -15.075074 -14.997544 -15.28555  -15.664357 -16.320786 -17.24781
0:  -17.979267 -18.538448 -18.66148  -18.470818 -18.263561 -18.232449
0:  -18.295433 -17.938133]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.7146077 -6.805757  -6.549396  -6.0345206 -5.515076  -5.2335067
0:  -5.173595  -5.255757  -5.631121  -5.9618278 -6.386081  -7.008598
0:  -7.5266232 -7.9547687 -8.101793  -7.9018226 -7.4466014 -7.0003343
0:  -7.0162034 -6.825335 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.600094  4.7706814 5.1353374 5.5764837 5.956702  6.169825  6.3064075
0:  6.406013  6.3539085 6.208166  5.934596  5.4847183 5.1889515 5.0989923
0:  5.3810606 6.0312395 6.905507  7.6888924 6.5839524 6.8207397]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.9451308  -2.7444558  -2.272716   -1.6608157  -1.1245828  -0.7821264
0:  -0.59022474 -0.46269703 -0.5691824  -0.66580343 -0.85555315 -1.2780738
0:  -1.6312366  -1.9221749  -1.9910455  -1.7576289  -1.3650584  -0.9820118
0:  -1.9344058  -1.7446437 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4543571 1.4032083 1.5333352 1.7415318 2.0150127 2.2584682 2.9562182
0:  3.7255695 4.410116  4.9441533 4.994011  4.882365  4.7010574 4.8461657
0:  5.325543  5.990284  6.422393  6.4919143 6.1923428 6.1493464]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.785027 16.12477  16.614822 17.087343 17.396336 17.513802 17.877548
0:  18.215485 18.637583 19.057285 19.241968 19.317787 19.4402   19.804508
0:  20.46586  21.37052  22.219227 22.771696 21.536419 21.979626]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.21585  16.42064  16.597458 16.62299  16.507626 16.155478 16.275904
0:  16.31633  16.416937 16.380585 15.915287 15.434521 14.979139 14.990486
0:  15.430548 16.194818 16.909454 17.319523 16.063818 16.173594]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.364616 17.71331  18.018599 18.190254 18.15583  17.907726 17.943924
0:  17.985704 18.061756 18.076233 17.775202 17.386604 17.070683 16.981758
0:  17.203568 17.668993 18.166628 18.495388 16.980434 17.145487]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.8834767 2.8274922 3.0355942 3.4548643 3.8889706 4.216218  4.3902197
0:  4.585041  4.557087  4.5557156 4.4406395 3.933825  3.370643  2.7226682
0:  2.3030648 2.3875203 2.9858716 3.8201468 2.6565337 2.81245  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.298496  11.585207  12.056082  12.511643  12.805002  12.969176
0:  13.234922  13.617106  13.979002  14.329836  14.5143585 14.410828
0:  14.346177  14.32781   14.605384  15.202496  15.981333  16.63974
0:  14.897293  15.064161 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.28899  18.580633 18.696169 18.527674 18.219177 17.765196 17.930489
0:  18.121649 18.371084 18.534582 18.212053 17.92156  17.679327 17.965502
0:  18.697369 19.710743 20.53414  20.937065 21.604713 21.969103]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.934647  -4.8578734 -4.41616   -3.6488385 -2.8388076 -2.229548
0:  -1.9853253 -1.9277148 -2.3363967 -2.7882323 -3.378181  -4.265096
0:  -5.0688386 -5.832519  -6.323938  -6.3665433 -6.0253224 -5.502771
0:  -6.0681043 -6.2286067]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.312833  12.959612  12.573833  11.908295  11.092571  10.054503
0:   9.606731   9.22142    9.050241   8.877938   8.313822   7.827031
0:   7.4238353  7.762046   8.775447  10.286483  11.682747  12.663902
0:  15.031109  15.909372 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [46.781273 46.732525 46.405373 45.972553 45.5627   45.01116  45.267128
0:  45.41947  45.710342 45.641483 44.84481  44.195396 43.67926  43.782997
0:  44.59344  45.58451  46.268944 46.297184 44.596413 44.688942]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.9045715 -8.076106  -7.8985844 -7.3847847 -6.7806454 -6.2783155
0:  -6.086317  -5.9765205 -6.272728  -6.5715733 -6.977951  -7.7918167
0:  -8.558685  -9.381681  -9.923064  -9.919046  -9.354141  -8.48392
0:  -8.608795  -8.610663 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.005735  -4.2009826 -4.023826  -3.4963899 -2.908565  -2.423966
0:  -2.192615  -2.0333333 -2.2831254 -2.5731673 -3.0248075 -3.845817
0:  -4.621982  -5.4046845 -5.8637795 -5.8189816 -5.3310876 -4.6317296
0:  -5.8038898 -5.900741 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.16916275  0.05974722  0.5770154   1.1985216   1.7022629   2.0055037
0:   2.2698789   2.4180293   2.4148197   2.4173632   2.274541    2.002211
0:   1.7987218   1.7455502   1.9743905   2.5044758   3.1152322   3.6366801
0:   3.7393079   4.180332  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.85887   -5.9595942 -5.734265  -5.21625   -4.6543574 -4.209824
0:  -4.11731   -4.0381474 -4.3414702 -4.58672   -4.9168715 -5.574822
0:  -6.19857   -6.876844  -7.2760253 -7.218682  -6.6820397 -5.9190965
0:  -6.290478  -6.371615 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.95794  26.22586  26.48869  26.604568 26.562988 26.293453 26.436016
0:  26.507748 26.619114 26.62688  26.213814 25.755192 25.428276 25.434555
0:  25.909851 26.644718 27.345596 27.717155 25.418663 25.446636]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.602764 20.88638  21.15875  21.248661 21.040089 20.65826  20.33117
0:  19.936947 19.347214 18.537275 17.407349 16.168491 15.199837 14.65667
0:  14.768734 15.278381 15.967289 16.583622 16.408306 16.761122]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.175607  -5.2127433 -4.931556  -4.4031453 -3.8863196 -3.4908051
0:  -3.2885966 -3.1084838 -3.2702165 -3.5058742 -3.9283376 -4.6527944
0:  -5.2684007 -5.75862   -5.853317  -5.4863734 -4.7915196 -4.0650067
0:  -4.216534  -4.063865 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.634573 22.644085 22.414812 21.849888 21.23     20.488962 20.568506
0:  20.70766  21.049953 21.174212 20.661354 20.242887 19.760542 19.85318
0:  20.448517 21.21222  21.751919 21.712055 21.713339 21.790548]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.553876 14.683115 15.028651 15.377362 15.660952 15.854144 16.149817
0:  16.48365  16.71508  16.923645 16.965872 16.83461  16.85461  17.014866
0:  17.538626 18.289917 19.099766 19.69849  16.956543 17.093006]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2344766e+00 -2.1964421e+00 -1.8578792e+00 -1.2715888e+00
0:  -6.7962122e-01 -2.1990967e-01  2.9668808e-03  1.1854267e-01
0:  -1.4300680e-01 -4.3465567e-01 -8.8072920e-01 -1.5591245e+00
0:  -2.1753564e+00 -2.7705283e+00 -3.0887980e+00 -3.0725522e+00
0:  -2.8016591e+00 -2.4712367e+00 -4.1839871e+00 -4.2272143e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.501698  5.2974825 5.276285  5.2013288 4.917881  4.41693   3.9590414
0:  3.5118961 3.0456889 2.7343993 2.319819  1.7839131 1.2960858 1.0195494
0:  1.0478435 1.3897729 1.7825885 2.023992  1.1064291 1.3559084]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.2348   -15.426516 -15.174597 -14.568068 -13.917282 -13.457533
0:  -13.282547 -13.290543 -13.697601 -14.154821 -14.820059 -15.783432
0:  -16.593502 -17.219748 -17.441742 -17.210997 -16.755098 -16.250107
0:  -17.787191 -18.108702]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [51.67788  52.06205  51.927208 51.473583 50.83505  50.051357 50.271675
0:  50.540226 51.030525 51.164043 50.565834 50.0942   49.73158  49.990192
0:  50.905476 51.791977 52.179195 51.64355  49.716377 50.275764]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.015075  13.138059  13.498072  13.873095  14.169349  14.286154
0:  14.451776  14.490234  14.272268  13.84089   13.063975  12.041061
0:  11.117926  10.450534  10.168956  10.308258  10.547993  10.7154255
0:   7.9019656  7.8167143]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [50.30133  50.189182 49.77555  49.219826 48.700394 48.041054 48.130493
0:  48.13354  48.209827 47.908394 46.969814 46.137127 45.491646 45.360847
0:  45.88485  46.474064 46.7727   46.558304 45.21606  45.05069 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.181942   -1.2645922  -1.0392151  -0.5758691  -0.10226965  0.20131826
0:   0.10674715 -0.09218884 -0.72910357 -1.306798   -1.9320574  -2.7906046
0:  -3.5688295  -4.3188505  -4.73398    -4.7230997  -4.233425   -3.5630274
0:  -5.6818247  -5.937934  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.0670948  -0.08498812  0.10588217  0.44539165  0.6498003   0.68527985
0:   0.43883324  0.2831936  -0.09126663 -0.33915043 -0.5919032  -1.1439672
0:  -1.6344671  -2.11026    -2.2065058  -1.7208748  -0.6801219   0.51759577
0:   0.69602346  0.61013174]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.68489075 -0.7106433  -0.48230028 -0.10231972  0.21016884  0.35421085
0:   0.50759363  0.6820841   0.78369236  0.9500661   1.0562801   0.9733925
0:   0.9980564   1.185113    1.6956143   2.6160035   3.7655084   4.8467937
0:   5.045006    5.4595428 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.88781   12.995068  13.145935  13.278594  13.381351  13.297681
0:  13.678137  13.9899845 14.37879   14.639656  14.49442   14.206142
0:  13.881861  13.852432  14.126759  14.715889  15.343094  15.758242
0:  13.902484  13.7607155]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.35244  13.491902 13.58403  13.523804 13.300888 12.883415 12.736079
0:  12.644274 12.657028 12.759905 12.706388 12.594953 12.57443  12.83383
0:  13.464983 14.379945 15.375629 16.175434 15.957621 16.268305]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.375819  9.506954  9.77038  10.152132 10.512556 10.78294  11.184879
0:  11.596727 11.923639 12.139576 12.129744 11.821741 11.58085  11.473751
0:  11.706176 12.267742 12.916816 13.378605 11.328604 11.102659]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.409033  18.27915   18.16853   17.97039   17.705406  17.2243
0:  17.109598  16.90729   16.733273  16.444872  15.787844  15.104113
0:  14.476202  14.242213  14.420414  14.947123  15.480997  15.8032875
0:  14.674237  14.603429 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.069304    0.803525    0.80622053  1.1117449   1.4431825   1.674706
0:   1.5717244   1.558608    1.2161508   1.0005999   0.7246289  -0.02573776
0:  -0.87239504 -1.8787274  -2.6425424  -2.7531772  -2.0935931  -1.0127492
0:  -1.5656447  -1.8369737 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5232825  -0.30243492  0.22178268  0.79038143  1.2729259   1.5554252
0:   1.9564085   2.2366233   2.339715    2.27607     1.8232913   1.0928774
0:   0.3787675  -0.02139378 -0.08298779  0.1596365   0.31213522  0.22601748
0:  -1.7788281  -1.7558722 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.5179224 5.4705997 5.6045065 5.871358  6.0753345 6.086253  6.1575413
0:  6.117313  5.9863763 5.772709  5.337882  4.720584  4.15265   3.8111048
0:  3.7813768 4.0759697 4.4537187 4.7100477 2.1042175 1.6789379]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.32675  35.492146 35.390697 35.0059   34.714478 34.453438 35.149166
0:  35.77681  36.429203 36.53513  35.753216 34.771156 33.938652 33.604576
0:  34.008327 34.835564 35.669716 36.17914  35.870792 35.78416 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.5550866 2.6025238 2.8896842 3.3237343 3.6740496 3.8456492 3.885133
0:  3.8830445 3.6791108 3.4688444 3.1243787 2.565207  2.0830574 1.7142859
0:  1.6666946 1.9926405 2.513112  2.9866552 1.9865737 2.0097404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.45685  16.541403 16.565592 16.41486  16.27365  16.135242 16.95045
0:  17.857082 18.926159 19.75681  19.865036 19.941093 19.914364 20.55437
0:  21.748095 23.201054 24.394138 24.923859 25.030075 25.492352]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.215952 16.30037  16.496618 16.606838 16.44985  16.037548 15.616197
0:  15.296251 14.999273 14.793084 14.545026 14.116633 13.814812 13.622591
0:  13.783827 14.289217 14.940296 15.443102 14.354897 14.588159]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.629495  6.6966596 7.0436134 7.531391  7.9878063 8.292208  8.531228
0:  8.69359   8.594938  8.512987  8.32642   7.968617  7.738675  7.565588
0:  7.625607  7.9373913 8.324956  8.597648  7.7823315 7.930579 ]
0: validation loss for strategy=forecast at epoch 8 : 0.34729132056236267
0: validation loss for velocity_u : 0.15578784048557281
0: validation loss for velocity_v : 0.204780712723732
0: validation loss for specific_humidity : 0.22186952829360962
0: validation loss for velocity_z : 0.5550100207328796
0: validation loss for temperature : 0.10372042655944824
0: validation loss for total_precip : 0.8425794243812561
0: 9 : 18:33:58 :: batch_size = 96, lr = 1.6825304701668386e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 9, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3391, -0.3476, -0.3665, -0.3530, -0.3000, -0.2464, -0.2084, -0.1877, -0.1971, -0.2316, -0.2592, -0.2525,
0:         -0.2069, -0.1413, -0.0834, -0.0506, -0.0441, -0.0442, -0.2722, -0.3094, -0.3525, -0.3535, -0.3096, -0.2644,
0:         -0.2430], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2492, 1.3465, 1.4116, 1.4584, 1.5159, 1.5898, 1.6863, 1.8180, 1.9842, 2.1833, 2.4091, 2.6404, 2.8675, 3.1000,
0:         3.3381, 3.5683, 3.7689, 3.9001, 1.3004, 1.4192, 1.4746, 1.4942, 1.5334, 1.6046, 1.7078], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5720, -0.5719, -0.5456, -0.5313, -0.5270, -0.5320, -0.5448, -0.5639, -0.5860, -0.6060, -0.6261, -0.6465,
0:         -0.6635, -0.6817, -0.7004, -0.7195, -0.7324, -0.7409, -0.6004, -0.5970, -0.5626, -0.5373, -0.5266, -0.5329,
0:         -0.5453], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.9994,  0.9344,  0.8033,  0.6957,  0.3460, -0.0800, -0.2425, -0.2167, -0.1977, -0.1551,  0.0052,  0.3269,
0:          0.7427,  1.0700,  1.2404,  1.3368,  1.4029,  1.5116,  0.8055,  0.6643,  0.3684,  0.0366, -0.2559, -0.3669,
0:         -0.2840], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 1.3134,  1.3141,  1.2984,  1.2742,  1.2390,  1.1863,  1.1245,  1.0720,  1.0318,  1.0005,  0.9862,  0.9927,
0:          1.0021,  0.9942,  0.9620,  0.9051,  0.8208,  0.6960,  0.5033,  0.2216, -0.1316, -0.4824, -0.7262, -0.8000,
0:         -0.7322], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2559, -0.2559, -0.2559, -0.2240, -0.1780, -0.2299, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559,
0:         -0.2559, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559, -0.2559,
0:         -0.2559], device='cuda:0')
0: [DEBUG] Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2559,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2559,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2559,     nan,     nan,     nan,     nan,     nan,     nan, -0.2559,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2559,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2559,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2559,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2559,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2559,     nan,     nan,     nan, -0.2559,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2559,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2547,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2559,     nan,     nan,     nan,     nan,     nan,
0:         -0.2559,     nan,     nan,     nan,     nan,     nan,     nan, -0.2559, -0.2559,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 9, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7083, -0.7232, -0.7210, -0.7007, -0.6760, -0.6512, -0.6365, -0.6116, -0.6055, -0.6010, -0.6047, -0.6354,
0:         -0.6698, -0.7050, -0.7215, -0.7069, -0.6574, -0.5949, -0.6453, -0.6923, -0.7127, -0.7092, -0.6920, -0.6707,
0:         -0.6482], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2110, -0.1843, -0.1491, -0.1062, -0.0466,  0.0179,  0.0826,  0.1622,  0.2544,  0.3502,  0.4393,  0.5161,
0:          0.5394,  0.4988,  0.4027,  0.3045,  0.2321,  0.2050, -0.2396, -0.1940, -0.1498, -0.1054, -0.0547,  0.0014,
0:          0.0659], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7021, -0.6933, -0.6795, -0.6574, -0.6355, -0.6177, -0.6163, -0.6218, -0.6342, -0.6497, -0.6646, -0.6779,
0:         -0.6796, -0.6779, -0.6768, -0.6725, -0.6748, -0.6751, -0.6980, -0.6883, -0.6694, -0.6416, -0.6187, -0.6065,
0:         -0.5990], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0850,  0.1366,  0.2750,  0.1250, -0.0021,  0.0806,  0.1624,  0.1759,  0.1991,  0.2584,  0.2785,  0.3069,
0:          0.4352,  0.4252,  0.3144,  0.3853,  0.4411,  0.3214,  0.0784,  0.0399,  0.0627,  0.0428,  0.0053,  0.0374,
0:          0.1012], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.9293, 0.9298, 0.9381, 0.9505, 0.9757, 1.0152, 1.0660, 1.1106, 1.1417, 1.1509, 1.1403, 1.1131, 1.0795, 1.0456,
0:         1.0076, 0.9639, 0.9259, 0.8976, 0.8815, 0.8771, 0.8762, 0.8750, 0.8670, 0.8461, 0.8038], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2067, -0.1979, -0.1989, -0.1969, -0.1941, -0.1965, -0.1923, -0.1898, -0.1899, -0.2101, -0.2000, -0.2054,
0:         -0.1991, -0.1992, -0.1986, -0.1986, -0.1946, -0.1936, -0.2217, -0.2177, -0.2107, -0.2059, -0.1986, -0.2001,
0:         -0.2031], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1812053918838501; velocity_v: 0.2415045201778412; specific_humidity: 0.1664116531610489; velocity_z: 0.5552552938461304; temperature: 0.1256306767463684; total_precip: 0.4826708734035492; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19486857950687408; velocity_v: 0.2690068185329437; specific_humidity: 0.1733836680650711; velocity_z: 0.6812556385993958; temperature: 0.12693750858306885; total_precip: 0.8275919556617737; 
0: epoch: 9 [1/5 (20%)]	Loss: 0.65513 : 0.30294 :: 0.20059 (2.85 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1907660961151123; velocity_v: 0.2485228329896927; specific_humidity: 0.2568458318710327; velocity_z: 0.6549726724624634; temperature: 0.17972616851329803; total_precip: 0.9594615697860718; 
0: epoch: 9 [2/5 (40%)]	Loss: 0.95946 : 0.37962 :: 0.19993 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2301599681377411; velocity_v: 0.2951658070087433; specific_humidity: 0.25559577345848083; velocity_z: 0.6594429612159729; temperature: 0.19042383134365082; total_precip: 0.730496346950531; 
0: epoch: 9 [3/5 (60%)]	Loss: 0.73050 : 0.35747 :: 0.19867 (16.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2167765498161316; velocity_v: 0.2902545928955078; specific_humidity: 0.2427574098110199; velocity_z: 0.6347473859786987; temperature: 0.16050156950950623; total_precip: 1.0547112226486206; 
0: epoch: 9 [4/5 (80%)]	Loss: 1.05471 : 0.39866 :: 0.20380 (16.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [4.14848328e-05 3.24249268e-05 2.28881836e-05 2.09808350e-05
0:  2.19345093e-05 2.38418579e-05 2.43186951e-05 2.52723694e-05
0:  2.43186951e-05 2.52723694e-05 2.09808350e-05 1.28746033e-05
0:  9.53674316e-06 1.52587891e-05 2.09808350e-05 2.19345093e-05
0:  1.66893005e-05 1.14440918e-05 1.09672546e-05 1.09672546e-05
0:  1.23977661e-05 1.38282776e-05 1.38282776e-05 1.28746033e-05
0:  1.23977661e-05 1.14440918e-05 1.28746033e-05 1.90734863e-05
0:  1.85966492e-05 2.47955322e-05 2.57492065e-05 2.09808350e-05
0:  2.24113464e-05 1.71661377e-05 2.62260437e-05 2.67028809e-05
0:  1.71661377e-05 2.00271606e-05 1.95503235e-05 2.09808350e-05
0:  2.00271606e-05 2.38418579e-05 3.95774841e-05 2.14576721e-05
0:  1.76429749e-05 1.76429749e-05 1.33514404e-05 1.28746033e-05
0:  2.28881836e-05 3.00407410e-05 2.86102295e-05 2.62260437e-05
0:  3.24249268e-05 2.38418579e-05 2.19345093e-05 3.09944153e-05
0:  3.71932983e-05 3.09944153e-05 3.62396240e-05 4.95910645e-05
0:  5.10215723e-05 5.38825989e-05 8.34465027e-05 8.15391541e-05
0:  8.10623169e-05 9.01222229e-05 9.91821289e-05 9.77516174e-05
0:  1.04904175e-04 1.07288361e-04 1.08242035e-04 1.11579895e-04
0:  1.16825104e-04 1.46389008e-04 1.59263611e-04 1.56879425e-04
0:  1.50680542e-04 1.42574310e-04 1.12533569e-04 1.23023987e-04
0:  1.25408173e-04 1.06334686e-04 1.07288361e-04 1.89781189e-04
0:  2.26974487e-04 2.16007233e-04 1.76906586e-04 1.19686127e-04
0:  9.91821289e-05 1.17301941e-04 1.19209290e-04 8.29696655e-05
0:  5.53131104e-05 4.33921814e-05 5.05447388e-05 4.10079956e-05
0:  2.24113464e-05 3.38554382e-05 4.43458557e-05 6.24656677e-05
0:  7.82012939e-05 7.91549683e-05 6.86645508e-05 1.27792358e-04
0:  1.59740448e-04 2.47955322e-04 3.59058380e-04 3.96728516e-04
0:  3.71932983e-05 2.86102295e-05 2.05039978e-05 2.33650208e-05
0:  2.71797180e-05 2.90870667e-05 2.90870667e-05 3.29017639e-05
0:  2.90870667e-05 3.14712524e-05 2.67028809e-05 1.47819519e-05
0:  8.10623169e-06 1.19209290e-05 1.81198120e-05 1.85966492e-05
0:  1.38282776e-05 1.76429749e-05 1.14440918e-05 1.14440918e-05
0:  1.23977661e-05 1.52587891e-05 1.62124634e-05 1.76429749e-05
0:  1.81198120e-05 1.66893005e-05 1.43051147e-05 1.62124634e-05
0:  1.71661377e-05 2.86102295e-05 2.95639038e-05 2.00271606e-05
0:  1.66893005e-05 2.14576721e-05 4.10079956e-05 4.14848328e-05
0:  2.33650208e-05 1.85966492e-05 2.52723694e-05 2.76565552e-05
0:  2.67028809e-05 2.24113464e-05 2.57492065e-05 1.71661377e-05
0:  2.33650208e-05 2.33650208e-05 1.66893005e-05 1.43051147e-05
0:  1.28746033e-05 1.85966492e-05 1.71661377e-05 8.58306885e-06
0:  6.67572021e-06 2.00271606e-05 2.33650208e-05 3.24249268e-05
0:  4.72068787e-05 4.95910645e-05 6.19888306e-05 8.10623169e-05
0:  8.24928284e-05 6.62803650e-05 5.76972961e-05 9.05990601e-05
0:  9.10758972e-05 1.00135803e-04 1.17778778e-04 8.58306885e-05
0:  8.34465027e-05 8.24928284e-05 8.34465027e-05 8.63075256e-05
0:  1.01089478e-04 1.00612640e-04 1.01089478e-04 9.91821289e-05
0:  9.44137573e-05 1.02043152e-04 1.08718872e-04 1.48773193e-04
0:  1.51157379e-04 1.16348267e-04 1.31607056e-04 1.10149384e-04
0:  1.15394592e-04 1.04427338e-04 7.77244568e-05 5.34057617e-05
0:  4.86373865e-05 9.48905945e-05 9.63211060e-05 5.38825989e-05]
0: Target values (first 200):
0: [7.27595761e-12 7.27595761e-12 7.27595761e-12 4.76837158e-07
0:  4.76837158e-07 9.53681592e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 1.43051147e-06 1.43051147e-06 1.90734863e-06
0:  4.76837158e-06 2.86102295e-06 2.38418579e-06 1.90734863e-06
0:  4.29153442e-06 1.14440918e-05 1.28746033e-05 1.43051147e-05
0:  1.57356262e-05 1.71661377e-05 1.90734863e-05 1.09672546e-05
0:  1.19209290e-05 1.28746033e-05 1.14440918e-05 1.66893005e-05
0:  2.62260437e-05 3.33786011e-05 3.67164612e-05 3.91006470e-05
0:  4.48226929e-05 3.48091125e-05 4.62532043e-05 5.05447388e-05
0:  4.43458557e-05 5.53131104e-05 6.96182251e-05 7.86781311e-05
0:  7.72476196e-05 6.72340393e-05 5.34057617e-05 5.53131104e-05
0:  5.86509705e-05 6.19888306e-05 5.81741333e-05 4.81605566e-05
0:  5.34057617e-05 5.19752502e-05 4.86373865e-05 4.62532043e-05
0:  4.05311584e-05 5.43594360e-05 6.43730164e-05 6.48498535e-05
0:  5.86509705e-05 5.29289246e-05 5.05447388e-05 5.57899475e-05
0:  6.19888306e-05 6.67572021e-05 7.48634338e-05 7.29560852e-05
0:  6.77108765e-05 6.81877136e-05 6.67572021e-05 5.24520874e-05
0:  4.72068787e-05 3.33786011e-05 2.24113464e-05 2.14576721e-05
0:  2.00271606e-05 5.53131104e-05 6.81877136e-05 1.01566315e-04
0:  1.57833099e-04 1.95980072e-04 1.76906586e-04 1.52111053e-04
0:  1.29699707e-04 1.13487244e-04 9.72747803e-05 8.34465027e-05
0:  7.77244568e-05 1.08718872e-04 1.43527985e-04 1.39236450e-04
0:  1.90734863e-04 2.32219696e-04 2.08377838e-04 1.43527985e-04
0:  1.03473663e-04 2.23636627e-04 2.96115875e-04 3.17096710e-04
0:  2.72274017e-04 1.60217285e-04 1.64508820e-04 1.33991241e-04
0:  1.04427338e-04 8.77380371e-05 5.19752502e-05 6.43730164e-05
0:  5.34057617e-05 3.91006470e-05 2.76565552e-05 7.27595761e-12
0:  4.29153442e-06 4.29153442e-06 8.58306885e-06 4.76837158e-07
0:  9.53681592e-07 9.53681592e-07 9.53681592e-07 9.53681592e-07
0:  1.43051147e-06 2.86102295e-06 3.33786011e-06 2.38418579e-06
0:  1.90734863e-06 1.90734863e-06 2.38418579e-06 2.38418579e-06
0:  1.43051147e-06 2.86102295e-06 4.76837158e-06 6.19888306e-06
0:  7.62939453e-06 9.05990601e-06 9.05990601e-06 1.09672546e-05
0:  1.90734863e-05 2.00271606e-05 1.38282776e-05 1.38282776e-05
0:  1.85966492e-05 2.43186951e-05 2.76565552e-05 2.81333923e-05
0:  4.62532043e-05 5.10215796e-05 7.77244568e-05 8.24928284e-05
0:  6.43730164e-05 5.14984094e-05 7.96318054e-05 8.44001770e-05
0:  8.29696655e-05 7.53402710e-05 4.62532043e-05 4.57763635e-05
0:  5.14984094e-05 5.43594360e-05 5.48362732e-05 4.86373865e-05
0:  4.62532043e-05 3.95774841e-05 3.67164612e-05 3.71932983e-05
0:  5.81741333e-05 4.91142273e-05 5.62667847e-05 5.62667847e-05
0:  4.95910645e-05 4.48226929e-05 7.86781311e-05 8.91685486e-05
0:  9.58442688e-05 9.77516174e-05 7.39097595e-05 6.38961792e-05
0:  5.72204590e-05 5.72204590e-05 6.53266907e-05 7.15255737e-05
0:  5.38825989e-05 3.38554382e-05 2.33650208e-05 2.19345093e-05
0:  2.81333923e-05 1.95503235e-05 1.57356262e-05 4.91142273e-05
0:  1.19209290e-04 1.70707703e-04 1.71661377e-04 1.42574310e-04
0:  1.20162964e-04 1.03950500e-04 1.03950500e-04 6.91413879e-05
0:  6.86645508e-05 1.00135803e-04 1.63555145e-04 2.00748444e-04
0:  1.90258026e-04 2.25067139e-04 2.00748444e-04 1.17301941e-04]
0: Prediction values (first 20):
0: [5.814533  6.0426726 6.5516014 7.187534  7.728367  8.120914  8.491507
0:  8.791129  8.901461  8.996378  8.91029   8.563391  8.291984  8.114467
0:  8.257825  8.722524  9.333059  9.834675  8.359051  8.40197  ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.468, max = 2.294, mean = 0.074
0:          sample (first 20): tensor([-0.0119,  0.0075,  0.0506,  0.1044,  0.1503,  0.1835,  0.2149,  0.2403,  0.2496,  0.2577,  0.2504,  0.2210,
0:          0.1980,  0.1830,  0.1951,  0.2345,  0.2862,  0.3287,  0.0410,  0.0380])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.179131  5.2934237 5.5725493 5.8738227 6.00392   5.872051  5.82288
0:  5.723805  5.535676  5.292527  4.8188047 4.1152706 3.494582  3.112292
0:  3.0500634 3.3315816 3.6627445 3.821529  1.0878358 0.946805 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.329607   8.750607   9.444987  10.32522   11.176399  11.902228
0:  12.593554  13.291957  13.751457  14.171723  14.445255  14.4323015
0:  14.506619  14.586035  15.001092  15.739848  16.669428  17.47188
0:  15.453888  15.752053 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.324597   -9.561133   -9.346397   -8.700348   -7.942769   -7.28897
0:   -6.9917593  -6.803399   -7.0470448  -7.282588   -7.624238   -8.336611
0:   -8.995262   -9.668217  -10.023849   -9.797257   -9.012964   -7.9715767
0:   -7.7962685  -7.744752 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.23080254 -0.13150787  0.23575497  0.6587496   1.0535426   1.3607497
0:   1.8798947   2.4015932   2.8366585   3.2080748   3.3036432   3.208462
0:   3.1744034   3.4389951   3.95926     4.6767554   5.209404    5.4206886
0:   3.5879347   3.4534512 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.999382   1.4362416  2.1188598  2.924386   3.777458   4.492178
0:   5.5088363  6.513355   7.576456   8.573471   9.32436    9.9255085
0:  10.470697  11.209782  12.087709  13.178385  14.262444  15.143787
0:  14.12648   14.781622 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8334937 -6.955995  -6.7619348 -6.2245574 -5.5917106 -4.9988303
0:  -4.688152  -4.358848  -4.356149  -4.2835975 -4.252933  -4.585589
0:  -4.902119  -5.29689   -5.4541793 -5.1124463 -4.227614  -3.0897336
0:  -2.572291  -2.4674659]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [44.46738  45.19713  45.83537  46.594776 47.13817  47.33141  47.583664
0:  47.29833  46.317535 44.361736 41.40786  37.953915 34.85742  32.18296
0:  30.513664 29.448174 28.724674 28.257988 31.669796 31.820591]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.953371 27.21793  27.552122 27.902552 28.179129 28.20447  28.46541
0:  28.537643 28.551018 28.313211 27.717403 27.06415  26.616385 26.427874
0:  26.642775 27.0338   27.366108 27.469616 24.927637 24.62024 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.413242 18.528776 18.46093  18.088102 17.575    16.919506 17.029675
0:  17.208097 17.559877 17.81652  17.522305 17.27817  17.013268 17.361298
0:  18.16489  19.26857  20.081581 20.415466 21.017141 21.166288]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.219294   -8.399385   -8.117434   -7.4821553  -6.851894   -6.428739
0:   -6.354236   -6.480716   -7.019982   -7.5678716  -8.232943   -9.215601
0:  -10.100182  -10.902834  -11.36091   -11.291611  -10.802677  -10.074989
0:   -8.9768715  -8.961699 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.566555 27.029825 27.583027 28.014454 28.308367 28.359753 28.680483
0:  28.846691 28.944664 28.83106  28.292244 27.63157  27.132597 26.883038
0:  27.130444 27.651384 28.196669 28.59888  25.419987 25.542137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.441839  9.356326  9.46326   9.695437  9.858805  9.908188  9.899933
0:  9.808551  9.481484  9.216113  8.849713  8.332974  7.9018745 7.538369
0:  7.4400363 7.6185718 7.911965  8.186131  7.382963  7.1741357]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.234903 27.861013 27.218807 26.355276 25.519255 24.520144 24.427666
0:  24.325535 24.399069 24.237812 23.406181 22.773235 22.192524 22.234772
0:  22.875555 23.710936 24.280506 24.26506  24.126326 24.089067]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.8726425 6.2020044 6.795057  7.4011574 7.821049  8.033699  8.258782
0:  8.4074545 8.394085  8.45632   8.348973  8.051744  7.804818  7.6653194
0:  7.7530756 8.12062   8.562234  8.899136  6.412091  6.3681097]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.240177   8.471355   8.814169   9.168802   9.436755   9.637478
0:   9.968933  10.367938  10.732441  11.1315365 11.41691   11.578869
0:  11.886396  12.348905  13.0796385 13.997791  14.922922  15.703413
0:  15.340338  15.905563 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.33969   9.299601  9.382477  9.618207  9.8530445 9.990485  9.862366
0:  9.652357  9.057146  8.550474  8.060912  7.534134  7.081776  6.669238
0:  6.484494  6.5856605 6.93329   7.4672375 6.819197  6.7317095]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8619761  -2.6746879  -2.168141   -1.5395331  -1.0817065  -0.91433096
0:  -1.0718446  -1.3722224  -2.0393996  -2.6475558  -3.3419042  -4.260349
0:  -5.017385   -5.66974    -5.9752736  -5.824061   -5.3546977  -4.7935977
0:  -5.540357   -5.566569  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.099505 11.217126 11.494066 11.888021 12.185752 12.346779 12.274418
0:  12.179256 11.805763 11.556177 11.27129  10.834805 10.452377 10.07391
0:  10.018993 10.336952 10.974804 11.716848 11.098846 11.298967]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.961818  -10.829482  -10.199283   -9.23081    -8.307553   -7.617611
0:   -7.136671   -6.8799324  -6.959054   -7.0058603  -7.1809855  -7.5616236
0:   -7.8019524  -8.005768   -7.9461365  -7.6712694  -7.3794823  -7.1372576
0:   -7.250574   -7.154173 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.861982 18.08581  18.376913 18.576702 18.65198  18.604088 18.809404
0:  18.974937 19.080864 18.992838 18.54644  17.978586 17.546497 17.46493
0:  17.870682 18.634987 19.426428 20.06353  17.958628 18.046621]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.018431  12.302349  12.399687  12.364098  12.35753   12.348052
0:  13.016319  13.657763  14.3805065 14.8712845 14.852711  14.702824
0:  14.591863  14.9535675 15.8178    17.032177  18.293945  19.30272
0:  18.14898   18.193836 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0396805 3.054989  3.334683  3.7310853 3.9786794 3.9909732 4.0053077
0:  3.9841733 3.8953366 3.8027563 3.5740614 3.1101854 2.696313  2.4593716
0:  2.4487302 2.7385674 3.0318449 3.0948417 0.6207781 0.6944113]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3214536  -2.0895467  -1.4638772  -0.6102147   0.13839626  0.5913992
0:   0.7429247   0.68868303  0.27388334 -0.21237707 -0.87819624 -1.8648162
0:  -2.7979894  -3.705298   -4.3725     -4.627506   -4.5932374  -4.4346204
0:  -5.775685   -5.716887  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7457101 2.6501803 2.7461529 2.936757  3.1224074 3.2503166 3.4958198
0:  3.7610383 3.9640691 4.1275554 4.111478  3.9110775 3.8015335 3.890164
0:  4.2949004 4.954601  5.658204  6.156873  5.403561  5.5921326]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.961006 17.118502 17.322641 17.41948  17.420544 17.218424 17.374159
0:  17.431837 17.561928 17.593275 17.29711  16.933384 16.617027 16.541029
0:  16.813147 17.423573 18.12285  18.695808 17.15343  17.300285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.2384834 7.370349  7.665765  7.967193  8.157826  8.166827  8.31246
0:  8.448685  8.449416  8.433941  8.126489  7.6110225 7.089345  6.759622
0:  6.7402925 7.0298195 7.3927493 7.6027446 6.4146657 6.6153307]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -6.288053   -6.5845475  -6.526921   -6.113663   -5.638229   -5.284558
0:   -5.4067216  -5.5511956  -6.143272   -6.6276393  -7.1584573  -8.108324
0:   -9.074141  -10.164473  -10.99902   -11.209782  -10.690573   -9.746086
0:  -10.375265  -10.742416 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.476903 -10.429662 -10.111577  -9.612071  -9.154597  -8.850793
0:   -8.607358  -8.397322  -8.452005  -8.543517  -8.863348  -9.46779
0:   -9.978984 -10.327254 -10.358455 -10.061404  -9.726289  -9.459037
0:  -10.406985 -10.387868]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.598692 26.970499 27.262218 27.395004 27.47135  27.399195 27.92194
0:  28.383926 28.936302 29.317917 29.20385  29.103016 29.082392 29.43052
0:  30.207832 31.188042 32.07493  32.573864 31.294794 31.62588 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.334581 19.262032 19.161911 19.030775 18.92731  18.660854 18.96032
0:  19.12843  19.357576 19.334047 18.823608 18.271042 17.798498 17.71275
0:  18.03231  18.656301 19.28436  19.658318 16.963469 16.988947]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.950617  -6.056549  -5.9314413 -5.6545115 -5.4509363 -5.387538
0:  -5.2026014 -5.0671997 -5.042701  -5.065983  -5.38293   -5.9400864
0:  -6.460305  -6.6556873 -6.4436073 -5.927863  -5.4442005 -5.1828346
0:  -6.162817  -6.2296457]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.032595 14.741058 15.456741 15.950157 16.111048 16.083004 16.034473
0:  16.115473 16.068043 15.962177 15.680979 15.16309  14.784098 14.541496
0:  14.703323 15.244829 16.028406 16.763151 14.434912 14.644615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.674942  1.7975941 2.145471  2.6457982 3.064162  3.340678  3.429573
0:  3.6032364 3.5820265 3.6836622 3.7561407 3.5146878 3.2685015 2.966684
0:  2.9364629 3.4085891 4.3608613 5.4872804 5.6250763 6.079922 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.351023 21.820595 22.263737 22.434467 22.352165 22.097023 21.839094
0:  21.810184 21.728394 21.788816 21.789396 21.55393  21.387032 21.232601
0:  21.316166 21.63067  22.064766 22.46722  19.74757  20.18696 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.193111 16.11191  15.967314 15.632694 15.307846 14.86639  15.087896
0:  15.297436 15.637583 15.794836 15.436182 15.042246 14.614702 14.63607
0:  15.118736 15.922983 16.635809 17.01516  16.31255  16.33062 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.504158 16.679178 17.035313 17.417751 17.764599 18.027235 18.574265
0:  19.183908 19.82714  20.373398 20.59343  20.555424 20.462732 20.434269
0:  20.7228   21.182909 21.668629 21.931473 19.142529 18.985168]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.142296 17.239916 17.441824 17.56974  17.581285 17.422695 17.500265
0:  17.561413 17.632845 17.686989 17.508896 17.186699 16.969042 16.927818
0:  17.218758 17.795292 18.448502 18.912601 17.467287 17.55069 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.966854 11.932146 11.969896 11.997518 12.007284 11.996842 12.2939
0:  12.614521 12.909451 13.090056 12.91804  12.669108 12.504777 12.721321
0:  13.372616 14.275436 15.122456 15.680234 14.791243 14.981292]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.582974  8.758165  9.161806  9.531791  9.685963  9.580094  9.48133
0:  9.357191  9.124179  8.857354  8.349884  7.5593176 6.781353  6.224931
0:  5.9890165 6.1336493 6.3915095 6.5196667 4.4629984 4.3092737]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.15213156  0.37566662  1.0069604   1.8045235   2.4182315   2.6781979
0:   2.6342382   2.3663945   1.6720452   0.82880354 -0.28101683 -1.7378349
0:  -3.0641346  -4.1252794  -4.6879234  -4.5918255  -4.1128683  -3.5937252
0:  -4.470956   -4.501624  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.601086  14.449415  14.351833  14.17951   14.025294  13.726927
0:  13.891243  13.959427  14.108377  14.081812  13.677948  13.287809
0:  13.052328  13.2951765 14.104604  15.266668  16.417324  17.313625
0:  16.744768  16.841087 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3624582  -2.3243504  -1.9276619  -1.3477135  -0.7938447  -0.43036413
0:  -0.19777203 -0.14893627 -0.39477682 -0.76893616 -1.3106971  -2.079421
0:  -2.700697   -3.13161    -3.2856398  -3.107894   -2.8510408  -2.6916885
0:  -5.4770565  -5.751111  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.616204 12.928098 13.252935 13.335508 13.007088 12.370161 11.682144
0:  11.131851 10.557875 10.100936  9.653639  9.002139  8.512651  8.194386
0:   8.237764  8.766331  9.551776 10.36821   8.036955  8.151275]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.43332  24.76755  25.135014 25.433538 25.593157 25.507786 25.838375
0:  26.10982  26.490099 26.853315 26.859306 26.907314 27.01191  27.559563
0:  28.50392  29.639912 30.580256 31.065819 30.460882 30.471037]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.599522 26.562824 26.439802 26.227222 25.860008 25.352478 25.144388
0:  24.942453 24.830015 24.614912 24.085903 23.507189 23.09472  22.938509
0:  23.293297 23.889    24.518917 24.967812 24.805258 24.9236  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3829799 -2.73457   -2.8623366 -2.8521323 -2.874642  -3.0375361
0:  -3.1354947 -3.298016  -3.532165  -3.773871  -4.1389995 -4.635164
0:  -5.0047345 -5.118628  -4.9515977 -4.5139155 -4.121756  -3.9030871
0:  -4.8947673 -4.8898377]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.47801   -4.7017145 -4.639575  -4.3472424 -4.014472  -3.7617521
0:  -3.6346283 -3.4824266 -3.58037   -3.6219025 -3.7131357 -4.069359
0:  -4.3194027 -4.574376  -4.568713  -4.1262712 -3.3321867 -2.3854198
0:  -1.7792768 -1.1467705]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.945874  8.645478  8.525313  8.520384  8.449898  8.185501  7.7268224
0:  7.293462  6.651857  6.1389594 5.5894566 4.855936  4.1544256 3.4195864
0:  2.8894753 2.6793094 2.7016058 2.809708  1.5827141 1.1923394]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5894227 -3.712134  -3.5431523 -3.2649255 -3.044908  -2.9592862
0:  -2.8229623 -2.7237878 -2.777349  -2.8909678 -3.2235055 -3.7452855
0:  -4.199514  -4.424147  -4.354545  -4.055306  -3.7904677 -3.655643
0:  -4.1143317 -3.8261142]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.3625607  7.7845     8.233257   8.629164   8.946127   9.087887
0:   9.747265  10.366393  10.994249  11.484733  11.578562  11.650412
0:  11.775072  12.468479  13.585005  14.964743  16.23075   17.052845
0:  18.592455  19.080784 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.6950097  -0.70767355 -0.43604183  0.01176357  0.3814702   0.5712652
0:   0.5317545   0.46495438  0.11678362 -0.18291092 -0.5782399  -1.2738705
0:  -1.8830795  -2.4752789  -2.7464209  -2.5084114  -1.8818226  -1.1650486
0:  -2.0213728  -1.9461155 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.138798   9.980479   9.90257    9.849254   9.697046   9.406115
0:   9.214132   8.993984   8.664502   8.39786    7.9815526  7.379988
0:   6.831714   6.3535843  6.116231   6.052086   6.010156   5.788874
0:   4.0277205  4.0540795]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.92374563 1.0051279  1.3466244  1.7380605  2.036128   2.1613479
0:  2.3441114  2.435567   2.3983803  2.3805532  2.1742578  1.8182564
0:  1.5783467  1.5577254  1.7967815  2.3206773  2.8151102  3.1463406
0:  1.8718057  2.0069342 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.073854 26.061928 25.852789 25.476402 25.143412 24.662119 24.963776
0:  25.150583 25.5477   25.69989  25.314644 25.085993 25.007534 25.453281
0:  26.416695 27.619152 28.65216  29.20934  29.322098 29.590755]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.366417 34.14389  33.74773  33.223923 32.757515 32.184708 32.39919
0:  32.51767  32.780376 32.764545 32.141808 31.661152 31.36701  31.587013
0:  32.329433 33.270317 33.97227  34.260654 34.205227 34.251945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.701841  -8.592997  -8.243713  -7.754768  -7.3664584 -7.2045016
0:  -7.221372  -7.3162417 -7.6518044 -7.8989635 -8.081203  -8.25456
0:  -8.117458  -7.7559342 -7.180226  -6.489638  -5.921947  -5.6274667
0:  -6.6606584 -6.7797704]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.771422  -4.8239036 -4.537536  -3.9685826 -3.3878608 -2.9259248
0:  -2.7269435 -2.5675378 -2.7205415 -2.874021  -3.150456  -3.756638
0:  -4.365145  -4.9807935 -5.286116  -5.1420064 -4.5836406 -3.8595824
0:  -3.5930438 -3.6113744]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.008158  11.144091  11.502501  11.933556  12.284509  12.461349
0:  12.575939  12.6767435 12.604353  12.551087  12.36956   11.971817
0:  11.677378  11.3808    11.384254  11.64365   12.017054  12.282035
0:  11.0459385 11.216982 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.903553  12.948686  13.032201  13.103651  13.113926  13.014906
0:  13.237019  13.477886  13.7961445 13.99783   13.95018   13.776836
0:  13.718711  13.947914  14.574949  15.4777565 16.416292  17.158306
0:  16.569761  16.75102  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.671541 26.749853 26.78294  26.661963 26.38664  25.862185 25.628416
0:  25.307016 24.999533 24.549583 23.758728 22.931211 22.246542 21.815016
0:  21.839773 22.183926 22.606628 22.932116 21.65021  21.62887 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.0495114 6.0870647 6.1350045 6.097722  5.989753  5.8002043 5.8837113
0:  6.119104  6.411616  6.7151546 6.8504763 6.802809  6.776433  6.9825554
0:  7.4487247 8.172202  8.891094  9.348883  7.728942  8.108104 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.433636 12.237359 12.099182 11.876654 11.502916 10.978876 10.655828
0:  10.327099  9.98864   9.62927   9.12031   8.52901   8.103416  8.029998
0:   8.346697  8.966605  9.660849 10.149711  8.806713  8.804518]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.4970813 5.5363665 5.766403  5.9825964 6.0121455 5.8470144 5.7238207
0:  5.566574  5.365877  5.1349134 4.763029  4.219249  3.7836473 3.5654297
0:  3.7257373 4.279683  5.020797  5.764976  4.518801  4.6012554]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.920193  9.372383  9.927733 10.37174  10.623458 10.666568 10.930739
0:  11.169704 11.450462 11.652208 11.61557  11.455891 11.395781 11.592227
0:  12.148152 12.972408 13.797722 14.364918 12.393714 12.704617]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.280306  12.387505  12.607733  12.900463  13.178713  13.318151
0:  13.70109   13.992401  14.172514  14.0681305 13.545722  12.756807
0:  11.997967  11.463259  11.311563  11.527898  11.931037  12.292322
0:   9.564591   9.426036 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.777323 25.92695  26.028723 26.11149  26.068785 25.821415 25.965572
0:  26.050737 26.169655 26.0619   25.542164 24.958744 24.550343 24.502234
0:  24.921574 25.640184 26.361969 26.79615  26.27269  26.576458]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.68384    8.846758   9.196707   9.612572   9.989249  10.189478
0:  10.650259  11.08743   11.484016  11.756136  11.680151  11.319176
0:  10.976925  10.876259  11.167213  11.8607025 12.675973  13.313297
0:  11.967403  12.013569 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.0621605  9.07912    9.065914   8.82967    8.487168   7.988715
0:   8.108041   8.258906   8.607994   8.842252   8.640408   8.447302
0:   8.211578   8.571079   9.386984  10.532075  11.433555  11.857267
0:  12.211014  12.301547 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.591171 17.654646 17.550915 17.101711 16.556839 15.930138 16.1975
0:  16.567457 17.100012 17.40039  16.9771   16.626348 16.201754 16.458014
0:  17.253191 18.255848 18.85948  18.821903 19.50988  19.59777 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.830793 22.915728 22.93452  22.78066  22.54233  22.176796 22.42775
0:  22.66574  23.012804 23.157263 22.778679 22.400434 22.103424 22.32507
0:  23.050386 24.040243 24.926691 25.387188 24.141731 24.24362 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.229149  15.994219  15.665604  15.298424  14.999012  14.5907955
0:  15.088285  15.481197  16.163416  16.572115  16.372461  16.311205
0:  16.274647  16.915182  18.13951   19.683392  20.997326  21.759193
0:  22.396502  22.641743 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -3.251102   -3.6393933  -3.680859   -3.5096517  -3.3507934  -3.3358426
0:   -3.5108075  -3.824535   -4.5318074  -5.2829843  -6.236818   -7.4765906
0:   -8.577242   -9.57509   -10.233864  -10.48698   -10.501362  -10.399256
0:  -11.084068  -10.9590435]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.6039085  8.087163   8.713197   9.2850685  9.675379   9.920339
0:  10.346775  10.887579  11.3507595 11.778801  11.946073  11.819253
0:  11.779281  11.897483  12.346119  13.102087  13.872797  14.397972
0:  12.03224   12.299777 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.05383825  0.10705042  0.4037943   0.7999172   1.0981455   1.2025924
0:   1.2612901   1.3526568   1.2776465   1.243278    1.0387115   0.53427124
0:   0.08718443 -0.31748724 -0.441422   -0.18888044  0.25599623  0.6548114
0:  -0.25778866 -0.12299347]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.093749   -3.1420631  -2.8124561  -2.151557   -1.4651899  -0.9176779
0:  -0.79701805 -0.7012825  -1.0311007  -1.2880049  -1.6008725  -2.3577576
0:  -3.11055    -4.0346413  -4.6816635  -4.713091   -4.020792   -2.946885
0:  -3.6439486  -3.762578  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3142967 2.3275313 2.661006  3.2714396 3.8082952 4.191905  4.2685013
0:  4.358983  4.141839  3.9423058 3.6168954 2.8938537 2.1723418 1.3967719
0:  0.9760289 1.1349597 1.8703227 2.8098483 1.6204205 1.510469 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.439459   8.4693165  8.623995   8.773508   8.814682   8.624047
0:   8.769651   8.884602   9.124291   9.291127   9.158626   8.872879
0:   8.7175045  8.88203    9.53609   10.55122   11.637951  12.438644
0:   9.346266   9.387839 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.82658625 -0.74011755 -0.30712223  0.44908094  1.2898221   2.014134
0:   2.5034142   2.837659    2.7195783   2.5458922   2.2095118   1.5786424
0:   1.0292034   0.5081506   0.2526641   0.33638048  0.67649174  1.1051602
0:  -0.5179615  -0.56799793]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3135867 -3.2879624 -2.9870286 -2.4839463 -2.0613875 -1.8481774
0:  -1.8362718 -1.8946147 -2.265592  -2.6129222 -3.093244  -3.8681645
0:  -4.5795226 -5.2388396 -5.5608106 -5.3996205 -4.9176307 -4.3294263
0:  -4.3273444 -4.229137 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8192267  -1.8749452  -1.6773047  -1.3555336  -1.0540972  -0.8740969
0:  -0.5919652  -0.32348108 -0.19715357 -0.10237312 -0.2182293  -0.53761005
0:  -0.7486868  -0.75490713 -0.47691774  0.1026063   0.67047787  1.0565357
0:  -0.49212742 -0.50084543]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.494448  -6.5817466 -6.199659  -5.4671035 -4.679482  -4.0527215
0:  -3.6288877 -3.3216643 -3.3794188 -3.4859843 -3.7825084 -4.4302363
0:  -5.0722947 -5.6967998 -6.0464025 -5.9567504 -5.529344  -4.9588304
0:  -6.0133233 -5.9001346]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8787937  -0.65083504 -0.14315844  0.5544076   1.2462492   1.8878655
0:   2.4253988   3.0076315   3.323464    3.6016607   3.7761767   3.6305082
0:   3.58496     3.566323    3.8507483   4.5239677   5.511505    6.48489
0:   5.8051524   6.380776  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.813234 26.851004 26.749035 26.42066  26.070961 25.590807 25.929956
0:  26.242455 26.687872 26.904392 26.467356 26.181845 25.95694  26.386564
0:  27.402575 28.647552 29.584476 29.928429 30.100283 30.136951]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.345923   2.4439573  2.778505   3.2449214  3.5536964  3.6662214
0:  3.5005913  3.4128246  3.1002433  2.9002957  2.627071   1.9890676
0:  1.3413625  0.6506462  0.30027008 0.5533042  1.3767061  2.4284315
0:  1.4419932  1.4872408 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.055291  10.016015  10.172775  10.320387  10.20422    9.844424
0:   9.527323   9.323638   9.130023   8.941958   8.577763   7.9122925
0:   7.312186   6.8958087  6.945479   7.475586   8.279574   8.976138
0:   6.9191694  6.8889184]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.6369796  6.877068   7.4035196  8.167406   8.890608   9.470134
0:   9.75052    9.936385   9.852528   9.856449   9.834151   9.620652
0:   9.440628   9.190399   9.184614   9.512064  10.183802  10.882065
0:   9.88032    9.896234 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.194427 24.1142   23.919992 23.493029 22.963947 22.30034  22.202051
0:  22.160713 22.223    22.327438 22.06678  21.81717  21.669416 21.9189
0:  22.59472  23.525719 24.388308 24.950932 23.647358 23.78113 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.6458845 -7.7090282 -7.3571978 -6.612339  -5.8070555 -5.1570864
0:  -4.712079  -4.4370756 -4.601513  -4.8137865 -5.2691402 -6.082803
0:  -6.866723  -7.5780096 -7.9678144 -7.8571396 -7.417082  -6.821785
0:  -7.2710114 -7.3422003]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.8630214 6.215849  6.7078443 7.192109  7.50904   7.6657043 7.6526995
0:  7.696831  7.5199413 7.353535  7.0561194 6.519947  5.9927826 5.477593
0:  5.270112  5.413078  5.81477   6.237553  4.5566463 4.8151503]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.005871  -7.91405   -7.399231  -6.627988  -5.8685503 -5.323659
0:  -5.011037  -4.837989  -5.03724   -5.2614956 -5.6788826 -6.3765907
0:  -6.954749  -7.4221325 -7.524535  -7.2164736 -6.706806  -6.1382704
0:  -5.0732603 -4.7375164]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3794975  -2.36656    -2.0392294  -1.5479183  -1.1342292  -0.91673946
0:  -0.9336047  -1.0609717  -1.5065651  -1.9045696  -2.4022102  -3.113082
0:  -3.731392   -4.258497   -4.4793043  -4.2490387  -3.6422062  -2.8916788
0:  -3.3517056  -3.2335196 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6736922  -1.6369014  -1.3328424  -0.85898733 -0.42199278 -0.09845924
0:   0.37567854  0.79834557  1.0853453   1.2960124   1.2327986   0.90279007
0:   0.6882844   0.6399598   0.8766217   1.3925524   1.8697324   2.2277155
0:   1.2342563   1.4358368 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.478405 20.73271  20.990858 21.142868 21.137651 20.928919 21.15476
0:  21.322338 21.516504 21.570406 21.176832 20.731064 20.362625 20.441282
0:  20.98569  21.868692 22.713022 23.267603 22.674541 22.943432]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.336966 22.551483 22.796255 22.987305 23.178532 23.196602 23.815495
0:  24.329536 24.905764 25.299955 25.180416 25.128542 25.140726 25.633505
0:  26.537363 27.666542 28.578257 29.073988 28.073149 28.297792]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.370533 20.219933 20.091965 19.962385 19.812363 19.516064 19.718084
0:  19.858974 20.137379 20.261538 19.98571  19.710674 19.517046 19.697992
0:  20.357887 21.267431 22.125954 22.725632 21.679688 21.634398]
0: validation loss for strategy=forecast at epoch 9 : 0.3769436478614807
0: validation loss for velocity_u : 0.14100801944732666
0: validation loss for velocity_v : 0.2054191678762436
0: validation loss for specific_humidity : 0.21284760534763336
0: validation loss for velocity_z : 0.5331913828849792
0: validation loss for temperature : 0.10674110054969788
0: validation loss for total_precip : 1.062454342842102
0: 10 : 18:37:52 :: batch_size = 96, lr = 1.6414931416261842e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 10, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5541, -0.5531, -0.5521, -0.5508, -0.5497, -0.5483, -0.5469, -0.5454, -0.5438, -0.5421, -0.5403, -0.5385,
0:         -0.5367, -0.5347, -0.5328, -0.5307, -0.5285, -0.5264, -0.5203, -0.5205, -0.5205, -0.5203, -0.5200, -0.5197,
0:         -0.5192], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0124, 0.0141, 0.0158, 0.0170, 0.0183, 0.0193, 0.0201, 0.0208, 0.0214, 0.0218, 0.0220, 0.0220, 0.0220, 0.0218,
0:         0.0216, 0.0212, 0.0206, 0.0199, 0.0604, 0.0646, 0.0685, 0.0725, 0.0760, 0.0794, 0.0825], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5585, -0.5584, -0.5583, -0.5581, -0.5577, -0.5574, -0.5571, -0.5566, -0.5563, -0.5560, -0.5556, -0.5553,
0:         -0.5549, -0.5545, -0.5542, -0.5538, -0.5534, -0.5530, -0.5687, -0.5685, -0.5684, -0.5683, -0.5681, -0.5679,
0:         -0.5678], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2917, 0.2895, 0.2895, 0.2872, 0.2850, 0.2850, 0.2828, 0.2805, 0.2805, 0.2783, 0.2783, 0.2761, 0.2761, 0.2761,
0:         0.2761, 0.2738, 0.2738, 0.2738, 0.2025, 0.1980, 0.1936, 0.1891, 0.1847, 0.1802, 0.1757], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.1549, 0.1561, 0.1574, 0.1584, 0.1597, 0.1608, 0.1619, 0.1627, 0.1638, 0.1647, 0.1656, 0.1665, 0.1671, 0.1681,
0:         0.1688, 0.1697, 0.1704, 0.1711, 0.1718, 0.1725, 0.1732, 0.1739, 0.1746, 0.1752, 0.1757], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1660, -0.1637, -0.1637, -0.1637, -0.1637, -0.1637, -0.1637, -0.1637, -0.1637, -0.1823, -0.1800, -0.1800,
0:         -0.1800, -0.1776, -0.1776, -0.1776, -0.1776, -0.1753, -0.1870, -0.1870, -0.1846, -0.1823, -0.1823, -0.1800,
0:         -0.1800], device='cuda:0')
0: [DEBUG] Epoch 10, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2405,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan, -0.2417,     nan,
0:         -0.2417, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2382,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2428,     nan,
0:         -0.2428,     nan,     nan, -0.2428,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2382,     nan,     nan,     nan, -0.2382,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan, -0.2393,
0:             nan,     nan,     nan, -0.2417, -0.2417,     nan,     nan, -0.2417, -0.2417,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2428,     nan, -0.2428, -0.2428,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2428,     nan])
0: [DEBUG] Epoch 10, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1059, 0.1355, 0.1695, 0.1975, 0.2087, 0.2085, 0.2179, 0.2238, 0.2256, 0.2222, 0.2031, 0.1703, 0.1476, 0.1374,
0:         0.1553, 0.1977, 0.2444, 0.2805, 0.1968, 0.2025, 0.2106, 0.2146, 0.2003, 0.1927, 0.1938], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0285,  0.0569,  0.0558,  0.0366,  0.0045, -0.0294, -0.0637, -0.0715, -0.0502, -0.0315, -0.0169, -0.0246,
0:         -0.0591, -0.1204, -0.1818, -0.1983, -0.1743, -0.1211, -0.0686, -0.0190,  0.0029, -0.0044, -0.0346, -0.0827,
0:         -0.1153], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6722, -0.6710, -0.6671, -0.6586, -0.6477, -0.6444, -0.6544, -0.6660, -0.6781, -0.6891, -0.6957, -0.6982,
0:         -0.6952, -0.6878, -0.6875, -0.6854, -0.6929, -0.6936, -0.6800, -0.6734, -0.6625, -0.6439, -0.6346, -0.6386,
0:         -0.6460], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1597,  0.2253,  0.2865,  0.1901,  0.1191,  0.1246,  0.1100,  0.0794,  0.0407,  0.0232,  0.0037,  0.0202,
0:          0.0628,  0.0091, -0.0661,  0.0062,  0.0914,  0.0318,  0.2165,  0.1554,  0.1490,  0.0853,  0.0700,  0.0900,
0:          0.1102], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5835, -0.6197, -0.6268, -0.6134, -0.5925, -0.5867, -0.5933, -0.6059, -0.6121, -0.6101, -0.6028, -0.5951,
0:         -0.5897, -0.5825, -0.5798, -0.5802, -0.5875, -0.5903, -0.5795, -0.5586, -0.5387, -0.5329, -0.5354, -0.5377,
0:         -0.5282], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2040, -0.1968, -0.1998, -0.1928, -0.1875, -0.1913, -0.1853, -0.1854, -0.1849, -0.2141, -0.2040, -0.2026,
0:         -0.1950, -0.1946, -0.1940, -0.1945, -0.1913, -0.1887, -0.2275, -0.2215, -0.2112, -0.2045, -0.1963, -0.1980,
0:         -0.2034], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1883666217327118; velocity_v: 0.2559494078159332; specific_humidity: 0.2716424763202667; velocity_z: 0.8220888376235962; temperature: 0.14661285281181335; total_precip: 1.3675073385238647; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20046375691890717; velocity_v: 0.26584145426750183; specific_humidity: 0.22862304747104645; velocity_z: 0.6514617204666138; temperature: 0.17834743857383728; total_precip: 0.7399880886077881; 
0: epoch: 10 [1/5 (20%)]	Loss: 1.05375 : 0.40901 :: 0.20205 (2.50 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19011183083057404; velocity_v: 0.26082077622413635; specific_humidity: 0.2307082712650299; velocity_z: 0.5637961030006409; temperature: 0.14806735515594482; total_precip: 0.8813830614089966; 
0: epoch: 10 [2/5 (40%)]	Loss: 0.88138 : 0.34525 :: 0.20063 (16.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19911475479602814; velocity_v: 0.2619118094444275; specific_humidity: 0.22166429460048676; velocity_z: 0.607355535030365; temperature: 0.1454383283853531; total_precip: 0.7969387769699097; 
0: epoch: 10 [3/5 (60%)]	Loss: 0.79694 : 0.33795 :: 0.19940 (16.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18207859992980957; velocity_v: 0.2576770782470703; specific_humidity: 0.23037034273147583; velocity_z: 0.5097119212150574; temperature: 0.13670122623443604; total_precip: 0.6417275071144104; 
0: epoch: 10 [4/5 (80%)]	Loss: 0.64173 : 0.29332 :: 0.20268 (16.03 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [8.1062317e-06 1.6689301e-05 3.5285950e-05 6.0558319e-05 5.5313110e-05
0:  5.1021576e-05 4.8637390e-05 4.9114227e-05 4.6253204e-05 2.0027161e-05
0:  1.7166138e-05 2.7179718e-05 3.4809113e-05 3.5762787e-05 2.7656555e-05
0:  1.5258789e-05 5.7220459e-06 1.2397766e-05 9.0599060e-06 0.0000000e+00
0:  9.5367432e-06 1.3828278e-05 1.0013580e-05 5.7220459e-06 2.8610229e-06
0:  1.1444092e-05 1.0967255e-05 5.2452087e-06 5.7220459e-06 7.6293945e-06
0:  1.0013580e-05 6.1988831e-06 1.4305115e-06 1.9073486e-06 2.8610229e-06
0:  3.8146973e-06 4.2915344e-06 4.2915344e-06 3.8146973e-06 4.7683716e-06
0:  8.5830688e-06 2.6226044e-05 3.4332275e-05 3.9100643e-05 1.5735626e-05
0:  2.8610229e-06 2.8610229e-06 5.7220459e-06 1.0013580e-05 1.9550323e-05
0:  1.6689301e-05 8.5830688e-06 2.0503998e-05 3.5285950e-05 5.2452087e-05
0:  9.3936920e-05 1.2302399e-04 5.8174133e-05 2.1934509e-05 0.0000000e+00
0:  6.6757202e-06 9.0599060e-06 5.2452087e-06 2.3841858e-06 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 9.5367432e-07 1.9073486e-06 2.3841858e-06 2.8610229e-06
0:  2.8610229e-06 1.4305115e-06 1.9073486e-06 3.8146973e-06 5.7220459e-06
0:  7.1525574e-06 9.0599060e-06 1.1444092e-05 2.0027161e-05 1.2397766e-05
0:  8.5830688e-06 1.9073486e-06 2.8610229e-06 4.7683716e-06 4.7683716e-06
0:  4.2915344e-06 2.8610229e-06 3.3378601e-06 5.7220459e-06 1.0013580e-05
0:  9.5367432e-06 7.6293945e-06 2.8610229e-06 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 9.5367432e-07 9.5367432e-07 9.5367432e-07
0:  9.5367432e-07 9.5367432e-07 9.5367432e-07 1.1920929e-05 2.0503998e-05
0:  3.4809113e-05 5.4359436e-05 5.6266785e-05 5.8650970e-05 6.2942505e-05
0:  4.6253204e-05 6.6757202e-06 4.3869019e-05 7.4386597e-05 3.6716461e-05
0:  3.1471252e-05 3.8623806e-05 2.9087067e-05 1.9073486e-05 1.0013580e-05
0:  6.6757202e-06 1.4305115e-06 1.9073486e-06 5.2452087e-06 3.3378601e-06
0:  1.2874603e-05 2.9563904e-05 1.8119812e-05 1.2874603e-05 1.0013580e-05
0:  6.1988831e-06 3.3378601e-06 1.5735626e-05 2.0027161e-05 6.1988831e-06
0:  2.3841858e-06 1.9073486e-06 2.8610229e-06 1.0967255e-05 3.6716461e-05
0:  2.4318695e-05 2.8610229e-06 2.3841858e-06 2.3841858e-06 2.8610229e-06
0:  7.6293945e-06 9.0599060e-06 4.7683716e-06 7.6293945e-06 2.7179718e-05
0:  2.4318695e-05 3.8146973e-06 7.1525574e-06 8.1062317e-06 2.2411346e-05
0:  3.8623806e-05 5.7220459e-05 7.8201294e-05 9.9182129e-05 7.4386597e-05
0:  5.1975250e-05 4.8160553e-05 3.1948090e-05 2.0980835e-05 5.2452087e-06
0:  4.2915344e-06 1.0013580e-05 3.8146973e-06 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 4.7683716e-07 1.9073486e-06 5.7220459e-06
0:  5.2452087e-06 2.8610229e-06 4.2915344e-06 4.7683716e-06 2.3841858e-06
0:  1.4305115e-06 1.4305115e-06 2.3841858e-06 5.2452087e-06 1.1444092e-05
0:  9.0599060e-06 5.2452087e-06 4.2915344e-06 3.3378601e-06 9.0599060e-06
0:  9.0599060e-06 4.7683716e-06 5.7220459e-06 5.7220459e-06 2.8610229e-06]
0: Target values (first 200):
0: [1.38282776e-05 8.58306885e-06 6.19888306e-06 6.19888306e-06
0:  5.24520874e-06 4.29153442e-06 3.81469727e-06 5.72204590e-06
0:  7.15255737e-06 1.38282776e-05 2.09808350e-05 3.33786011e-05
0:  2.33650208e-05 2.09808350e-05 2.95639038e-05 3.00407410e-05
0:  3.24249268e-05 4.48226965e-05 5.19752502e-05 5.48362732e-05
0:  1.10149384e-04 1.40666962e-04 1.36852264e-04 8.15391541e-05
0:  2.57492065e-05 1.43051147e-05 8.58306885e-06 5.24520874e-06
0:  2.86102295e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 2.86102295e-06 5.72204590e-06
0:  8.10623169e-06 8.10623169e-06 4.76837158e-06 2.86102295e-06
0:  2.38418579e-06 6.19888306e-06 4.76837158e-06 4.76837158e-07
0:  0.00000000e+00 9.53674316e-07 2.38418579e-06 1.43051147e-06
0:  4.76837158e-07 9.53674316e-07 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 2.38418579e-06 7.62939453e-06 8.58306885e-06
0:  1.09672546e-05 2.86102295e-05 3.62396240e-05 3.67164612e-05
0:  4.48226965e-05 5.14984131e-05 5.57899475e-05 5.24520874e-05
0:  4.29153442e-05 2.19345093e-05 1.04904175e-05 3.33786011e-06
0:  8.10623169e-06 1.38282776e-05 1.95503235e-05 2.62260437e-05
0:  3.62396240e-05 5.48362732e-05 8.63075256e-05 1.29222870e-04
0:  2.13146210e-04 2.59876251e-04 2.54154205e-04 2.30312347e-04
0:  2.23159790e-04 2.77042389e-04 3.77655029e-04 5.01155853e-04
0:  7.07149506e-04 7.91072845e-04 7.30991364e-04 5.64098358e-04
0:  3.85284424e-04 2.33173370e-04 1.45435333e-04 1.02043152e-04
0:  1.09672546e-04 9.91821289e-05 6.43730164e-05 2.81333923e-05
0:  7.62939453e-06 4.52995264e-05 4.95910645e-05 3.48091125e-05
0:  8.10623169e-06 6.67572021e-06 5.24520874e-06 4.76837158e-06
0:  3.81469727e-06 3.33786011e-06 2.86102295e-06 4.29153442e-06
0:  7.62939453e-06 1.38282776e-05 2.05039978e-05 1.66893005e-05
0:  1.52587891e-05 1.85966492e-05 2.38418579e-05 2.67028809e-05
0:  2.90870667e-05 3.14712524e-05 2.95639038e-05 4.14848364e-05
0:  6.29425049e-05 7.15255737e-05 7.00950623e-05 5.91278076e-05
0:  4.19616663e-05 4.19616663e-05 7.96318054e-05 5.67436218e-05
0:  1.14440918e-05 4.76837158e-06 9.53674316e-07 9.53674316e-07
0:  4.76837158e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.90734863e-06 4.29153442e-06
0:  1.00135803e-05 1.28746033e-05 8.10623169e-06 5.24520874e-06
0:  3.33786011e-06 3.33786011e-06 1.43051147e-06 1.43051147e-06
0:  4.29153442e-06 1.04904175e-05 5.72204590e-06 2.38418579e-06
0:  8.58306885e-06 7.62939453e-06 2.86102295e-06 9.53674316e-07
0:  9.53674316e-07 6.67572021e-06 1.38282776e-05 2.14576721e-05
0:  4.10079956e-05 5.91278076e-05 6.81877136e-05 6.72340393e-05
0:  6.38961792e-05 5.43594360e-05 5.05447388e-05 5.43594360e-05
0:  3.38554382e-05 3.33786011e-06 1.90734863e-06 4.76837158e-07
0:  4.76837158e-07 1.43051147e-06 2.38418579e-06 4.29153442e-06
0:  9.53674316e-06 1.90734863e-05 2.81333923e-05 4.19616663e-05
0:  9.87052917e-05 1.33037567e-04 1.59263611e-04 1.81198120e-04
0:  2.23636627e-04 3.12328339e-04 4.27246094e-04 6.06060028e-04]
0: Prediction values (first 20):
0: [26.639503 26.483795 26.100548 25.526718 24.966282 24.268803 24.41914
0:  24.471905 24.641487 24.5806   23.905998 23.343105 22.82639  22.924648
0:  23.535408 24.438354 25.085209 25.196888 24.170147 24.092266]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.289, max = 2.748, mean = 0.866
0:          sample (first 20): tensor([1.5565, 1.5445, 1.5150, 1.4709, 1.4278, 1.3742, 1.3857, 1.3898, 1.4028, 1.3982, 1.3463, 1.3030, 1.2633, 1.2708,
0:         1.3178, 1.3872, 1.4370, 1.4455, 1.4679, 1.5179])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9791558 4.186626  4.572585  5.010124  5.23119   5.250837  5.14217
0:  5.18529   5.0815425 5.070064  4.9261966 4.41691   3.9258373 3.4693918
0:  3.4062822 3.9235823 4.913909  6.031064  4.5688496 4.6693945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.134218  4.4992485 5.0917025 5.73988   6.209052  6.456375  6.54826
0:  6.598466  6.4832115 6.382623  6.207229  5.806119  5.489053  5.320412
0:  5.535035  6.2627544 7.3690367 8.592384  7.7719693 8.172854 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3159184  -2.3105044  -2.0315747  -1.5313292  -0.9761195  -0.5741224
0:  -0.3788514  -0.25826597 -0.4255023  -0.57487345 -0.8028698  -1.3138404
0:  -1.8309374  -2.325992   -2.6070504  -2.476829   -1.9581285  -1.2835908
0:  -2.100059   -2.012689  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.156766 11.820246 12.529026 13.144597 13.636036 13.908167 14.454187
0:  14.865077 15.285585 15.572419 15.573466 15.4791   15.497567 15.832486
0:  16.489096 17.434982 18.31932  19.063213 18.129753 18.741615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.243872  15.034309  14.960005  14.889196  14.716919  14.383738
0:  14.035292  13.696205  13.214397  12.741062  12.135992  11.34267
0:  10.683298  10.2346115 10.177529  10.555401  11.1336155 11.702608
0:  10.301867  10.080568 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.6228614 -7.5752645 -7.158072  -6.439195  -5.678698  -5.033033
0:  -4.6859393 -4.332956  -4.3311687 -4.2397237 -4.2198815 -4.5553107
0:  -4.8533797 -5.264543  -5.467648  -5.195805  -4.4790883 -3.5752187
0:  -4.008693  -3.887116 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.966941  -8.128462  -7.8618774 -7.2142973 -6.52689   -5.9714146
0:  -5.7959375 -5.6553917 -5.8825293 -6.0280743 -6.269927  -6.909837
0:  -7.5574493 -8.266464  -8.688891  -8.49068   -7.7275677 -6.7004805
0:  -6.6528587 -6.7600203]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.891346 29.165169 29.384293 29.46294  29.459919 29.407326 29.918198
0:  30.477818 31.133831 31.629921 31.662743 31.625721 31.81443  32.28032
0:  33.185684 34.20947  35.05024  35.536537 32.982407 33.24474 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1433854  -2.9868007  -2.6491294  -2.1854892  -1.7670469  -1.4544053
0:  -1.2484775  -1.0564728  -1.1152453  -1.1718669  -1.3279624  -1.7037077
0:  -2.0181966  -2.2839427  -2.2898679  -1.9319606  -1.2984939  -0.6002984
0:  -0.7201576  -0.39275503]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.526861  -12.600634  -12.313471  -11.611004  -10.762415  -10.015831
0:   -9.44734    -9.065395   -9.136124   -9.2708     -9.643328  -10.3546505
0:  -10.912874  -11.412195  -11.601431  -11.416664  -11.023876  -10.5415535
0:  -12.068413  -12.214266 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.945078 17.781757 17.835382 18.026917 18.149666 18.097267 18.25694
0:  18.422653 18.53545  18.637165 18.448118 18.066675 17.71489  17.584747
0:  17.883331 18.535141 19.275751 19.863876 17.517797 17.300117]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.204153  15.359804  15.35886   15.05352   14.672842  14.1988535
0:  14.559326  15.028049  15.691923  16.150692  15.994404  15.85133
0:  15.612316  15.961836  16.776686  17.829245  18.543705  18.673805
0:  18.605484  18.668064 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.3897233  -7.2869515  -6.967257   -6.6781316  -6.3836246  -6.137277
0:  -5.4385476  -4.7052264  -3.9730124  -3.3534436  -3.1302752  -3.057756
0:  -3.0255437  -2.5432982  -1.8053184  -0.8448458  -0.23368788 -0.03496075
0:  -1.4149365  -1.3162332 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4198542  -3.2256517  -2.58634    -1.7046003  -0.75971365  0.02449369
0:   0.79291344  1.4312587   1.7773957   2.12189     2.322298    2.3450418
0:   2.5161686   2.7387357   3.158169    3.796188    4.469746    5.0504293
0:   2.1581378   2.5570335 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8340144  -1.9558454  -1.7511144  -1.1909356  -0.546371    0.00785017
0:   0.25787926  0.5521178   0.4781146   0.51933146  0.5119004   0.09628677
0:  -0.32008076 -0.8899541  -1.2358117  -1.0559001  -0.31101513  0.7022271
0:   0.71673393  0.6875458 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.213488  12.161102  12.315235  12.581715  12.753525  12.706753
0:  12.541084  12.339844  11.834775  11.410362  10.860252  10.044179
0:   9.295685   8.499856   7.9808893  7.814475   7.923515   8.096807
0:   5.8168955  5.652646 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4964705  -1.4696565  -1.0672297  -0.38741064  0.18178844  0.49862385
0:   0.40481758  0.27469778 -0.18243742 -0.5384908  -0.969995   -1.8249311
0:  -2.7253203  -3.7132678  -4.3880324  -4.4011483  -3.751577   -2.771276
0:  -3.9243665  -4.2378354 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.9610164 2.6952085 2.5942252 2.662081  2.8555808 3.0073307 3.388222
0:  3.8248863 4.084156  4.3636594 4.403093  4.2439637 4.1696553 4.238755
0:  4.5772467 5.1745887 5.811679  6.1765947 7.993686  8.11847  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.39505386 -0.33508968  0.01178026  0.4961338   0.93760204  1.2266245
0:   1.5145717   1.7371123   1.7050147   1.6241755   1.2940412   0.6594181
0:   0.03492117 -0.4436636  -0.57338524 -0.23632097  0.3586135   1.0152025
0:  -1.308671   -1.5218396 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.697467   9.845935  10.150127  10.422524  10.620356  10.652683
0:  10.907865  11.129335  11.364138  11.578859  11.550066  11.383753
0:  11.255367  11.374737  11.7907505 12.47809   13.088476  13.4835825
0:  11.660164  11.78075  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.02782297 -0.12404442  0.06482458  0.52768373  1.0315809   1.4476013
0:   1.6291099   1.786387    1.6531224   1.5619135   1.402648    0.93884706
0:   0.51456785  0.07644844 -0.06962061  0.2900629   1.1384268   2.1578608
0:   1.5004148   1.6424799 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.499828 27.48522  27.179287 26.57409  25.905743 24.945644 24.787899
0:  24.56033  24.487362 24.160482 23.106003 22.17825  21.242794 20.870674
0:  21.061476 21.534256 21.826822 21.722414 22.045513 22.013731]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6577191  -2.4400268  -1.9240079  -1.1888099  -0.6442642  -0.36459684
0:  -0.51780224 -0.7872186  -1.4569182  -2.225183   -3.2203555  -4.61634
0:  -6.033685   -7.339729   -8.196627   -8.380774   -7.9592795  -7.202172
0:  -7.839541   -8.054878  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.9434395 7.8475533 7.976095  8.2074795 8.277243  8.110481  7.886013
0:  7.712133  7.428444  7.17721   6.766349  6.077573  5.4159164 4.765025
0:  4.4457417 4.5663843 5.017309  5.52066   4.437757  4.4151936]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5744715  -1.977057   -1.90165    -1.2082081  -0.30815077  0.49485445
0:   0.7839532   0.7628555   0.00817442 -0.9648447  -2.1848178  -3.8681674
0:  -5.462624   -6.915817   -7.7330003  -7.4795213  -6.188583   -4.4262443
0:  -2.7909207  -3.3884263 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.37573004 -0.2681265   0.1149497   0.6878524   1.1733184   1.4986753
0:   1.5887308   1.7234683   1.6049514   1.5619674   1.4403057   1.0410509
0:   0.67707205  0.30198717  0.18552065  0.45114708  1.0316968   1.6567149
0:   0.72030544  0.8332367 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.449156 15.576883 15.68083  15.657833 15.525955 15.333464 15.591507
0:  15.890465 16.27531  16.43362  16.247799 15.948273 15.798363 15.968605
0:  16.580006 17.447151 18.240374 18.904598 16.94563  17.26653 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.058769 15.3407   15.739006 16.13852  16.410145 16.534985 16.977179
0:  17.448444 17.864464 18.218029 18.270561 18.104013 18.08216  18.299652
0:  18.917274 19.845139 20.77462  21.47348  19.270187 19.479284]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.372973   6.5055213  6.7335505  6.9742074  7.0903053  7.108204
0:   7.174803   7.3544097  7.479248   7.7005463  7.8179684  7.6701455
0:   7.5247717  7.418445   7.5873194  8.167378   9.080864  10.011301
0:   8.728713   8.99801  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.260123  4.2361145 4.422827  4.805419  5.1123075 5.337256  5.252364
0:  5.3112464 5.109009  5.038188  4.93826   4.457288  3.935299  3.3435695
0:  3.0992525 3.4377108 4.4102297 5.5941744 5.084525  5.305661 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.447437 16.711945 16.708607 16.321156 15.744432 15.039839 15.158972
0:  15.342466 15.649244 15.767012 15.269062 14.770809 14.257914 14.429533
0:  15.085889 16.080338 16.86411  17.143614 17.368357 17.654684]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.537789 11.837608 12.534054 13.366395 14.040785 14.527984 15.061064
0:  15.662186 16.296486 16.962227 17.495892 17.714909 18.00473  18.341007
0:  19.03344  20.121649 21.39655  22.472595 22.171127 22.424366]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.78866  19.13736  19.637897 20.046263 20.329762 20.34106  20.55455
0:  20.689709 20.826118 20.933743 20.82106  20.511124 20.342678 20.330864
0:  20.653797 21.286655 22.082325 22.818077 21.477701 21.66586 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.980938 27.217922 27.401077 27.484592 27.514172 27.393581 27.88961
0:  28.245106 28.711246 28.942047 28.682999 28.44569  28.404953 28.735428
0:  29.521729 30.502548 31.32994  31.7831   31.01427  31.140396]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.229149 17.25579  17.296444 17.213602 17.070177 16.78231  16.918028
0:  17.052006 17.204681 17.292152 16.978168 16.554363 16.12977  16.037601
0:  16.35936  17.011322 17.714096 18.229717 17.586918 17.541172]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8996873 -6.665055  -6.071049  -5.229429  -4.381782  -3.6966462
0:  -3.2509513 -2.880978  -2.8546376 -2.817     -2.8949409 -3.252256
0:  -3.5236611 -3.7882094 -3.7817855 -3.366097  -2.5840793 -1.6977701
0:  -1.9261584 -1.626523 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.5580187 4.8630724 5.3671336 5.9169755 6.251638  6.3719335 6.3351393
0:  6.392985  6.279383  6.2198296 6.0501175 5.531399  5.0570183 4.6048417
0:  4.5079136 4.9238462 5.7349234 6.599956  5.6598797 5.984021 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.772591  10.88576   11.180911  11.446682  11.4850235 11.231746
0:  11.053956  10.907949  10.787682  10.656902  10.342557   9.804808
0:   9.373706   9.091206   9.200782   9.724226  10.416288  11.017312
0:   8.8273535  8.749897 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.444187 28.797464 29.231125 29.57186  29.753605 29.695078 29.80506
0:  29.832663 29.838894 29.687286 29.275406 28.678072 28.254349 27.90995
0:  27.923159 28.144579 28.43821  28.681942 26.257538 26.051952]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1820726 6.1094847 6.3193135 6.6634927 7.0063868 7.1900115 7.4382606
0:  7.5582376 7.506506  7.4076643 7.0782146 6.5222254 5.9825335 5.55408
0:  5.310079  5.329318  5.3753867 5.350985  3.1792939 2.978701 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.873777  15.581669  15.370785  15.140808  14.811876  14.31674
0:  14.002317  13.652048  13.23815   12.732829  11.97871   11.054998
0:  10.241013   9.704014   9.593314   9.865317  10.260072  10.567001
0:   7.9607673  7.7997003]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.07676  19.73212  19.289534 18.613327 17.908995 17.067028 17.056942
0:  17.10459  17.340342 17.432396 16.906828 16.45755  15.940908 16.018652
0:  16.518457 17.232918 17.598152 17.423294 17.133883 17.268108]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.750179 15.83062  15.961464 16.006338 15.911154 15.694794 15.650297
0:  15.642118 15.615236 15.490175 15.178452 14.67487  14.308975 14.104227
0:  14.27294  14.71139  15.272303 15.795477 13.646965 13.609585]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8884392 -4.0622005 -3.9522066 -3.5745878 -3.159834  -2.8435326
0:  -2.6560698 -2.5127535 -2.6631737 -2.817833  -3.0952215 -3.6256104
0:  -4.0467215 -4.3738728 -4.439996  -4.116351  -3.5982609 -3.0924501
0:  -3.8942175 -3.9799461]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.6646042  2.2065382  3.0807319  4.263801   5.56507    6.7871356
0:   8.250544   9.565403  10.552895  11.202451  11.36694   11.023581
0:  10.692993  10.585819  10.822506  11.458027  12.2152    12.795894
0:  11.635547  11.708472 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.209148 21.356056 21.470161 21.466225 21.469097 21.377232 22.126263
0:  22.881947 23.768936 24.371086 24.274906 24.069561 23.85477  24.192848
0:  25.086704 26.248314 27.189285 27.559917 26.477669 26.474163]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.2709737 -6.2196517 -5.7707934 -5.118057  -4.46318   -3.9883304
0:  -3.5320926 -3.2197862 -3.1584382 -3.0813403 -3.1925826 -3.5340276
0:  -3.7511706 -3.8635888 -3.742456  -3.3791032 -3.0268483 -2.7466016
0:  -3.2957797 -2.9514089]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.7715816 6.8642807 7.164253  7.505106  7.7327976 7.7759705 7.974332
0:  8.1719475 8.350347  8.50991   8.459571  8.201135  7.9742746 7.907877
0:  8.080184  8.504545  8.933743  9.170424  7.3853545 7.372354 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.654394   5.112605   4.7870235  4.722677   4.826454   4.9936075
0:   5.3759317  5.7882133  6.157768   6.5086565  6.7482033  6.8556366
0:   7.1977224  7.7678194  8.663437   9.835653  11.003563  11.972691
0:  10.298031  10.525322 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.799265 23.90083  23.782047 23.294758 22.737974 22.01225  22.191906
0:  22.403044 22.76494  22.901255 22.291199 21.721758 21.078104 21.079922
0:  21.667688 22.580736 23.271873 23.380941 22.027527 22.156078]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.7196693   6.916336    6.971853    6.6763773   6.1421323   5.4346447
0:   5.157159    4.987451    4.8390465   4.593233    3.9145684   3.134931
0:   2.2987523   1.987751    2.0014014   2.2955847   2.4122672   2.1996818
0:  -0.04713154 -0.206779  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.607128 14.826903 15.068999 15.121727 15.008268 14.776541 14.996614
0:  15.280539 15.570229 15.800093 15.634327 15.336189 15.006695 15.024972
0:  15.321564 15.828455 16.17285  16.182598 14.679834 14.872067]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.6437464   1.3849869   1.347055    1.5394139   1.7036076   1.7039256
0:   1.4584675   1.2199287   0.695539    0.35122585  0.05815983 -0.45838594
0:  -0.8330488  -1.1966252  -1.2779698  -0.86827326  0.00344181  1.0357485
0:   0.69570684  0.5806718 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.92708254  0.69004965  0.7322073   1.0980616   1.4769993   1.7639632
0:   1.6887407   1.56495     1.0376358   0.47996044 -0.1822896  -1.2432103
0:  -2.235107   -3.169148   -3.6495728  -3.399343   -2.4429412  -1.1811433
0:  -0.98223543 -1.2254643 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.174618 23.010397 22.561054 21.67591  20.693916 19.672747 19.522139
0:  19.580677 19.826366 19.94831  19.443138 19.009155 18.5686   18.807085
0:  19.677544 20.86759  21.847956 22.325012 23.63468  23.70717 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.839611 28.07114  28.26986  28.587376 29.165373 29.73164  30.982725
0:  31.891495 32.470856 32.407227 31.440065 30.353827 29.337849 28.840696
0:  28.908197 29.213718 29.34728  29.047417 25.915415 25.526445]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.343184  9.208656  9.153271  9.067102  8.884577  8.563711  8.476054
0:  8.339043  8.09543   7.785228  7.192401  6.3912554 5.7271047 5.4418917
0:  5.592652  6.0266557 6.416522  6.5895414 4.995563  4.9650054]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.2637825 4.0513587 3.8837502 3.6021984 3.275051  2.8562934 3.0627089
0:  3.3503451 3.818848  4.157853  3.9906106 3.740491  3.386272  3.5518765
0:  4.215888  5.2347937 6.160523  6.698928  7.2773113 7.341017 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.643309 27.882133 28.017525 27.992313 27.821259 27.42676  27.449963
0:  27.382236 27.391413 27.25116  26.707323 26.182842 25.777876 25.75886
0:  26.23838  26.992275 27.739315 28.208748 25.86793  25.910643]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.005468  6.181429  6.551655  6.994667  7.2746706 7.3292046 7.216228
0:  7.1920033 6.978794  6.877584  6.7012043 6.1988125 5.703923  5.140545
0:  4.823729  4.9452124 5.464996  6.044478  4.647956  4.599637 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.601527 12.648569 12.853331 13.088161 13.269566 13.384771 13.679377
0:  14.068655 14.466572 14.811888 14.941538 14.779238 14.640542 14.588665
0:  14.789906 15.239405 15.779518 16.139803 13.568374 13.811834]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.384317 21.558645 21.80538  21.99588  22.064108 22.043089 22.214727
0:  22.440123 22.649641 22.847492 22.853197 22.725574 22.763845 22.997465
0:  23.606716 24.435137 25.272247 25.845455 23.45823  23.483875]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.519585  2.7360177 2.7585146 2.5518348 2.2341933 1.7879329 2.0852575
0:  2.473992  3.084013  3.640698  3.745221  3.6655712 3.3950808 3.678788
0:  4.3242984 5.425952  6.5286155 7.3405547 6.9475403 7.4115615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.244719  15.918434  15.611547  15.22909   14.677235  13.862619
0:  13.255562  12.589182  11.890473  11.125147  10.086945   8.94659
0:   7.9258432  7.4051194  7.426393   7.878734   8.46124    8.817735
0:   6.7887645  6.5693216]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.507343   8.052542   7.4971147  6.7944913  6.0940466  5.3103867
0:   5.3138475  5.309886   5.4540467  5.3979664  4.763555   4.192886
0:   3.6464264  3.9211156  4.85504    6.277153   7.574647   8.468859
0:  11.610671  11.666548 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.1560421  -0.10252953 -0.15296173 -0.05636597 -0.02995634 -0.1870637
0:  -0.33535004 -0.5955548  -1.1131101  -1.6842237  -2.4669328  -3.4485898
0:  -4.2790847  -4.9115367  -5.1535287  -5.080685   -4.8967576  -4.7514634
0:  -6.112577   -6.221212  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.235691 22.176613 21.869957 21.25423  20.661057 19.962196 20.020147
0:  20.043697 20.194027 20.110641 19.483398 19.028322 18.6697   18.88008
0:  19.632505 20.567522 21.306278 21.638779 22.631126 22.901188]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.590853  12.433804  12.346727  12.249053  12.132138  11.86861
0:  11.959387  11.969852  12.016159  11.908457  11.480995  11.0215435
0:  10.676959  10.785049  11.407431  12.398138  13.420858  14.254627
0:  13.627705  13.58259  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8667235 1.7794542 1.966392  2.4634871 2.99018   3.4838057 3.7325675
0:  4.0482626 4.0581675 4.152801  4.166748  3.8066266 3.4171221 2.9082456
0:  2.6753364 2.9652615 3.8472257 4.9905767 4.751551  4.6455207]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.004887 21.92069  21.746084 21.431396 21.145996 20.759293 21.202705
0:  21.646698 22.261454 22.69709  22.510044 22.526108 22.623508 23.422646
0:  24.774376 26.314117 27.488308 28.009975 28.389011 28.792042]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.239857  -14.398916  -14.15638   -13.583836  -13.088242  -12.834606
0:  -12.9358425 -13.285469  -14.068983  -14.827196  -15.701778  -16.760149
0:  -17.569782  -18.251175  -18.47226   -18.271336  -17.853891  -17.294992
0:  -15.694897  -15.657959 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.990093 29.969826 29.747215 29.279747 28.769417 27.968609 27.755035
0:  27.373363 27.096283 26.642365 25.712967 24.880207 24.140541 23.801842
0:  23.902573 24.288872 24.660522 24.816635 24.798763 24.768932]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.735386 24.983093 25.165028 25.249783 25.336147 25.2691   25.865665
0:  26.370354 26.904018 27.106037 26.653885 26.135502 25.541164 25.310707
0:  25.453747 25.857624 26.054058 25.926857 26.088097 26.278767]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.445282 27.387629 27.210855 26.823128 26.3449   25.749094 25.745144
0:  25.747515 25.807646 25.779318 25.34532  24.94052  24.629717 24.767937
0:  25.350311 26.165781 26.847013 27.164522 26.612268 26.759497]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6679084 2.2697873 2.2335062 2.343823  2.4358692 2.4013944 2.399041
0:  2.4065151 2.3445554 2.236269  2.0048494 1.5367732 1.2487283 1.1956625
0:  1.6107283 2.4691827 3.5331888 4.474677  3.0039048 2.5725198]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.289597  14.980738  14.76586   14.587275  14.372782  13.99395
0:  14.0590515 14.090834  14.159409  14.072231  13.514111  12.822462
0:  12.080532  11.731148  11.852243  12.3087435 12.734232  12.886343
0:  10.582396  10.423352 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.138889 13.126537 13.259886 13.415152 13.512079 13.438298 13.5816
0:  13.668333 13.613232 13.478216 13.01343  12.406284 11.914297 11.737099
0:  11.92875  12.406292 12.803799 13.01879   9.321134  8.903326]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.848452  12.729137  12.699329  12.570135  12.168285  11.492194
0:  10.912781  10.395838   9.869063   9.450956   8.942828   8.2674265
0:   7.7382064  7.461761   7.5347624  8.006713   8.601489   9.102223
0:   7.582351   7.5505285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.963122 -12.163758 -11.985816 -11.417359 -10.754745 -10.203589
0:   -9.97915   -9.779891 -10.017968 -10.243426 -10.609906 -11.394841
0:  -12.052661 -12.695786 -13.003032 -12.776608 -12.157678 -11.348939
0:  -11.736562 -12.035001]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.38302   10.154644  10.0719595 10.087134  10.1549015 10.20176
0:  10.851116  11.512865  12.172789  12.544325  12.454023  12.151218
0:  12.054866  12.555925  13.626272  15.117388  16.55483   17.533087
0:  14.273262  14.323797 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.741453  -11.911222  -11.6505165 -11.004749  -10.293015   -9.766291
0:   -9.616802   -9.588345   -9.988695  -10.344137  -10.777349  -11.514074
0:  -12.107485  -12.703737  -13.051964  -12.943922  -12.467209  -11.791641
0:  -12.361765  -12.45509  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.462534  5.4537444 5.734517  6.2044196 6.6008906 6.8378744 6.8703976
0:  6.893996  6.6809993 6.569243  6.4111266 5.9942474 5.6374846 5.265749
0:  5.177644  5.524225  6.272047  7.0752287 5.955888  6.0865   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.182552  7.2448635 7.544452  7.9116845 8.052727  7.876233  7.5703807
0:  7.195946  6.613493  6.054587  5.3510075 4.38508   3.4895773 2.7984228
0:  2.5215063 2.7779498 3.3263316 3.8895879 1.5234184 1.2796874]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.274082 33.384777 33.283936 33.007225 32.744083 32.262276 32.476986
0:  32.461628 32.492004 32.189648 31.343975 30.60471  30.097534 30.142128
0:  30.797394 31.760548 32.686085 33.216557 33.146095 33.26243 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0872316 3.1037588 3.271937  3.5249243 3.6895466 3.7174063 3.682841
0:  3.6116748 3.3007278 3.0046315 2.564608  1.9967079 1.5415382 1.3474374
0:  1.5232024 1.9869556 2.526507  2.9760008 2.7102222 2.863854 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.3174853 -6.286493  -5.8728194 -5.1463094 -4.4426436 -3.9644723
0:  -3.671823  -3.4813905 -3.6634612 -3.8341117 -4.2472177 -4.9737926
0:  -5.6165624 -6.145741  -6.297792  -5.996946  -5.494229  -4.9667516
0:  -5.706075  -5.6926637]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.324615  -5.5585046 -5.395655  -4.878817  -4.284633  -3.8286386
0:  -3.7816281 -3.763595  -4.2093625 -4.5546365 -4.9571977 -5.7627864
0:  -6.517286  -7.3783813 -7.935861  -7.9140983 -7.264419  -6.312181
0:  -6.5630727 -6.5925565]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.82704496 1.0453248  1.5456257  2.241816   2.820006   3.1865308
0:  3.2808573  3.3437712  3.0850744  2.8280735  2.4338536  1.6688924
0:  0.9690275  0.31478262 0.07565928 0.46007442 1.3817625  2.4755778
0:  2.408638   2.6727257 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.389936    5.3592377   5.3807898   5.479265    5.3550086   4.9015245
0:   4.502127    3.9038734   3.2961583   2.6152477   1.7328448   0.87239695
0:   0.05898237 -0.14954662  0.20391226  0.954782    1.9708638   2.8115153
0:   2.7568636   3.5192184 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.843987 19.164246 19.556622 19.791313 19.727589 19.32798  19.253723
0:  19.089561 19.006306 18.987434 18.668114 18.340553 18.15947  18.361563
0:  18.994963 19.922749 20.775608 21.357113 19.207312 19.481024]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6220927  -2.6369443  -2.3224468  -1.6698308  -0.9899435  -0.3998928
0:  -0.1321683   0.1606102   0.10765314  0.08249664 -0.04175425 -0.581038
0:  -1.1793771  -1.8920231  -2.3408198  -2.2573934  -1.575561   -0.6350298
0:  -1.0622683  -1.2036147 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.643264   3.6063077  3.8574243  4.317279   4.668479   4.797563
0:  4.562093   4.4052553  3.9864402  3.7556481  3.518033   2.8494892
0:  2.0819793  1.1764078  0.54636574 0.59477234 1.3941474  2.5622892
0:  1.4520531  1.2608981 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.797523   -2.9956698  -2.8708892  -2.5080638  -2.180985   -1.9675107
0:  -1.8100152  -1.6299701  -1.6244435  -1.5027604  -1.4388433  -1.5732236
0:  -1.6578174  -1.6383834  -1.3780808  -0.7692013   0.00966072  0.7853198
0:   0.4155178   0.5829115 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.183243  7.174297  7.3822455 7.634114  7.7962623 7.802647  7.9216332
0:  7.9930615 7.9661264 7.931603  7.664956  7.2416334 6.8930683 6.7889476
0:  6.973031  7.4256067 7.861494  8.1432085 7.1720486 7.046238 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.665295   -8.83844    -8.634489   -8.100517   -7.537048   -7.1326547
0:   -7.075619   -7.070208   -7.423899   -7.6997876  -8.023085   -8.681404
0:   -9.25699    -9.868559  -10.212004  -10.046675   -9.442413   -8.623121
0:   -8.771921   -8.778524 ]
0: validation loss for strategy=forecast at epoch 10 : 0.3458971083164215
0: validation loss for velocity_u : 0.17210282385349274
0: validation loss for velocity_v : 0.22162027657032013
0: validation loss for specific_humidity : 0.20158733427524567
0: validation loss for velocity_z : 0.5047847628593445
0: validation loss for temperature : 0.10712862014770508
0: validation loss for total_precip : 0.868158757686615
0: 11 : 18:41:53 :: batch_size = 96, lr = 1.601456723537741e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 11, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1424, -1.1492, -1.1568, -1.1588, -1.1533, -1.1562, -1.1800, -1.2157, -1.2435, -1.2520, -1.2472, -1.2371,
0:         -1.2210, -1.2000, -1.1795, -1.1615, -1.1500, -1.1431, -1.0685, -1.0797, -1.1032, -1.1200, -1.1186, -1.1135,
0:         -1.1298], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1303, -0.1383, -0.1174, -0.0801, -0.0533, -0.0552, -0.0778, -0.0997, -0.1132, -0.1206, -0.1219, -0.1235,
0:         -0.1284, -0.1429, -0.1661, -0.1870, -0.2015, -0.2049, -0.0744, -0.0776, -0.0561, -0.0289, -0.0230, -0.0481,
0:         -0.0856], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0929, -0.0031,  0.1293,  0.2342,  0.4983,  0.5698,  0.4300,  0.1812,  0.1855,  0.1895,  0.1844,  0.1670,
0:          0.1805,  0.2299,  0.3227,  0.4303,  0.5157,  0.5676, -0.3177, -0.0665,  0.1513,  0.5312,  0.7512,  0.7249,
0:          0.7004], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3158, 0.3647, 0.4558, 0.5158, 0.5603, 0.7315, 0.9960, 1.0760, 0.7893, 0.4136, 0.2424, 0.2780, 0.4136, 0.5114,
0:         0.4758, 0.4114, 0.3891, 0.4002, 0.2335, 0.3869, 0.5803, 0.6337, 0.4981, 0.5581, 0.8426], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.7483, 1.8384, 1.9248, 1.9430, 1.8770, 1.8305, 1.8779, 1.9270, 1.9120, 1.8894, 1.8856, 1.8910, 1.8902, 1.8635,
0:         1.7754, 1.6543, 1.5220, 1.4037, 1.3066, 1.2186, 1.1463, 1.0809, 1.0640, 1.1035, 1.2041], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.2486,  0.3779,  0.4449,  0.3732,  0.4684,  0.5213,  0.6095,  0.0075, -0.0148,  0.0381,  0.3085,  0.4955,
0:          1.0139,  1.5101,  0.7447,  1.4807,  1.1609,  0.5049,  0.3450,  0.6577,  1.5524,  2.1990,  3.4441,  2.6916,
0:          4.4810], device='cuda:0')
0: [DEBUG] Epoch 11, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,  0.8599,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          2.8010,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          8.6700,     nan,     nan,  5.1183,     nan,     nan, 11.3541,     nan,     nan,     nan,     nan,     nan,
0:          9.1415,     nan,     nan, 14.1135,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  8.4984, 11.2307,     nan,     nan,     nan,  1.6817,  4.7174,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  7.4955,     nan,     nan, 10.1561,     nan, 11.5246,
0:         10.8262,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         10.2266,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          3.5511,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2627,     nan,  0.1216,     nan,
0:         -0.0430,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  3.0632,     nan,     nan,     nan,     nan,     nan,     nan, 28.8883,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.7659,     nan,
0:          7.4367,     nan,     nan,     nan,     nan,  3.7827,  1.8828,     nan,  1.6664,     nan,     nan, 10.8533,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 11, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.3969, -1.4240, -1.4146, -1.3574, -1.2928, -1.2321, -1.2012, -1.1856, -1.2085, -1.2254, -1.2402, -1.2741,
0:         -1.2908, -1.3133, -1.3185, -1.2867, -1.2108, -1.1213, -1.3256, -1.4207, -1.4688, -1.4614, -1.4094, -1.3383,
0:         -1.2720], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3055, -0.2493, -0.2144, -0.1872, -0.1686, -0.1585, -0.1686, -0.1796, -0.1804, -0.1863, -0.1774, -0.1776,
0:         -0.2094, -0.2788, -0.3626, -0.4065, -0.3972, -0.3383, -0.3592, -0.2769, -0.2110, -0.1643, -0.1333, -0.1368,
0:         -0.1453], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 1.3805,  1.1720,  0.9225,  0.7119,  0.5295,  0.3559,  0.2667,  0.1896,  0.1511,  0.1610,  0.1622,  0.1350,
0:          0.1278,  0.0855, -0.0436, -0.0768, -0.1019, -0.0557,  2.0370,  1.8613,  1.6566,  1.3729,  1.1088,  0.8221,
0:          0.5764], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1037,  0.7234,  1.3733,  1.2955,  0.8249,  1.0582,  1.2485,  0.6297,  0.3737,  0.4634,  0.0491, -0.5590,
0:         -0.5740, -0.4585, -0.8974, -0.8962, -0.2504, -0.0885,  1.1387,  1.7096,  1.6723,  1.4991,  0.9231,  0.6433,
0:          0.9517], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4244, 0.4012, 0.3605, 0.3071, 0.2712, 0.2914, 0.3770, 0.5205, 0.7097, 0.9322, 1.1842, 1.4670, 1.7762, 2.0801,
0:         2.3384, 2.4968, 2.5364, 2.4553, 2.2910, 2.1066, 1.9615, 1.8747, 1.8126, 1.7118, 1.5204], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([1.8032, 1.9257, 2.0724, 2.2218, 2.3380, 2.4228, 2.4639, 2.4894, 2.5196, 2.5988, 2.7486, 3.0615, 3.3219, 3.5406,
0:         3.8322, 3.9433, 3.9670, 3.8936, 3.6490, 3.9598, 4.4873, 4.8359, 5.3479, 5.6499, 5.8529], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.17398568987846375; velocity_v: 0.2067531943321228; specific_humidity: 0.25435107946395874; velocity_z: 0.47649627923965454; temperature: 0.154535710811615; total_precip: 0.6032981276512146; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21465745568275452; velocity_v: 0.2964745759963989; specific_humidity: 0.20570330321788788; velocity_z: 0.6745814085006714; temperature: 0.18235889077186584; total_precip: 0.5575016736984253; 
0: epoch: 11 [1/5 (20%)]	Loss: 0.58040 : 0.29852 :: 0.20632 (2.72 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19104371964931488; velocity_v: 0.22354348003864288; specific_humidity: 0.20643241703510284; velocity_z: 0.65537029504776; temperature: 0.15203876793384552; total_precip: 0.7309346795082092; 
0: epoch: 11 [2/5 (40%)]	Loss: 0.73093 : 0.32636 :: 0.20434 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18861858546733856; velocity_v: 0.2605549097061157; specific_humidity: 0.2616041302680969; velocity_z: 0.5046132802963257; temperature: 0.15158875286579132; total_precip: 0.929415762424469; 
0: epoch: 11 [3/5 (60%)]	Loss: 0.92942 : 0.34867 :: 0.20389 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24904009699821472; velocity_v: 0.3351300358772278; specific_humidity: 0.21575069427490234; velocity_z: 0.6583369374275208; temperature: 0.15247181057929993; total_precip: 0.7285266518592834; 
0: epoch: 11 [4/5 (80%)]	Loss: 0.72853 : 0.35469 :: 0.21128 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [2.70366669e-04 1.04904175e-04 1.57833099e-04 1.74522400e-04
0:  1.04904175e-04 6.15119934e-05 6.24656677e-05 1.70707703e-04
0:  2.70843506e-04 2.55584717e-04 1.44481659e-04 1.33037567e-04
0:  1.16825104e-04 1.34944916e-04 1.40666962e-04 5.14984094e-05
0:  4.76837158e-05 5.24520874e-06 9.53674316e-07 1.43051147e-06
0:  2.38418579e-06 5.72204590e-06 2.95639038e-05 8.48770142e-05
0:  2.30789185e-04 2.35080719e-04 8.34465027e-05 3.86238098e-05
0:  3.00407410e-05 3.48091125e-05 4.43458557e-05 1.30176544e-04
0:  2.09808350e-04 1.11103058e-04 1.91211700e-04 3.29971313e-04
0:  3.55720520e-04 4.04357910e-04 4.92095947e-04 5.34534454e-04
0:  4.71591949e-04 4.10556793e-04 6.55651093e-04 6.03675842e-04
0:  5.47885895e-04 3.61442566e-04 1.85966492e-04 1.80244446e-04
0:  1.10626221e-04 4.57763635e-05 3.38554382e-05 2.81333923e-05
0:  3.29017639e-05 6.19888306e-05 7.62939453e-05 8.34465027e-05
0:  8.39233398e-05 6.81877136e-05 4.48226929e-05 2.71797180e-05
0:  2.14576721e-05 3.05175781e-05 4.00543213e-05 7.48634338e-05
0:  1.64031982e-04 3.36170197e-04 5.72681427e-04 6.28948212e-04
0:  5.14030457e-04 4.39643860e-04 3.39031219e-04 2.57968903e-04
0:  1.46865845e-04 1.15394592e-04 1.48296356e-04 1.80244446e-04
0:  2.45094299e-04 3.01361084e-04 2.55584717e-04 1.84059143e-04
0:  1.15871429e-04 1.97410583e-04 1.04427338e-04 3.29017639e-05
0:  2.19345093e-05 1.43051147e-06 1.43051147e-06 1.43051147e-06
0:  0.00000000e+00 4.76837158e-07 1.90734863e-06 5.00679016e-05
0:  8.29696655e-05 5.29289246e-05 3.00407410e-05 5.24520874e-06
0:  3.33786011e-06 1.43051147e-06 2.86102295e-06 2.86102295e-06
0:  7.15255737e-06 1.66893005e-05 2.24113464e-05 2.43186951e-05
0:  3.43322754e-05 3.91006470e-05 6.91413879e-05 2.39372253e-04
0:  6.94274902e-04 5.29766083e-04 5.73635101e-04 5.97476959e-04
0:  4.83512878e-04 4.36306000e-04 1.63555145e-04 4.35352325e-04
0:  3.51905823e-04 2.76088715e-04 1.41620636e-04 1.64508820e-04
0:  1.33037567e-04 5.19752539e-05 3.67164612e-05 2.71797180e-05
0:  4.81605530e-05 5.72204590e-06 4.76837158e-07 9.53674316e-07
0:  2.38418579e-06 4.76837158e-06 1.66893005e-05 7.24792480e-05
0:  1.49726868e-04 2.35080719e-04 5.57899475e-05 4.86373865e-05
0:  3.05175781e-05 1.38282776e-05 1.43051147e-05 3.86238098e-05
0:  8.34465027e-05 1.48773193e-04 3.05652618e-04 4.78744507e-04
0:  3.73363495e-04 3.52859497e-04 3.78131866e-04 4.52041626e-04
0:  4.88758087e-04 4.20570374e-04 3.74317169e-04 1.02186191e-03
0:  9.73701419e-04 5.70774078e-04 3.57151031e-04 1.56879425e-04
0:  7.77244568e-05 6.43730164e-05 7.39097595e-05 9.96589661e-05
0:  9.39369202e-05 8.91685486e-05 1.04904175e-04 1.10626221e-04
0:  8.01086426e-05 6.19888306e-05 4.95910645e-05 2.57492065e-05
0:  2.81333923e-05 4.05311584e-05 3.81469727e-05 6.29425049e-05
0:  1.33514404e-04 2.42710114e-04 5.18798828e-04 6.40392303e-04
0:  4.13417816e-04 4.22477722e-04 2.71797180e-04 1.73091888e-04
0:  1.31607056e-04 8.24928284e-05 6.43730164e-05 7.96318054e-05
0:  9.20295715e-05 1.53541565e-04 1.77860260e-04 5.72204590e-05
0:  7.20024109e-05 4.29153442e-05 2.47955322e-05 1.81198120e-05
0:  9.53674316e-06 3.81469727e-06 3.81469727e-06 2.86102295e-06
0:  9.53674316e-07 3.81469727e-06 1.33514404e-05 1.03950500e-04]
0: Target values (first 200):
0: [9.95635986e-04 1.33132935e-03 1.42097473e-03 1.47247314e-03
0:  1.13010406e-03 5.77926578e-04 1.71661377e-04 9.53674316e-07
0:  1.81198120e-05 8.10623169e-05 2.47001648e-04 3.06129456e-04
0:  5.41687012e-04 7.03811646e-04 5.95092773e-04 6.15119934e-04
0:  5.70297241e-04 4.25338745e-04 3.15666199e-04 1.44958496e-04
0:  5.72204590e-05 4.76837158e-05 4.48226929e-05 2.19345093e-05
0:  1.04904175e-05 7.62939453e-06 1.62124634e-05 2.47955322e-05
0:  4.29153442e-05 9.91821289e-05 1.69754028e-04 2.29835510e-04
0:  2.40325928e-04 2.57492065e-04 2.91824341e-04 5.62667847e-04
0:  8.46862793e-04 8.23020935e-04 4.14848328e-04 1.97410583e-04
0:  1.81198120e-04 1.75476074e-04 2.38418579e-04 4.38690186e-04
0:  4.32968140e-04 6.50405884e-04 6.87599182e-04 6.31332397e-04
0:  4.64439392e-04 1.94549561e-04 9.05990601e-05 5.53131104e-05
0:  4.76837158e-06 3.81469727e-06 2.86102295e-06 3.81469727e-06
0:  1.90734863e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  2.57492065e-05 1.22070312e-04 2.03132629e-04 2.13623047e-04
0:  2.26974487e-04 1.47819519e-04 2.30789185e-04 2.79426575e-04
0:  2.52723694e-04 2.26020813e-04 1.65939331e-04 1.09672546e-04
0:  9.53674316e-05 7.82012939e-05 4.00543213e-05 4.38690186e-05
0:  2.86102295e-05 6.00814819e-05 1.05857849e-04 1.83105469e-04
0:  2.69889832e-04 4.05311584e-04 4.91142273e-04 4.57763672e-04
0:  3.36647034e-04 2.56538391e-04 1.99317932e-04 1.87873840e-04
0:  1.97410583e-04 2.29835510e-04 2.71797180e-04 2.59399414e-04
0:  2.17437744e-04 1.58309937e-04 1.29699707e-04 1.08718872e-04
0:  9.91821289e-05 1.78337097e-04 1.62124634e-04 4.29153442e-05
0:  8.58306885e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.62532043e-04 8.73565674e-04 1.18160248e-03 1.28078461e-03
0:  1.30844116e-03 8.57353210e-04 5.50270081e-04 2.32696533e-04
0:  1.77383423e-04 6.10351562e-05 3.11851501e-04 5.26428223e-04
0:  8.24928284e-04 9.89913824e-04 6.80923462e-04 4.55856323e-04
0:  6.04629517e-04 4.49180603e-04 2.72750854e-04 1.26838684e-04
0:  8.96453857e-05 8.29696655e-05 5.91278076e-05 3.14712524e-05
0:  1.23977661e-05 1.62124634e-05 3.05175781e-05 3.52859497e-05
0:  3.52859497e-05 6.19888306e-05 1.27792358e-04 1.66893005e-04
0:  1.85966492e-04 1.98364258e-04 3.12805176e-04 5.09262085e-04
0:  5.34057559e-04 4.13894653e-04 2.88009644e-04 1.40190125e-04
0:  9.82284546e-05 7.15255737e-05 3.71932983e-05 1.14440918e-04
0:  1.99317932e-04 3.34739685e-04 4.27246094e-04 4.29153442e-04
0:  2.72750854e-04 1.23023987e-04 9.15527344e-05 1.43051147e-05
0:  5.72204590e-06 5.72204590e-06 1.90734863e-06 2.86102295e-06
0:  1.90734863e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.90734863e-06 9.82284546e-05 2.08854675e-04
0:  1.88827515e-04 1.60217285e-04 2.04086304e-04 2.01225281e-04
0:  2.20298767e-04 2.25067139e-04 2.04086304e-04 1.43051147e-04
0:  1.02043152e-04 4.67300451e-05 1.23977661e-05 1.23977661e-05
0:  1.14440918e-05 1.71661377e-05 4.67300451e-05 9.15527344e-05
0:  1.59263611e-04 2.27928162e-04 3.60488892e-04 3.80516052e-04
0:  2.55584717e-04 1.18255615e-04 9.91821289e-05 1.08718872e-04]
0: Prediction values (first 20):
0: [-4.82626   -4.925918  -4.676167  -4.1519885 -3.663156  -3.343267
0:  -3.2214513 -3.1232572 -3.3628726 -3.5845523 -3.9969382 -4.7909427
0:  -5.528992  -6.194699  -6.48467   -6.228521  -5.615997  -4.8992257
0:  -5.8968816 -6.021073 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.653, max = 2.404, mean = -0.227
0:          sample (first 20): tensor([-0.8925, -0.9009, -0.8797, -0.8351, -0.7935, -0.7662, -0.7559, -0.7475, -0.7679, -0.7868, -0.8219, -0.8895,
0:         -0.9523, -1.0090, -1.0336, -1.0118, -0.9597, -0.8987, -0.8339, -0.8904])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.4332395   5.2422347   5.0839763   4.844303    4.5514727   4.153586
0:   4.097638    4.0647125   3.9784808   3.6923122   2.9272757   1.930634
0:   0.8602991   0.24392271  0.14688778  0.5509305   1.0927153   1.5138698
0:  -0.7350178  -1.663136  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.429098 13.568011 13.895577 14.247461 14.43773  14.454351 14.554989
0:  14.605146 14.598677 14.550516 14.2595   13.817318 13.482811 13.426513
0:  13.808117 14.49868  15.19526  15.719538 12.928406 12.926598]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.990779   9.828711   9.915293  10.059614  10.073188   9.859903
0:   9.774971   9.657993   9.536216   9.446941   9.199112   8.760164
0:   8.395048   8.201057   8.332529   8.838049   9.4568615  9.952263
0:   8.339645   8.191131 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0281172  -1.1506486  -0.95874596 -0.43652916  0.07867622  0.4157014
0:   0.31960297  0.20151758 -0.31845474 -0.68416214 -1.0627174  -1.8131037
0:  -2.5620193  -3.4136238  -3.9431553  -3.8223343  -2.9861002  -1.792819
0:  -2.1559238  -2.1820726 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.472881  -8.749334  -8.609726  -7.948582  -7.076637  -6.184027
0:  -5.467092  -4.9081283 -4.8986726 -5.114098  -5.6031756 -6.528332
0:  -7.2415586 -7.884109  -8.132845  -7.9099994 -7.3644996 -6.734056
0:  -8.310551  -8.799288 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.471789  -8.4359045 -7.9720078 -7.1523046 -6.2762175 -5.5557747
0:  -5.0679016 -4.705179  -4.7196836 -4.7475615 -4.900713  -5.38159
0:  -5.7206697 -6.029255  -6.0725794 -5.7078495 -5.116954  -4.4863815
0:  -5.6842175 -5.6159954]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0069518  -2.658752   -1.8229976  -0.6945443   0.45397282  1.5169287
0:   2.6490905   3.657279    4.428271    5.045993    5.3273716   5.2243986
0:   5.0613146   4.944668    4.988924    5.2612004   5.5116763   5.6110334
0:   2.4101949   2.0874805 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.259651  7.1057916 7.228307  7.482498  7.6719112 7.656779  7.82557
0:  7.9354906 7.9488916 7.9195113 7.642091  7.226399  6.909572  6.9412565
0:  7.3276243 8.054567  8.693381  9.075636  7.5873175 7.3706326]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.3415017 7.3314104 7.5413227 7.9023113 8.181864  8.292511  8.499495
0:  8.681162  8.809243  8.817976  8.599917  8.103016  7.711318  7.52288
0:  7.715603  8.283094  9.03026   9.588051  8.5516615 8.347744 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.160776  -8.161392  -7.770451  -7.0666766 -6.2878833 -5.610171
0:  -4.989718  -4.522697  -4.358001  -4.295748  -4.4106364 -4.752585
0:  -4.956015  -5.017629  -4.8369975 -4.3468976 -3.8001733 -3.2837825
0:  -3.9132142 -3.830936 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.75299  21.175173 21.420336 21.356493 21.204432 20.982683 21.489311
0:  22.035576 22.695751 23.203382 23.193954 23.171131 23.151598 23.639755
0:  24.544128 25.686668 26.641115 27.14231  26.511795 26.934109]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.804146  10.676069  10.626034  10.547745  10.33446    9.971622
0:   9.802414   9.627178   9.34914    9.068533   8.557705   7.8841553
0:   7.313523   7.030481   7.0516777  7.4078994  7.7990017  8.025535
0:   7.0634193  7.1581945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9831343  -3.9273024  -3.5505676  -2.9431343  -2.3371658  -1.8315029
0:  -1.3316412  -0.86096716 -0.6026645  -0.41617298 -0.3921008  -0.57176495
0:  -0.6038523  -0.49529982 -0.10954857  0.5449343   1.2175636   1.7384491
0:   0.68421507  0.8985796 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.6349     9.892297  10.303581  10.687424  10.900913  10.951078
0:  11.097836  11.216055  11.270485  11.31378   11.1516285 10.822231
0:  10.593106  10.55143   10.843214  11.409277  11.999444  12.370717
0:  10.603493  10.89514  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.897206  -6.9925036 -6.7734294 -6.299563  -5.8550677 -5.653437
0:  -5.695769  -5.922417  -6.479156  -6.9903283 -7.583999  -8.309261
0:  -8.854416  -9.230324  -9.34651   -9.176035  -8.91223   -8.656605
0:  -9.665466  -9.717703 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5816731 -1.7302003 -1.6334567 -1.4025803 -1.2549067 -1.2885013
0:  -1.3495693 -1.4888396 -1.8100772 -2.0497613 -2.4054122 -2.89776
0:  -3.266006  -3.389584  -3.2337317 -2.835073  -2.4626136 -2.2817607
0:  -2.7775445 -2.8038926]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.955908 30.055042 30.161573 30.22603  30.185192 29.847006 29.79138
0:  29.445162 29.034092 28.28038  27.097836 25.872454 24.905802 24.249252
0:  24.11908  24.335737 24.614237 24.788065 23.965368 24.024904]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.522384  13.653938  13.91174   14.218773  14.448879  14.5913515
0:  14.926973  15.238829  15.526002  15.751162  15.783709  15.633215
0:  15.644661  15.840664  16.362844  17.148146  17.940828  18.517986
0:  17.056976  17.243168 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.18056059 0.3974719  0.95900345 1.7321014  2.3746095  2.7603765
0:  2.9374833  2.9335456  2.6266685  2.353353   1.9913297  1.4321623
0:  1.0741682  0.8927593  1.0525312  1.6221395  2.340397   2.9477305
0:  1.6029859  1.784833  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.931921 25.2268   25.597307 25.872965 25.876675 25.577988 25.364742
0:  25.103199 24.769213 24.355059 23.713676 22.88013  22.185225 21.665928
0:  21.513622 21.65123  21.81015  21.793898 18.198513 17.670933]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.269024 24.15601  24.084858 23.87957  23.578163 23.047558 22.954514
0:  22.753752 22.51818  22.122837 21.254894 20.331848 19.52029  19.266846
0:  19.590584 20.32658  21.067467 21.549694 20.79722  20.893982]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.446916  -7.5751643 -7.451017  -7.1079125 -6.758328  -6.5328183
0:  -6.259047  -6.0333676 -5.995806  -6.0364747 -6.3467164 -6.941524
0:  -7.394797  -7.6532016 -7.579165  -7.189506  -6.8420978 -6.6165833
0:  -8.089804  -8.234272 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7146163 -5.0139976 -4.981341  -4.6798806 -4.4004045 -4.4119515
0:  -4.4188    -4.668259  -5.1620345 -5.655845  -6.296988  -7.057195
0:  -7.568773  -7.8448024 -7.8047023 -7.509584  -7.238256  -7.1143737
0:  -5.9404035 -6.238993 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.8147993 8.04745   8.434804  8.8877325 9.222625  9.392484  9.453379
0:  9.470647  9.268652  9.0931635 8.802351  8.30935   7.8711815 7.457897
0:  7.34451   7.572686  8.09795   8.653038  7.493145  7.6860228]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.4329114   0.53400517  0.9682145   1.5292721   1.9800258   2.216583
0:   2.3964813   2.4448566   2.2612586   2.1087089   1.7680244   1.176559
0:   0.6888666   0.36791897  0.3430643   0.64145136  0.99295855  1.2641873
0:  -0.48490238 -0.34518337]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1284838  -0.9479928  -0.5309024   0.0157938   0.49654913  0.8143225
0:   1.0081081   1.1874328   1.0783634   0.9847007   0.775229    0.33057117
0:   0.03991413 -0.18959475 -0.11205721  0.29264116  0.86576176  1.4172735
0:   1.5869012   1.8258247 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.037163  11.270811  11.788442  12.2853565 12.525637  12.457921
0:  12.415564  12.360313  12.337645  12.288862  12.071568  11.528944
0:  11.119264  10.842227  10.944066  11.491922  12.236598  12.803576
0:  10.127878  10.0521965]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.58348  14.684136 14.961604 15.27048  15.498899 15.670671 16.003365
0:  16.337833 16.52726  16.635359 16.490177 16.166721 15.95334  15.864042
0:  16.126701 16.600292 17.085    17.36914  14.505476 14.760981]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8259206  -1.8634019  -1.6038241  -1.0273223  -0.44120026 -0.02850056
0:   0.05644941  0.04628277 -0.38700247 -0.8083892  -1.3519506  -2.2227626
0:  -3.0461802  -3.886951   -4.4549966  -4.5391994  -4.162166   -3.5615811
0:  -4.5349813  -4.6599364 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.752678 21.868544 21.93074  21.911478 21.73922  21.467386 21.48159
0:  21.559462 21.727798 21.827238 21.718958 21.48453  21.405888 21.534302
0:  22.08567  22.90554  23.855976 24.651012 22.666328 23.003445]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.516998  13.60473   13.86871   14.09066   14.203108  14.151693
0:  14.251699  14.283739  14.260581  14.251505  14.069681  13.695635
0:  13.4267845 13.300393  13.457117  13.923204  14.466057  14.901392
0:  13.273145  13.311176 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5400434 6.7639065 7.142638  7.5779166 7.8309565 7.8914022 7.7345295
0:  7.5761375 7.117518  6.7656565 6.306421  5.642994  5.0705547 4.527547
0:  4.362336  4.5749674 5.0788813 5.6170506 4.9524465 5.229452 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.208128  12.029407  11.981997  11.9832325 11.99057   11.776623
0:  11.855326  11.826761  11.8656845 11.832881  11.518833  11.185457
0:  10.997265  11.163147  11.724991  12.519094  13.332388  13.952146
0:  12.2497225 12.070612 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.026863  -6.9384737 -6.4465494 -5.650093  -4.8680573 -4.2332315
0:  -3.7445097 -3.3566914 -3.294548  -3.2586327 -3.4009275 -3.8999443
0:  -4.2869225 -4.5833454 -4.5548973 -4.029954  -3.2650146 -2.4380665
0:  -3.1811361 -3.03539  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.561823 12.56516  12.647667 12.66972  12.622255 12.396904 12.535217
0:  12.582701 12.671776 12.665669 12.298231 11.879532 11.534531 11.583878
0:  12.065544 12.902922 13.727243 14.339977 13.652603 13.759189]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.7360697  8.016819   8.270355   8.267906   8.189429   8.003904
0:   8.508249   9.067714   9.780228  10.315025  10.3030205 10.200233
0:  10.068216  10.459309  11.249907  12.341861  13.358768  14.032449
0:  13.201197  13.410997 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.20083  38.096695 37.679047 36.982357 36.194595 35.28159  35.347797
0:  35.407463 35.63851  35.54222  34.679253 33.93646  33.35357  33.53849
0:  34.427307 35.506187 36.151802 35.92922  33.744198 33.647743]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.060865 28.158587 27.947649 27.350897 26.736578 26.06173  26.452595
0:  26.923595 27.611563 28.012138 27.668753 27.46312  27.265135 27.776358
0:  28.863146 30.161694 31.086008 31.26775  31.579693 31.90672 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.642085  6.6403084 6.8090544 7.0959797 7.3272552 7.4049697 7.5478244
0:  7.6658273 7.6792216 7.631485  7.413259  7.040718  6.7638507 6.705859
0:  7.0201044 7.6873407 8.505719  9.255962  8.411846  8.325266 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.391551  8.406039  8.657538  8.93286   9.036525  8.8597555 8.700903
0:  8.465276  8.17911   7.908204  7.5142784 6.941231  6.457332  6.089574
0:  6.091226  6.540801  7.2691555 8.03569   7.2648063 7.239655 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.810049  -7.862486  -7.566618  -6.8824778 -6.0864277 -5.349285
0:  -4.9466305 -4.535671  -4.496255  -4.3894415 -4.3310394 -4.6822586
0:  -5.065341  -5.5549517 -5.8053083 -5.484338  -4.5433664 -3.2991714
0:  -3.0431576 -3.125636 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.0760427 1.9477785 1.955787  2.0410461 2.1419415 2.0949578 2.3140256
0:  2.468095  2.6327796 2.720729  2.5620425 2.3196049 2.1482115 2.3649204
0:  2.9698923 3.89383   4.8175817 5.511375  4.20268   4.1899586]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.900294   9.23803    9.748121  10.217324  10.448169  10.460746
0:  10.450169  10.408585  10.178135  10.001629   9.652257   9.1854315
0:   8.823859   8.636698   8.848668   9.318979   9.801485  10.140673
0:   9.228859   9.625231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.003391 -13.083017 -12.710608 -11.939749 -11.18586  -10.708712
0:  -10.494522 -10.571472 -11.108442 -11.640617 -12.330784 -13.221706
0:  -13.880594 -14.317251 -14.356958 -13.918699 -13.357349 -12.783796
0:  -12.406273 -12.170795]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.813048  -8.050081  -7.957848  -7.6179194 -7.1472297 -6.689033
0:  -6.0354524 -5.44546   -5.0221295 -4.632257  -4.45659   -4.404595
0:  -4.225476  -3.777141  -3.0666652 -2.1888576 -1.5148416 -1.0889735
0:  -2.413981  -2.4200826]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.7376218  2.9956887  3.536786   4.1778946  4.708147   5.06999
0:   5.5283675  5.966926   6.3391213  6.7886143  7.0894084  7.2701077
0:   7.4932003  7.895412   8.565575   9.45608   10.285444  10.826195
0:   9.947372  10.3460865]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.3511047  6.55993    6.6911316  6.5425816  6.2935     6.010064
0:   6.34261    6.776031   7.286756   7.598652   7.401553   7.1431274
0:   6.8521395  7.0712094  7.7067046  8.624839   9.43064    9.893102
0:  10.3767395 10.729188 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.6739764 -4.8858495 -4.718085  -4.2243276 -3.6642814 -3.1940856
0:  -2.903479  -2.5644183 -2.538097  -2.4945035 -2.6142125 -3.190123
0:  -3.7587667 -4.397717  -4.7891717 -4.644737  -4.0839257 -3.3321748
0:  -5.137132  -5.441604 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.198446 23.012554 22.75554  22.44231  22.128817 21.609669 21.64963
0:  21.437628 21.238821 20.793533 19.919928 19.2339   18.849302 19.135717
0:  20.176739 21.67931  23.161638 24.285028 24.58955  24.62486 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.434663 -12.59961  -12.28144  -11.486302 -10.569695  -9.797008
0:   -9.457482  -9.257279  -9.54098   -9.811925 -10.195722 -10.949851
0:  -11.643462 -12.351216 -12.790271 -12.70617  -12.120025 -11.261363
0:  -10.797744 -10.856511]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.760937 15.831919 15.905888 15.892086 15.664326 15.222656 14.890795
0:  14.674203 14.497805 14.438877 14.298296 13.97451  13.691963 13.457559
0:  13.437939 13.671608 14.016306 14.208693 11.766693 11.526117]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.768902 17.49391  17.288525 17.135046 16.987871 16.705736 16.920897
0:  17.067072 17.263203 17.300888 16.889048 16.35867  15.807232 15.552271
0:  15.698563 16.117838 16.523052 16.6831   14.727299 14.243623]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.546652 17.583155 17.590307 17.461763 17.18876  16.90595  16.890232
0:  17.0542   17.258024 17.456787 17.502817 17.391693 17.480267 17.741701
0:  18.390654 19.247166 20.099121 20.720291 18.361935 18.52148 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.105475  -8.160938  -7.654628  -6.851988  -6.0397825 -5.4100246
0:  -4.9764767 -4.786792  -4.991354  -5.248801  -5.7196255 -6.4956903
0:  -7.125475  -7.606819  -7.7894287 -7.6187334 -7.378989  -7.2089386
0:  -9.333366  -9.422778 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.428542  -6.7222795 -6.633713  -6.277996  -5.89019   -5.637906
0:  -5.1455336 -4.751583  -4.4973164 -4.3594484 -4.574463  -5.08986
0:  -5.53349   -5.743478  -5.5258694 -5.0818205 -4.666936  -4.439682
0:  -5.853325  -5.7506523]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.748259  9.556821  9.495187  9.495874  9.43275   9.307222  9.144045
0:  9.0870075 8.833149  8.524708  8.071108  7.431811  6.807622  6.300191
0:  6.1036777 6.1714134 6.391507  6.566659  4.391905  4.0334234]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.303593  -10.390741  -10.075523   -9.382841   -8.564924   -7.874351
0:   -7.508593   -7.276668   -7.4859786  -7.67783    -7.9654994  -8.553537
0:   -9.014698   -9.511274   -9.783905   -9.658968   -9.197433   -8.569071
0:   -9.479658   -9.615761 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.91975  15.886583 15.872543 15.84174  15.729471 15.460176 15.448723
0:  15.458557 15.472641 15.437462 15.191401 14.834919 14.601206 14.62376
0:  14.963232 15.543232 16.147324 16.540865 13.980663 13.576959]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.179835  -11.1314535 -10.640403   -9.828027   -9.001742   -8.43304
0:   -8.235233   -8.224745   -8.620476   -8.951318   -9.345031   -9.953179
0:  -10.424193  -10.781001  -10.860063  -10.50184    -9.811408   -9.016471
0:   -9.007077   -8.915521 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.98726463 1.4285069  2.034328   2.7781005  3.5044396  4.0621176
0:  4.5831547  4.9985905  5.107483   5.165291   5.0160823  4.5653687
0:  4.232046   3.9456077  3.9285145  4.1872206  4.5632467  4.9259453
0:  3.8316948  4.086178  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.394806 40.62919  40.596348 40.210114 39.684124 38.85276  38.584835
0:  38.066456 37.493515 36.58069  35.031353 33.551502 32.226124 31.441029
0:  31.228525 31.329037 31.339499 31.100342 30.097416 29.92819 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7653298  -1.6644683  -1.2346606  -0.53031397  0.1913023   0.8091502
0:   1.1739302   1.471694    1.3993106   1.3203597   1.1440444   0.73791695
0:   0.49056578  0.32630157  0.52242565  1.0842347   1.92486     2.764773
0:   2.2022305   2.6942344 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.691378 20.607439 20.664309 20.723885 20.6198   20.340818 20.28117
0:  20.252035 20.28249  20.206108 19.757627 19.154722 18.742468 18.68452
0:  19.274725 20.192232 21.163742 22.028872 18.680948 18.240898]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.463269    5.3102746   5.3905106   5.561743    5.5249634   5.241621
0:   4.824654    4.4808326   3.9957902   3.611339    3.1242805   2.3131337
0:   1.5294476   0.7922554   0.4178896   0.59000444  1.1735625   1.9013424
0:   0.22586012 -0.06669855]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.078885  4.343338  4.754523  5.121185  5.2904406 5.2359843 5.1621814
0:  5.1775937 5.169485  5.173251  5.0958796 4.8641987 4.7701707 4.82477
0:  5.148534  5.7294126 6.3489046 6.761185  4.9915085 5.2415137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.199983  7.8638315 7.6117687 7.351742  7.0218477 6.525859  6.4040093
0:  6.288406  6.239573  6.1286817 5.654929  5.0665913 4.4688787 4.289322
0:  4.6001196 5.260415  5.9292884 6.3343673 4.313519  3.9344926]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.67323446 -0.3829975  -0.18109894 -0.24046564 -0.43904924 -0.7375827
0:  -0.32584715  0.11617279  0.6420884   0.94690466  0.6811352   0.39330053
0:   0.17961216  0.6943159   1.7725306   3.2064788   4.5376587   5.406639
0:   5.854944    6.450046  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.41428375 0.2447362  0.35409594 0.72549105 1.1843791  1.5849109
0:  1.9677496  2.3200078  2.3992367  2.3433886  2.0809126  1.5740337
0:  1.2142901  1.0096235  1.1050982  1.5322795  2.0487857  2.4684882
0:  1.3394351  1.0412054 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.55077  36.47235  36.041122 35.397156 34.757465 33.934658 34.054794
0:  34.11749  34.32151  34.236076 33.502598 32.984337 32.609913 32.903816
0:  33.84739  35.024567 35.9857   36.264023 37.241302 37.43963 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.347652 16.210783 16.25112  16.306843 16.264923 16.063446 16.255909
0:  16.406315 16.659357 16.77959  16.57121  16.248146 16.113644 16.262363
0:  16.87871  17.866093 18.90936  19.838612 18.386744 18.273228]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.414179 16.777134 16.256954 15.880621 15.582336 15.117325 15.210192
0:  15.155432 15.175884 15.013838 14.344972 13.641726 12.967352 12.679062
0:  12.850968 13.427597 14.028481 14.465882 13.096018 12.870483]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.516228  -10.647144  -10.3637085  -9.684965   -8.920748   -8.297623
0:   -7.956906   -7.734734   -7.931805   -8.124113   -8.451328   -9.079567
0:   -9.606604  -10.105778  -10.367075  -10.225655   -9.808966   -9.279169
0:   -9.825445  -10.042295 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.628356  6.2138243 5.902862  5.6212416 5.4209433 5.3140907 5.716115
0:  6.2658815 6.8536534 7.399731  7.625851  7.6472197 7.671492  7.967038
0:  8.506081  9.174846  9.581216  9.606503  6.3766656 6.318013 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.8098545 10.006437  10.269359  10.506813  10.62471   10.559809
0:  10.74346   10.878587  11.008965  11.080498  10.897047  10.623839
0:  10.530062  10.820997  11.572519  12.655681  13.656193  14.329315
0:  12.478016  12.48613  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.866848 25.610353 25.129576 24.498812 23.842054 22.971447 22.913984
0:  22.774483 22.73612  22.469126 21.546099 20.828987 20.13832  20.105022
0:  20.60782  21.334957 21.811121 21.741726 21.846151 21.763636]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.4364195 14.828658  15.338187  15.776665  16.068043  16.159832
0:  16.621708  16.993563  17.286549  17.362148  16.904076  16.262915
0:  15.648686  15.425982  15.666513  16.29224   16.97089   17.537773
0:  15.930794  16.29244  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.7438812  -0.41486788  0.18807745  0.9338498   1.5923777   2.0961394
0:   2.567406    3.0511754   3.2994552   3.5451584   3.6316435   3.4290211
0:   3.309755    3.1941428   3.3312564   3.7721872   4.317406    4.8023944
0:   3.108498    3.4527435 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.333618 -11.13125  -10.479498  -9.607376  -8.888189  -8.485563
0:   -8.338964  -8.479511  -8.953587  -9.376457  -9.896671 -10.642109
0:  -11.233784 -11.640947 -11.723141 -11.364075 -10.791153 -10.07728
0:   -9.496832  -8.765755]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.709444  5.953697  6.417207  6.940011  7.349788  7.5379205 7.718657
0:  7.867219  7.8695893 7.8584223 7.6955547 7.336191  7.055553  6.9358425
0:  7.092487  7.5033717 8.000274  8.381913  6.06866   6.2373476]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.055954 19.599907 19.09333  18.547728 18.063288 17.371675 17.384226
0:  17.286901 17.34822  17.200315 16.49735  15.900864 15.328436 15.242613
0:  15.660543 16.30231  16.797995 16.8685   15.347155 14.925395]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.413245 15.514027 15.73138  15.911533 15.94022  15.755121 15.890303
0:  15.979063 16.057163 16.094728 15.765928 15.334825 14.91702  14.840069
0:  15.144872 15.770299 16.399708 16.785885 15.586992 15.617829]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.157295 16.940174 16.900452 16.928188 16.866257 16.5666   16.534008
0:  16.375275 16.19628  15.916991 15.322024 14.604257 14.001106 13.729055
0:  13.935863 14.564592 15.284224 15.841837 13.415558 13.171454]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.664494  -6.8619227 -6.7160535 -6.1966147 -5.549395  -4.9673944
0:  -4.6713424 -4.4529204 -4.6852336 -4.945703  -5.386806  -6.1941514
0:  -6.9379315 -7.618528  -7.9831977 -7.768124  -7.048941  -6.085914
0:  -5.7228417 -5.585855 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.966534 19.877583 19.986492 20.099922 20.102348 19.91415  19.887917
0:  19.828793 19.717949 19.627403 19.342361 18.88805  18.588215 18.424286
0:  18.583782 19.036945 19.508434 19.759745 18.094833 17.975994]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8581638  -2.6064157  -1.9647212  -1.0469012  -0.17874718  0.50413656
0:   1.004488    1.2873988   1.1574659   0.97100306  0.5574894  -0.10126448
0:  -0.6531563  -1.0688319  -1.1388655  -0.8491788  -0.38710976  0.08001041
0:  -0.6107054  -0.5093951 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.972983   9.997343  10.078623  10.186918  10.250429  10.265363
0:  10.550127  10.818449  11.030813  11.204622  11.203651  11.137804
0:  11.227842  11.615444  12.299614  13.172777  13.936331  14.443661
0:  13.364384  13.5917015]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.1744184  6.2541256  6.4520802  6.6297626  6.793235   6.8547983
0:   7.2324867  7.573374   7.9144926  8.181257   8.146516   8.027051
0:   7.9284697  8.159318   8.744444   9.616471  10.489954  11.110765
0:  10.633249  10.820602 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.42024803 0.47902632 0.78481245 1.2485075  1.6237569  1.8595552
0:  1.9878349  2.1427484  2.0474682  1.9459972  1.6580858  1.0658708
0:  0.53319454 0.12339926 0.10318899 0.52074385 1.1859441  1.8529043
0:  1.0080032  1.1104546 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.100456 24.152206 24.147602 23.990866 23.835442 23.625307 23.944681
0:  24.243841 24.631645 24.920666 24.866951 24.853952 25.017906 25.564543
0:  26.612944 27.905191 29.220387 30.301851 29.501043 29.78385 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.184731  14.668861  14.028965  13.3282585 12.536115  11.772488
0:  11.318163  11.072516  10.959637  10.929039  10.860135  10.793076
0:  10.903631  11.251736  11.899307  12.703907  13.450541  14.022402
0:  13.7491455 13.797607 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.573969 16.575062 16.767317 16.94373  16.851427 16.433851 16.050545
0:  15.687864 15.296963 14.932728 14.385416 13.608963 12.843161 12.168421
0:  11.842091 11.902981 12.156696 12.342989  8.828845  8.661553]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3992686  -1.2525668  -0.80527544 -0.21521235  0.2569027   0.49151516
0:   0.6851959   0.770648    0.6643567   0.512692    0.1680727  -0.38697147
0:  -0.80459976 -1.0497546  -0.9859648  -0.5755315  -0.10261345  0.24540901
0:  -1.1973491  -1.0599685 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.154447  -4.330693  -4.1272264 -3.492128  -2.7934957 -2.2229238
0:  -2.089315  -1.968308  -2.2415204 -2.4237523 -2.6708817 -3.3267007
0:  -4.0587296 -4.875959  -5.391126  -5.268539  -4.4410424 -3.248177
0:  -2.9540696 -3.0620904]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.943792 21.499924 21.829752 21.822138 21.635696 21.319826 21.541536
0:  21.830626 22.265026 22.602703 22.559586 22.497105 22.455606 22.740023
0:  23.350649 24.1321   24.798496 25.151243 24.752089 25.277615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.76556  26.449425 26.356102 26.430603 26.470451 26.479649 26.72324
0:  27.091087 27.498924 27.746773 27.607002 27.388042 27.366116 27.511246
0:  28.112438 28.688885 29.164879 29.450294 27.524849 26.958748]
0: validation loss for strategy=forecast at epoch 11 : 0.30460965633392334
0: validation loss for velocity_u : 0.15366767346858978
0: validation loss for velocity_v : 0.22979490458965302
0: validation loss for specific_humidity : 0.1786975860595703
0: validation loss for velocity_z : 0.5126244425773621
0: validation loss for temperature : 0.10959336161613464
0: validation loss for total_precip : 0.6432802081108093
0: 12 : 18:45:52 :: batch_size = 96, lr = 1.5623968034514547e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 12, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4483, 0.4421, 0.4358, 0.4279, 0.4166, 0.4033, 0.3890, 0.3755, 0.3655, 0.3609, 0.3611, 0.3634, 0.3666, 0.3700,
0:         0.3730, 0.3758, 0.3778, 0.3790, 0.4176, 0.4148, 0.4111, 0.4047, 0.3959, 0.3843, 0.3704], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2458, -0.2572, -0.2655, -0.2749, -0.2832, -0.2936, -0.3056, -0.3198, -0.3369, -0.3477, -0.3534, -0.3559,
0:         -0.3575, -0.3630, -0.3656, -0.3724, -0.3864, -0.4019, -0.2759, -0.2859, -0.2934, -0.3015, -0.3074, -0.3150,
0:         -0.3251], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1009, -0.1208, -0.1410, -0.1632, -0.1861, -0.2070, -0.2247, -0.2341, -0.2385, -0.2379, -0.2449, -0.2450,
0:         -0.2525, -0.2586, -0.2607, -0.2575, -0.2392, -0.2237, -0.2592, -0.2811, -0.2983, -0.3181, -0.3401, -0.3585,
0:         -0.3706], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3092,  0.1999,  0.0787,  0.0549,  0.0409,  0.0192,  0.1382,  0.2010,  0.2345,  0.2702,  0.1750,  0.1188,
0:          0.0701,  0.0441,  0.1696,  0.2886,  0.4336,  0.5947,  0.3211,  0.2313,  0.1188,  0.0517, -0.0111, -0.0338,
0:          0.0420], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.7737, 0.7760, 0.7795, 0.7829, 0.7861, 0.7913, 0.7999, 0.8121, 0.8252, 0.8375, 0.8482, 0.8539, 0.8561, 0.8547,
0:         0.8506, 0.8480, 0.8487, 0.8514, 0.8562, 0.8596, 0.8589, 0.8532, 0.8439, 0.8311, 0.8150], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313,
0:         -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313,
0:         -0.2313], device='cuda:0')
0: [DEBUG] Epoch 12, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2313,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2302,     nan,     nan,     nan,     nan, -0.2144, -0.2313,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2201,     nan,     nan,     nan, -0.2313,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2279,     nan,
0:             nan, -0.2313,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2313,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2313,
0:             nan, -0.2313, -0.2313, -0.2313,     nan, -0.2290,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2201,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2313, -0.2313,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2088,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2313,     nan, -0.1954,     nan,
0:             nan,     nan,     nan,     nan, -0.2313,     nan, -0.2290,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2156,     nan,     nan,     nan, -0.2313,     nan,     nan,     nan, -0.2290,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2313,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 12, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5031, 0.4939, 0.4860, 0.4722, 0.4461, 0.3970, 0.3875, 0.3764, 0.3837, 0.3820, 0.3477, 0.3016, 0.2565, 0.2361,
0:         0.2539, 0.3047, 0.3705, 0.4280, 0.6035, 0.6273, 0.6433, 0.6319, 0.5755, 0.5225, 0.4743], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1948, 0.2137, 0.2073, 0.1887, 0.1647, 0.1477, 0.1303, 0.1345, 0.1593, 0.1826, 0.2017, 0.2039, 0.1744, 0.1202,
0:         0.0602, 0.0290, 0.0368, 0.0842, 0.1586, 0.2061, 0.2288, 0.2283, 0.2086, 0.1729, 0.1553], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6340, -0.6215, -0.6106, -0.5954, -0.5807, -0.5697, -0.5697, -0.5709, -0.5752, -0.5833, -0.5878, -0.5899,
0:         -0.5858, -0.5769, -0.5770, -0.5724, -0.5779, -0.5722, -0.6344, -0.6156, -0.6040, -0.5892, -0.5780, -0.5726,
0:         -0.5675], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5292, 0.6711, 0.7778, 0.6202, 0.5146, 0.6658, 0.7048, 0.6012, 0.5785, 0.5830, 0.5270, 0.5886, 0.6845, 0.5627,
0:         0.4513, 0.5714, 0.6481, 0.5545, 0.6987, 0.6325, 0.6676, 0.5693, 0.4589, 0.5122, 0.5446], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.9253, 0.9338, 0.9284, 0.9109, 0.8909, 0.8776, 0.8763, 0.8788, 0.8845, 0.8883, 0.8847, 0.8773, 0.8624, 0.8429,
0:         0.8197, 0.7919, 0.7663, 0.7501, 0.7468, 0.7559, 0.7699, 0.7828, 0.7907, 0.7923, 0.7896], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1892, -0.1840, -0.1866, -0.1732, -0.1681, -0.1692, -0.1668, -0.1714, -0.1749, -0.1990, -0.1920, -0.1849,
0:         -0.1745, -0.1746, -0.1774, -0.1813, -0.1804, -0.1820, -0.2122, -0.2039, -0.1972, -0.1858, -0.1793, -0.1849,
0:         -0.1899], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.19638867676258087; velocity_v: 0.29649654030799866; specific_humidity: 0.21081052720546722; velocity_z: 0.7074877023696899; temperature: 0.1646580547094345; total_precip: 1.01498281955719; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2064347267150879; velocity_v: 0.3199702203273773; specific_humidity: 0.25076496601104736; velocity_z: 0.6597899198532104; temperature: 0.1978168785572052; total_precip: 1.023036003112793; 
0: epoch: 12 [1/5 (20%)]	Loss: 1.01901 : 0.40225 :: 0.19852 (2.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20780141651630402; velocity_v: 0.2521451413631439; specific_humidity: 0.2487512230873108; velocity_z: 0.4641505479812622; temperature: 0.1580360233783722; total_precip: 0.6925737261772156; 
0: epoch: 12 [2/5 (40%)]	Loss: 0.69257 : 0.30314 :: 0.20154 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23194119334220886; velocity_v: 0.3061107397079468; specific_humidity: 0.1909506767988205; velocity_z: 0.6219004988670349; temperature: 0.1569967120885849; total_precip: 0.49635493755340576; 
0: epoch: 12 [3/5 (60%)]	Loss: 0.49635 : 0.29938 :: 0.20320 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20520032942295074; velocity_v: 0.25705915689468384; specific_humidity: 0.22786331176757812; velocity_z: 0.5597150921821594; temperature: 0.14143787324428558; total_precip: 0.49072954058647156; 
0: epoch: 12 [4/5 (80%)]	Loss: 0.49073 : 0.27964 :: 0.20329 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [3.38554382e-05 2.76565552e-05 1.19209290e-05 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 1.43051147e-06
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.43051147e-06
0:  1.23977661e-05 2.57492065e-05 1.85966492e-05 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  6.67572021e-06 8.58306885e-06 1.43051147e-06 2.38418579e-06
0:  1.90734863e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 7.15255737e-06 5.05447388e-05
0:  2.19345093e-05 1.95503235e-05 1.85966492e-05 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  9.53674316e-07 4.86373901e-05 5.24520874e-05 2.00271606e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 9.53674316e-07 8.58306885e-06 1.47819519e-05
0:  2.67028809e-05 6.19888306e-06 4.76837158e-06 4.29153442e-06
0:  1.43051147e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.43051147e-06 1.43051147e-06 9.53674316e-07
0:  0.00000000e+00 2.86102295e-06 2.86102295e-06 0.00000000e+00
0:  1.09672546e-05 2.62260437e-05 1.85966492e-05 1.28746033e-05
0:  1.28746033e-05 5.53131104e-05 8.10623169e-05 1.11579895e-04
0:  9.87052917e-05 7.48634338e-05 7.96318054e-05 4.86373901e-05
0:  5.96046448e-05 5.14984131e-05 2.76565552e-05 3.57627869e-05
0:  1.76429749e-05 1.19209290e-05 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 1.00135803e-05 4.10079956e-05
0:  5.67436218e-05 1.23977661e-05 5.72204590e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.29153442e-06 1.90734863e-06 4.76837158e-06
0:  1.09672546e-05 9.53674316e-06 2.38418579e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 2.05039978e-05
0:  1.85966492e-05 1.04904175e-05 1.09672546e-05 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.14440918e-05 1.14440918e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 4.76837158e-07 4.76837158e-06
0:  1.04904175e-05 4.29153442e-06 1.90734863e-06 1.90734863e-06
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 4.76837158e-07 4.29153442e-06
0:  1.43051147e-06 0.00000000e+00 0.00000000e+00 2.38418579e-06]
0: Target values (first 200):
0: [1.4305115e-06 1.9073486e-06 1.9073486e-06 2.8610229e-06 4.7683716e-06
0:  9.5367432e-07 2.8610229e-06 1.4305115e-06 4.7683716e-07 4.7683716e-07
0:  2.8610229e-06 7.1525574e-06 1.1920929e-05 1.4781952e-05 6.1988831e-06
0:  1.1920929e-05 1.2874603e-05 1.3828278e-05 1.5735626e-05 1.9073486e-05
0:  1.1920929e-05 4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 5.2452087e-06 1.7166138e-05 1.8119812e-05 1.1920929e-05
0:  4.7683716e-06 4.7683716e-07 3.8146973e-06 2.7656555e-05 3.4332275e-05
0:  3.8146973e-05 4.9591064e-05 4.4822693e-05 3.0040741e-05 9.5367432e-06
0:  8.1062317e-06 9.5367432e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07
0:  3.8146973e-06 5.7220459e-06 2.5272369e-05 2.3841858e-05 1.5735626e-05
0:  5.7220459e-06 3.0040741e-05 5.9127808e-05 3.8623810e-05 5.1975250e-05
0:  5.0544739e-05 1.5735626e-05 9.5367432e-06 5.2452087e-06 3.8146973e-06
0:  2.3841858e-06 4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.6757202e-06
0:  1.9073486e-05 4.5299530e-05 5.1021576e-05 8.6784363e-05 1.0681152e-04
0:  8.1062317e-05 2.0503998e-05 1.4305115e-06 9.5367432e-07 9.5367432e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3378601e-06 2.3841858e-06
0:  2.8610229e-06 5.2452087e-06 4.2915344e-06 3.3378601e-06 1.4305115e-06
0:  9.5367432e-07 1.4305115e-06 1.4305115e-06 1.9073486e-06 5.7220459e-06
0:  8.1062317e-06 6.6757202e-06 9.5367432e-07 9.5367432e-06 1.1920929e-05
0:  4.2915344e-06 4.2915344e-06 5.2452087e-06 9.5367432e-07 4.7683716e-07
0:  4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  7.1525574e-06 1.2397766e-05 1.0013580e-05 2.8610229e-06 0.0000000e+00
0:  0.0000000e+00 1.9073486e-06 5.7220459e-06 5.2452087e-06 1.1444092e-05
0:  1.1444092e-05 4.7683716e-07 4.7683716e-07 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4305115e-06 0.0000000e+00
0:  8.1062317e-06 8.1062317e-06 2.3841858e-06 1.2397766e-05 3.3855438e-05
0:  6.8664551e-05 5.6266785e-05 5.1975250e-05 4.7683716e-05 5.1975250e-05
0:  5.0544739e-05 4.3392181e-05 3.2901764e-05 3.0517578e-05 7.2956085e-05
0:  6.6757202e-05 1.9073486e-05 1.9073486e-06 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.5367432e-07]
0: Prediction values (first 20):
0: [6.695618  6.7403865 6.9550495 7.27122   7.411691  7.3735847 7.1301656
0:  6.999449  6.7069464 6.5297    6.278112  5.6630125 5.001916  4.3553133
0:  4.079131  4.4287014 5.364834  6.52186   5.37434   5.44736  ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.869, max = 0.509, mean = -0.633
0:          sample (first 20): tensor([-0.0744, -0.0705, -0.0516, -0.0237, -0.0114, -0.0147, -0.0362, -0.0477, -0.0734, -0.0890, -0.1112, -0.1654,
0:         -0.2236, -0.2806, -0.3049, -0.2741, -0.1916, -0.0897,  0.0027, -0.0316])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -3.2226071  -3.8844075  -4.130629   -3.9652133  -3.7488217  -3.7409225
0:   -4.1721797  -4.7217565  -5.6854215  -6.5133805  -7.2994313  -8.331026
0:   -9.214014  -10.142302  -10.869205  -11.205584  -11.207045  -10.983595
0:  -13.256195  -14.363905 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.997578 23.208424 23.35455  23.318838 23.17183  22.83122  22.926714
0:  23.080046 23.305645 23.439602 23.27187  23.008574 22.869286 22.984203
0:  23.437958 24.086605 24.708435 24.994825 22.966204 23.190235]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.9555736 -7.9781384 -7.7324243 -7.3793216 -7.0245194 -6.817531
0:  -6.5515294 -6.337297  -6.2408233 -6.0942025 -6.068767  -6.115476
0:  -5.9649158 -5.4968495 -4.697063  -3.6227527 -2.6290298 -1.8121896
0:  -2.4005399 -2.2516122]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.769957 14.883043 15.245575 15.649824 15.869633 15.921783 15.958315
0:  16.036182 16.050728 16.039446 15.891636 15.515585 15.328033 15.263876
0:  15.649736 16.351538 17.14996  17.841171 15.766563 15.933636]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.584747 19.675446 19.595606 19.273754 18.925873 18.503426 18.88071
0:  19.238026 19.759916 20.079885 19.862844 19.69297  19.52657  19.890202
0:  20.737442 21.841213 22.830605 23.376884 24.166885 24.39127 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.672581  12.603786  12.665899  12.692528  12.628462  12.357312
0:  12.360916  12.300224  12.305874  12.231428  11.893809  11.453922
0:  11.119106  11.06061   11.405858  12.108152  12.9367485 13.649944
0:  12.292398  12.309272 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.479292  5.5325427 5.813964  6.1743455 6.3760915 6.3802533 6.4126477
0:  6.512238  6.504285  6.573112  6.51183   6.193849  5.9792757 5.962353
0:  6.297171  7.071986  7.9987884 8.780015  7.41899   7.6936297]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.7643085 -6.98134   -6.79719   -6.1913123 -5.438621  -4.8133426
0:  -4.5626187 -4.4457984 -4.8345056 -5.2009463 -5.67431   -6.506313
0:  -7.2383456 -7.9724164 -8.428368  -8.266369  -7.5358195 -6.511437
0:  -8.329939  -8.696602 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.6934624 7.706314  7.9239078 8.272739  8.484523  8.468072  8.305634
0:  8.10329   7.7189064 7.376049  6.985136  6.3649793 5.783013  5.273462
0:  5.061041  5.2850256 5.8838525 6.597695  4.960087  4.739121 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.4741745 15.66522   15.993946  16.356188  16.597801  16.720804
0:  17.036383  17.395184  17.614435  17.724035  17.549711  17.266083
0:  17.112463  17.209267  17.719944  18.483324  19.192389  19.703804
0:  17.810534  18.087967 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.783258 27.924322 27.705921 27.197748 26.63906  25.993843 26.42615
0:  26.823574 27.467054 27.805096 27.405874 27.177608 26.98723  27.49256
0:  28.637442 30.034191 31.222954 31.672691 33.388733 33.84938 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.2161646 7.0721245 6.9844766 6.7526474 6.391635  5.9404087 5.951505
0:  6.079053  6.2477937 6.3459606 6.004639  5.4896164 4.8557405 4.6446834
0:  4.755027  5.09994   5.2422047 5.041753  3.0799515 3.161309 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.286422 11.644348 12.207891 12.753807 13.181796 13.456294 13.90974
0:  14.323265 14.65601  14.958424 15.085257 14.96559  15.051766 15.306876
0:  15.852863 16.6766   17.475946 18.070496 15.791311 16.237797]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.7006397 -4.1803017 -4.304528  -4.2593746 -4.3848066 -4.7828054
0:  -5.300803  -5.9900055 -6.9236603 -7.6878176 -8.452054  -9.240638
0:  -9.658231  -9.744195  -9.507922  -8.989965  -8.62117   -8.414969
0:  -7.3179913 -6.9208155]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.707354   -8.754471   -8.440577   -7.782043   -7.1189494  -6.671642
0:   -6.604983   -6.65608    -7.14224    -7.5870214  -8.173534   -9.029078
0:   -9.767971  -10.421982  -10.722961  -10.594866  -10.179964   -9.710209
0:   -9.797174   -9.694422 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.62167  38.260525 37.393936 36.347004 35.277523 34.117737 34.072647
0:  34.112106 34.370796 34.245872 33.24433  32.496475 31.759588 31.768045
0:  32.547382 33.391148 33.86625  33.506992 34.49841  34.374985]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.705679  -8.799812  -8.539549  -7.9427943 -7.2785535 -6.78336
0:  -6.5447383 -6.4849057 -6.8410325 -7.1841044 -7.5858016 -8.136999
0:  -8.473644  -8.651638  -8.53857   -8.086874  -7.454125  -6.865598
0:  -6.5282044 -6.420979 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.47463   -6.497762  -6.1898723 -5.6202035 -5.0643754 -4.6340737
0:  -4.2999167 -4.093271  -4.174147  -4.2721543 -4.533069  -5.0167603
0:  -5.382131  -5.6495366 -5.703338  -5.506334  -5.2764697 -5.103645
0:  -5.5066953 -5.3069873]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.18325    -0.82254124 -0.17975473  0.7956786   1.8309412   2.7904124
0:   3.5478444   4.284596    4.6454554   5.004462    5.156644    4.865808
0:   4.4618816   3.8546941   3.3669693   3.2358372   3.4301653   3.7422218
0:   1.1817069   1.2583346 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.389458  10.4527235 10.706195  10.975918  11.077353  10.915241
0:  10.924221  10.886156  10.848319  10.782864  10.479208   9.990442
0:   9.580972   9.378378   9.559031  10.172307  10.949287  11.590771
0:   9.752533   9.777246 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.72422  32.033974 32.08601  31.955883 31.784336 31.382141 31.766861
0:  32.01202  32.258278 32.191746 31.44751  30.887486 30.391842 30.536015
0:  31.195305 32.047638 32.677567 32.697422 31.994316 32.040447]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5629649  -1.3979373  -0.91149235 -0.27103662  0.28861666  0.6412344
0:   0.9741044   1.1293182   1.0205994   0.84809256  0.44366598 -0.12867546
0:  -0.57665396 -0.800437   -0.71083117 -0.33844185  0.03130054  0.27661324
0:  -0.97441196 -0.95257044]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8688111  -3.763393   -3.4176612  -2.8119693  -2.2147164  -1.7543163
0:  -1.4128361  -1.1360312  -1.1764989  -1.2135196  -1.3923564  -1.8541789
0:  -2.205171   -2.5166497  -2.4431758  -1.8140564  -0.8204622   0.2947364
0:   0.66779566  0.9245982 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.362144 23.1138   22.57647  21.704578 20.79679  19.75778  19.709883
0:  19.679409 19.896166 19.853313 19.157152 18.594812 18.035204 18.179165
0:  18.928112 20.003149 20.814787 21.02968  22.330114 22.23589 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.707968  9.479044  9.437535  9.484753  9.448726  9.2549    9.200245
0:   9.113408  9.045836  8.972507  8.794076  8.457949  8.306078  8.449138
0:   8.935287  9.763876 10.660598 11.376034  8.406277  8.284838]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.645763   7.0006633  7.5355473  8.054639   8.4465885  8.628033
0:   8.857321   8.96465    8.918449   8.781143   8.471194   8.052897
0:   7.8906417  8.047715   8.639736   9.620998  10.725113  11.64765
0:  10.7644615 11.022966 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.02321768 -0.21658278 -0.01421833  0.49917364  1.094768    1.4573765
0:   1.6846948   1.5854173   1.0696921   0.5717745  -0.0513978  -0.7890401
0:  -1.3214488  -1.6663394  -1.6984258  -1.443234   -1.0664029  -0.7575989
0:  -1.4882169  -1.4686971 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.185219 26.956562 26.430607 25.635674 24.889149 24.011326 24.21426
0:  24.393642 24.767857 24.857204 24.14867  23.628487 23.052505 23.18977
0:  23.911991 24.851078 25.428482 25.258232 25.528042 25.454845]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [50.82995  50.913563 50.53049  50.104874 49.56726  48.894466 49.145226
0:  49.286034 49.67     49.58363  48.851116 48.1447   47.679184 47.646156
0:  48.23208  48.859814 49.16752  48.935482 48.4987   48.521355]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.85245   6.9609156 7.1230745 7.2346907 7.2159257 7.0198812 6.9594755
0:  6.8709235 6.6843514 6.4596386 5.9745216 5.3804913 4.867097  4.6818132
0:  4.846805  5.2795906 5.7060633 5.931878  4.942882  5.138053 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1256604 6.27063   6.6377516 7.046444  7.357746  7.502454  7.7237997
0:  7.957809  8.138493  8.296942  8.296068  8.123401  7.9471827 7.843233
0:  7.8699636 8.060271  8.276262  8.400968  6.5133753 6.6322346]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1896758 -3.3443646 -3.2916389 -2.9942002 -2.675116  -2.4419742
0:  -2.3075233 -2.1994767 -2.4081197 -2.6039004 -2.9324422 -3.56679
0:  -4.0956407 -4.5976434 -4.8215976 -4.643951  -4.211942  -3.6763158
0:  -4.241812  -4.2232423]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.5343256 7.508358  7.709564  8.054547  8.330742  8.449732  8.545105
0:  8.542511  8.307416  8.020757  7.537802  6.842256  6.1823716 5.723755
0:  5.5798616 5.7137704 5.8953495 5.944165  3.9298823 3.7295198]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.10092   5.219018  5.4981484 5.773875  5.8672433 5.8317347 5.874953
0:  6.070403  6.299361  6.555557  6.6822267 6.5714965 6.4477544 6.439794
0:  6.744314  7.3448133 8.0904045 8.78468   7.5839005 8.218291 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.339648  -10.113282   -9.506472   -8.638953   -7.7536297  -7.069209
0:   -6.2769833  -5.687087   -5.4095     -5.2536488  -5.40034    -5.8104563
0:   -6.0700502  -6.0683136  -5.723034   -5.1152077  -4.5217237  -4.131551
0:   -6.3060656  -6.193904 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.3926158 0.9401474 1.8055363 2.7111692 3.4024673 3.7963922 4.158372
0:  4.4239316 4.544533  4.5533667 4.3613324 3.9438248 3.6658585 3.669326
0:  4.0827312 4.891658  5.7935443 6.599657  4.0622625 4.3327675]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.4761963  -0.56358576 -0.34602833  0.16658926  0.7399759   1.1715899
0:   1.3873434   1.4809089   1.1839995   0.99705267  0.7120123   0.23931456
0:  -0.12318468 -0.48665142 -0.5095196  -0.16172552  0.5176697   1.3126869
0:   1.5109067   1.5267744 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.638832  14.520112  14.322697  14.017471  13.674929  13.179745
0:  13.393509  13.547709  13.817701  13.9298115 13.474081  13.032978
0:  12.631203  12.869536  13.668224  14.789828  15.76001   16.266058
0:  15.649693  15.616575 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.383291   6.7394924  7.2374463  7.781691   8.2266245  8.539048
0:   8.921986   9.34371    9.658861  10.022114  10.324083  10.394306
0:  10.646768  10.961046  11.505333  12.344999  13.251655  14.040434
0:  12.380983  12.661694 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.3775263 3.4452822 3.6425657 3.8653412 4.0433207 4.0939684 4.4516625
0:  4.793669  5.0056815 5.135058  4.829898  4.3411884 3.8401513 3.6818478
0:  3.8640313 4.3093452 4.666574  4.7156754 2.571255  2.6375203]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.0776014  7.648029   8.516045   9.572025  10.586342  11.306931
0:  12.142831  12.884331  13.647511  14.356539  14.966099  15.3439865
0:  15.818383  16.489841  17.454802  18.646763  19.905422  20.857101
0:  19.285122  19.862982 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.36315   8.598885  8.999462  9.476316  9.773589  9.846594  9.833254
0:  9.810581  9.623547  9.436316  9.101219  8.496515  7.9101424 7.3301053
0:  7.0302744 7.1020427 7.46294   7.8688793 6.688981  6.5625477]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.2392845 -5.0292034 -4.545388  -3.8212194 -3.1288166 -2.655426
0:  -2.465767  -2.4141874 -2.8458767 -3.2667694 -3.8330793 -4.6346455
0:  -5.225153  -5.695793  -5.817333  -5.5016947 -4.9247284 -4.335642
0:  -6.5277147 -6.856801 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.546772 16.689163 16.878956 16.919876 16.834356 16.485895 16.442616
0:  16.257372 16.122627 15.895521 15.395292 14.809803 14.383684 14.306646
0:  14.640157 15.323534 16.025702 16.538086 14.481278 14.395266]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.120218 17.957878 17.624237 17.09264  16.579086 16.026793 16.328785
0:  16.637424 17.113352 17.38688  17.11573  16.91545  16.742    17.208412
0:  18.194342 19.471798 20.559422 21.129051 22.35068  22.424946]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.210023  2.3203673 2.671225  3.1454818 3.4718244 3.597126  3.6898232
0:  3.7427027 3.5709589 3.4168973 3.088039  2.4367406 1.8664756 1.43787
0:  1.343945  1.7068372 2.272025  2.8254302 1.1942191 1.3265748]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.433434 17.640991 17.994425 18.40343  18.725008 18.873898 19.271708
0:  19.603561 19.875763 19.933514 19.655182 19.189165 18.857904 18.776434
0:  19.197372 19.972628 20.885103 21.635763 18.35326  18.269674]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [48.18335  48.379715 48.24163  47.923782 47.534195 46.873775 47.11071
0:  47.26589  47.59716  47.740555 47.28366  46.936993 46.653496 46.78823
0:  47.283974 47.792526 47.95812  47.515774 45.82377  46.12303 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.9211082 7.135337  7.2047725 7.1760526 6.9791064 6.678396  6.6527734
0:  6.708773  6.6564755 6.5848207 6.226406  5.646771  5.181033  4.991464
0:  5.172224  5.655053  6.1157207 6.362907  4.337295  4.1688766]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2615352  -1.0994644  -0.5950899   0.25639343  1.1807418   1.9396777
0:   2.426219    2.800497    2.7869883   2.8696172   2.9040709   2.728016
0:   2.6145496   2.416861    2.4580278   2.8192313   3.4752386   4.175014
0:   3.8419082   3.9318233 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.375614 21.905674 22.503544 22.873081 22.921614 22.573376 22.353165
0:  21.890491 21.294043 20.43507  19.05146  17.33642  15.649212 14.313915
0:  13.679962 13.738689 14.284827 14.89646  12.881248 13.129919]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4523087  1.4995813  1.7388563  2.0924497  2.3090672  2.3659506
0:  2.453714   2.5354981  2.388682   2.224743   1.7981844  1.113133
0:  0.57816267 0.23049355 0.32454824 0.8400006  1.4631796  2.0146255
0:  1.4672976  1.608037  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.9722805 14.16267   14.588703  15.04615   15.314398  15.350821
0:  15.396353  15.485945  15.5036335 15.549929  15.41642   14.991243
0:  14.602175  14.27289   14.319464  14.73299   15.299049  15.704218
0:  13.92244   14.123053 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.0063577 -6.192369  -6.013593  -5.448071  -4.8002295 -4.2605815
0:  -4.12429   -4.0335593 -4.402968  -4.7101035 -5.0758243 -5.8349595
0:  -6.527908  -7.2878575 -7.7899957 -7.747703  -7.1561704 -6.247433
0:  -6.3218827 -6.5038896]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.487621  8.485058  8.638966  8.757588  8.727405  8.485582  8.461882
0:  8.395936  8.309644  8.235819  7.9032807 7.3886104 6.922619  6.760791
0:  6.9308023 7.483076  8.079526  8.4714    7.5347576 7.749845 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3365784 4.485013  4.6828957 4.7273755 4.6348186 4.4200835 4.7500234
0:  5.1649346 5.6196604 5.980097  5.8245564 5.5178638 5.1060963 5.2111444
0:  5.712548  6.539599  7.1554327 7.350151  7.123398  7.288005 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3945894  -1.7090664  -1.740324   -1.415308   -1.0393047  -0.75781155
0:  -0.75216055 -0.7542486  -1.1411891  -1.4816475  -1.9410896  -2.7472706
0:  -3.5136652  -4.2478046  -4.6358156  -4.426759   -3.6827202  -2.7225952
0:  -2.8289156  -2.7280054 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.19372463 0.24107695 0.5261774  0.96707153 1.3715906  1.6410952
0:  2.016357   2.3823862  2.649334   2.8984718  2.9532766  2.818026
0:  2.7981591  2.9535975  3.3927636  4.1259875  4.875556   5.4719768
0:  5.473771   5.690796  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.8761563 -5.831076  -5.4237676 -4.700667  -4.0021853 -3.519321
0:  -3.2645898 -3.1096349 -3.343453  -3.5271573 -3.863153  -4.521814
0:  -5.046249  -5.5405927 -5.7324214 -5.495971  -5.030557  -4.4657874
0:  -5.327312  -5.2513204]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.30637  27.8059   28.378359 28.7212   28.712698 28.467587 28.14262
0:  27.92036  27.628847 27.351067 26.94471  26.29951  25.83791  25.553682
0:  25.70748  26.147648 26.620209 26.88669  23.561945 23.821497]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1745114 6.0970025 6.1539507 6.272663  6.3016944 6.1720586 6.362997
0:  6.5657396 6.814968  7.0598383 7.021757  6.8489757 6.660904  6.8207855
0:  7.2984676 8.064693  8.771806  9.154992  7.573971  7.658923 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.076859 17.945995 17.644238 17.2233   16.659313 15.888138 15.566487
0:  15.175649 14.803339 14.267527 13.325489 12.286295 11.29237  10.850278
0:  10.87611  11.361668 11.990982 12.430768 12.159281 12.174982]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.177204 26.317823 26.437145 26.581484 26.73374  26.751955 27.298578
0:  27.649708 28.00359  28.081438 27.553843 27.114887 26.83234  27.026018
0:  27.762297 28.680748 29.472527 29.984459 29.783783 29.76997 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.5542974 5.447772  5.620144  5.9563465 6.2620296 6.4330206 6.577846
0:  6.6540728 6.5971875 6.5070467 6.3529553 6.0545006 5.9563074 6.1045175
0:  6.571573  7.3794236 8.275642  9.00592   8.176304  8.464792 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.9776244  5.6186533  6.223657   6.679614   6.9813137  7.005208
0:   7.2232494  7.2616477  7.2802067  7.098782   6.5752683  5.991447
0:   5.51766    5.626538   6.3299303  7.6311927  9.227808  10.843924
0:  13.060652  14.745783 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.4640384 -7.48694   -7.1238685 -6.382916  -5.5417953 -4.835507
0:  -4.4936595 -4.246142  -4.477077  -4.709247  -5.0595164 -5.788339
0:  -6.42895   -7.111131  -7.5174365 -7.4111304 -6.8182836 -5.9744
0:  -6.7709985 -6.832912 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.85567  21.938782 21.663162 20.978973 19.978601 18.863544 18.36468
0:  18.300753 18.657328 19.395731 20.031187 20.63182  21.23619  22.119005
0:  23.29479  24.679443 26.029488 27.002817 25.577137 26.24022 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.496393 31.183086 30.703722 30.025486 29.26879  28.320734 27.970766
0:  27.62925  27.43487  27.114458 26.317392 25.528101 24.782566 24.516378
0:  24.79648  25.428865 26.023232 26.281199 25.848969 25.808294]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.91821  -16.71276  -16.011486 -14.9975   -14.057879 -13.535214
0:  -13.201581 -13.152162 -13.543202 -13.944491 -14.66043  -15.700179
0:  -16.674046 -17.393337 -17.723251 -17.590378 -17.26764  -17.017456
0:  -17.063744 -16.939568]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.1965599  0.35811043 0.75259733 1.3201866  1.8460343  2.2337532
0:  2.6613326  3.031194   3.1581366  3.2195077  3.0173116  2.5665426
0:  2.1582613  1.8541055  1.7825496  1.9423885  2.100039   2.0902424
0:  0.30836487 0.19681072]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.78204   -6.839722  -6.632915  -6.175404  -5.7477317 -5.500316
0:  -5.591208  -5.773126  -6.3483663 -6.868112  -7.451635  -8.260944
0:  -8.908377  -9.459217  -9.693842  -9.479372  -8.912785  -8.171783
0:  -8.1348915 -8.110426 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.756493  15.636637  15.306513  14.763024  14.189801  13.528127
0:  13.8196335 14.216736  14.8651285 15.308674  15.032577  14.886305
0:  14.579592  14.914452  15.757265  16.783941  17.456978  17.427357
0:  19.538704  19.564016 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9370942  -3.8833346  -3.5200925  -2.8710036  -2.1116366  -1.4900222
0:  -0.99649525 -0.6592207  -0.7121959  -0.78030634 -0.98293877 -1.3951325
0:  -1.639566   -1.8428426  -1.8049245  -1.5696816  -1.2675195  -1.0744967
0:  -2.1496758  -2.0298743 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.071917   9.104395   9.659996  10.705748  11.708574  12.390583
0:  12.483481  12.222696  11.335389  10.336475   9.189302   7.8334236
0:   6.664657   5.6324873  5.0734944  5.0616293  5.452617   5.9527807
0:   3.2290874  2.838612 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.25953  19.37735  19.525454 19.619638 19.553753 19.324318 19.24778
0:  19.242332 19.172516 19.098322 18.858088 18.453424 18.183367 18.107328
0:  18.451864 19.132801 20.007362 20.731255 18.060093 18.012993]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3184924 6.501813  6.8814044 7.37576   7.775563  7.989725  8.239526
0:  8.390438  8.385973  8.325492  8.07113   7.6986055 7.455862  7.481802
0:  7.842428  8.460962  9.024498  9.361947  9.011993  9.203736 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.5841684 -7.5031757 -6.9901295 -6.172817  -5.38048   -4.779435
0:  -4.2433233 -3.877429  -3.8848724 -3.8941946 -4.1650314 -4.725483
0:  -5.1649084 -5.432089  -5.4031935 -4.9539948 -4.432331  -3.9138517
0:  -4.8730917 -4.6646137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1730266 -4.0947795 -3.7063975 -3.0419822 -2.4403644 -2.0532908
0:  -1.9018416 -1.9132171 -2.3354306 -2.8469663 -3.5661674 -4.5593534
0:  -5.3914084 -6.0926404 -6.4781613 -6.479014  -6.318217  -6.1303725
0:  -7.9552894 -8.187918 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.50608  16.778519 17.123692 17.396341 17.354988 16.991684 16.452925
0:  16.006983 15.476495 15.106598 14.724587 14.058871 13.425905 12.804785
0:  12.528639 12.753779 13.403223 14.098631 11.529646 11.647551]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.271521  15.009772  14.616102  14.109697  13.680317  13.080499
0:  13.329443  13.491386  13.947435  14.229778  13.978524  13.9017
0:  13.83035   14.3986225 15.506429  16.873087  18.0386    18.578762
0:  18.753448  18.801704 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.2681956 -6.360167  -6.1409774 -5.6098332 -4.9996963 -4.4928184
0:  -4.249685  -4.087445  -4.376307  -4.7312675 -5.296515  -6.189305
0:  -6.9899993 -7.7001357 -8.091396  -8.0122795 -7.566071  -6.9710793
0:  -8.34235   -8.562862 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.4395323  7.710519   8.140963   8.629281   8.984076   9.165434
0:   9.406113   9.591799   9.652603   9.70177    9.5420265  9.2180395
0:   8.986071   8.941139   9.239846   9.837238  10.511578  11.021011
0:   9.904735  10.220261 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1180124 -2.1828165 -1.98211   -1.6411486 -1.3952608 -1.3299742
0:  -1.1851711 -1.1184034 -1.2464485 -1.3636351 -1.7504234 -2.3696117
0:  -2.954647  -3.2885685 -3.3294296 -3.0592604 -2.8105536 -2.6908464
0:  -3.9240036 -3.904255 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.4547014 -5.546589  -5.265258  -4.650814  -4.0207024 -3.5879803
0:  -3.469468  -3.4689999 -3.8402815 -4.1701336 -4.582698  -5.316676
0:  -5.984598  -6.718611  -7.235832  -7.3284016 -7.0501633 -6.5393753
0:  -7.6179924 -7.6454954]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1791868 2.1311474 2.2995229 2.4807916 2.570311  2.5277138 2.7829275
0:  3.0739257 3.3516645 3.5942357 3.5054111 3.1983478 2.9428256 3.0828018
0:  3.6708038 4.6145735 5.447176  5.9297667 4.758579  4.8545027]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2661114  -3.2474632  -2.8221846  -2.0221114  -1.2443027  -0.6685004
0:  -0.4034977  -0.28148556 -0.5092325  -0.752645   -1.1018128  -1.7614183
0:  -2.3285074  -2.791689   -2.9049091  -2.5140343  -1.7636952  -0.9200816
0:  -1.5399327  -1.327323  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.75032  29.989685 30.078915 29.945614 29.699436 29.213455 29.455887
0:  29.668274 30.016834 30.225754 29.802898 29.503456 29.15057  29.338154
0:  29.998161 30.835049 31.435184 31.54137  31.08923  31.076878]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.536964   -8.252062   -7.6114926  -6.727308   -5.960606   -5.4733176
0:  -5.2016826  -5.188524   -5.5054097  -5.7700095  -6.0850663  -6.5366216
0:  -6.783685   -6.738189   -6.2676654  -5.303346   -4.1070457  -2.8098807
0:   0.16997385  0.8961029 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.026836 34.21123  34.14739  33.8066   33.284283 32.562515 32.359055
0:  32.1053   31.96267  31.611176 30.82491  30.041088 29.475998 29.399567
0:  29.909428 30.686752 31.415012 31.831554 30.30188  30.382397]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.0081854 5.108518  5.385029  5.7191286 5.9198337 5.9469976 6.1858478
0:  6.412141  6.5196705 6.5642266 6.285686  5.8206096 5.40783   5.341178
0:  5.6749105 6.367974  7.088637  7.599306  6.810836  6.9944515]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4178243 -3.8104405 -3.8391194 -3.3999314 -2.8338113 -2.323605
0:  -2.262236  -2.2853408 -2.8486333 -3.4295163 -4.129393  -5.293135
0:  -6.4462895 -7.631127  -8.478451  -8.620665  -8.00025   -6.931331
0:  -7.1028714 -7.4287276]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8683791  -1.008656   -0.9029546  -0.6035619  -0.37042475 -0.29660654
0:  -0.4469428  -0.56461954 -0.92932653 -1.1804442  -1.4721961  -2.0683408
0:  -2.581204   -3.0724797  -3.2858176  -3.0044446  -2.3387542  -1.4934554
0:  -1.8247876  -1.844893  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.501363  10.2855    10.060345   9.80224    9.466476   9.045796
0:   9.063516   9.1003065  9.112135   8.952201   8.329106   7.548743
0:   6.712345   6.3242445  6.291572   6.497977   6.561392   6.246441
0:   3.9106383  3.6179934]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0782557  -1.9929004  -1.6058111  -1.0298367  -0.48197794 -0.12114286
0:   0.156672    0.356246    0.33082914  0.3235278   0.1989193  -0.12378073
0:  -0.3063469  -0.4294095  -0.30171967  0.14566422  0.68640757  1.1369982
0:   0.588603    0.7448149 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.140901   7.271471   7.528712   7.8547583  8.134481   8.288948
0:   8.710023   9.058418   9.32851    9.504226   9.36197    9.084827
0:   8.891072   9.05021    9.586727  10.394705  11.125481  11.547505
0:  10.74221   10.97561  ]
0: validation loss for strategy=forecast at epoch 12 : 0.39374828338623047
0: validation loss for velocity_u : 0.14835254848003387
0: validation loss for velocity_v : 0.21943068504333496
0: validation loss for specific_humidity : 0.2091440111398697
0: validation loss for velocity_z : 0.6371751427650452
0: validation loss for temperature : 0.11653434485197067
0: validation loss for total_precip : 1.0318530797958374
0: 13 : 18:49:50 :: batch_size = 96, lr = 1.5242895643428828e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 13, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([4.4991e-05, 1.7245e-02, 3.2479e-02, 4.5911e-02, 5.7542e-02, 6.7534e-02, 7.6052e-02, 8.3096e-02, 8.8993e-02,
0:         9.3907e-02, 9.8002e-02, 1.0144e-01, 1.0423e-01, 1.0668e-01, 1.0865e-01, 1.1029e-01, 1.1176e-01, 1.1291e-01,
0:         4.7877e-02, 5.7542e-02, 6.5896e-02, 7.2776e-02, 7.8345e-02, 8.2768e-02, 8.6044e-02], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9748, 1.0071, 1.0345, 1.0572, 1.0755, 1.0895, 1.1001, 1.1074, 1.1122, 1.1143, 1.1148, 1.1135, 1.1113, 1.1085,
0:         1.1051, 1.1014, 1.0975, 1.0939, 1.1262, 1.1435, 1.1564, 1.1653, 1.1702, 1.1719, 1.1707], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6698, -0.6648, -0.6567, -0.6485, -0.6403, -0.6337, -0.6272, -0.6207, -0.6135, -0.6052, -0.5969, -0.5887,
0:         -0.5812, -0.5739, -0.5665, -0.5593, -0.5524, -0.5455, -0.6458, -0.6373, -0.6289, -0.6204, -0.6112, -0.6015,
0:         -0.5919], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1317, 0.1539, 0.1762, 0.1984, 0.2207, 0.2385, 0.2518, 0.2630, 0.2674, 0.2652, 0.2607, 0.2496, 0.2363, 0.2185,
0:         0.2007, 0.1851, 0.1695, 0.1584, 0.0360, 0.0672, 0.1005, 0.1384, 0.1762, 0.2118, 0.2429], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.4545, 0.4554, 0.4562, 0.4575, 0.4586, 0.4601, 0.4613, 0.4630, 0.4647, 0.4666, 0.4690, 0.4712, 0.4741, 0.4770,
0:         0.4804, 0.4838, 0.4875, 0.4914, 0.4954, 0.4994, 0.5040, 0.5079, 0.5121, 0.5161, 0.5197], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574,
0:         -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574, -0.2574,
0:         -0.2574], device='cuda:0')
0: [DEBUG] Epoch 13, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2574,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2574,     nan,     nan, -0.2574,     nan, -0.2574,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan, -0.2574,
0:             nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,
0:         -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,
0:             nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan, -0.2574,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,
0:             nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2574, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,
0:         -0.2574,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2574, -0.2574, -0.2574,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 13, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0438, -0.0282, -0.0114, -0.0016,  0.0054,  0.0040,  0.0329,  0.0631,  0.0884,  0.1089,  0.0901,  0.0491,
0:         -0.0062, -0.0487, -0.0677, -0.0646, -0.0537, -0.0620, -0.0146, -0.0026,  0.0081,  0.0121, -0.0038, -0.0040,
0:          0.0134], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4911, -0.4525, -0.4504, -0.4767, -0.5092, -0.5384, -0.5721, -0.5852, -0.5673, -0.5540, -0.5209, -0.4861,
0:         -0.4801, -0.5059, -0.5542, -0.5810, -0.5797, -0.5480, -0.6090, -0.5391, -0.5096, -0.5167, -0.5473, -0.5931,
0:         -0.6231], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6825, -0.6813, -0.6808, -0.6761, -0.6720, -0.6697, -0.6764, -0.6814, -0.6840, -0.6889, -0.6927, -0.6941,
0:         -0.6881, -0.6835, -0.6845, -0.6854, -0.6960, -0.6963, -0.6852, -0.6814, -0.6749, -0.6658, -0.6646, -0.6670,
0:         -0.6715], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5706, 0.6440, 0.6612, 0.5410, 0.4281, 0.4291, 0.4531, 0.4705, 0.4961, 0.5258, 0.4976, 0.4899, 0.6076, 0.6206,
0:         0.5767, 0.7209, 0.7903, 0.6748, 0.6305, 0.5659, 0.5414, 0.4381, 0.3741, 0.4187, 0.4744], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.3614, 0.3428, 0.3377, 0.3296, 0.3277, 0.3300, 0.3421, 0.3562, 0.3685, 0.3754, 0.3727, 0.3625, 0.3501, 0.3390,
0:         0.3215, 0.3024, 0.2851, 0.2816, 0.2903, 0.3094, 0.3228, 0.3284, 0.3231, 0.3066, 0.2780], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2011, -0.1967, -0.2013, -0.1889, -0.1836, -0.1840, -0.1823, -0.1886, -0.1922, -0.2144, -0.2051, -0.1985,
0:         -0.1910, -0.1898, -0.1940, -0.1986, -0.1952, -0.1991, -0.2298, -0.2176, -0.2107, -0.2043, -0.1945, -0.2036,
0:         -0.2073], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.19356344640254974; velocity_v: 0.2566083073616028; specific_humidity: 0.23999729752540588; velocity_z: 0.5955255627632141; temperature: 0.16282014548778534; total_precip: 0.8783138394355774; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19126524031162262; velocity_v: 0.27773329615592957; specific_humidity: 0.21767492592334747; velocity_z: 0.5768135190010071; temperature: 0.158835768699646; total_precip: 0.7617260813713074; 
0: epoch: 13 [1/5 (20%)]	Loss: 0.82002 : 0.34155 :: 0.20004 (2.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19823016226291656; velocity_v: 0.2676306366920471; specific_humidity: 0.2579807937145233; velocity_z: 0.6444602012634277; temperature: 0.16332806646823883; total_precip: 1.167020559310913; 
0: epoch: 13 [2/5 (40%)]	Loss: 1.16702 : 0.41331 :: 0.20707 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18422889709472656; velocity_v: 0.2623095214366913; specific_humidity: 0.1996036171913147; velocity_z: 0.5699496865272522; temperature: 0.14494109153747559; total_precip: 0.6843738555908203; 
0: epoch: 13 [3/5 (60%)]	Loss: 0.68437 : 0.30707 :: 0.19992 (16.04 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19563060998916626; velocity_v: 0.2876571714878082; specific_humidity: 0.2049153447151184; velocity_z: 0.5776242017745972; temperature: 0.1510271281003952; total_precip: 0.6253149509429932; 
0: epoch: 13 [4/5 (80%)]	Loss: 0.62531 : 0.30594 :: 0.20692 (15.91 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 3.81469727e-06
0:  2.67028809e-05 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-06
0:  1.14440918e-05 8.58306885e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 2.19345093e-05 1.23023987e-04
0:  3.91960144e-04 1.25408173e-03 3.06510925e-03 3.63826728e-03
0:  4.95338440e-03 3.47232819e-03 3.08704376e-03 3.30257416e-03
0:  2.22396851e-03 8.97407532e-04 2.52723694e-04 2.17437744e-04
0:  2.07901001e-04 1.85966492e-04 1.24931335e-04 1.23023987e-04
0:  7.72476196e-05 4.00543213e-05 1.00135803e-04 2.26020813e-04
0:  8.53538513e-04 1.60598755e-03 2.40993500e-03 1.31320953e-03
0:  7.11441040e-04 6.79969788e-04 9.41276550e-04 7.74383545e-04
0:  2.58445740e-04 1.97410583e-04 3.63349915e-04 9.02175903e-04
0:  5.73158264e-04 1.83105469e-04 2.03132629e-04 9.96589661e-04
0:  9.36508179e-04 1.06811523e-04 2.04086304e-04 2.18391418e-04
0:  3.49998474e-04 2.17437744e-04 1.13487244e-04 7.72476196e-05
0:  9.44137573e-05 6.67572021e-05 3.24249268e-05 8.10623169e-05
0:  4.95910645e-05 1.90734863e-05 7.15255737e-05 1.33514404e-05
0:  2.09808350e-05 1.23977661e-05 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 2.86102295e-06 5.72204590e-06
0:  1.90734863e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 1.90734863e-06 1.23977661e-05
0:  7.72476196e-05 9.82284546e-05 6.00814819e-05 5.14984131e-05
0:  6.38961792e-05 6.77108765e-05 3.52859497e-05 1.52587891e-05
0:  2.00271606e-05 4.10079956e-05 6.38961792e-05 1.51634216e-04
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-06 2.86102295e-06 0.00000000e+00 0.00000000e+00
0:  4.76837158e-06 2.86102295e-06 9.53674316e-07 1.90734863e-06
0:  2.86102295e-06 3.81469727e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  2.86102295e-06 7.62939453e-06 1.52587891e-05 4.67300415e-05
0:  8.58306885e-06 9.53674316e-06 6.58035278e-05 2.13623047e-04
0:  7.39097595e-04 1.65843964e-03 2.38990784e-03 3.45039391e-03
0:  2.68745422e-03 1.85775757e-03 1.48105621e-03 2.51579285e-03
0:  2.03514099e-03 9.33647156e-04 1.75476074e-04 1.98364258e-04
0:  2.69889832e-04 2.29835510e-04 1.67846680e-04 1.61170959e-04
0:  1.30653381e-04 8.29696655e-05 8.39233398e-05 1.87873840e-04
0:  3.53813171e-04 1.02138519e-03 1.44863129e-03 1.79672241e-03
0:  1.27601624e-03 1.21688843e-03 1.03759766e-03 5.96046448e-04
0:  4.24385071e-04 8.54492188e-04 9.59396362e-04 1.91307068e-03
0:  1.45244598e-03 8.66889954e-04 7.21931458e-04 9.41276550e-04
0:  6.33239746e-04 2.65121460e-04 5.38825931e-04 1.69754028e-04
0:  8.01086426e-05 1.15394592e-04 4.54902649e-04 4.78744507e-04
0:  6.82830811e-04 5.09262085e-04 1.45912170e-04 2.57492065e-05
0:  1.14440918e-05 8.58306885e-06 1.33514404e-05 0.00000000e+00
0:  1.90734863e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 2.86102295e-06 7.62939453e-06
0:  1.23977661e-05 1.90734863e-06 0.00000000e+00 0.00000000e+00]
0: Target values (first 200):
0: [2.50816345e-04 1.13010406e-04 2.51770020e-04 2.72750854e-04
0:  2.47955322e-05 1.62124634e-04 2.76565552e-05 4.24385071e-05
0:  1.09195709e-04 7.00950623e-05 7.24792480e-05 1.79290771e-04
0:  3.50952148e-04 8.13007355e-04 2.24065781e-03 8.89778137e-04
0:  3.90529633e-04 2.18486786e-03 1.52873993e-03 7.31945038e-04
0:  8.98361206e-04 2.50816345e-04 2.38895416e-04 6.48498535e-05
0:  3.03459167e-03 1.42574310e-03 5.68866730e-04 3.32355499e-04
0:  4.21524048e-04 5.88417053e-04 5.60283661e-04 6.81400299e-04
0:  6.92367554e-04 7.46250153e-04 1.08337402e-03 5.10215759e-04
0:  7.29560852e-05 1.47819519e-05 1.66893005e-05 1.90734863e-06
0:  3.43322754e-05 2.74181366e-04 4.13417816e-04 5.19275665e-04
0:  7.88211823e-04 1.09863281e-03 9.57965909e-04 3.83853912e-04
0:  2.47955322e-05 4.29153442e-06 6.67572021e-06 4.86373901e-05
0:  6.81877136e-05 8.96453857e-05 5.76972961e-05 2.33650208e-05
0:  9.05990601e-06 1.90734863e-06 3.81469727e-06 2.28881836e-05
0:  8.58306885e-06 2.43186951e-05 3.15046310e-03 2.05612183e-03
0:  3.77178192e-04 9.02652740e-04 1.99699402e-03 2.22396851e-03
0:  2.05516815e-04 1.06811523e-04 7.05718994e-05 1.04904175e-04
0:  5.62667847e-05 3.43322754e-05 6.72340393e-05 9.39369202e-05
0:  1.67369843e-04 2.09331512e-04 2.57492065e-05 2.76565552e-05
0:  7.62939453e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.43051147e-06 8.10623169e-06
0:  5.24520874e-06 4.29153442e-06 1.43051147e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  2.38418579e-06 9.53674316e-06 2.38418579e-06 2.38418579e-06
0:  8.46385956e-04 1.98364258e-04 3.32832336e-04 1.87397003e-04
0:  2.47955322e-05 2.90870667e-05 3.71932983e-05 3.43322754e-05
0:  3.29017639e-05 2.52723694e-05 3.22818756e-04 5.88417053e-04
0:  4.90188599e-04 7.77721405e-04 9.20772552e-04 1.00231171e-03
0:  2.39372253e-04 2.95639038e-05 7.15255737e-05 1.18732452e-04
0:  1.01566315e-04 4.14848328e-05 3.76701355e-05 3.76701355e-05
0:  2.02369690e-03 1.75857544e-03 5.84125519e-04 4.76837158e-04
0:  1.62363052e-03 2.38943100e-03 2.92396545e-03 1.93405151e-03
0:  1.18780136e-03 2.17247009e-03 1.33419037e-03 4.08172607e-04
0:  2.57492065e-05 5.24520874e-06 2.38418579e-06 1.71661377e-05
0:  9.01222229e-05 2.19821930e-04 6.41345978e-04 6.65187836e-04
0:  7.50541687e-04 1.03330612e-03 8.10623169e-04 2.33650208e-04
0:  3.43322754e-05 1.23977661e-05 2.38418579e-06 9.05990601e-06
0:  2.00271606e-05 8.96453857e-05 7.29560852e-05 3.00407410e-05
0:  8.10623169e-06 1.00135803e-05 3.43322754e-05 4.38690186e-05
0:  3.81469727e-06 1.49583817e-03 1.68228149e-03 1.04808807e-03
0:  2.46047974e-04 6.72340393e-04 1.02949142e-03 1.57785416e-03
0:  9.65118408e-04 1.70230865e-04 1.18732452e-04 5.14984131e-05
0:  2.86102295e-05 5.72204590e-05 9.29832458e-05 4.67300415e-05
0:  3.86238098e-05 2.05039978e-05 7.15255737e-06 4.76837158e-06
0:  4.76837158e-06 9.05990601e-06 2.05039978e-05 1.81198120e-05
0:  3.33786011e-06 1.43051147e-06 4.86373901e-05 4.00543213e-05
0:  4.48226929e-05 2.52723694e-05 1.62124634e-05 1.09672546e-05]
0: Prediction values (first 20):
0: [-5.3436213 -5.31802   -5.0154047 -4.4937525 -4.0506907 -3.7967882
0:  -3.7176514 -3.6984873 -3.9492197 -4.1433554 -4.370743  -4.802183
0:  -5.0078793 -5.1247177 -5.0227957 -4.642271  -4.1789703 -3.7759657
0:  -5.165201  -5.04261  ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.353, max = 0.844, mean = -0.164
0:          sample (first 20): tensor([-1.0158, -1.0137, -0.9892, -0.9468, -0.9108, -0.8902, -0.8838, -0.8822, -0.9026, -0.9184, -0.9368, -0.9719,
0:         -0.9886, -0.9981, -0.9898, -0.9589, -0.9213, -0.8885, -0.9463, -0.9847])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.101019 17.556324 17.883226 18.004694 17.960081 17.698923 17.776653
0:  17.754112 17.691837 17.466833 16.893032 16.253082 15.697073 15.54496
0:  15.7715   16.317583 16.905579 17.289818 15.580935 15.651144]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.6651115   1.8510132   2.3057737   2.9743536   3.524361    3.818763
0:   3.9493442   3.8940961   3.4711444   2.9852684   2.2975516   1.4053521
0:   0.6846633   0.16289186  0.03506422  0.31894732  0.7529764   1.1181564
0:  -0.34787083 -0.3865633 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.80803156 -0.76077795 -0.40829802  0.19835377  0.8056402   1.244945
0:   1.601366    1.8174691   1.7411427   1.5354624   1.0627847   0.21014309
0:  -0.6960392  -1.6358862  -2.38941    -2.761054   -2.9113426  -2.8465595
0:  -4.151249   -4.48195   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.701916  5.4228997 5.21054   5.0644956 4.950236  4.785117  4.8093076
0:  4.912513  4.902096  5.0360336 5.059047  4.886426  4.886861  4.9681473
0:  5.2368307 5.7839823 6.3494887 6.920704  5.7492495 5.856201 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.068735  15.352075  15.730703  16.229755  16.59031   16.816433
0:  16.862522  16.960674  16.846992  16.846067  16.803322  16.478554
0:  16.132475  15.755524  15.664936  15.982726  16.687098  17.440735
0:  15.2843485 15.585137 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.0453863   1.1001425   1.3831367   1.8630991   2.304493    2.5597973
0:   2.531107    2.482605    2.0692472   1.7484632   1.3237133   0.57019186
0:  -0.13490725 -0.8659544  -1.3235726  -1.2921948  -0.83028555 -0.14427185
0:  -1.0425091  -1.1471701 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.017708 32.20779  32.299126 32.20824  31.997234 31.56247  31.52422
0:  31.343487 31.135815 30.716522 29.842503 29.007423 28.328613 28.026085
0:  28.13956  28.461239 28.731094 28.789955 27.228928 27.253746]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3306184  -2.3285766  -2.0334554  -1.4307723  -0.82263184 -0.3578782
0:  -0.21137142 -0.05961275 -0.2780676  -0.4163847  -0.65749264 -1.310266
0:  -1.9831958  -2.7628903  -3.29034    -3.2604012  -2.6729026  -1.7991338
0:  -2.808415   -3.0818458 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0643048  -1.2271838  -1.102633   -0.686666   -0.2392497   0.09083176
0:   0.2788968   0.46123075  0.36904335  0.36578274  0.31582117 -0.03867865
0:  -0.28053093 -0.5558176  -0.59711075 -0.18700027  0.55805683  1.4421325
0:   0.6702256   0.74699354]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.039907 24.036554 24.028744 23.990135 23.858055 23.534279 23.676195
0:  23.763521 23.894152 23.885403 23.373472 22.74316  22.104704 21.766092
0:  21.870153 22.278873 22.827328 23.126724 20.797726 20.761822]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.3712935 -6.3508906 -5.9695973 -5.3065104 -4.648565  -4.1747804
0:  -3.978252  -3.868548  -4.1273627 -4.3613744 -4.6738524 -5.296367
0:  -5.743976  -6.1535907 -6.280046  -5.9606977 -5.389704  -4.752237
0:  -5.8107867 -5.7876973]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.067654 19.231228 19.259262 19.062737 18.823786 18.421726 18.65758
0:  18.855074 19.097492 19.192337 18.750523 18.357426 18.014307 18.213146
0:  18.857857 19.788353 20.613518 21.130835 20.269184 20.384914]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.84619  -10.960647 -10.642919  -9.955437  -9.190076  -8.574325
0:   -8.206851  -8.111721  -8.565545  -9.117818  -9.922271 -11.009709
0:  -11.867406 -12.579377 -12.881173 -12.780271 -12.506585 -12.117585
0:  -11.816545 -11.665406]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.41286  30.630241 30.274166 29.558018 28.815353 27.99445  28.475487
0:  28.958954 29.575321 29.713352 28.769669 27.909805 26.98322  26.937256
0:  27.70858  28.795269 29.604515 29.594715 30.566349 30.931492]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.05704  20.223516 20.428013 20.521414 20.48563  20.347094 20.620594
0:  20.896057 21.174488 21.342316 21.104101 20.757755 20.562471 20.754599
0:  21.481514 22.576792 23.766567 24.732332 23.704062 23.943926]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8852091  -0.8070774  -0.53341913 -0.1316433   0.11449718  0.13318348
0:  -0.08440351 -0.40362692 -1.0104094  -1.6081896  -2.3009968  -3.2015362
0:  -3.9839125  -4.650507   -4.9891176  -4.89324    -4.485071   -3.9705915
0:  -5.301296   -5.331971  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.506926 19.732603 20.092133 20.481339 20.654188 20.602442 20.369358
0:  20.13432  19.660624 19.203585 18.621845 17.825073 17.1737   16.641365
0:  16.528847 16.794582 17.319921 17.785841 14.87146  14.703901]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.497902  5.656697  6.049604  6.505405  6.7485356 6.730283  6.625179
0:  6.6154203 6.5074954 6.524003  6.463545  6.0831037 5.715352  5.341848
0:  5.243417  5.6134033 6.32257   7.0843883 6.0696783 6.2434874]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.895565  2.843689  3.0452948 3.471099  3.807096  3.9485896 4.172699
0:  4.287225  4.249201  4.205171  3.9202166 3.4726467 3.0786672 2.9107394
0:  3.0676656 3.5238838 3.9931355 4.3064003 3.8302913 3.8492258]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.834333  16.611393  16.407192  16.209297  15.965067  15.55632
0:  15.624456  15.650873  15.74349   15.741756  15.329403  14.784849
0:  14.237981  14.044791  14.327505  15.027328  15.8641205 16.501604
0:  14.630356  14.580601 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.743731  -8.133564  -8.201645  -7.9748693 -7.6809916 -7.522388
0:  -7.301432  -7.184319  -7.297155  -7.4042945 -7.696567  -8.12719
0:  -8.395983  -8.368406  -8.047014  -7.470242  -6.9394536 -6.64208
0:  -8.225042  -8.330362 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.523529  4.489273  4.6491003 4.983031  5.2404995 5.337596  5.2036076
0:  5.1442966 4.8497863 4.6715612 4.4296994 3.8462198 3.2620187 2.6200738
0:  2.230527  2.3089757 2.823975  3.4994483 1.611598  1.3110723]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [49.01177  48.769623 48.16783  47.584568 47.027176 46.28583  46.568085
0:  46.65036  46.920498 46.68493  45.6496   44.755665 43.9733   43.822285
0:  44.234142 44.74206  44.90298  44.379776 44.45784  44.22181 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.7930655 6.837618  7.004472  7.277918  7.3714633 7.285474  7.124897
0:  7.045637  6.8358774 6.6849513 6.399702  5.802063  5.2084837 4.661222
0:  4.4654536 4.7250905 5.365058  6.0640736 5.1544695 5.1105704]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.23822    -9.692551   -9.687283   -9.45591    -9.166212   -8.975981
0:   -8.655367   -8.444767   -8.484707   -8.500895   -8.772316   -9.224373
0:   -9.45998    -9.365282   -8.928745   -8.17584    -7.557531   -7.1511292
0:  -10.135475  -10.433231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.50878  28.72252  28.856667 28.83948  28.738064 28.489256 28.966335
0:  29.395653 29.787178 29.858974 29.203608 28.598074 27.952559 27.942255
0:  28.455463 29.127747 29.459375 29.243704 29.219402 29.17443 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.891057  12.845638  12.890091  12.927843  12.972276  12.868392
0:  13.279654  13.596874  13.918281  14.066387  13.729749  13.357269
0:  13.0332575 13.249915  13.987156  15.096876  16.160889  16.889624
0:  16.644096  16.704155 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7709787 3.4664173 3.261905  3.0928876 2.8728476 2.5514216 2.5882487
0:  2.6867394 2.8208747 2.8857532 2.6124277 2.2215004 1.8619595 1.953196
0:  2.473215  3.2710648 4.0215364 4.4338865 3.829319  3.8646917]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8670802  -0.7012801  -0.24221563  0.42141485  0.95180655  1.215826
0:   1.3295064   1.2456746   0.780118    0.31411362 -0.35636282 -1.2569356
0:  -2.0096097  -2.58184    -2.8109202  -2.5899358  -2.2029033  -1.7612681
0:  -2.2633128  -2.15624   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.375332  10.5082035 10.696996  10.80433   10.821943  10.644171
0:  10.856357  11.032173  11.247939  11.415531  11.209964  10.953274
0:  10.658499  10.789753  11.33354   12.199632  13.054946  13.5935955
0:  12.919521  13.141372 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.659273  13.67811   13.660259  13.6187935 13.50786   13.230953
0:  13.116554  12.898009  12.53508   12.061623  11.309376  10.470484
0:   9.726622   9.327443   9.370906   9.807167  10.389984  10.866054
0:   9.648977   9.641149 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.292906  6.49001   6.8853636 7.323669  7.5444255 7.5783625 7.514401
0:  7.6149044 7.589414  7.687002  7.6863112 7.3377175 7.001486  6.6502004
0:  6.5904574 7.0223413 7.837965  8.700561  7.1724443 7.154681 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.058339    2.0637946   2.3622756   2.9498892   3.4935932   3.877655
0:   3.9822712   4.064537    3.7071373   3.3114085   2.7443376   1.7740703
0:   0.8873992   0.04380655 -0.40722656 -0.24380922  0.42448092  1.2634268
0:  -0.31607342 -0.69192076]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6841302 -3.5350652 -3.0965104 -2.4263606 -1.8350415 -1.4506922
0:  -1.3046875 -1.2536674 -1.479003  -1.6688704 -1.901833  -2.3750367
0:  -2.8190217 -3.2386603 -3.485869  -3.4100747 -3.050324  -2.575873
0:  -3.1792579 -2.873468 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7713304 3.5303433 3.255395  2.8029068 2.2780824 1.7476668 1.9231339
0:  2.21551   2.6733487 3.017951  2.8688126 2.669629  2.4411378 2.8578012
0:  3.7680888 5.028007  5.994417  6.482289  6.8349543 6.877151 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.256172 37.815243 38.175957 38.242958 38.082302 37.66418  37.699276
0:  37.725945 37.838814 37.81499  37.416798 37.040363 36.915012 37.123726
0:  37.90617  38.838245 39.67299  40.196278 39.081554 39.701473]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.1906    9.648885 10.235371 10.786367 11.094364 11.059977 11.050491
0:  10.923302 10.701165 10.439451 10.021264  9.399819  8.894835  8.597543
0:   8.629052  8.979245  9.420305  9.757671  8.053467  8.070961]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.328381  6.3928    6.654798  7.025456  7.2921844 7.35809   7.497018
0:  7.5293694 7.4274197 7.2987213 6.959401  6.3991547 5.9018335 5.615781
0:  5.649302  6.0143175 6.441386  6.711918  5.1493783 5.1552935]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.579969   2.6157722  2.8932147  3.4336061  4.076345   4.749899
0:   5.6426406  6.5596285  7.474031   8.357125   9.160965   9.868446
0:  10.755234  11.854603  13.252832  14.812288  16.289406  17.400837
0:  15.383501  15.76259  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -5.6269145  -5.781297   -5.68391    -5.4354663  -5.3565526  -5.587747
0:   -5.9310155  -6.5163856  -7.313008   -8.108734   -8.969049   -9.803007
0:  -10.257397  -10.172359   -9.643103   -8.755489   -7.9510627  -7.4008718
0:   -8.156807   -8.021528 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4749331 -3.3097787 -2.8977294 -2.2649398 -1.64924   -1.214448
0:  -0.9773135 -0.8488307 -1.0589461 -1.2363601 -1.5145574 -2.0322948
0:  -2.4285483 -2.7889    -2.916843  -2.7088866 -2.2939897 -1.8160205
0:  -3.5979247 -3.6010547]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7171168  -2.7371116  -2.440187   -1.8992839  -1.3885899  -1.0428548
0:  -0.8559427  -0.87234116 -1.233738   -1.5503545  -2.0008798  -2.6227098
0:  -3.1104026  -3.4683576  -3.5664449  -3.2819142  -2.80094    -2.262875
0:  -2.2164416  -1.9341445 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9161725  -3.9190726  -3.5499358  -2.7171893  -1.6767249  -0.6875601
0:   0.03644037  0.69628143  0.87774277  1.0805879   1.1511235   0.93233156
0:   0.77193356  0.46756983  0.42363262  0.64139605  1.2123208   1.8047824
0:   2.4762745   2.5986595 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.789251  5.979551  6.300624  6.623679  6.8524795 6.9010553 7.161492
0:  7.365507  7.5810475 7.706456  7.5250673 7.1971807 6.934733  6.940554
0:  7.2958655 7.944397  8.56645   9.031979  6.321301  6.390694 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.989964 20.877321 20.59475  20.091393 19.53299  18.731901 18.787415
0:  18.781115 18.973223 18.87174  18.041014 17.26358  16.403887 16.183468
0:  16.511929 17.191841 17.657024 17.698843 17.94413  17.87573 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.887473   -7.8575125  -7.383362   -6.5350704  -5.677582   -5.0459967
0:   -4.785309   -4.672903   -4.96677    -5.269082   -5.6853447  -6.4528294
0:   -7.162117   -7.882698   -8.365954   -8.333925   -7.910528   -7.276268
0:   -9.817187  -10.373103 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1505785 2.511939  2.9970348 3.4646494 3.8893661 4.187928  4.8500576
0:  5.449374  5.962928  6.228238  6.0921383 5.7563586 5.4707336 5.556639
0:  6.0547247 6.7807074 7.4934545 8.024887  6.260214  6.497219 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.42639  11.5201   11.538856 11.375696 11.204604 10.97232  11.489189
0:  12.037683 12.717027 13.210159 13.175696 13.111863 12.997494 13.402311
0:  14.20297  15.233952 16.016657 16.33895  16.360798 16.435076]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.8596945 13.037178  13.300879  13.526697  13.642689  13.569904
0:  13.7916565 13.978565  14.158801  14.319162  14.184216  13.933051
0:  13.7321    13.854413  14.323123  15.080937  15.837442  16.31995
0:  14.337999  14.494091 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.126967  11.667286  11.296148  10.981472  10.532602   9.946389
0:   9.673191   9.476456   9.354953   9.270098   8.948913   8.418408
0:   7.8958707  7.5923014  7.6406455  8.030059   8.558327   8.944562
0:   6.604657   6.4893146]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.695451  4.9232793 5.361369  5.8758907 6.215803  6.35291   6.341678
0:  6.4025974 6.266367  6.212794  6.0631948 5.5857453 5.1373887 4.684571
0:  4.5347047 4.8721724 5.541218  6.2382426 4.826976  5.0509663]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.820183  11.777035  11.886116  12.015938  12.002731  11.85239
0:  11.7971325 11.7499485 11.628426  11.5184    11.274458  10.887814
0:  10.64495   10.538153  10.742258  11.218069  11.712687  12.024311
0:  10.335631  10.307511 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.669056  -6.610961  -6.2366266 -5.5120544 -4.766671  -4.2582893
0:  -4.1599684 -4.2676344 -4.9236736 -5.511858  -6.233795  -7.1103916
0:  -7.927396  -8.764127  -9.128572  -9.028738  -8.401239  -7.532444
0:  -6.020897  -5.9142265]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1906672  -3.0540133  -2.6422348  -1.9866085  -1.3065329  -0.7758007
0:  -0.40606022 -0.09609938 -0.13852262 -0.19008303 -0.40224028 -0.9130726
0:  -1.3102903  -1.6898203  -1.7455435  -1.3976984  -0.7638941  -0.03114653
0:  -0.35229778 -0.07392359]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.163603  13.412573  13.7692175 14.045271  14.155663  14.096943
0:  14.285738  14.474009  14.67268   14.819496  14.717266  14.493081
0:  14.41534   14.576746  15.113462  15.929003  16.759123  17.388994
0:  14.803499  15.082891 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.882815  14.284073  14.71784   15.041706  15.156635  15.079257
0:  15.106817  15.167194  15.216503  15.273207  15.225817  15.0416565
0:  15.058002  15.29768   15.958763  16.936575  18.057888  18.966793
0:  17.51233   17.933548 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -4.520807   -4.9985347  -5.148992   -5.027493   -4.868815   -4.881804
0:   -5.111833   -5.4073844  -6.0088744  -6.570933   -7.190309   -8.023567
0:   -8.718349   -9.279596   -9.582949   -9.4749775  -9.109053   -8.683798
0:  -10.805227  -11.439018 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.562148 -10.737212 -10.535968  -9.961869  -9.310483  -8.786154
0:   -8.59824   -8.499666  -8.822317  -9.11245   -9.469649 -10.128509
0:  -10.639625 -11.151143 -11.440915 -11.330958 -10.922317 -10.326959
0:  -10.526597 -10.589   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.448977  -4.225987  -3.7344594 -3.2276587 -2.883552  -2.7224584
0:  -2.6046119 -2.5314288 -2.5470872 -2.6029725 -2.7979102 -3.16783
0:  -3.4088912 -3.4589171 -3.2106433 -2.6224828 -1.9738679 -1.3947959
0:  -3.2059188 -2.7967849]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.965672   7.3978996  7.92172    8.401058   8.77795    9.103727
0:   9.622355  10.17417   10.687884  11.066975  11.233541  11.293642
0:  11.480427  11.903506  12.650891  13.615526  14.558613  15.323273
0:  13.725199  14.240549 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3687277 4.54557   4.9938326 5.554505  5.9384136 6.1480346 6.210098
0:  6.2377124 6.17056   6.2109213 6.2598243 6.126749  6.1125345 6.173504
0:  6.4585342 7.0399146 7.814503  8.608822  6.914837  6.89147  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.512791 15.486338 15.744629 16.110487 16.403511 16.53838  16.882048
0:  17.24009  17.656202 18.061148 18.290771 18.31301  18.453978 18.703161
0:  19.287348 20.151575 21.084898 21.794    19.922838 20.001057]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.1882744  7.0965285  6.9889936  6.805563   6.5653377  6.3160214
0:   6.4927044  6.788725   7.140172   7.452374   7.417137   7.3103733
0:   7.206575   7.555745   8.3087225  9.386345  10.415728  11.160772
0:   9.116866   9.1791   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0999107 3.0925741 3.3419325 3.6670668 3.77539   3.6176693 3.434009
0:  3.337307  3.1880152 3.1801565 3.0798612 2.7044713 2.390139  2.1881142
0:  2.3015933 2.8365197 3.5067205 4.034013  2.7877018 3.0596292]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.053799  14.962704  14.819122  14.540634  14.256672  13.883498
0:  14.355677  14.9022045 15.651976  16.249437  16.234207  16.246986
0:  16.177065  16.664549  17.62611   18.778824  19.55425   19.6888
0:  19.479559  19.490927 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.481277 -13.66115  -13.301027 -12.441874 -11.411043 -10.54516
0:   -9.984213  -9.717266  -9.973146 -10.210714 -10.53495  -10.986254
0:  -11.131117 -11.05335  -10.590687  -9.753235  -8.833372  -8.06432
0:   -8.924147  -8.859016]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.092469   3.5461664  3.0770197  2.5616908  1.8484998  0.8540182
0:   0.0621171 -0.821702  -1.6900678 -2.60222   -3.7960658 -5.094321
0:  -6.216167  -6.837007  -6.85957   -6.430302  -6.009606  -5.7413135
0:  -7.285361  -8.09026  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.4008203 -4.3607955 -4.0599217 -3.5848165 -3.1637073 -2.9901147
0:  -3.004994  -3.197311  -3.7672124 -4.3428073 -5.051892  -6.0138793
0:  -6.845468  -7.513059  -7.8904586 -7.8876824 -7.7057242 -7.455898
0:  -6.919024  -6.7038503]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.137497 20.26923  20.511028 20.724443 20.800583 20.68029  20.884096
0:  20.979156 21.096926 21.105917 20.717201 20.246872 19.895363 19.897585
0:  20.367987 21.147438 21.938148 22.484468 21.252115 21.362827]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.530645 18.601027 18.715294 18.746477 18.524712 18.053637 17.749628
0:  17.398685 17.065523 16.673935 16.027536 15.231775 14.54624  14.089512
0:  14.097261 14.457258 14.913095 15.194653 13.653877 13.510286]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3958335 -3.3417106 -3.03684   -2.5396037 -2.15879   -1.9962039
0:  -1.9552293 -1.9812121 -2.2549167 -2.4613824 -2.7621293 -3.282659
0:  -3.6597257 -3.923143  -3.8736835 -3.4207072 -2.7531266 -2.0571036
0:  -2.4205384 -2.261949 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.07417154 -1.0596104  -1.8506975  -2.3948078  -2.773138   -3.048109
0:  -2.767808   -2.4427166  -2.1412354  -1.8948231  -1.8797951  -1.965971
0:  -1.7884684  -0.99812365  0.3191681   2.0071182   3.572695    4.610822
0:   3.312057    3.0714002 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.4894915 -6.5778685 -6.3420353 -5.693199  -4.9386625 -4.276904
0:  -4.0139236 -3.810546  -4.0522947 -4.215069  -4.427984  -4.9950914
0:  -5.550377  -6.196602  -6.618535  -6.513464  -5.827284  -4.807048
0:  -5.145341  -5.3255205]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.49012   14.828499  15.288827  15.655452  15.727292  15.541816
0:  15.377601  15.321907  15.227422  15.193469  15.006097  14.538309
0:  14.095499  13.722284  13.674059  13.970409  14.437656  14.778004
0:  12.5922985 12.806208 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.605396 26.161179 26.674582 27.058764 27.284742 27.50126  28.128742
0:  28.846106 29.586912 30.323164 30.74629  31.115435 31.616446 32.500744
0:  33.790485 35.212475 36.439713 37.1705   34.974464 35.53485 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.52052    -4.7714615  -4.849759   -4.8220205  -4.729309   -4.680728
0:  -4.03724    -3.4045753  -2.6797733  -2.12471    -2.0579414  -2.1781235
0:  -2.3730974  -2.0463963  -1.3527565  -0.34651327  0.45395756  0.82684803
0:   0.23959208  0.37301397]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1292198 3.1107914 3.3158948 3.61695   3.9074159 4.074541  4.3874946
0:  4.664939  4.8702526 5.024784  4.9736347 4.7543983 4.614562  4.7152987
0:  5.1507845 5.899154  6.693351  7.3448176 4.265395  4.249331 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.57873  38.570377 38.48258  38.476204 38.45159  38.216602 38.711956
0:  39.07277  39.617657 39.86688  39.495396 39.110878 38.92893  39.110954
0:  39.88242  40.863205 41.7739   42.23928  41.839603 41.817776]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.5890808e-03 -4.1522694e-01 -6.2769890e-01 -5.7525015e-01
0:  -3.5774612e-01 -1.9268703e-01  4.7508621e-01  1.0595288e+00
0:   1.7140646e+00  2.2311773e+00  2.3427572e+00  2.3694930e+00
0:   2.4536242e+00  2.9872656e+00  3.9908109e+00  5.3603587e+00
0:   6.6901822e+00  7.7278152e+00  6.7928619e+00  6.7194252e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.668438 16.60803  16.401623 15.933598 15.415892 14.860577 15.038855
0:  15.330286 15.75532  16.00942  15.655415 15.368322 15.071304 15.421772
0:  16.308064 17.458488 18.357983 18.743717 18.856628 18.98198 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.121645  17.82982   17.456188  16.854     15.907099  14.631638
0:  13.345497  12.102459  10.821214   9.777506   8.841714   7.9514713
0:   7.48669    7.447399   7.9197803  8.881748  10.053668  11.162316
0:  10.146936   9.920051 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.7989502   1.3058524   0.943439    0.6402235   0.3302555  -0.11338329
0:  -0.14293098 -0.18642426 -0.19376993 -0.27665472 -0.7002058  -1.2489142
0:  -1.7524238  -1.720614   -1.1888633  -0.19427347  0.8392081   1.648479
0:   0.718225    0.46893597]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.759592 -10.853056 -10.537403  -9.86178   -9.153109  -8.624758
0:   -8.433907  -8.351967  -8.66806   -8.91626   -9.226957  -9.815767
0:  -10.233343 -10.639641 -10.770273 -10.456848  -9.838099  -9.077908
0:   -9.225262  -8.941307]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.706654  -9.676516  -9.2735195 -8.574881  -7.864927  -7.3108544
0:  -6.9355397 -6.73988   -6.933105  -7.19959   -7.688336  -8.486809
0:  -9.069052  -9.451839  -9.392406  -8.89661   -8.298     -7.7143636
0:  -8.769344  -8.657585 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8739314 -4.164496  -4.1380973 -3.7842488 -3.3644786 -3.025413
0:  -2.8682952 -2.7974257 -3.0649886 -3.3097882 -3.634108  -4.148373
0:  -4.54561   -4.8510594 -4.896871  -4.572     -4.0170255 -3.3371577
0:  -4.0377545 -4.164952 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.964471 12.511108 12.098254 11.698038 11.332928 10.860197 10.844654
0:  10.781892 10.82996  10.794389 10.415436  9.980934  9.624625  9.679983
0:  10.109008 10.81927  11.419663 11.721456  9.287077  8.830509]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.590654 15.058327 15.608938 16.039455 16.296228 16.381039 16.625317
0:  16.912607 17.168821 17.347298 17.243189 17.048685 16.966028 17.225826
0:  17.971752 18.99139  20.045546 20.851074 18.347351 18.628899]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.784819  3.6814249 3.7888594 4.0454493 4.2568626 4.3571677 4.5426607
0:  4.7417336 4.7716036 4.680781  4.324002  3.749063  3.2145288 2.928245
0:  3.0390506 3.5204754 4.1251965 4.671505  3.7844949 3.728369 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.202951 21.651983 21.055143 20.394556 19.655529 18.651093 18.241545
0:  17.713238 17.303854 16.72939  15.642864 14.534774 13.462832 12.977623
0:  13.081638 13.635073 14.232401 14.515705 12.576599 12.032244]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.589116  10.862151  11.387682  11.985498  12.33494   12.35145
0:  12.035011  11.502384  10.603589   9.699684   8.752866   7.753212
0:   7.0238605  6.5726953  6.56654    7.0086718  7.720407   8.406988
0:   6.45711    6.383036 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.280731 31.05383  30.656902 30.174946 29.73811  29.219349 29.463818
0:  29.62009  29.941551 30.02288  29.556866 29.19186  28.993011 29.297043
0:  30.171501 31.300457 32.291958 32.875813 31.847134 31.866455]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.311054  7.1971264 7.2898545 7.47942   7.5651307 7.491109  7.4476604
0:  7.3908787 7.2143736 7.0254936 6.6581416 6.061512  5.5372744 5.1724195
0:  5.1464596 5.5033484 6.0358076 6.4992933 5.0131087 4.939538 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.931147 22.011284 22.177473 22.327694 22.351551 22.180737 22.186989
0:  22.111073 21.986568 21.77102  21.286598 20.716263 20.261856 20.0545
0:  20.215115 20.5764   20.940031 21.097803 19.373    19.16094 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.598957 37.53488  37.302406 37.07983  36.845078 36.424324 36.69694
0:  36.827477 37.102688 37.155518 36.647655 36.16588  35.878178 36.026535
0:  36.70949  37.579266 38.390606 38.83059  38.670193 38.653236]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.878253  12.304567  12.958247  13.765264  14.5105715 15.133702
0:  15.810554  16.395765  16.82753   17.223392  17.487469  17.58222
0:  17.876812  18.279884  18.944561  19.776712  20.589954  21.139309
0:  18.834671  18.816675 ]
0: validation loss for strategy=forecast at epoch 13 : 0.3590683937072754
0: validation loss for velocity_u : 0.15850181877613068
0: validation loss for velocity_v : 0.2350866049528122
0: validation loss for specific_humidity : 0.17519555985927582
0: validation loss for velocity_z : 0.5858845114707947
0: validation loss for temperature : 0.10564716905355453
0: validation loss for total_precip : 0.894094705581665
0: 14 : 18:53:56 :: batch_size = 96, lr = 1.4871117700906175e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 14, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7014, -0.6785, -0.6572, -0.6321, -0.6086, -0.5875, -0.5629, -0.5312, -0.4914, -0.4524, -0.4182, -0.3907,
0:         -0.3654, -0.3381, -0.3130, -0.2858, -0.2577, -0.2326, -0.6917, -0.6687, -0.6446, -0.6159, -0.5875, -0.5606,
0:         -0.5341], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5486, 0.5640, 0.5776, 0.5929, 0.6052, 0.6141, 0.6226, 0.6357, 0.6578, 0.6843, 0.7155, 0.7432, 0.7617, 0.7791,
0:         0.7913, 0.8007, 0.8087, 0.8118, 0.5764, 0.5867, 0.5961, 0.6061, 0.6177, 0.6259, 0.6306], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7027, -0.7084, -0.7139, -0.7196, -0.7255, -0.7323, -0.7401, -0.7477, -0.7528, -0.7545, -0.7566, -0.7557,
0:         -0.7560, -0.7564, -0.7579, -0.7601, -0.7619, -0.7638, -0.7068, -0.7100, -0.7149, -0.7202, -0.7255, -0.7316,
0:         -0.7373], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2820,  0.4220,  0.3779,  0.2776,  0.1872,  0.0087, -0.0673, -0.1258, -0.2415, -0.3175, -0.3462, -0.2988,
0:         -0.2558, -0.1886, -0.1467, -0.1136, -0.0464, -0.0310,  0.3294,  0.4352,  0.3327,  0.1950,  0.1200,  0.0307,
0:          0.0252], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.3434, 1.1971, 1.0192, 0.8640, 0.7154, 0.5610, 0.3951, 0.2594, 0.1794, 0.1688, 0.2312, 0.3020, 0.3467, 0.3695,
0:         0.3675, 0.3540, 0.3467, 0.3397, 0.3439, 0.3625, 0.3708, 0.3749, 0.3712, 0.3903, 0.4283], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.3033,  0.2853,  0.4409,  0.4657,  0.4635,  0.3507,  0.1928,  0.1093,  0.0575,  0.2514,  0.2627,  0.3056,
0:          0.3033,  0.1838,  0.1296,  0.0214, -0.0688, -0.0553,  0.0597,  0.0642,  0.0958,  0.0687,  0.0326, -0.0621,
0:         -0.1252], device='cuda:0')
0: [DEBUG] Epoch 14, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.0553,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2312,     nan,     nan,     nan,     nan,     nan,     nan,  0.0710,     nan,     nan,     nan, -0.2155,
0:             nan,     nan,     nan,     nan,  0.0823,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0666,     nan,     nan,     nan,     nan, -0.0756,  0.0349,     nan,     nan,
0:             nan,     nan,     nan, -0.1591,     nan,     nan,     nan,     nan,     nan, -0.1613,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2064,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0688,     nan,     nan,     nan,     nan,     nan, -0.1681,     nan,     nan,     nan,
0:          0.2176,     nan,     nan,     nan, -0.0531,     nan,     nan, -0.1320,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0779,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0260,     nan,     nan,  0.0507, -0.1749,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1410,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.0191,     nan,     nan,     nan,     nan,     nan,     nan, -0.2087,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 14, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5534, -0.5466, -0.5224, -0.4874, -0.4620, -0.4546, -0.4685, -0.4949, -0.5531, -0.6148, -0.6835, -0.7644,
0:         -0.8208, -0.8530, -0.8481, -0.8047, -0.7484, -0.6975, -0.5076, -0.5395, -0.5450, -0.5310, -0.5165, -0.5042,
0:         -0.5016], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1414,  0.1935,  0.2188,  0.2146,  0.1876,  0.1506,  0.1067,  0.0792,  0.0730,  0.0726,  0.0769,  0.0761,
0:          0.0523,  0.0062, -0.0451, -0.0694, -0.0608, -0.0271,  0.0885,  0.1557,  0.1955,  0.2026,  0.1866,  0.1501,
0:          0.1172], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8643, -0.8578, -0.8504, -0.8313, -0.8148, -0.8088, -0.8110, -0.8202, -0.8332, -0.8480, -0.8545, -0.8532,
0:         -0.8422, -0.8238, -0.8106, -0.8017, -0.8059, -0.8141, -0.8547, -0.8443, -0.8289, -0.8058, -0.7891, -0.7822,
0:         -0.7832], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3471, 0.4436, 0.5244, 0.4867, 0.3759, 0.5063, 0.5917, 0.5366, 0.5365, 0.4352, 0.3469, 0.3685, 0.3973, 0.4363,
0:         0.4033, 0.4823, 0.6608, 0.6027, 0.4145, 0.4036, 0.4253, 0.4437, 0.3650, 0.5165, 0.6229], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.6889, 1.7629, 1.7927, 1.7758, 1.7416, 1.7130, 1.6800, 1.6227, 1.5392, 1.4448, 1.3585, 1.3029, 1.2774, 1.2759,
0:         1.2816, 1.2835, 1.2823, 1.2752, 1.2584, 1.2387, 1.2276, 1.2368, 1.2612, 1.2881, 1.3059], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1110, -0.1051, -0.1010, -0.0936, -0.0880, -0.0808, -0.0798, -0.0788, -0.0807, -0.1278, -0.1182, -0.1070,
0:         -0.0983, -0.0932, -0.0897, -0.0925, -0.0864, -0.0775, -0.1443, -0.1316, -0.1183, -0.1101, -0.1007, -0.0989,
0:         -0.0968], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.17803630232810974; velocity_v: 0.2582467198371887; specific_humidity: 0.2153303623199463; velocity_z: 0.6415882706642151; temperature: 0.1671203374862671; total_precip: 0.8468677997589111; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20786359906196594; velocity_v: 0.3152530789375305; specific_humidity: 0.16831110417842865; velocity_z: 0.6350810527801514; temperature: 0.12511014938354492; total_precip: 0.8413353562355042; 
0: epoch: 14 [1/5 (20%)]	Loss: 0.84410 : 0.34929 :: 0.20421 (2.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1835334599018097; velocity_v: 0.2738471031188965; specific_humidity: 0.1731829196214676; velocity_z: 0.4357838034629822; temperature: 0.1281471699476242; total_precip: 0.39988788962364197; 
0: epoch: 14 [2/5 (40%)]	Loss: 0.39989 : 0.23292 :: 0.20572 (16.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22448043525218964; velocity_v: 0.2737042307853699; specific_humidity: 0.2708466947078705; velocity_z: 0.5535622239112854; temperature: 1.1426217555999756; total_precip: 0.627330482006073; 
0: epoch: 14 [3/5 (60%)]	Loss: 0.62733 : 0.48111 :: 0.19656 (16.04 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2087615728378296; velocity_v: 0.28010058403015137; specific_humidity: 0.19669479131698608; velocity_z: 0.5021995902061462; temperature: 0.20489253103733063; total_precip: 0.5557708740234375; 
0: epoch: 14 [4/5 (80%)]	Loss: 0.55577 : 0.28986 :: 0.19911 (16.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [7.10487366e-05 6.91413879e-05 6.43730164e-05 5.96046448e-05
0:  5.81741333e-05 5.53131104e-05 4.91142273e-05 4.72068750e-05
0:  4.57763672e-05 4.14848328e-05 3.86238062e-05 3.71932983e-05
0:  3.48091125e-05 2.95639038e-05 2.81333923e-05 2.71797180e-05
0:  2.33650208e-05 2.09808350e-05 2.00271606e-05 1.76429749e-05
0:  1.38282776e-05 1.14440918e-05 1.00135803e-05 9.53674316e-06
0:  8.58306885e-06 6.67572021e-06 6.67572021e-06 6.19888306e-06
0:  5.24520874e-06 4.76837158e-06 4.29153442e-06 3.81469727e-06
0:  3.33786011e-06 2.86102295e-06 2.38418579e-06 1.90734863e-06
0:  1.43051147e-06 1.43051147e-06 9.53674316e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 4.76837158e-07
0:  9.53674316e-07 1.43051147e-06 2.86102295e-06 3.33786011e-06
0:  2.86102295e-06 2.38418579e-06 1.43051147e-06 1.90734863e-06
0:  2.38418579e-06 2.38418579e-06 2.38418579e-06 2.38418579e-06
0:  2.86102295e-06 3.33786011e-06 3.33786011e-06 4.29153442e-06
0:  5.24520874e-06 6.19888306e-06 8.10623169e-06 1.14440918e-05
0:  1.83105469e-04 1.68323517e-04 1.62124634e-04 1.55925751e-04
0:  1.49726868e-04 1.44004822e-04 1.35421753e-04 1.23500824e-04
0:  1.14917755e-04 1.10149384e-04 1.05381012e-04 1.00135803e-04
0:  9.48905945e-05 8.48770142e-05 7.48634338e-05 6.86645508e-05
0:  6.43730164e-05 6.00814819e-05 5.62667847e-05 5.24520874e-05
0:  4.33921814e-05 3.62396240e-05 3.33786011e-05 3.09944153e-05
0:  2.90870667e-05 2.71797180e-05 2.43186951e-05 1.95503235e-05
0:  1.66893005e-05 1.57356262e-05 1.47819519e-05 1.43051147e-05
0:  1.33514404e-05 1.14440918e-05 8.58306885e-06 7.62939453e-06
0:  6.67572021e-06 6.19888306e-06 5.72204590e-06 4.76837158e-06
0:  3.33786011e-06 1.90734863e-06 1.90734863e-06 1.43051147e-06
0:  1.43051147e-06 9.53674316e-07 9.53674316e-07 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 4.76837158e-07 9.53674316e-07
0:  2.38418579e-06 2.86102295e-06 2.86102295e-06 2.86102295e-06]
0: Target values (first 200):
0: [1.43051147e-05 1.23977661e-05 1.14440918e-05 1.04904175e-05
0:  1.14440918e-05 1.33514404e-05 1.43051147e-05 1.71661377e-05
0:  1.90734863e-05 2.19345093e-05 2.28881836e-05 2.28881836e-05
0:  2.19345093e-05 2.19345093e-05 2.38418579e-05 2.67028809e-05
0:  2.67028809e-05 2.67028809e-05 2.76565552e-05 2.86102295e-05
0:  3.05175781e-05 3.05175781e-05 3.05175781e-05 2.86102295e-05
0:  2.67028809e-05 2.28881836e-05 2.00271606e-05 1.62124634e-05
0:  1.33514404e-05 1.04904175e-05 8.58306885e-06 6.67572021e-06
0:  5.72204590e-06 4.76837158e-06 3.81469727e-06 2.86102295e-06
0:  2.86102295e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  9.53674316e-07 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  9.53674316e-07 9.53674316e-07 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 2.86102295e-06 2.86102295e-06
0:  3.81469727e-06 3.81469727e-06 2.86102295e-06 2.86102295e-06
0:  1.62124634e-05 1.43051147e-05 1.23977661e-05 1.04904175e-05
0:  1.14440918e-05 1.23977661e-05 1.33514404e-05 1.52587891e-05
0:  1.71661377e-05 2.09808350e-05 2.38418579e-05 2.38418579e-05
0:  2.28881836e-05 2.47955322e-05 2.38418579e-05 2.19345093e-05
0:  2.19345093e-05 2.28881836e-05 2.57492065e-05 2.76565552e-05
0:  3.05175781e-05 3.24249268e-05 3.24249268e-05 3.24249268e-05
0:  3.05175781e-05 2.67028809e-05 2.38418579e-05 2.00271606e-05
0:  1.52587891e-05 1.23977661e-05 8.58306885e-06 6.67572021e-06
0:  4.76837158e-06 2.86102295e-06 2.86102295e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 0.00000000e+00]
0: Prediction values (first 20):
0: [-4.101704   -4.602155   -4.7908187  -4.680281   -4.4222245  -4.1655564
0:  -3.766087   -3.3873658  -3.1561532  -2.8875012  -2.7895055  -2.7605662
0:  -2.6394668  -2.2833247  -1.7344241  -1.0624719  -0.5427213  -0.29986143
0:  -1.557425   -1.5160966 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.183, max = 3.401, mean = 0.515
0:          sample (first 20): tensor([-0.9283, -0.9701, -0.9859, -0.9767, -0.9551, -0.9336, -0.9002, -0.8685, -0.8492, -0.8267, -0.8185, -0.8161,
0:         -0.8060, -0.7762, -0.7303, -0.6741, -0.6306, -0.6103, -0.8588, -0.9203])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.2172923 6.3838935 6.7672153 7.3768744 7.937929  8.340571  8.57195
0:  8.7654705 8.671398  8.622762  8.478867  8.08074   7.721401  7.3132977
0:  7.1192613 7.20509   7.533295  7.883292  6.268979  6.189869 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.304968  4.2391796 4.3482065 4.517111  4.7242794 4.792457  5.177669
0:  5.5712423 5.9237423 6.216755  6.1845455 5.897411  5.6087117 5.6085887
0:  5.891146  6.502697  7.165259  7.7079315 7.865383  7.913534 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.81857  20.765465 20.665306 20.578537 20.502934 20.327385 20.681063
0:  20.990837 21.332424 21.521595 21.272738 20.977707 20.692478 20.838955
0:  21.356443 22.116282 22.84903  23.197292 21.222986 21.173203]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.74812  24.41378  23.926266 23.432331 22.977093 22.353569 22.610521
0:  22.720554 22.994316 22.92976  22.149052 21.440937 20.75695  20.65322
0:  21.121256 21.876291 22.457628 22.546782 21.091915 20.687607]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.05358  20.34459  20.552153 20.516394 20.360664 20.145615 20.415813
0:  20.755428 21.171492 21.505001 21.4901   21.465845 21.53594  22.040232
0:  22.932688 24.07673  25.172575 26.001926 25.63761  26.023682]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.82251   12.6820135 12.527869  12.370218  12.2142315 11.880518
0:  12.059725  12.101564  12.203294  12.13273   11.677113  11.205376
0:  10.857262  11.071323  11.814049  12.995588  14.289829  15.291006
0:  14.906368  15.0047   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5860572 1.6235809 1.8860455 2.3767471 2.814126  3.0846283 3.1205478
0:  3.182567  2.9929633 2.9080915 2.7759142 2.3226013 1.8603301 1.3388562
0:  1.0491061 1.2672238 1.9600844 2.8461366 1.6177087 1.3782606]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.552029 23.81409  23.88792  23.809265 23.703938 23.567978 24.370102
0:  25.160835 26.01958  26.531204 26.276363 26.028297 25.808945 26.36182
0:  27.538692 29.016022 30.19334  30.651348 30.83163  30.773632]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.854682   4.770639   4.9738264  5.332038   5.7103367  5.9868975
0:   6.664519   7.302137   8.009725   8.63774    8.929855   9.060462
0:   9.247194   9.714771  10.506651  11.602493  12.722645  13.69517
0:  11.8706455 12.278476 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.7252035  9.395201   9.088163   8.817283   8.594354   8.209361
0:   8.428655   8.596656   9.054678   9.40486    9.457714   9.582626
0:   9.910612  10.732447  12.143394  13.927835  15.772795  17.27108
0:  18.348795  18.519054 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.05993   9.154384  9.378225  9.686268  9.790194  9.613995  9.224622
0:  8.845001  8.231336  7.711875  7.0978527 6.1624913 5.238724  4.287212
0:  3.672354  3.59806   4.040844  4.665022  2.7952547 2.5634513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0153213 -3.9161792 -3.5118155 -2.9431968 -2.4235253 -2.0953984
0:  -1.8384662 -1.6663022 -1.8386836 -2.0128236 -2.4170318 -3.0844808
0:  -3.6070914 -4.030226  -4.115355  -3.9019456 -3.6208901 -3.3999734
0:  -4.202499  -4.0429177]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.630351  10.7797365 10.937207  10.968548  10.893216  10.669806
0:  10.869738  11.064305  11.286995  11.403387  11.046286  10.550409
0:  10.020071   9.933078  10.275615  11.022101  11.749893  12.247417
0:  10.652063  10.833324 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.29120255 -0.44475174 -0.31942368  0.11972904  0.59415674  0.8937955
0:   0.87196445  0.7547579   0.20607424 -0.26360512 -0.7884412  -1.5748878
0:  -2.2114434  -2.7949376  -3.0166316  -2.7396283  -1.9950175  -1.1259265
0:  -1.8312964  -1.8816915 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.64055157 -0.40397215  0.0241766   0.50583124  0.92767715  1.1653905
0:   1.6085963   1.9834714   2.268207    2.506206    2.415669    2.2077737
0:   2.0039415   2.1305199   2.5442128   3.166129    3.6044667   3.7247326
0:   2.477141    2.7187486 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.237973  -11.526765  -11.558303  -11.368701  -11.142045  -11.0477495
0:  -10.724639  -10.4659605 -10.191026   -9.974005   -9.996577  -10.172004
0:  -10.275662  -10.051862   -9.518828   -8.659978   -7.8057528  -7.1060457
0:   -9.178532   -9.317433 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.483675  5.2933583 5.2695847 5.340373  5.362505  5.418848  5.578869
0:  5.9452195 6.203315  6.5191483 6.65109   6.4325027 6.071379  5.631079
0:  5.3951874 5.45228   5.79307   6.254363  3.233673  2.8869214]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.518204 33.93636  32.20414  30.692265 29.36952  28.171322 27.723345
0:  27.031462 26.182001 24.802313 22.716114 20.577768 18.803797 17.808296
0:  17.718033 18.151562 18.725458 18.962288 16.85034  16.26352 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.8032403   2.9850092   3.4913926   4.1593757   4.6381817   4.7870927
0:   4.6921577   4.3841486   3.7005758   2.9511294   1.9930372   0.7902942
0:  -0.22646046 -1.0458016  -1.426579   -1.3518863  -1.1119084  -0.9128747
0:  -2.8957353  -2.9476752 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.092491 20.051813 19.975998 19.766666 19.331856 18.433353 17.936596
0:  17.188705 16.438091 15.583666 14.320932 13.144992 12.160362 11.869572
0:  12.179465 12.981451 13.683048 14.016907 11.578235 11.317793]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.8317394 3.2985635 4.011874  4.82171   5.5014486 6.0203066 6.438925
0:  6.8532047 7.0451636 7.2547007 7.364566  7.129831  6.968405  6.822789
0:  6.8688836 7.278639  7.867077  8.400807  6.8470526 6.8597894]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.7520809   0.56635714  0.5484204   0.8181782   1.1750846   1.4743066
0:   1.4034548   1.2939606   0.6909852   0.14446878 -0.4965291  -1.4210038
0:  -2.2972288  -3.173215   -3.7163515  -3.6485767  -2.9925718  -2.0030837
0:  -2.949294   -3.1312418 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.701846  10.919506  10.330809   9.937703   9.645942   9.472286
0:   9.624237   9.931427  10.204917  10.516264  10.663302  10.6679945
0:  10.857046  11.279457  12.052325  12.987624  13.832283  14.2305565
0:   9.695159   8.432997 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.295885  19.831968  19.58678   19.405373  19.061811  18.379921
0:  17.921394  17.34885   16.794111  16.114084  15.014311  13.696767
0:  12.45801   11.455304  11.038956  11.108652  11.455727  11.7443695
0:   9.54879    9.23457  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.0744762 1.3394189 1.7188716 2.0511136 2.3151617 2.4377217 2.8514662
0:  3.2037442 3.4659133 3.5719223 3.2715302 2.7647521 2.3835936 2.3824153
0:  2.8096714 3.5465927 4.1813345 4.568009  1.5201378 1.7326331]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8000464 -2.8697667 -2.6663928 -2.1954322 -1.7493978 -1.4249768
0:  -1.3110471 -1.1555295 -1.3096051 -1.4246411 -1.6661601 -2.241939
0:  -2.767652  -3.3063092 -3.5147405 -3.243816  -2.618812  -1.8666959
0:  -2.5168653 -2.586989 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [44.72266  45.306347 45.915615 46.484367 46.664627 46.30099  45.930958
0:  45.19203  44.176144 42.7856   40.807854 38.7012   36.899387 35.269775
0:  34.300766 33.49603  32.656063 31.765932 29.273275 28.87616 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.9265954 3.0781624 3.508148  4.134386  4.678132  5.0766616 5.2630477
0:  5.4723477 5.429888  5.4272904 5.3166127 4.8560658 4.385661  3.857713
0:  3.614047  3.8271654 4.4326844 5.140149  3.4111037 3.3440144]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1850433   1.1728163   1.3789759   1.6660542   1.9327483   2.045116
0:   2.1768832   2.1703396   1.8717098   1.478426    0.9002609   0.21740866
0:  -0.16011763 -0.1205554   0.3818164   1.2828751   2.2521353   3.0974
0:   1.5556908   1.7184029 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.138148  8.109077  8.164234  8.200309  8.190452  8.169041  8.567244
0:   9.002477  9.350475  9.571501  9.361513  8.970796  8.669469  8.887994
0:   9.564423 10.555977 11.415386 11.853262  9.637208  9.924278]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.751778  17.33444   16.813042  16.17758   15.5273075 14.754053
0:  14.687654  14.584903  14.62237   14.46904   13.7447815 13.069883
0:  12.378538  12.321537  12.826118  13.63341   14.309317  14.511989
0:  13.449722  13.218866 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.8854795 -7.940864  -7.6023393 -6.919456  -6.2531285 -5.7603374
0:  -5.6430087 -5.57254   -5.868632  -6.084643  -6.38184   -7.071984
0:  -7.7104926 -8.377848  -8.724333  -8.412165  -7.5584598 -6.428904
0:  -7.0013576 -7.1482644]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.705095  15.016668  15.299796  15.314322  14.975814  14.312877
0:  13.712238  13.12883   12.523926  11.967621  11.314131  10.5189
0:   9.9867115  9.839078  10.271733  11.290705  12.6564045 14.03196
0:  13.620857  14.048305 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.4819    -4.370373  -3.9135227 -3.1406975 -2.3979921 -1.8640099
0:  -1.6699715 -1.5894794 -1.8981953 -2.1754103 -2.5765505 -3.2923794
0:  -3.9309092 -4.536825  -4.834299  -4.630145  -4.081957  -3.4581308
0:  -4.3880777 -4.5094976]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.27898  23.388813 23.456305 23.366581 23.229795 22.97576  23.287823
0:  23.544647 23.831682 23.97649  23.647886 23.326786 23.084726 23.328438
0:  24.05155  25.04687  25.960588 26.498335 25.613155 25.69193 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.48558  22.674728 22.863113 22.897345 22.751757 22.463339 22.525486
0:  22.64703  22.810112 22.904251 22.662355 22.229027 21.889534 21.753712
0:  22.079304 22.741653 23.59116  24.308022 22.415997 22.77743 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.61331  20.406973 20.333918 20.21771  19.95834  19.566097 19.242214
0:  19.023518 18.773308 18.541647 18.159773 17.625555 17.201954 16.941319
0:  17.06712  17.440977 17.923656 18.2882   15.865629 15.341845]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.728295   8.85109    9.109135   9.443033   9.706768   9.773958
0:  10.20104   10.507534  10.7764435 10.887823  10.558485  10.125224
0:   9.704615   9.743557  10.193881  11.018327  11.811541  12.347066
0:  10.665006  10.66898  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.1859016 7.0557337 7.110418  7.284944  7.423274  7.477433  7.6196723
0:  7.849226  7.954949  8.050712  7.9042177 7.3994966 6.8311734 6.2169943
0:  5.8128777 5.753775  5.932762  6.1793604 4.7082543 4.910995 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.666462 15.709942 15.675823 15.427425 15.078875 14.579838 14.718235
0:  14.84404  15.059893 15.125347 14.653694 14.129488 13.590401 13.622497
0:  14.167676 15.108889 16.016598 16.520622 15.959875 15.976444]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.285921  16.191597  16.08301   15.862413  15.636007  15.324352
0:  15.645798  15.925262  16.238102  16.301413  15.7621    15.2452755
0:  14.728645  14.711191  15.165649  15.860847  16.465467  16.753588
0:  16.244755  16.166433 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.5167866 -6.9146347 -6.982918  -6.654356  -6.1994705 -5.8328495
0:  -5.8098927 -5.819886  -6.241741  -6.560362  -6.887084  -7.5061407
0:  -8.035028  -8.571922  -8.87145   -8.673461  -8.013519  -7.099391
0:  -6.8491507 -7.059269 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.7364991e+00  2.7618511e+00  3.0386639e+00  3.4666505e+00
0:   3.7445810e+00  3.7640924e+00  3.6347277e+00  3.4393959e+00
0:   2.9280963e+00  2.4598122e+00  1.8400626e+00  8.8597250e-01
0:  -3.0469894e-03 -8.2068586e-01 -1.3079658e+00 -1.2549810e+00
0:  -8.5531235e-01 -3.5081100e-01 -2.2058830e+00 -2.2960892e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.086321  7.0018744 7.088559  7.1941843 7.1763897 6.922956  7.0068717
0:  7.034586  7.0965614 7.1103716 6.7515354 6.195773  5.6163116 5.378356
0:  5.5287437 6.114035  6.7539864 7.220023  5.5847745 5.5573015]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.065963  12.501204  13.207752  13.955793  14.387075  14.302809
0:  13.868086  13.092798  11.9768505 10.837772   9.630701   8.323592
0:   7.220539   6.3067455  5.721989   5.490964   5.369607   5.1918416
0:   2.6806407  2.3999004]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.989197   -5.007572   -4.724181   -4.2064085  -3.5648699  -3.0017576
0:  -2.378065   -1.8525152  -1.6321077  -1.4724531  -1.5052643  -1.719903
0:  -1.7869978  -1.6032543  -1.0591826  -0.26336956  0.6866112   1.5090251
0:  -1.5310001  -1.3783741 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.0362906  -4.7572856  -4.1360555  -3.2690778  -2.4661603  -1.8551784
0:  -1.317286   -0.93325377 -0.92604685 -1.0646739  -1.4624858  -2.1218858
0:  -2.558622   -2.6695838  -2.270454   -1.4197555  -0.41120148  0.43444204
0:   0.11661291  0.5977483 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.351813  -8.175324  -7.648408  -6.8466434 -6.119957  -5.674407
0:  -5.484869  -5.450922  -5.7566633 -6.0248384 -6.387377  -6.976427
0:  -7.4009953 -7.723791  -7.780278  -7.432413  -6.9216595 -6.422427
0:  -7.6884336 -7.747384 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.21008205 -0.20370913  0.18147564  0.8789191   1.5953159   2.1761441
0:   2.707305    3.079947    3.1340408   3.1383634   2.8954806   2.3698764
0:   1.8781691   1.4289517   1.1820078   1.2159915   1.3426208   1.4671807
0:   0.22179413  0.1667881 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.5634742  2.6179993  2.8139708  3.1429992  3.4156058  3.5381756
0:  3.45954    3.3390622  2.926999   2.5780847  2.1643581  1.5473857
0:  1.0289664  0.58534193 0.41891813 0.6322918  1.1241221  1.6987243
0:  0.31717587 0.34171152]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.983453  13.392061  13.003853  12.783218  12.530351  12.064078
0:  11.781756  11.412277  11.044111  10.625167   9.981213   9.260144
0:   8.61683    8.20336    8.190277   8.522247   8.999363   9.400332
0:   7.8900557  7.735751 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7283826  -1.872231   -1.6405444  -0.99436665 -0.2920065   0.27093363
0:   0.4697194   0.65864325  0.38931656  0.16709185 -0.17462301 -0.9467244
0:  -1.7148757  -2.534346   -3.0417953  -2.9095979  -2.1773267  -1.1534524
0:  -2.108294   -2.3898711 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.795863  12.076266  12.472738  12.767626  12.848833  12.701653
0:  12.736208  12.774422  12.753399  12.6901455 12.292999  11.670379
0:  11.074598  10.823187  10.996021  11.548439  12.16957   12.619171
0:  11.211702  11.418346 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3150134  -2.3338628  -1.9962611  -1.3240151  -0.53679276  0.16917706
0:   0.88957167  1.5085583   1.82144     2.0269206   1.9920774   1.6711886
0:   1.3774848   1.0623727   0.9215474   0.9179411   0.94283056  0.87574625
0:  -1.0505505  -1.0606933 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.229371   -2.1092367  -1.6731615  -0.9124031  -0.10837126  0.55398035
0:   1.1023698   1.5948949   1.7712278   1.9245887   1.885674    1.465539
0:   1.0589552   0.70372295  0.64194345  1.0185809   1.6813092   2.3243709
0:   0.8655877   0.9761362 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6346669 -1.6307454 -1.3168383 -0.8471632 -0.5696292 -0.6053567
0:  -0.7425256 -1.0032711 -1.4764509 -1.9073238 -2.4895034 -3.2801852
0:  -3.948666  -4.360558  -4.4002376 -3.9636025 -3.3790174 -2.8211255
0:  -3.61305   -3.319758 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.2300024 -4.1893644 -3.8475018 -3.2303114 -2.6042595 -2.166813
0:  -1.974638  -1.8410044 -2.0867863 -2.2633018 -2.5284882 -3.121004
0:  -3.5875807 -4.0658264 -4.288478  -4.0630693 -3.5498905 -2.8665304
0:  -3.315299  -3.3863525]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.5434837 2.6951952 3.1349306 3.7564926 4.270176  4.593999  4.8237085
0:  4.9000525 4.6929913 4.4857593 4.094694  3.4908848 3.016663  2.6918
0:  2.6479988 2.9120045 3.2253222 3.4958258 1.9944282 2.190312 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.254135  14.9813385 14.896069  14.870436  14.73111   14.401146
0:  14.2507    14.1315975 14.0236435 13.938639  13.621852  13.100606
0:  12.631797  12.364955  12.524994  13.120891  13.958814  14.741146
0:  13.676479  13.447351 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.77758    9.963435  10.3890295 10.868912  11.25667   11.482916
0:  11.82316   12.075022  12.181665  12.245463  12.050734  11.676729
0:  11.408505  11.398279  11.667259  12.196808  12.664356  12.88325
0:  10.523906  10.538921 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4252918 3.382151  3.5824008 3.9386442 4.2625303 4.4740834 4.874942
0:  5.2009425 5.382573  5.456381  5.278392  4.9163914 4.6376476 4.6330166
0:  4.957895  5.5006375 5.974739  6.2491665 3.9022012 3.862213 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.7700043   0.48914576  0.52005243  0.79444313  1.1255593   1.3770375
0:   1.5271859   1.6060061   1.3943253   1.1479907   0.7933903   0.29681444
0:   0.00439072 -0.09572458  0.13650179  0.68142414  1.3047247   1.8542004
0:  -0.2526102  -0.61323357]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.546576  14.401009  14.183607  13.832142  13.4352865 12.9063015
0:  13.019703  13.063938  13.158091  13.018441  12.308365  11.64006
0:  10.996746  10.97842   11.463587  12.286539  13.031843  13.364292
0:  12.882886  12.774218 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.1362123 1.2275829 1.5944209 2.1768422 2.7400656 3.1760876 3.3910432
0:  3.5141509 3.2314827 3.033174  2.728491  2.14083   1.6422753 1.1799183
0:  1.0759044 1.411922  2.1033669 2.936577  2.4316735 2.5725193]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.809895  18.490198  18.166723  17.817734  17.415028  16.846394
0:  16.61911   16.305954  15.975506  15.490892  14.590066  13.508295
0:  12.4406185 11.754964  11.580055  11.83308   12.238976  12.501785
0:   8.730082   7.9881372]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.635265 16.029837 16.503237 16.981354 17.383213 17.7061   18.496107
0:  19.19453  19.890297 20.471107 20.67019  20.803902 20.951761 21.52665
0:  22.466135 23.49685  24.340237 24.635096 24.894386 25.337519]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.302416 20.980652 20.305182 19.453136 18.712471 17.967724 18.46033
0:  18.956657 19.745922 20.162218 19.791306 19.631935 19.470858 20.104584
0:  21.403122 22.951588 24.214031 24.660324 27.592617 27.747414]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.8170438 4.046627  4.4439526 4.9009585 5.2222543 5.3280277 5.36891
0:  5.450179  5.345072  5.2618403 5.0228944 4.5334334 4.1239414 3.7904894
0:  3.748987  4.080021  4.6180077 5.120248  3.9688077 4.1655207]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5618606 6.6823153 6.952667  7.2872176 7.470647  7.430005  7.5724955
0:  7.66033   7.6720695 7.616391  7.258821  6.722852  6.199602  5.949288
0:  6.0504875 6.4777794 6.905076  7.1372957 5.3159585 5.3724775]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.33593  16.278528 15.966562 15.413496 14.835033 14.233526 14.621254
0:  14.964733 15.509699 15.77915  15.397612 15.14834  14.903421 15.415752
0:  16.502928 17.872713 18.981    19.440845 21.15112  21.175219]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.501396 27.782993 28.099518 28.3684   28.597366 28.822845 29.429749
0:  30.135563 30.878597 31.553703 31.939236 32.33289  32.940674 33.766632
0:  34.990322 36.155514 37.1205   37.712337 34.07786  34.355125]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.464378   -8.466814   -8.103981   -7.4738374  -6.767388   -6.116072
0:  -5.408566   -4.737927   -4.2725472  -3.7967143  -3.4836326  -3.383677
0:  -3.1345096  -2.8037314  -2.2126498  -1.3533864  -0.49072647  0.31554842
0:  -0.71465063 -0.35872793]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.963674  10.928068  11.077014  11.109392  10.884626  10.333578
0:   9.79232    9.136351   8.368599   7.7147818  6.94161    6.1585493
0:   5.533188   5.2664533  5.3002687  5.619485   5.828066   5.824582
0:   4.2670403  4.0023847]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.9049015 7.773679  7.772441  7.8001475 7.7637897 7.536043  7.7546377
0:  7.950822  8.219127  8.392185  8.101221  7.6719055 7.13029   6.937417
0:  7.189229  7.7349615 8.278268  8.587317  6.8624034 6.649072 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.165861 17.376518 17.646433 17.848583 17.827961 17.595951 17.458715
0:  17.411362 17.38892  17.352812 17.161194 16.794762 16.528316 16.446579
0:  16.772812 17.436127 18.225616 18.903751 16.54662  16.599203]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.941563 32.648663 32.31158  31.977661 31.775406 31.443886 31.938221
0:  32.186882 32.46719  32.46347  31.833466 31.26878  30.874142 31.02657
0:  31.66275  32.55746  33.355675 33.791885 32.984463 32.96274 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.252705  9.361397  9.676966 10.094744 10.393436 10.526431 10.755608
0:  10.951261 11.02594  11.080017 10.896761 10.546152 10.27899  10.194883
0:  10.419243 10.921512 11.425416 11.716475 10.142864 10.255513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.4882336 -6.3683147 -5.9314566 -5.1901875 -4.492751  -4.0542164
0:  -4.1205616 -4.3329306 -5.007183  -5.6062183 -6.2086573 -7.077633
0:  -7.812643  -8.548294  -8.950263  -8.761541  -8.031433  -7.015889
0:  -7.1767454 -7.2871504]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-23.140614 -23.498402 -23.386671 -22.79609  -22.204716 -21.8498
0:  -22.20718  -22.790451 -23.971567 -25.079094 -26.25417  -27.696873
0:  -28.954033 -30.051407 -30.599438 -30.504414 -29.789627 -28.648357
0:  -28.021194 -28.200394]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.922766 15.019588 15.187504 15.345415 15.495056 15.484861 15.955698
0:  16.356956 16.852785 17.211067 17.141521 16.95675  16.802141 16.976383
0:  17.621    18.623306 19.753202 20.699617 20.620363 20.903074]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.732858  -6.8317895 -6.5471587 -6.022693  -5.5009217 -5.1238446
0:  -4.9312186 -4.8646846 -5.096165  -5.332026  -5.6335006 -6.1223664
0:  -6.3723683 -6.44988   -6.1478925 -5.4513755 -4.6475396 -3.9104257
0:  -4.8154235 -4.756655 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4341106 4.516109  4.993584  5.7344995 6.375086  6.824758  7.028968
0:  7.194412  7.1294465 7.0893316 7.0217047 6.671154  6.4068537 6.259066
0:  6.4115868 6.976571  7.836559  8.70514   7.6029935 7.6835485]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.1960099   2.0431058   2.1790864   2.4564946   2.5422342   2.370469
0:   2.1316197   1.8291302   1.2821393   0.7587457   0.01921272 -0.98261213
0:  -1.8433561  -2.4052396  -2.4804978  -2.056603   -1.4685688  -0.9486537
0:  -2.665275   -3.198422  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.105783 25.87958  25.670773 25.466448 25.350674 25.195784 25.556787
0:  25.867321 26.22041  26.3705   26.058567 25.865107 25.876383 26.37068
0:  27.48149  28.840818 30.180979 31.231573 31.743484 32.08185 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.071285   12.676599   12.494133   12.491503   12.476717   12.109989
0:  11.752972   11.179149   10.361586    9.545505    8.454879    7.1778703
0:   5.836743    4.5784082   3.5624516   2.766969    2.215612    1.5687165
0:  -0.29215193 -0.8962784 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.26984215 0.46245766 0.86776066 1.3537774  1.8071551  2.1224551
0:  2.6198287  3.0666268  3.3944604  3.6146564  3.576051   3.3248053
0:  3.2718613  3.5064948  4.1792803  5.126421   6.0301533  6.7493234
0:  5.6976233  5.7957907 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.242571 15.469704 15.843359 16.225082 16.47652  16.600166 16.833183
0:  17.115843 17.32183  17.503727 17.504498 17.29004  17.163416 17.158789
0:  17.523808 18.166044 18.89692  19.486492 17.31937  17.536385]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [41.228855 41.15541  40.872993 40.562153 40.20542  39.615776 39.745316
0:  39.708294 39.828564 39.67266  38.881214 38.183357 37.667454 37.60854
0:  38.1618   38.867455 39.44216  39.528553 37.643417 37.565994]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.911591  -8.960543  -8.603594  -7.853601  -7.0281625 -6.3562617
0:  -6.109654  -6.0552316 -6.485619  -6.856961  -7.2513785 -7.8579526
0:  -8.320087  -8.786381  -9.022972  -8.886333  -8.417292  -7.7776694
0:  -8.848486  -9.051636 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.397102 11.492386 11.746065 12.024231 12.218476 12.284557 12.552758
0:  12.790259 12.964243 13.063804 12.864134 12.514258 12.204097 12.10318
0:  12.348436 12.877627 13.469591 13.902025 12.322824 12.414215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6201177  -1.8615994  -1.8050456  -1.491694   -1.149168   -0.9512534
0:  -0.7527175  -0.65954876 -0.850471   -1.0680895  -1.4976125  -2.192099
0:  -2.7685266  -3.1747127  -3.3303456  -3.1192365  -2.8078418  -2.483808
0:  -5.0204997  -5.1028733 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.08704  17.99221  17.799088 17.59305  17.34138  16.933056 17.215319
0:  17.460394 17.925838 18.205456 17.993898 17.6194   17.182858 17.092428
0:  17.383348 18.01509  18.704905 19.235598 17.033985 17.35286 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.000074 16.153952 16.398636 16.56143  16.502201 16.231886 16.136065
0:  16.037588 15.930023 15.7605   15.379141 14.869194 14.527689 14.441294
0:  14.730045 15.243111 15.797724 16.15612  14.015875 13.877831]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.58317   -4.7496777 -4.5683575 -4.0255375 -3.4751453 -3.1315212
0:  -3.1952868 -3.3751936 -4.029272  -4.639615  -5.3442173 -6.367427
0:  -7.2630167 -8.14456   -8.696244  -8.664842  -8.17042   -7.408165
0:  -8.088562  -8.276245 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.055416 19.013535 18.934547 18.902706 18.988213 19.054955 19.773819
0:  20.384415 20.966953 21.255598 20.948484 20.685413 20.53569  20.914574
0:  21.874317 23.189003 24.552452 25.629425 25.904354 26.246628]
0: validation loss for strategy=forecast at epoch 14 : 0.35969454050064087
0: validation loss for velocity_u : 0.1595437079668045
0: validation loss for velocity_v : 0.2288825660943985
0: validation loss for specific_humidity : 0.15123380720615387
0: validation loss for velocity_z : 0.713017463684082
0: validation loss for temperature : 0.10408312827348709
0: validation loss for total_precip : 0.8014066815376282
0: 15 : 18:57:54 :: batch_size = 96, lr = 1.4508407513079195e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 15, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2644, 0.2525, 0.2404, 0.2279, 0.2149, 0.2016, 0.1882, 0.1747, 0.1610, 0.1472, 0.1334, 0.1198, 0.1064, 0.0931,
0:         0.0801, 0.0673, 0.0550, 0.0431, 0.3747, 0.3617, 0.3481, 0.3340, 0.3195, 0.3046, 0.2892], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3394, 0.3411, 0.3430, 0.3453, 0.3478, 0.3504, 0.3529, 0.3550, 0.3569, 0.3584, 0.3590, 0.3586, 0.3573, 0.3550,
0:         0.3514, 0.3468, 0.3407, 0.3333, 0.2312, 0.2306, 0.2306, 0.2312, 0.2323, 0.2340, 0.2361], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3786, -0.3824, -0.3863, -0.3901, -0.4031, -0.4160, -0.4289, -0.4419, -0.4550, -0.4680, -0.4810, -0.4933,
0:         -0.4986, -0.5019, -0.5052, -0.5085, -0.5118, -0.5150, -0.3659, -0.3653, -0.3648, -0.3665, -0.3789, -0.3904,
0:         -0.4020], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5892, -0.6058, -0.6257, -0.6489, -0.6754, -0.7030, -0.7318, -0.7616, -0.7914, -0.8190, -0.8455, -0.8699,
0:         -0.8908, -0.9096, -0.9251, -0.9372, -0.9450, -0.9505, -0.6135, -0.6279, -0.6445, -0.6622, -0.6809, -0.7008,
0:         -0.7207], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.3936, -1.3955, -1.3977, -1.4001, -1.4025, -1.4055, -1.4083, -1.4115, -1.4153, -1.4189, -1.4233, -1.4275,
0:         -1.4324, -1.4373, -1.4426, -1.4480, -1.4535, -1.4592, -1.4651, -1.4708, -1.4766, -1.4823, -1.4880, -1.4936,
0:         -1.4990], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0580, -0.1149, -0.1695, -0.2265, -0.2265, -0.2288, -0.2288, -0.2312, -0.2312, -0.1292, -0.1482, -0.1695,
0:         -0.2099, -0.2099, -0.2146, -0.2170, -0.2193, -0.2217, -0.1648, -0.1790, -0.1861, -0.1932, -0.2004, -0.2075,
0:         -0.2146], device='cuda:0')
0: [DEBUG] Epoch 15, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2348,     nan,
0:             nan,     nan, -0.2383,     nan, -0.2419,     nan,     nan, -0.2419, -0.2419,     nan,     nan, -0.2443,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2217,     nan, -0.2063,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1660,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1316,     nan,     nan,     nan, -0.2051, -0.2324,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2253, -0.2288,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2205, -0.2229, -0.2253,     nan,     nan,     nan,     nan,     nan,     nan, -0.2110,     nan,     nan,
0:             nan, -0.2431,     nan, -0.2443,     nan,     nan, -0.2431,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0699,     nan, -0.1494,     nan,     nan, -0.2466,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2490,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2431,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2502,     nan,     nan,     nan,     nan, -0.2502,     nan,     nan,     nan, -0.2395,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2431, -0.2443,     nan])
0: [DEBUG] Epoch 15, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.4809,  0.4358,  0.4019,  0.3715,  0.3529,  0.3125,  0.3155,  0.3024,  0.3028,  0.2760,  0.1898,  0.1038,
0:          0.0148, -0.0393, -0.0456, -0.0140,  0.0315,  0.0543,  0.6305,  0.6330,  0.6252,  0.6004,  0.5280,  0.4688,
0:          0.4224], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6752, -0.6081, -0.5673, -0.5463, -0.5360, -0.5506, -0.5964, -0.6382, -0.6487, -0.6374, -0.5948, -0.5353,
0:         -0.4964, -0.4871, -0.5034, -0.4969, -0.4732, -0.4359, -0.8841, -0.8207, -0.7769, -0.7538, -0.7529, -0.7873,
0:         -0.8289], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6971, -0.7059, -0.7148, -0.7174, -0.7132, -0.7113, -0.7135, -0.7126, -0.7126, -0.7122, -0.7120, -0.7090,
0:         -0.6982, -0.6907, -0.6874, -0.6864, -0.6929, -0.6934, -0.7068, -0.7164, -0.7209, -0.7169, -0.7153, -0.7145,
0:         -0.7122], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4865, 1.5626, 1.5772, 1.5099, 1.3802, 1.3786, 1.3146, 1.1585, 1.1114, 1.0441, 0.9830, 1.1015, 1.1733, 0.9796,
0:         0.8050, 0.8431, 0.8443, 0.6482, 1.8336, 1.7306, 1.6554, 1.6447, 1.6000, 1.6621, 1.6119], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.6642, -1.7155, -1.7179, -1.6821, -1.6309, -1.5991, -1.5891, -1.6074, -1.6320, -1.6652, -1.7212, -1.7739,
0:         -1.8161, -1.8211, -1.7977, -1.7686, -1.7664, -1.7865, -1.8128, -1.8257, -1.8307, -1.8405, -1.8649, -1.9046,
0:         -1.9439], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1570, -0.1608, -0.1677, -0.1589, -0.1544, -0.1584, -0.1576, -0.1616, -0.1674, -0.1771, -0.1734, -0.1647,
0:         -0.1623, -0.1590, -0.1644, -0.1708, -0.1715, -0.1773, -0.1888, -0.1771, -0.1786, -0.1708, -0.1638, -0.1734,
0:         -0.1774], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1873830258846283; velocity_v: 0.2776873707771301; specific_humidity: 0.24534785747528076; velocity_z: 0.5355228185653687; temperature: 0.186112642288208; total_precip: 0.34879302978515625; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20882514119148254; velocity_v: 0.3076431453227997; specific_humidity: 0.19384324550628662; velocity_z: 0.5703641176223755; temperature: 0.15576711297035217; total_precip: 0.5685299634933472; 
0: epoch: 15 [1/5 (20%)]	Loss: 0.45866 : 0.28037 :: 0.21052 (2.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20249679684638977; velocity_v: 0.2445053607225418; specific_humidity: 0.20706239342689514; velocity_z: 0.6137619614601135; temperature: 0.1448817104101181; total_precip: 0.5186452269554138; 
0: epoch: 15 [2/5 (40%)]	Loss: 0.51865 : 0.28767 :: 0.20413 (15.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17314431071281433; velocity_v: 0.2511594891548157; specific_humidity: 0.21285071969032288; velocity_z: 0.5496917963027954; temperature: 0.16284681856632233; total_precip: 0.5326305627822876; 
0: epoch: 15 [3/5 (60%)]	Loss: 0.53263 : 0.27933 :: 0.20249 (15.86 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18599119782447815; velocity_v: 0.2789921760559082; specific_humidity: 0.18471823632717133; velocity_z: 0.6436172127723694; temperature: 0.16375665366649628; total_precip: 0.584040105342865; 
0: epoch: 15 [4/5 (80%)]	Loss: 0.58404 : 0.30595 :: 0.20254 (15.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [1.43051147e-06 1.43051147e-06 1.43051147e-06 9.53674316e-07
0:  9.53674316e-07 1.43051147e-06 1.90734863e-06 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.33786011e-06
0:  9.05990601e-06 3.33786011e-06 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 1.90734863e-06
0:  1.90734863e-06 0.00000000e+00 1.43051147e-06 5.24520874e-06
0:  1.04904175e-05 1.04904175e-05 8.10623169e-06 3.81469727e-06
0:  2.38418579e-06 2.38418579e-06 4.29153442e-06 7.15255737e-06
0:  7.15255737e-06 2.86102295e-06 9.05990601e-06 8.10623169e-06
0:  9.53674316e-07 1.43051147e-06 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 4.76837158e-07 0.00000000e+00 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 9.53674316e-07 1.43051147e-06
0:  9.53674316e-07 2.86102295e-06 5.24520874e-06 7.15255737e-06
0:  8.58306885e-06 1.00135803e-05 1.04904175e-05 1.14440918e-05
0:  1.14440918e-05 1.19209290e-05 1.28746033e-05 1.43051147e-05
0:  1.62124634e-05 1.90734863e-05 2.05039978e-05 2.14576721e-05
0:  2.00271606e-05 1.95503235e-05 1.85966492e-05 2.05039978e-05
0:  2.14576721e-05 2.09808350e-05 2.24113464e-05 2.76565552e-05
0:  3.19480896e-05 3.24249268e-05 2.47955322e-05 1.28746033e-05
0:  1.19209290e-05 1.28746033e-05 1.38282776e-05 1.81198120e-05
0:  2.67028809e-05 4.48226929e-05 4.05311584e-05 3.62396240e-05
0:  3.81469727e-06 2.38418579e-06 1.43051147e-06 9.53674316e-07
0:  1.43051147e-06 1.90734863e-06 1.43051147e-06 1.43051147e-06
0:  2.38418579e-06 3.81469727e-06 1.43051147e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 4.76837158e-07 5.24520874e-06
0:  1.47819519e-05 5.24520874e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 4.76837158e-07 1.43051147e-06 8.58306885e-06
0:  2.09808350e-05 1.47819519e-05 9.05990601e-06 4.76837158e-06
0:  3.33786011e-06 2.86102295e-06 3.33786011e-06 6.67572021e-06
0:  8.58306885e-06 8.58306885e-06 6.67572021e-06 5.72204590e-06
0:  5.24520874e-06 3.33786011e-06 2.38418579e-06 1.90734863e-06
0:  9.53674316e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 9.53674316e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 9.53674316e-07
0:  1.43051147e-06 1.43051147e-06 9.53674316e-07 1.90734863e-06
0:  3.33786011e-06 7.62939453e-06 1.23977661e-05 1.66893005e-05
0:  1.81198120e-05 2.00271606e-05 2.19345093e-05 2.09808350e-05
0:  1.81198120e-05 1.38282776e-05 1.38282776e-05 1.52587891e-05
0:  1.76429749e-05 2.05039978e-05 2.24113464e-05 2.33650208e-05
0:  2.43186951e-05 2.62260437e-05 2.95639038e-05 2.90870667e-05]
0: Target values (first 200):
0: [1.71661377e-05 2.09808350e-05 1.71661377e-05 1.90734863e-06
0:  9.53674316e-06 2.09808350e-05 4.19616699e-05 2.86102295e-05
0:  1.81198120e-05 8.58306885e-06 4.76837158e-06 3.81469727e-06
0:  5.72204590e-06 8.58306885e-06 9.53674316e-06 7.62939453e-06
0:  9.53674316e-06 8.58306885e-06 3.81469727e-06 4.76837158e-06
0:  4.76837158e-06 2.86102295e-06 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 9.53674316e-07 0.00000000e+00
0:  3.81469727e-06 3.81469727e-06 3.81469727e-06 8.58306885e-06
0:  6.96182251e-05 2.12669373e-04 3.06129456e-04 3.37600737e-04
0:  3.11851501e-04 2.36511230e-04 1.56402588e-04 3.24249268e-05
0:  1.71661377e-05 1.52587891e-05 3.05175781e-05 3.05175781e-05
0:  2.67028809e-05 1.52587891e-05 1.33514404e-05 1.14440918e-05
0:  1.33514404e-05 9.53674316e-06 8.58306885e-06 1.33514404e-05
0:  1.43051147e-05 2.19345093e-05 3.71932983e-05 3.33786011e-05
0:  4.10079956e-05 6.29425049e-05 3.33786011e-05 2.09808350e-05
0:  1.71661377e-05 2.67028809e-05 5.62667847e-05 1.23977661e-04
0:  8.01086426e-05 8.39233398e-05 1.22070312e-04 6.67572021e-05
0:  4.38690186e-05 2.09808350e-05 2.28881836e-05 1.33514404e-05
0:  1.33514404e-05 9.53674316e-06 9.53674316e-06 1.90734863e-05
0:  1.81198120e-05 2.28881836e-05 3.05175781e-05 8.48770142e-05
0:  7.53402710e-05 9.53674316e-07 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 1.90734863e-06
0:  4.76837158e-06 7.62939453e-06 6.67572021e-06 1.90734863e-06
0:  9.53674316e-07 1.43051147e-05 4.67300415e-05 9.82284546e-05
0:  1.26838684e-04 1.23023987e-04 1.71661377e-04 1.93595886e-04
0:  1.33514404e-05 7.62939453e-06 5.72204590e-06 8.58306885e-06
0:  2.00271606e-05 2.09808350e-05 1.33514404e-05 5.72204590e-06
0:  2.86102295e-06 2.86102295e-06 5.72204590e-06 7.62939453e-06
0:  9.53674316e-06 1.04904175e-05 1.23977661e-05 1.43051147e-05
0:  9.53674316e-06 5.72204590e-06 3.81469727e-06 2.86102295e-06
0:  1.90734863e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 1.90734863e-06 2.86102295e-06
0:  3.24249268e-05 3.14712524e-05 9.53674316e-07 9.53674316e-07
0:  1.14440918e-05 4.19616699e-05 6.86645508e-05 9.15527344e-05
0:  1.11579895e-04 8.01086426e-05 7.15255737e-05 7.43865967e-05
0:  4.86373901e-05 3.91006470e-05 4.76837158e-05 4.57763672e-05
0:  4.38690186e-05 4.19616699e-05 2.57492065e-05 1.52587891e-05
0:  9.53674316e-06 5.72204590e-06 2.86102295e-06 2.86102295e-06
0:  1.90734863e-06 1.90734863e-06 3.81469727e-06 1.90734863e-06
0:  9.53674316e-07 2.86102295e-06 2.86102295e-06 5.72204590e-06
0:  8.58306885e-06 8.58306885e-06 8.58306885e-06 1.33514404e-05
0:  1.81198120e-05 4.57763672e-05 9.15527344e-05 1.14440918e-04
0:  1.55448914e-04 2.05993652e-04 1.07765198e-04 5.53131104e-05
0:  5.62667847e-05 4.19616699e-05 2.95639038e-05 2.09808350e-05
0:  1.33514404e-05 2.09808350e-05 4.19616699e-05 3.71932983e-05
0:  2.76565552e-05 1.43051147e-05 6.67572021e-06 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [-0.01376438  0.03195906  0.3055439   0.58107996  0.77009106  0.84011745
0:   1.3231263   1.883522    2.464223    3.0276463   3.1871898   3.1046283
0:   2.982775    3.2682822   3.8651364   4.7907524   5.539904    5.9160404
0:   5.641101    5.939099  ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -0.982, max = 2.207, mean = 0.473
0:          sample (first 20): tensor([-0.5417, -0.5381, -0.5169, -0.4955, -0.4808, -0.4754, -0.4379, -0.3944, -0.3493, -0.3055, -0.2931, -0.2995,
0:         -0.3090, -0.2868, -0.2405, -0.1686, -0.1104, -0.0812, -0.4828, -0.4702])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.220059  -7.3283086 -7.062939  -6.396878  -5.654074  -5.0366855
0:  -4.729837  -4.551024  -4.8158116 -5.0689893 -5.4214835 -5.985213
0:  -6.3756785 -6.6933913 -6.7375884 -6.4141755 -5.8827434 -5.2883124
0:  -4.821243  -4.729285 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.9104095  7.039764   7.3749394  7.804762   8.173368   8.428805
0:   8.73842    8.927134   8.883901   8.802705   8.556199   8.261103
0:   8.177218   8.337924   8.808967   9.467928  10.040337  10.414753
0:   9.112065   9.397694 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.19845  32.728374 31.885405 30.91687  30.08396  29.22432  29.436255
0:  29.604803 29.946575 29.950153 29.259903 28.799809 28.443165 28.82815
0:  29.883156 31.223206 32.42985  32.999386 35.710346 35.80662 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1047635  -3.873814   -3.3780885  -2.6494613  -1.8940158  -1.3462071
0:  -0.9389367  -0.69562006 -0.8246064  -0.8972602  -1.1216979  -1.3956447
0:  -1.4999175  -1.3447814  -0.8016658  -0.0274806   0.7540417   1.3662009
0:   1.2553334   1.6530743 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.193185   4.4495745  4.9910145  5.637372   6.0956793  6.258021
0:  6.37258    6.431575   6.3104563  6.1621485  5.744919   4.9514284
0:  4.097831   3.3290863  2.8490448  2.7920885  2.9785395  3.1353326
0:  0.73198795 0.6672063 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.855236 17.09578  17.293365 17.317871 17.169876 16.772738 16.906567
0:  17.038893 17.309334 17.420649 17.01573  16.620037 16.210054 16.182682
0:  16.64078  17.297302 17.799892 18.032259 15.945574 16.044212]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.5673556  7.561565   7.862439   8.3621025  8.8628235  9.216219
0:   9.619589   9.929487  10.016071  10.112561  10.017766   9.716527
0:   9.519205   9.487989   9.798249  10.461437  11.271136  11.955443
0:  10.534429  10.491688 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.123783 17.109138 17.247362 17.45662  17.542892 17.454594 17.593475
0:  17.72284  17.832268 17.888474 17.669664 17.232069 16.877392 16.702553
0:  16.931564 17.507473 18.249401 18.834412 15.270496 15.144027]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.861956 27.1097   27.118277 26.890158 26.558586 26.020958 26.215399
0:  26.362894 26.657293 26.779148 26.321852 25.925707 25.553192 25.654932
0:  26.1938   27.04433  27.804955 28.175827 27.671553 28.024912]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.047943 15.217077 15.380251 15.4855   15.481096 15.394323 15.649441
0:  15.961338 16.285725 16.574558 16.544252 16.42168  16.367939 16.63505
0:  17.290766 18.13718  18.910864 19.31091  17.697927 17.729876]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.01809  35.463734 35.681046 35.662415 35.5049   35.050323 35.220467
0:  35.25032  35.352737 35.276142 34.69099  34.2425   33.907394 34.074425
0:  34.594654 35.215885 35.454628 35.151123 32.731663 32.88263 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.857237  20.164133  20.465414  20.474075  20.135038  19.456507
0:  18.693453  17.929575  17.09986   16.381521  15.643589  14.800209
0:  14.126801  13.591419  13.4206295 13.626736  14.070661  14.522095
0:  12.815475  13.126783 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.1092565   3.0405195   3.177152    3.507696    3.7553828   3.8017957
0:   3.6186364   3.4058487   2.8946192   2.4444985   1.9225602   1.0967002
0:   0.2772236  -0.49127245 -0.94803    -0.93006516 -0.48451805  0.10405779
0:  -0.82340336 -1.0927434 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3512044  -2.2735572  -1.9389052  -1.1488204  -0.10761881  0.89340067
0:   1.9929333   2.891072    3.3022842   3.6713583   3.7259774   3.469614
0:   3.3688989   3.2124834   3.2383063   3.4350474   3.6979425   3.8936553
0:   1.9024396   1.5874348 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.9914322  -3.0451994  -2.7201118  -2.0439425  -1.3189869  -0.7257862
0:  -0.3154788   0.04103374  0.03488111  0.01586914 -0.1628952  -0.66796064
0:  -1.0796242  -1.4952221  -1.6665878  -1.4203405  -0.9482517  -0.4252267
0:  -2.102497   -2.138217  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2015336 3.3279102 3.6407597 4.10346   4.458216  4.657255  4.686703
0:  4.6960726 4.4068856 4.149688  3.7324648 3.065538  2.4270258 1.8207183
0:  1.5354671 1.6648479 2.057113  2.4923248 1.5938139 1.5433893]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.404013  11.523615  11.841508  12.224461  12.431198  12.388003
0:  12.21014   11.995499  11.6412735 11.412402  11.131734  10.718616
0:  10.409704  10.148133  10.204799  10.600066  11.144707  11.656315
0:  10.725245  10.801327 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.0749774   0.23274326  0.65445614  1.2146602   1.6453514   1.8634272
0:   1.977149    2.0829873   1.9774618   1.9045987   1.714015    1.2227445
0:   0.8402109   0.51015806  0.4628253   0.81809616  1.3645926   1.8906484
0:   0.04675341 -0.0147562 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.804274   9.759077   9.879676  10.102906  10.3003235 10.34052
0:  10.783123  11.193985  11.700411  12.156114  12.320823  12.369066
0:  12.518718  12.989166  13.76728   14.774622  15.723032  16.316805
0:  14.82807   14.707421 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.9029515   3.1237354   3.5238008   4.0957923   4.519717    4.7133827
0:   4.48296     4.1064343   3.2288036   2.3236861   1.3276024   0.10930252
0:  -0.8981633  -1.6560698  -1.8567009  -1.3272533  -0.19694424  1.0720344
0:   1.0905833   1.0782032 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.58692  -10.56386  -10.092915  -9.302813  -8.592806  -8.151247
0:   -7.963222  -8.037054  -8.504393  -8.934397  -9.490204 -10.2181
0:  -10.730839 -10.998257 -10.917217 -10.415884  -9.831623  -9.231972
0:   -9.222948  -8.945099]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.251487  -6.3253145 -6.030739  -5.277689  -4.402032  -3.5668392
0:  -3.1155472 -2.682024  -2.7094927 -2.6752357 -2.7013159 -3.1250315
0:  -3.546029  -4.097501  -4.4332523 -4.253317  -3.5210638 -2.5111532
0:  -3.0734024 -3.269949 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.751621 11.006693 11.465149 11.920237 12.226019 12.35886  12.731161
0:  13.107342 13.448245 13.768795 13.795108 13.615305 13.463493 13.480452
0:  13.90191  14.677303 15.524322 16.213785 14.097548 14.197445]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.284636 21.443087 21.64106  21.7711   21.685684 21.364187 21.182806
0:  21.01716  20.878178 20.797533 20.617563 20.3298   20.227432 20.313164
0:  20.791912 21.54472  22.432505 23.133953 21.053604 21.342937]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.1179237 4.16908   4.458565  4.906369  5.332493  5.606134  5.8613567
0:  5.990768  5.9203386 5.8816905 5.734117  5.4537926 5.329636  5.2953224
0:  5.463003  5.789688  6.042235  6.1409187 5.042613  5.073045 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.147945 29.796062 29.484867 29.43795  29.646715 29.801395 30.917383
0:  31.847004 32.990242 33.73725  33.847233 34.014694 34.374104 35.192688
0:  36.51174  37.87385  38.964176 39.405884 37.968918 38.060738]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.3405356 7.621098  8.014158  8.544964  8.894007  9.052107  8.929087
0:  8.870946  8.596201  8.479322  8.384033  8.000566  7.596582  7.1548243
0:  7.035888  7.438067  8.397047  9.554557  8.947106  9.210537 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.5470252   0.4612813   0.5199084   0.56880903  0.45583773  0.18502331
0:   0.11421061  0.12100506  0.11868286 -0.00183916 -0.3852992  -0.8864045
0:  -1.2632036  -1.2788129  -0.8247633  -0.06507587  0.6724615   1.2381706
0:  -1.2334924  -1.3020716 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8864522  -2.1836543  -2.1340156  -1.7624726  -1.3040757  -0.91270113
0:  -0.5847659  -0.38214684 -0.43713188 -0.5424514  -0.7622237  -1.1580739
0:  -1.336163   -1.3508983  -1.0486712  -0.42554998  0.2786808   0.9376149
0:  -0.5663934  -0.6345315 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2935    -2.4903975 -2.4510937 -2.2259789 -2.105339  -2.2236528
0:  -2.560999  -3.0455055 -3.8813038 -4.655895  -5.495161  -6.409643
0:  -7.0311317 -7.360656  -7.231311  -6.619438  -5.794695  -4.959014
0:  -4.3593874 -4.0759535]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6472344 2.8123307 3.1847847 3.713755  4.161043  4.400052  4.452957
0:  4.453361  4.1771755 3.9452004 3.6374054 3.0759974 2.6031852 2.1718845
0:  2.003006  2.221952  2.70085   3.2150595 2.0498896 2.0872846]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3856487 4.2697053 4.539422  5.002105  5.403338  5.565387  5.827056
0:  5.963666  6.0726333 6.146441  6.043051  5.850515  5.8116927 6.1777544
0:  6.9503465 8.012381  8.92269   9.3979225 7.036435  6.859764 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7254891  -1.7253232  -1.3636222  -0.76115894 -0.26203632 -0.0352149
0:  -0.11849928 -0.39028835 -1.046494   -1.5598397  -2.121244   -2.8912272
0:  -3.5062432  -4.1348014  -4.5285916  -4.570888   -4.4351144  -4.1464486
0:  -5.193457   -5.4334216 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.261932  12.353527  12.441433  12.355292  11.967787  11.224548
0:  10.738728  10.203314   9.77537    9.38249    8.773692   8.133341
0:   7.6309767  7.623853   8.125818   9.055419  10.074838  10.807974
0:   9.127203   8.874476 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.9611704  4.6728964  5.689701   6.8785996  7.9954734  8.954088
0:  10.022853  11.0012665 11.789259  12.431616  12.738302  12.7862625
0:  12.881715  13.135449  13.717924  14.509505  15.212559  15.621639
0:  13.703588  14.002871 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.957047 20.747282 21.445444 22.071497 22.495872 22.61449  23.114859
0:  23.477617 23.824087 24.030714 23.932339 23.683226 23.635912 23.911161
0:  24.523449 25.331503 26.032982 26.378017 22.834324 22.800371]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.345541 18.638325 19.029825 19.341202 19.50591  19.444532 19.774763
0:  20.193    20.704576 21.141216 21.214218 21.140406 21.131449 21.476418
0:  22.366861 23.665709 25.048836 26.229761 24.515852 24.99421 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.979292  3.9848588 4.197027  4.480106  4.6856656 4.7009506 4.943802
0:  5.1268005 5.2585373 5.3259397 5.0904617 4.6925726 4.379901  4.4380827
0:  4.904355  5.742498  6.5907383 7.2223296 6.0168004 6.0353866]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2271135 3.568789  4.1496983 4.858149  5.5113254 6.059581  6.735899
0:  7.4545627 8.032367  8.564758  8.741784  8.480897  8.013138  7.4899826
0:  7.1522174 7.101717  7.1992197 7.285642  5.818762  6.2975616]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.294764   -2.1574697  -1.7044563  -0.9979658  -0.3582201   0.12352991
0:   0.43474102  0.6678219   0.5904455   0.48709154  0.23875713 -0.2536111
0:  -0.62348557 -0.9171128  -0.9002695  -0.55198765 -0.08439398  0.31692457
0:  -0.8297334  -0.54342365]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.83860207 -0.8351207  -0.5869341  -0.10711908  0.3175335   0.5477519
0:   0.51870537  0.41509867 -0.03051138 -0.44744682 -0.95190716 -1.7211323
0:  -2.373757   -2.9462352  -3.1724534  -2.8406653  -2.0818095  -1.1367593
0:  -1.756288   -1.8323908 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.22407  40.602127 40.78051  40.697693 40.38512  39.848923 39.70628
0:  39.511395 39.331287 39.10749  38.515465 37.925255 37.553104 37.475517
0:  37.833652 38.387066 38.93068  39.244064 36.857548 37.105766]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6089344 -5.7548914 -5.5261374 -5.0758114 -4.5627384 -4.199766
0:  -3.7497826 -3.4000874 -3.2519016 -3.127715  -3.260766  -3.6553526
0:  -3.8980656 -3.950078  -3.6335316 -2.9544992 -2.3059568 -1.8417606
0:  -4.3319035 -4.4208384]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.526766   -9.694374   -9.485556   -8.903034   -8.255083   -7.726332
0:   -7.4959893  -7.435683   -7.8833823  -8.330915   -8.943428   -9.791538
0:  -10.473253  -10.970034  -11.020733  -10.63056    -9.976337   -9.221453
0:   -8.727844   -8.826239 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.502247 35.584904 35.44658  35.175426 34.96322  34.617092 35.04369
0:  35.339687 35.668755 35.740715 35.193634 34.822582 34.52799  34.77141
0:  35.53645  36.464928 37.197933 37.40139  36.177856 36.408794]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.974457   7.175016   6.6341577  6.2966814  5.932249   5.4368434
0:   5.0003123  4.588989   4.113659   3.770146   3.3465273  2.7788367
0:   2.340765   2.080957   2.0458593  2.26959    2.5092971  2.555388
0:  -0.3540144 -0.9500804]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8272185 -3.8538136 -3.5468726 -2.946773  -2.3542066 -1.9512286
0:  -1.7276053 -1.6540971 -2.003594  -2.3496728 -2.8801122 -3.6945882
0:  -4.345038  -4.9152017 -5.128902  -4.931684  -4.526241  -4.051624
0:  -5.1300473 -5.195048 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.173265  -9.485519  -9.460312  -9.0235195 -8.406157  -7.8428226
0:  -7.2448373 -6.6924977 -6.349157  -5.9946513 -5.813311  -5.805636
0:  -5.685572  -5.4338374 -4.969918  -4.3773675 -3.8684382 -3.5263371
0:  -6.426637  -6.572909 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.362256  -6.451235  -6.151884  -5.511512  -4.837469  -4.3594036
0:  -4.2416897 -4.273795  -4.7213697 -5.1387577 -5.6435423 -6.4534206
0:  -7.124614  -7.775078  -8.130553  -8.017706  -7.5519724 -6.9253054
0:  -8.287288  -8.501019 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.470922   -4.475742   -4.1571584  -3.5361428  -2.8026128  -2.1524186
0:  -1.4615483  -0.8561883  -0.54290533 -0.2979145  -0.23890543 -0.42594528
0:  -0.46411324 -0.42618608 -0.15156507  0.36787367  0.8930254   1.293819
0:  -0.69988346 -0.65684795]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.2677755 6.424637  6.8834996 7.4653006 7.9126806 8.121553  8.302705
0:  8.451982  8.526039  8.633378  8.644308  8.438766  8.282283  8.230452
0:  8.398604  8.859915  9.398002  9.799206  7.520317  7.7527   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.3085122 3.3768594 3.651566  4.020285  4.3197913 4.502557  4.84558
0:  5.1752405 5.4043837 5.578549  5.501756  5.234349  5.052217  5.0458994
0:  5.3745923 5.9748187 6.5735755 7.0148363 5.879856  6.003871 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.029797 25.233324 25.398417 25.47142  25.468697 25.333689 25.790665
0:  26.200172 26.704935 27.09456  26.961237 26.898073 26.892105 27.409708
0:  28.41997  29.68948  30.818481 31.55549  31.153315 31.495867]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.83679  32.967407 33.099926 33.157673 33.168728 32.94604  33.189423
0:  33.27538  33.416782 33.404644 32.90453  32.380928 31.94357  31.897778
0:  32.337746 33.08533  33.906113 34.50775  33.383675 33.50987 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.781454 19.826498 19.958025 20.118713 20.23168  20.13688  20.387587
0:  20.483725 20.541475 20.37041  19.810135 19.12803  18.60234  18.400225
0:  18.697546 19.347023 20.135174 20.803587 19.320152 19.463558]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.894692  -4.0680585 -3.863562  -3.3604045 -2.9549346 -2.7162986
0:  -2.7025933 -2.8480153 -3.3588166 -3.804483  -4.3424134 -5.101892
0:  -5.651297  -6.0341954 -5.966043  -5.4224353 -4.66357   -3.87293
0:  -4.557654  -4.7659583]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.646493   8.531532   8.563946   8.594042   8.510796   8.283667
0:   8.196533   8.137119   8.099414   8.074603   7.936511   7.6071153
0:   7.433815   7.484899   7.8493004  8.595807   9.486008  10.29015
0:   8.438844   8.7532   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.247791  -6.553066  -6.474871  -5.962645  -5.3514466 -4.85486
0:  -4.6483045 -4.4737287 -4.679339  -4.8001313 -4.9613204 -5.419514
0:  -5.763457  -6.1518054 -6.3006606 -6.0243278 -5.4087214 -4.675225
0:  -4.543595  -4.571584 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.002037  7.0258822 7.317594  7.782287  8.162388  8.369846  8.506896
0:  8.498713  8.195248  7.905677  7.465217  6.794516  6.249934  5.822674
0:  5.7015405 5.9416475 6.3333435 6.712837  5.3620324 5.4233694]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.62884   6.597435  6.7544193 6.955144  7.0455637 6.931425  7.029863
0:  7.0990686 7.0959625 7.0581875 6.705744  6.167538  5.663623  5.443413
0:  5.5755243 6.085305  6.662375  7.0516677 5.783841  5.854519 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.953186 32.70564  32.095177 31.31302  30.600933 29.7621   30.053669
0:  30.234617 30.589272 30.492384 29.581247 28.880432 28.226534 28.361824
0:  29.249271 30.407274 31.354366 31.536514 33.058037 33.047607]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.380957  8.443099  8.538809  8.710049  8.823036  8.885152  9.032541
0:  9.237716  9.24623   9.209655  8.964692  8.483935  8.142198  7.9328313
0:  8.074294  8.578048  9.25046   9.826606  7.3420134 7.224251 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.566742 11.654207 11.860613 12.139206 12.346833 12.393128 12.685284
0:  12.945345 13.128604 13.24263  13.012545 12.622572 12.264378 12.190223
0:  12.448337 12.984491 13.485201 13.769526 11.18558  11.106558]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.6338086 -6.776812  -6.6129537 -6.1564984 -5.681975  -5.4119077
0:  -5.333383  -5.3499713 -5.650226  -5.8635163 -6.0893817 -6.516104
0:  -6.740515  -6.90231   -6.85368   -6.5640044 -6.3126817 -6.1058383
0:  -6.693977  -6.6599545]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.708752 -14.796537 -14.383015 -13.44425  -12.316004 -11.343567
0:  -10.775511 -10.415716 -10.59676  -10.840233 -11.221291 -11.870804
0:  -12.421404 -12.926958 -13.143116 -12.947821 -12.437582 -11.787062
0:  -11.447926 -11.618622]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.2014585  -4.84911    -4.148441   -3.1705203  -2.1603847  -1.2865386
0:  -0.49403524  0.27080774  0.7840123   1.2971201   1.6902423   1.80687
0:   1.9370551   2.048296    2.332996    2.8830187   3.5394816   4.119604
0:   3.9453027   4.3071766 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.350464  -4.2983227 -3.8922129 -3.1652155 -2.427555  -1.8684063
0:  -1.5476575 -1.3893328 -1.6668892 -1.9421129 -2.3446608 -3.0271797
0:  -3.484868  -3.8343506 -3.806944  -3.2832794 -2.5213728 -1.7019129
0:  -2.28024   -2.1912775]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.51341057 0.68606806 1.0695968  1.5331979  1.8626246  1.988378
0:  2.326086   2.630623   2.9422996  3.2157512  3.2707071  3.1501596
0:  3.1243227  3.4536302  4.1349516  5.1044493  5.9905457  6.552273
0:  4.6820736  4.917049  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.8765054 -7.790795  -7.3783174 -6.6672406 -5.964352  -5.488851
0:  -5.394792  -5.473431  -5.996121  -6.4910464 -7.0436864 -7.817837
0:  -8.406858  -8.898529  -9.093821  -8.846338  -8.320377  -7.667688
0:  -7.8807483 -7.7260995]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.0759506  7.1335664  7.3144364  7.4994545  7.6064467  7.5529666
0:   7.8272443  8.047693   8.2724495  8.412407   8.19508    7.870166
0:   7.589152   7.7087736  8.261076   9.157346  10.125031  10.860071
0:  10.196403  10.353849 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.6691055 7.723629  8.015356  8.328427  8.544082  8.586255  8.816884
0:  8.943925  8.91023   8.802051  8.422407  7.873546  7.4861345 7.4186196
0:  7.7032433 8.312641  8.918353  9.37363   7.4226604 7.555303 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.105947 19.583124 20.187141 20.649635 20.972057 21.091867 21.44768
0:  21.835735 22.366138 22.813974 23.104425 23.292517 23.71129  24.331789
0:  25.336885 26.519518 27.65896  28.581701 26.013742 26.449291]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.095786  12.904318  12.856134  12.98564   13.099247  13.131344
0:  13.055746  12.98192   12.6580105 12.347295  11.877089  11.134936
0:  10.388502   9.670403   9.279925   9.3007345  9.713976  10.236386
0:   8.729553   8.786793 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.024284 31.90115  31.397192 30.545479 29.567667 28.364363 28.164738
0:  27.973291 27.990938 27.82609  27.029415 26.443647 25.95499  26.20697
0:  27.158127 28.397078 29.398836 29.6692   29.299927 29.225847]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.241443  8.321262  8.639666  9.105462  9.511194  9.732036  9.827548
0:  9.913303  9.726065  9.578178  9.299601  8.743604  8.229361  7.6639633
0:  7.3240647 7.3225584 7.5462503 7.791626  5.428835  5.219071 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.709009   7.165936   7.671172   8.1626835  8.530461   8.758772
0:   9.052525   9.434553   9.655337   9.845709   9.796806   9.425312
0:   9.058768   8.802825   8.925969   9.532651  10.488693  11.480257
0:  10.096397  10.504806 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.532895  4.633258  4.900307  5.314654  5.6764708 5.878038  6.29781
0:  6.64943   6.962514  7.174931  7.111675  6.861291  6.656805  6.765073
0:  7.2269793 7.955209  8.686644  9.127441  7.0484295 6.885553 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.003193  11.073438  11.397227  11.799183  12.067434  12.1283245
0:  12.171913  12.11743   11.89937   11.775998  11.515103  11.157172
0:  10.908438  10.80113   11.029229  11.535467  12.091994  12.506866
0:  11.3022995 11.384389 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.1362767  -5.02578    -4.5577226  -3.7514796  -2.8876147  -2.161344
0:  -1.6577353  -1.2039905  -1.1165242  -1.0111661  -0.99664164 -1.3189168
0:  -1.5489945  -1.8134656  -1.8173728  -1.382833   -0.6280718   0.23588848
0:   0.03873491  0.25494146]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5366955  -1.7322397  -1.7599802  -1.7912474  -1.882586   -2.112381
0:  -1.8240099  -1.4940968  -0.98722124 -0.5309024  -0.472754   -0.52036667
0:  -0.65676546 -0.28412533  0.46848297  1.5448532   2.4471564   2.9409997
0:   3.0641217   3.1105406 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2273011  1.1417232  1.2254443  1.4816952  1.7140784  1.8778524
0:  1.8830719  1.9749827  1.8377061  1.810225   1.7494984  1.4320307
0:  1.1782031  0.9043627  0.87105227 1.2448969  1.9777155  2.829988
0:  1.3234491  1.4315848 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.336323 22.329988 22.092329 21.556782 20.98437  20.418886 20.77107
0:  21.186108 21.750214 22.07345  21.754427 21.484547 21.225594 21.672691
0:  22.721146 24.08611  25.272793 25.885704 26.70214  26.91226 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.808859 17.153767 17.701736 18.203564 18.529034 18.617975 19.005615
0:  19.329939 19.619339 19.674376 19.195225 18.397324 17.569954 17.027096
0:  16.881613 17.005766 17.045265 16.843487 14.002274 13.815861]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7628651  -1.9668198  -1.8511496  -1.4547582  -1.0874085  -0.8954158
0:  -0.78281784 -0.8222923  -1.1871696  -1.5851531  -2.1966228  -3.043908
0:  -3.7872744  -4.2491627  -4.3054304  -3.9105673  -3.3293858  -2.7668724
0:  -3.3734436  -3.273879  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.171431  -3.321237  -3.1537042 -2.6360087 -2.0711312 -1.6364303
0:  -1.5657325 -1.5088096 -1.8772502 -2.1429448 -2.4329867 -3.0652022
0:  -3.593779  -4.185228  -4.5073667 -4.2922482 -3.5591455 -2.551311
0:  -3.5909495 -3.7442393]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.510314   9.661609  10.057012  10.515913  10.869329  11.01706
0:  11.358849  11.579572  11.655045  11.7153225 11.489088  11.056197
0:  10.723204  10.691457  11.008147  11.602798  12.120554  12.379075
0:  10.323089  10.4894   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1166663  -1.1352735  -0.8525214  -0.23731136  0.4252243   0.97674656
0:   1.2176533   1.4779401   1.3594127   1.341156    1.2615962   0.82082367
0:   0.38435745 -0.12662554 -0.37308073 -0.04184246  0.84613323  1.9865589
0:   1.7468047   1.7354593 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.987648 38.721107 37.948437 36.946983 35.917225 34.731537 34.52471
0:  34.407974 34.45155  34.101566 32.85867  31.843452 30.729218 30.272537
0:  30.470398 30.713257 30.67004  29.913086 31.11568  31.062794]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3157964 6.3797307 6.601705  7.0184226 7.4024835 7.6475797 7.691325
0:  7.66279   7.2728934 6.9303145 6.4926534 5.907914  5.4368567 5.0595884
0:  4.935143  5.147547  5.559924  5.920876  4.442484  4.4721346]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.485027 36.613503 36.372528 35.802486 35.222378 34.55564  34.95964
0:  35.391357 35.963234 36.212532 35.615902 35.151966 34.69238  34.933994
0:  35.85741  37.066372 38.048138 38.348522 38.642803 38.894478]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.6633596 -4.6191187 -4.250964  -3.6843014 -3.2070723 -2.9667788
0:  -2.786069  -2.7162962 -2.9462967 -3.1567755 -3.5567436 -4.2294264
0:  -4.743512  -5.1359196 -5.215166  -4.8933105 -4.4692526 -4.0625057
0:  -4.8015966 -4.7269993]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.496931 28.766748 28.948833 29.017069 29.008184 28.88419  29.250462
0:  29.561602 29.893812 30.093262 29.85426  29.584564 29.461323 29.709818
0:  30.415007 31.386744 32.331947 33.030464 31.036554 31.22899 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.974782 11.845168 11.71291  11.427227 11.091934 10.591226 10.800316
0:  11.031137 11.405867 11.601341 11.226507 10.757811 10.205486 10.226017
0:  10.736494 11.587038 12.23399  12.397741 10.793215 10.524708]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.806011 23.753342 23.768883 23.810978 23.809204 23.58387  23.86766
0:  23.93185  24.003529 23.801655 22.996319 22.09829  21.342411 20.95671
0:  21.05782  21.511986 21.992542 22.244537 19.986614 19.719582]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.4828005 9.549267  9.7942505 9.999983  9.964226  9.640918  9.372856
0:  9.157931  8.870592  8.590381  8.118109  7.434677  6.8286133 6.4261823
0:  6.4488893 6.9461055 7.6682477 8.32644   6.5905714 6.715781 ]
0: validation loss for strategy=forecast at epoch 15 : 0.2862674593925476
0: validation loss for velocity_u : 0.14304742217063904
0: validation loss for velocity_v : 0.21796061098575592
0: validation loss for specific_humidity : 0.16794681549072266
0: validation loss for velocity_z : 0.49258914589881897
0: validation loss for temperature : 0.10564003139734268
0: validation loss for total_precip : 0.590420663356781
0: 16 : 19:01:52 :: batch_size = 96, lr = 1.4154543915199217e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 16, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2193,  0.2237,  0.2166,  0.1999,  0.1773,  0.1510,  0.1231,  0.0952,  0.0632,  0.0249, -0.0112, -0.0384,
0:         -0.0595, -0.0721, -0.0678, -0.0508, -0.0354, -0.0325,  0.2193,  0.2310,  0.2374,  0.2353,  0.2239,  0.2036,
0:          0.1759], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0897, -1.1533, -1.2196, -1.2801, -1.3263, -1.3626, -1.3991, -1.4433, -1.4978, -1.5603, -1.6272, -1.7044,
0:         -1.8011, -1.9169, -2.0457, -2.1827, -2.3212, -2.4488, -1.0775, -1.1477, -1.2215, -1.2867, -1.3328, -1.3622,
0:         -1.3851], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5029, -0.5167, -0.5337, -0.5505, -0.5659, -0.5774, -0.5874, -0.5948, -0.6037, -0.6170, -0.6265, -0.6304,
0:         -0.6287, -0.6227, -0.6181, -0.6079, -0.5994, -0.5933, -0.5033, -0.5152, -0.5322, -0.5523, -0.5651, -0.5766,
0:         -0.5811], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.1117,  1.0226,  0.9094,  0.6554,  0.3013,  0.0253, -0.0143,  0.0704, -0.1220, -0.7664, -1.4504, -1.5603,
0:         -0.8434,  0.5421,  2.2113,  3.6496,  4.3764,  4.2841,  1.1216,  1.1381,  1.0303,  0.7719,  0.4157,  0.0946,
0:         -0.0407], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2037, -0.1877, -0.1776, -0.1842, -0.2012, -0.2121, -0.2091, -0.1995, -0.2025, -0.2353, -0.2829, -0.3010,
0:         -0.2720, -0.2252, -0.1875, -0.1531, -0.1117, -0.0696, -0.0364, -0.0138, -0.0046, -0.0147, -0.0421, -0.0758,
0:         -0.1061], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441,
0:         -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441, -0.2441,
0:         -0.2441], device='cuda:0')
0: [DEBUG] Epoch 16, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2441,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2441,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2441,
0:             nan,     nan,     nan,     nan,     nan, -0.2441, -0.2441,     nan,     nan,     nan,     nan, -0.2441,
0:             nan,     nan,     nan, -0.2441,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,
0:         -0.2405,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2429,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2441,     nan,
0:             nan,     nan,     nan,     nan, -0.2429,     nan,     nan,     nan,     nan,     nan,     nan, -0.2441,
0:             nan,     nan,     nan,     nan,     nan, -0.2441,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2441, -0.2441,     nan,     nan,     nan, -0.2441,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2429,     nan,     nan, -0.2237,     nan,     nan,     nan,     nan,     nan, -0.2369,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2261,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2249,     nan, -0.2441,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2441,     nan,     nan,     nan,     nan,     nan,     nan, -0.2441,     nan,     nan,
0:             nan, -0.2441,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 16, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2135, 1.1889, 1.1735, 1.1423, 1.1151, 1.0805, 1.1079, 1.1262, 1.1528, 1.1583, 1.1242, 1.0852, 1.0643, 1.0920,
0:         1.1542, 1.2455, 1.3300, 1.3770, 1.2304, 1.2488, 1.2635, 1.2334, 1.1567, 1.0992, 1.0705], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3581, -0.3256, -0.3235, -0.3475, -0.3887, -0.4400, -0.5235, -0.5983, -0.6576, -0.6928, -0.6940, -0.6719,
0:         -0.6774, -0.7376, -0.8265, -0.8895, -0.8914, -0.8434, -0.3948, -0.3570, -0.3549, -0.3900, -0.4478, -0.5165,
0:         -0.5792], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2910, -0.2897, -0.2932, -0.3013, -0.3076, -0.3181, -0.3349, -0.3514, -0.3707, -0.3926, -0.4193, -0.4435,
0:         -0.4520, -0.4620, -0.4789, -0.4877, -0.5139, -0.5244, -0.3062, -0.3036, -0.3103, -0.3159, -0.3329, -0.3524,
0:         -0.3619], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5376, -0.3262, -0.2621, -0.3611, -0.4417, -0.3624, -0.2860, -0.3418, -0.2239, -0.1838, -0.5368, -0.8295,
0:         -0.6617, -0.2166,  0.0265,  0.2534,  0.6184,  0.5975, -0.3669, -0.4462, -0.5732, -0.6282, -0.7296, -0.7957,
0:         -0.7063], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.1627, 1.1859, 1.1995, 1.2009, 1.1999, 1.2030, 1.1978, 1.1773, 1.1459, 1.1170, 1.0978, 1.0967, 1.1062, 1.1204,
0:         1.1263, 1.1181, 1.0920, 1.0530, 1.0074, 0.9671, 0.9449, 0.9464, 0.9633, 0.9826, 0.9900], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1879, -0.1932, -0.2033, -0.1920, -0.1912, -0.1881, -0.1912, -0.1915, -0.1934, -0.2081, -0.2035, -0.1979,
0:         -0.1942, -0.1891, -0.1931, -0.1979, -0.1956, -0.2017, -0.2142, -0.2067, -0.2038, -0.1946, -0.1866, -0.1946,
0:         -0.1981], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2014409303665161; velocity_v: 0.2435959279537201; specific_humidity: 0.20034879446029663; velocity_z: 0.5847892165184021; temperature: 0.1610420048236847; total_precip: 0.7712174654006958; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17973357439041138; velocity_v: 0.2727344036102295; specific_humidity: 0.17848432064056396; velocity_z: 0.4902051091194153; temperature: 0.15506160259246826; total_precip: 0.6841040849685669; 
0: epoch: 16 [1/5 (20%)]	Loss: 0.72766 : 0.31008 :: 0.20097 (2.67 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20177000761032104; velocity_v: 0.26290151476860046; specific_humidity: 0.18589794635772705; velocity_z: 0.49663084745407104; temperature: 0.20130889117717743; total_precip: 0.3602575957775116; 
0: epoch: 16 [2/5 (40%)]	Loss: 0.36026 : 0.25060 :: 0.20028 (15.85 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16509683430194855; velocity_v: 0.26161786913871765; specific_humidity: 0.20114915072917938; velocity_z: 0.49871504306793213; temperature: 0.16174408793449402; total_precip: 0.6928386688232422; 
0: epoch: 16 [3/5 (60%)]	Loss: 0.69284 : 0.29640 :: 0.19594 (15.80 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20925039052963257; velocity_v: 0.2711859345436096; specific_humidity: 0.2308882772922516; velocity_z: 0.6072154641151428; temperature: 0.1683446615934372; total_precip: 0.8235395550727844; 
0: epoch: 16 [4/5 (80%)]	Loss: 0.82354 : 0.34960 :: 0.21121 (15.80 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.0000000e+00 4.7683716e-07 9.5367432e-07 5.2452087e-06 1.4305115e-05
0:  7.3432922e-05 8.7738037e-05 8.9168549e-05 1.6069412e-04 1.4543533e-04
0:  2.3317337e-04 7.1573257e-04 1.8210411e-03 7.1954727e-04 2.4366379e-04
0:  1.7452240e-04 1.9979477e-04 1.4257431e-04 9.5367432e-05 6.7234039e-05
0:  4.5776364e-05 1.4781952e-05 9.5367432e-07 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07
0:  9.5367432e-07 7.1525574e-06 2.3365021e-05 1.8596649e-05 2.7656555e-05
0:  2.9563904e-05 1.0013580e-05 4.8637390e-05 4.5299530e-05 1.9073486e-05
0:  4.9114227e-05 1.1920929e-04 1.3923645e-04 1.2207031e-04 6.3896179e-05
0:  8.4400177e-05 7.6770782e-05 5.4359436e-05 4.9591064e-05 1.9073486e-05
0:  2.4318695e-05 4.4345856e-05 7.0571899e-05 1.4352798e-04 1.6689301e-04
0:  1.8978119e-04 3.3187866e-04 3.2567978e-04 2.9277802e-04 2.3698807e-04
0:  2.4032593e-04 9.3460083e-05 2.0027161e-05 4.7683716e-06 3.3378601e-06
0:  3.3378601e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 3.8146973e-06 4.1961670e-05 6.2465668e-05
0:  6.4849854e-05 4.0531155e-05 1.5735626e-05 5.2452087e-06 0.0000000e+00
0:  0.0000000e+00 1.9073486e-06 5.7220459e-06 9.0599060e-06 1.3828278e-05
0:  2.3365021e-05 2.2411346e-05 1.8596649e-05 1.9073486e-05 7.1525574e-06
0:  2.8610229e-06 1.9073486e-06 4.7683716e-07 4.7683716e-07 1.9073486e-06
0:  5.7220459e-06 3.1471252e-05 7.9631805e-05 6.8187714e-05 3.2424927e-05
0:  3.1948090e-05 2.6702881e-05 1.7166138e-05 5.2452087e-06 5.2452087e-06
0:  5.2452087e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 9.5367432e-07 4.2915344e-06 5.2452087e-06
0:  4.7683716e-06 4.7683716e-06 1.9645691e-04 1.8978119e-04 4.0054321e-04
0:  1.3771057e-03 4.8780441e-04 2.6607513e-04 2.6273727e-04 1.7261505e-04
0:  1.0776520e-04 4.2915344e-05 3.3378601e-06 4.7683716e-06 1.4305115e-06
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 2.8610229e-06 9.0599060e-06 1.8596649e-05
0:  2.8610229e-05 1.9073486e-05 2.8610229e-05 4.6253208e-05 6.2465668e-05
0:  1.6593933e-04 1.4638901e-04 8.3446503e-05 5.7697296e-05 2.7179718e-05
0:  6.5803528e-05 1.0108948e-04 6.3896179e-05 8.1062317e-05 7.2956085e-05
0:  4.8637390e-05 1.3828278e-05 1.5735626e-05 1.7166138e-05 2.5749207e-05
0:  5.3405762e-05 8.4400177e-05 1.1444092e-04 1.3780594e-04 1.4495850e-04
0:  1.7118454e-04 2.0647049e-04 1.9741058e-04 8.0108643e-05 6.1511993e-05
0:  3.9577484e-05 2.0980835e-05 2.0980835e-05 5.2452087e-06 4.7683716e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 2.0503998e-05 4.1961670e-05 5.3405762e-05
0:  3.0517578e-05 1.2397766e-05 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  6.6757202e-06 1.8596649e-05 3.2901764e-05 2.8133392e-05 4.1484836e-05
0:  4.8637390e-05 2.3841858e-05 2.0980835e-05 1.5735626e-05 8.5830688e-06]
0: Target values (first 200):
0: [1.6922951e-03 2.5687218e-03 3.4561157e-03 4.2033195e-03 3.9062500e-03
0:  3.0035973e-03 2.4538040e-03 2.2654533e-03 2.0580292e-03 2.5172234e-03
0:  3.9024353e-03 5.2561760e-03 5.4893494e-03 3.4899714e-03 2.3293495e-03
0:  1.8286705e-03 1.5029907e-03 1.2135506e-03 9.0599054e-04 6.6566467e-04
0:  4.0531158e-04 2.6035309e-04 1.2397766e-04 6.1511993e-05 1.6212463e-05
0:  2.1934509e-05 1.3351440e-05 4.7683716e-07 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 7.6293945e-06 3.0517578e-05 6.3896179e-05 8.0108643e-05
0:  7.5817108e-05 7.2002411e-05 5.9127808e-05 2.4318695e-05 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 3.2424927e-05 3.5285950e-05 4.1484836e-05
0:  2.2029877e-04 2.3126602e-04 1.5449524e-04 1.4066696e-04 6.1035156e-05
0:  2.6226044e-05 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 2.3841858e-06 5.2452087e-06 2.8610229e-06 9.5367432e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 2.3841858e-06 1.5258789e-05
0:  2.3841858e-05 1.6212463e-05 3.3378601e-06 0.0000000e+00 4.7683716e-07
0:  2.8610229e-06 4.0054321e-05 5.8650970e-05 2.8610229e-05 2.7656555e-05
0:  1.8596649e-05 3.8146973e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 1.9073486e-06 4.7683716e-07 4.7683716e-07 4.7683716e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7447472e-03 2.4733543e-03
0:  2.9087067e-03 2.9134750e-03 2.3059845e-03 2.2354126e-03 2.0813942e-03
0:  1.9392967e-03 1.9526482e-03 2.7098656e-03 4.0106773e-03 4.8589706e-03
0:  4.4169426e-03 2.6979446e-03 1.9550323e-03 1.6179085e-03 1.3089180e-03
0:  7.5626373e-04 3.8671494e-04 1.8215179e-04 7.5340271e-05 2.6702881e-05
0:  8.1062317e-06 1.9073486e-06 4.7683716e-07 0.0000000e+00 0.0000000e+00
0:  4.7683716e-07 1.9073486e-06 4.7683716e-07 4.7683716e-07 8.1062317e-06
0:  2.9087067e-05 6.2942505e-05 1.1539459e-04 1.2731552e-04 3.7193298e-05
0:  2.5749207e-05 1.1444092e-05 4.7683716e-07 1.4305115e-06 1.1444092e-05
0:  1.1253357e-04 2.0313263e-04 2.0217896e-04 2.3317337e-04 1.9741058e-04
0:  1.2350082e-04 3.9100647e-05 1.0013580e-05 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 2.3841858e-06 1.9073486e-05 6.0081482e-05 3.3378601e-05
0:  1.1444092e-05 4.7683716e-07 1.4305115e-06 1.4305115e-05 2.7656555e-05
0:  2.8133392e-05 0.0000000e+00 2.2411346e-05 1.6212463e-05 2.3841858e-06]
0: Prediction values (first 20):
0: [ 3.276666    3.3839962   3.8400416   4.477263    5.003626    5.223658
0:   5.236327    4.949089    4.207097    3.3625546   2.2857494   0.85460806
0:  -0.45471525 -1.5831618  -2.251289   -2.3168263  -1.8783269  -1.1707482
0:  -1.354075   -1.3101068 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -3.143, max = 2.422, mean = -0.116
0:          sample (first 20): tensor([-0.3170, -0.3081, -0.2705, -0.2178, -0.1743, -0.1561, -0.1551, -0.1788, -0.2401, -0.3099, -0.3989, -0.5172,
0:         -0.6254, -0.7186, -0.7738, -0.7792, -0.7430, -0.6845, -0.2124, -0.2575])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.6919394  7.7438536  7.973875   8.341647   8.686534   8.948299
0:   9.434251   9.957022  10.432277  10.877651  11.101164  11.1147375
0:  11.236801  11.574526  12.238087  13.220854  14.24638   15.059445
0:  15.500372  15.724405 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.7927175  7.991045   8.278242   8.5275345  8.680115   8.703476
0:   9.124182   9.572727  10.060576  10.370288  10.253607   9.9590845
0:   9.645302   9.697598  10.191949  11.031994  11.96582   12.733408
0:  11.918718  12.119226 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.607327  -4.141728  -4.343248  -4.103973  -3.7254338 -3.4224105
0:  -3.381103  -3.443912  -3.8977523 -4.3332677 -4.784076  -5.433313
0:  -5.8499055 -6.103582  -6.028026  -5.4923015 -4.666926  -3.8053737
0:  -3.6471076 -3.8867679]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.94891   11.081482  11.334021  11.56609   11.774618  11.925803
0:  12.519167  13.093431  13.673539  14.0404825 13.922422  13.6151
0:  13.366185  13.636385  14.368934  15.460842  16.485228  17.206165
0:  16.58817   16.997475 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.6331425 6.4866276 6.2856975 5.8503675 5.2718344 4.692415  4.966891
0:  5.3752604 5.9508452 6.2911115 5.921963  5.499205  5.0071445 5.311902
0:  6.195497  7.4083943 8.249605  8.410911  9.419429  9.369707 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.2562003 -4.1955657 -3.8062892 -3.1019874 -2.3916783 -1.8490252
0:  -1.5347381 -1.3367095 -1.5134158 -1.7149959 -2.0363822 -2.6163583
0:  -3.0848804 -3.4542556 -3.5033555 -3.131351  -2.4992652 -1.8192477
0:  -1.8851686 -1.8887935]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.91936  16.965603 17.110298 17.211832 17.08619  16.694618 16.445614
0:  16.221899 16.02782  15.870381 15.540858 15.040517 14.670304 14.53896
0:  14.796307 15.374033 16.001884 16.381218 13.919792 13.864069]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4465392  2.3546038  2.30267    2.2213798  1.9911175  1.5769048
0:  1.3553405  1.1686621  1.00844    0.92091036 0.74004364 0.4288292
0:  0.22226524 0.24520206 0.6190767  1.2148356  1.9014101  2.4584887
0:  1.4380288  1.6555834 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.002628 24.49456  25.037457 25.5713   25.978302 26.320572 26.947815
0:  27.553108 28.11522  28.545193 28.757729 28.851269 29.101723 29.554005
0:  30.306602 31.15699  31.9803   32.584415 30.665062 31.180363]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.330185  -4.3997173 -4.127906  -3.5435867 -2.9163275 -2.3977356
0:  -2.1507874 -1.9710817 -2.180499  -2.4243279 -2.7684479 -3.4254398
0:  -3.953412  -4.4223833 -4.534334  -4.190303  -3.545436  -2.8142834
0:  -3.2464705 -3.3433495]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.069082 17.997232 17.886719 17.644878 17.347372 16.897392 17.318886
0:  17.687826 18.208254 18.493845 18.1084   17.78473  17.443142 17.81779
0:  18.701345 19.857489 20.727345 20.976688 20.71377  20.862877]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.11024  24.911804 24.989693 25.149784 25.195332 25.056255 25.210754
0:  25.392498 25.528751 25.64732  25.426537 24.924828 24.537502 24.333675
0:  24.621086 25.227589 25.87989  26.218212 24.991848 25.251907]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.854971  -11.779005  -11.2754345 -10.487774   -9.712351   -9.216024
0:   -8.719236   -8.480204   -8.499916   -8.457815   -8.584263   -8.78076
0:   -8.84207    -8.543282   -7.981878   -7.2214246  -6.6749854  -6.43543
0:   -7.842859   -7.553016 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.295673 30.296082 29.894375 29.24376  28.615963 27.96836  28.520004
0:  29.078026 29.768383 30.041267 29.38416  28.812222 28.167366 28.271347
0:  29.017471 30.028656 30.784672 30.78936  32.175224 32.324886]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9450822  -1.7789822  -1.3515635  -0.7615719  -0.24979591  0.03874826
0:   0.16106987  0.17930508 -0.10688496 -0.38008404 -0.80026865 -1.4721661
0:  -1.9999356  -2.4210591  -2.5276732  -2.2586093  -1.7560358  -1.240838
0:  -2.1045327  -1.91892   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.09703016 -0.30811834 -0.24080753  0.19171667  0.67573166  1.0855899
0:   1.1447163   1.2641854   1.0780392   1.0725079   1.0855594   0.76071453
0:   0.38025188 -0.15089464 -0.49723148 -0.3009014   0.5061078   1.6386309
0:   1.0950131   1.0107136 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4742188 3.651494  4.0935383 4.6757803 5.225169  5.607091  6.016947
0:  6.3757257 6.5964193 6.7837114 6.786269  6.59904   6.50955   6.552094
0:  6.862326  7.4354687 8.046213  8.495553  6.686391  6.590326 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5600367  -3.5364752  -3.2042408  -2.541912   -1.8275585  -1.2544041
0:  -0.916955   -0.65296793 -0.74843836 -0.7832813  -0.8580346  -1.1972122
0:  -1.4226298  -1.667376   -1.6698151  -1.2839656  -0.6205964   0.12826395
0:  -0.8231387  -0.8896365 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.864742  5.797564  5.8988714 6.1160583 6.249245  6.189165  6.061177
0:  5.924082  5.5870676 5.261048  4.8164215 4.164772  3.6026685 3.185197
0:  3.0716765 3.3361359 3.793291  4.2216535 3.1532936 3.054898 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.390205 26.971687 26.534405 26.296455 26.070675 25.610165 25.808601
0:  25.789715 25.875927 25.652153 24.921112 24.19246  23.686527 23.679886
0:  24.252766 25.064571 25.804802 26.127403 25.923111 25.51521 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.2073593 4.4560995 4.9700747 5.63753   6.1821747 6.493346  6.718088
0:  6.8378863 6.6888537 6.499733  6.12468   5.49835   4.9719057 4.5667906
0:  4.511097  4.832645  5.3190117 5.753961  3.801608  3.7553961]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1893086 -4.523516  -4.5901914 -4.291085  -3.9113355 -3.5601182
0:  -3.4660153 -3.3456588 -3.5703664 -3.7146173 -3.887033  -4.3676972
0:  -4.7453523 -5.1570916 -5.302325  -4.958565  -4.1734996 -3.185163
0:  -2.9645934 -3.0296073]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.155257  9.440276  9.945924 10.459656 10.961916 11.262168 11.752628
0:  12.168626 12.486073 12.761515 12.789878 12.668453 12.635799 12.801478
0:  13.281525 14.085054 14.973033 15.767402 14.317887 14.616632]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.6701288 1.9259844 2.4379807 3.0938785 3.6245627 3.940598  4.1485353
0:  4.246501  4.1187744 4.0309296 3.837301  3.4763658 3.2498412 3.0994124
0:  3.206896  3.566798  3.9948149 4.3217616 2.5068636 2.754148 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -2.9879017  -2.9747381  -2.5924263  -1.9172602  -1.4319491  -1.3413434
0:   -1.6909752  -2.3903112  -3.546177   -4.721936   -6.0279446  -7.5703564
0:   -8.954514  -10.11556   -10.827849  -10.9385195 -10.684451  -10.193692
0:   -9.809149   -9.938032 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.769732   -1.5782905  -1.0644145  -0.37286234  0.26058817  0.70305634
0:   1.1273375   1.4371653   1.5991259   1.7045794   1.690474    1.5108576
0:   1.5113158   1.7293148   2.2258253   3.0182667   3.7955866   4.3723564
0:   2.4693365   2.6661253 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.15414715 0.41778708 1.0313425  1.6286635  2.1148725  2.4689963
0:  3.0682628  3.654295   4.1404104  4.532873   4.5398493  4.2894373
0:  3.9812472  3.963198   4.1899242  4.672658   4.9986553  5.059651
0:  2.8605816  2.9854698 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.121761  17.98983   17.805204  17.440596  16.946703  16.364977
0:  16.271702  16.250729  16.293348  16.30428   15.891136  15.416498
0:  14.951288  15.008501  15.530066  16.373417  17.070238  17.38887
0:  15.2830715 15.181246 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.026142 36.041805 35.78302  35.28957  34.81888  34.224102 34.410202
0:  34.463272 34.5583   34.360683 33.55684  32.890842 32.356262 32.40916
0:  33.008556 33.889217 34.685383 35.09359  36.23994  36.466354]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.617054 19.30728  18.946234 18.596725 18.199072 17.610806 17.596552
0:  17.47046  17.38935  17.09955  16.29648  15.455738 14.68269  14.379635
0:  14.637783 15.315306 16.09788  16.665901 16.222734 15.840832]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.37810612 -0.31527328  0.03138685  0.6374073   1.2077589   1.6302233
0:   1.8873162   2.075986    1.928134    1.7474442   1.4413934   0.8769393
0:   0.4174571   0.00405169 -0.15752554  0.03846931  0.44331408  0.8659735
0:  -0.62686396 -0.65967464]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.5676918  -2.5404377  -2.2227526  -1.6701136  -1.16253    -0.8395281
0:  -0.89481163 -1.0099115  -1.4823437  -1.8753128  -2.3160257  -2.971776
0:  -3.5689259  -4.1082916  -4.333515   -4.089176   -3.4533758  -2.7163448
0:  -3.2287865  -3.2889185 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.56567  39.274296 38.711197 38.160297 37.619915 36.94027  37.397312
0:  37.864834 38.710163 39.270824 39.13358  39.093117 39.178883 39.75255
0:  40.786728 41.79571  42.36001  42.07336  39.92971  38.84886 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.046331   9.644247  10.496283  11.241602  11.662024  11.752248
0:  11.821138  11.98158   12.128485  12.378595  12.510592  12.423209
0:  12.385038  12.4744835 12.926636  13.761389  14.795747  15.736769
0:  13.160727  13.613567 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.411777 32.221104 31.678856 31.008991 30.459084 29.848877 30.374683
0:  30.890755 31.612934 31.965916 31.531948 31.26279  31.00333  31.429089
0:  32.469193 33.740204 34.799904 35.173317 36.633194 36.783813]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.5453944   1.3841233   1.411622    1.569891    1.6348128   1.5672851
0:   1.4528923   1.3455987   1.0329008   0.7428627   0.31011677 -0.3820758
0:  -1.0465584  -1.5597138  -1.703906   -1.3673849  -0.69676256  0.08216763
0:   0.04529762  0.23931313]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7821136  -2.3609157  -1.5005293  -0.2918949   0.91991806  1.9928386
0:   2.9035475   3.679277    4.0173426   4.148842    3.9304597   3.2221503
0:   2.470167    1.6882558   1.1853786   1.1646214   1.4208579   1.7736449
0:  -0.34314823 -0.48979998]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.62647   11.13612   11.833664  12.511883  12.97757   13.202345
0:  13.419455  13.658163  13.85102   14.174582  14.419758  14.348949
0:  14.237858  13.952126  13.580576  13.210617  12.66465   11.953781
0:   6.567831   6.4075203]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.901102   5.9933105  6.3379345  6.873809   7.400125   7.8372993
0:   8.362093   8.87523    9.238333   9.528816   9.587978   9.360868
0:   9.173878   9.086022   9.253099   9.710327  10.265998  10.727065
0:   9.037062   9.098021 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.3490505 15.489147  15.694443  15.841209  15.818129  15.519108
0:  15.329699  15.074356  14.778385  14.458752  13.918479  13.225868
0:  12.590651  12.144155  12.042603  12.274009  12.667647  12.957918
0:  10.960489  10.883085 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.55365   11.267555  11.14837   11.176331  11.142647  10.9081545
0:  10.958965  10.8591175 10.631359  10.192966   9.34301    8.375149
0:   7.560522   7.257657   7.4813566  8.1376295  8.825034   9.303709
0:   7.4269466  7.2439647]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2056355  -2.336763   -2.1778808  -1.6911626  -1.1560807  -0.75051594
0:  -0.6514945  -0.5606704  -0.83492184 -1.0038657  -1.2096753  -1.7429414
0:  -2.2186236  -2.7650456  -3.0602288  -2.8497758  -2.1732965  -1.2777972
0:  -1.7636104  -1.8136334 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.9538   38.144337 36.958546 35.639618 34.375515 33.038376 32.93479
0:  32.95835  33.188747 32.9034   31.844173 30.820004 29.640194 29.409122
0:  30.019371 30.910603 31.496277 30.930096 31.12838  30.867435]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.398592 25.624346 25.818874 25.84967  25.72824  25.459146 25.660423
0:  25.864609 26.19793  26.433744 26.28062  26.026934 25.863214 25.957703
0:  26.425066 27.086962 27.685331 28.023241 25.300797 25.456167]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.0467   35.059437 34.90769  34.659695 34.39655  33.902718 34.169987
0:  34.279434 34.492558 34.415504 33.71148  33.057983 32.447926 32.36773
0:  32.781994 33.439777 33.953407 33.947865 31.69188  31.51567 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.09336805  0.14307928  0.66001034  1.3022933   1.886303    2.3643317
0:   2.9953103   3.5441973   3.9558804   4.2046957   4.143675    4.027852
0:   4.0524683   4.4413505   5.217804    6.1976357   7.002128    7.571199
0:   6.040024    6.445013  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.018535  -4.074696  -3.849156  -3.4906363 -3.3055062 -3.3641791
0:  -3.3517604 -3.5028853 -3.8533874 -4.2453036 -4.900658  -5.793159
0:  -6.45721   -6.710542  -6.452023  -5.743502  -4.947906  -4.3101144
0:  -4.9351535 -4.9158235]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.041569  7.9686146 8.127327  8.377013  8.492167  8.393514  8.3128
0:  8.139963  7.781589  7.423043  6.848509  6.0905895 5.434947  5.012353
0:  4.9507785 5.2481236 5.5878434 5.7589817 4.341523  4.2821803]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.60164  33.76646  33.73482  33.60135  33.4314   33.064556 33.439434
0:  33.633907 33.935425 33.93584  33.28501  32.725594 32.28122  32.256523
0:  32.743084 33.432556 33.97684  34.142372 32.107235 31.981237]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3056798 -3.0828805 -2.64434   -2.1047769 -1.7257133 -1.5196757
0:  -1.5359797 -1.5356612 -1.8028903 -1.992948  -2.2071013 -2.6361003
0:  -2.9217563 -3.1162877 -3.0181155 -2.555574  -1.8658757 -1.1393666
0:  -1.8366151 -1.3097215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.22876   -6.3024387 -5.987878  -5.3564034 -4.7200327 -4.2983427
0:  -4.054307  -4.003277  -4.3843565 -4.754863  -5.2743063 -6.033737
0:  -6.5577116 -7.0057898 -7.1223345 -6.8968306 -6.585124  -6.242259
0:  -6.919894  -6.924491 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.880541   8.824022   8.882164   9.021517   9.183319   9.26109
0:   9.665074  10.055636  10.470568  10.748478  10.719698  10.585171
0:  10.467009  10.646536  11.210255  12.037667  12.88056   13.609653
0:  12.1204815 12.08893  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.448171  12.287212  12.2065525 12.119385  11.995306  11.758488
0:  11.982308  12.181163  12.408787  12.5179205 12.183279  11.774534
0:  11.3984165 11.515278  12.094538  13.029879  13.950562  14.512814
0:  13.689056  13.669222 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.873165 20.95636  20.979034 20.808668 20.498219 20.056568 20.050991
0:  20.034592 20.078728 19.929167 19.365084 18.715195 18.222338 18.123272
0:  18.523693 19.22724  19.9337   20.418375 18.249279 18.199602]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-19.174799 -19.146011 -18.721495 -17.936537 -17.107904 -16.467573
0:  -16.028627 -15.816231 -16.029879 -16.30109  -16.761963 -17.4511
0:  -17.880325 -18.064907 -17.830889 -17.265574 -16.731707 -16.364832
0:  -16.632128 -16.530407]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.182055 21.591457 22.12796  22.598162 22.933357 23.197903 23.479778
0:  23.87584  24.149933 24.377707 24.410563 24.309649 24.398094 24.629261
0:  25.272415 26.038877 26.761923 27.237709 24.96151  25.297735]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.789396  -9.922974  -9.673515  -9.012445  -8.192654  -7.4638076
0:  -7.025252  -6.668389  -6.833633  -6.9324822 -7.102512  -7.556763
0:  -7.885161  -8.216784  -8.237247  -7.7323604 -6.837479  -5.857017
0:  -6.326855  -6.4035277]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.0214   17.471409 18.021275 18.534632 18.877579 18.947504 19.124144
0:  19.246265 19.246159 19.189102 18.910244 18.450752 18.086472 17.892454
0:  18.021837 18.40277  18.855328 19.079645 16.656273 16.340656]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.613975  5.687867  5.9439287 6.290722  6.449259  6.374071  6.311221
0:  6.2359858 6.116458  6.096869  5.981301  5.6611843 5.379361  5.242866
0:  5.3691187 5.823805  6.413105  6.929032  5.900715  6.0387154]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.461908  5.524051  5.8665175 6.3460035 6.666061  6.742442  6.70461
0:  6.635822  6.3942475 6.0972114 5.5853953 4.7902513 4.0134115 3.3871176
0:  3.1457458 3.3711958 3.8607206 4.356724  2.5331216 2.3562016]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1891074 2.1285114 2.2889252 2.5133035 2.7396667 2.9167454 3.439045
0:  4.0048084 4.4451337 4.815996  4.793377  4.6116457 4.357028  4.4492655
0:  4.8090544 5.3180194 5.611724  5.5443826 3.6676538 3.6615806]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.36735   -6.686833  -6.83184   -6.7749095 -6.6871877 -6.6969705
0:  -6.305613  -5.958263  -5.6969275 -5.5411496 -5.8741546 -6.4142575
0:  -7.108134  -7.38497   -7.431192  -7.2194686 -7.0896077 -7.1861787
0:  -9.526609  -9.897621 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8145046 -7.1347885 -7.1111    -6.795515  -6.4461536 -6.2548428
0:  -6.138094  -6.1374617 -6.467496  -6.7434273 -7.165126  -7.704197
0:  -8.020444  -8.074436  -7.7492285 -7.152486  -6.580798  -6.1352177
0:  -8.698095  -9.1074705]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0289145  -1.99368    -1.6657476  -1.1633139  -0.77358913 -0.6061902
0:  -0.54216146 -0.50057554 -0.7014098  -0.8422437  -1.1395969  -1.7237282
0:  -2.2272797  -2.6525583  -2.7550707  -2.420638   -1.8598022  -1.3283691
0:  -2.125595   -2.02849   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.9720778 2.2306604 2.714172  3.2891707 3.7694263 4.123229  4.626509
0:  5.072877  5.3643856 5.607912  5.563048  5.2977324 5.1510286 5.195578
0:  5.584622  6.269099  6.965843  7.5214562 6.6564355 6.9401774]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.27806  21.525873 22.024134 22.521461 22.847082 22.867374 23.11325
0:  23.282825 23.519875 23.650566 23.4301   22.991451 22.669056 22.58664
0:  22.990915 23.79925  24.791693 25.627739 23.691595 23.685633]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.198326 19.935093 19.628855 19.290857 19.019205 18.73868  19.241596
0:  19.794952 20.4866   20.973234 20.814442 20.702679 20.574772 21.029778
0:  22.056074 23.346798 24.486698 25.15821  24.564411 24.57119 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.90412855 -0.7021136  -0.23135996  0.43660355  1.0407224   1.4906678
0:   1.8877144   2.1851802   2.2058077   2.2078862   2.04424     1.6602426
0:   1.4094272   1.2577004   1.4060764   1.8764348   2.4666991   3.0330591
0:   2.0636587   2.28341   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.82101  27.866333 27.784779 27.587807 27.44648  27.217747 27.727585
0:  28.169758 28.64925  28.82817  28.2883   27.860062 27.449505 27.633682
0:  28.387283 29.384624 30.24118  30.63964  30.428886 30.58913 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.217957 26.306835 26.283749 26.142847 25.92174  25.462078 25.524118
0:  25.393147 25.289797 24.948477 24.184864 23.483221 23.086414 23.21324
0:  23.968365 25.170023 26.53231  27.66026  27.262615 27.541243]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.690825  12.896662  13.197805  13.566452  13.773344  13.76568
0:  13.741379  13.584139  13.259143  12.8962345 12.401524  11.783419
0:  11.335581  11.136829  11.340754  11.920067  12.672802  13.385368
0:  12.119524  12.351453 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.239742  13.53273   13.977466  14.52562   14.894323  15.076788
0:  15.126783  15.201742  14.987459  14.781771  14.337968  13.556183
0:  12.704667  11.768132  11.134241  10.8999815 11.013049  11.217329
0:   8.104197   7.772214 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.692703  -6.512216  -5.910169  -5.0175147 -4.151933  -3.511639
0:  -3.136994  -2.9473796 -3.161961  -3.3698354 -3.6508813 -4.141978
0:  -4.4134192 -4.583277  -4.413757  -3.853166  -3.0968308 -2.343305
0:  -3.3395944 -3.2003636]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.521019 22.53138  22.563004 22.530766 22.340609 21.880657 21.715649
0:  21.486408 21.281584 21.002476 20.39281  19.699436 19.107569 18.762806
0:  18.866867 19.320316 19.988815 20.53783  18.822317 18.928219]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.302506  -5.251123  -4.88718   -4.202722  -3.4929461 -2.8971324
0:  -2.4280744 -2.0572648 -2.0738454 -2.1666636 -2.4931302 -3.152555
0:  -3.7670293 -4.3218446 -4.576614  -4.416512  -3.993774  -3.4461398
0:  -4.166528  -4.266834 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.0123334 -7.089768  -6.79954   -6.179912  -5.5299697 -5.032868
0:  -4.771868  -4.617453  -4.80815   -4.9700847 -5.2159677 -5.7292285
0:  -6.074713  -6.358248  -6.330149  -5.855331  -5.1140714 -4.3124785
0:  -4.869804  -4.5630164]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.020966   -2.0783644  -1.8588419  -1.3656535  -0.86662626 -0.53512335
0:  -0.4494171  -0.5557666  -1.0712175  -1.6274085  -2.2816858  -3.0863953
0:  -3.7018976  -4.127244   -4.239358   -3.9625235  -3.4807386  -2.9687176
0:  -3.7932253  -3.834508  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1381536  -1.9788518  -1.6609254  -1.1856246  -0.6983876  -0.173594
0:   0.68048334  1.6170077   2.4081135   3.0745955   3.3005626   3.3074255
0:   3.2776651   3.5908995   4.2748456   5.2130327   6.163198    6.8285007
0:   6.666902    7.1670523 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.047614  5.032059  5.3065314 5.6454144 5.8410816 5.8205423 5.9139395
0:  5.9549074 5.896342  5.8717194 5.6126823 5.1380343 4.670119  4.371139
0:  4.3506517 4.636059  4.9422917 5.063299  2.7380915 2.7385533]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.775371   -5.6687694  -5.2352605  -4.573686   -3.8493562  -3.2309737
0:  -2.452602   -1.7377429  -1.2384725  -0.8738289  -0.87716675 -1.1157413
0:  -1.2596307  -1.1861749  -0.8353238  -0.2630167   0.27857828  0.67447996
0:  -0.17790842 -0.0714488 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.073596    2.2445402   2.632215    3.0118592   3.1892173   3.0416074
0:   2.9631891   2.7128422   2.3913655   2.0610752   1.5536027   0.9651432
0:   0.4878297   0.3896389   0.6678629   1.3222556   2.0012774   2.4893432
0:  -0.1887908  -0.20776844]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.1290369 1.3081818 1.7134542 2.3431835 2.950213  3.3983536 3.672244
0:  3.873953  3.7849216 3.7522147 3.6696775 3.3627472 3.1435342 2.951423
0:  2.9892719 3.385517  4.0290155 4.697551  4.184972  4.489085 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.267357 24.356281 24.384678 24.226685 23.960445 23.46109  23.52248
0:  23.531525 23.579937 23.493172 22.83406  22.136969 21.410976 21.127563
0:  21.29058  21.774021 22.207596 22.325436 20.20769  20.105268]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.573956  5.5321894 5.8809247 6.4358416 6.978196  7.297357  7.699973
0:  7.947835  7.9322147 7.7921405 7.286329  6.4633837 5.69625   5.0967026
0:  4.8522463 4.961134  5.162537  5.3385096 4.435713  4.7959356]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.570908 27.92137  28.309973 28.64413  29.022945 29.291513 30.247612
0:  31.157677 32.106636 32.77986  32.84481  32.83411  32.855694 33.31843
0:  34.223473 35.344715 36.35069  36.853542 33.625042 33.978115]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.805229 15.834066 15.758236 15.500431 15.011862 14.354637 13.655127
0:  12.95437  12.217832 11.47662  10.691927  9.799067  9.174197  8.872679
0:   9.066543  9.646801 10.365379 11.094203  9.399077  9.491573]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.639545 26.214443 26.824455 27.494175 28.270378 29.16727  30.534496
0:  31.945133 33.18282  34.23783  34.846806 35.083862 35.224617 35.312737
0:  35.48166  35.771286 36.180412 36.466988 31.766071 32.083824]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3333797 2.5920181 3.0965836 3.6681821 4.034708  4.1369047 4.0841093
0:  4.050517  3.8198721 3.6579397 3.4001784 2.872029  2.4141073 2.0029159
0:  1.8826609 2.1784582 2.7240398 3.2695277 1.5470328 1.5228238]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.235581 13.541171 13.929123 14.323439 14.548854 14.579671 14.516666
0:  14.474641 14.265905 14.109947 13.855724 13.375771 12.935875 12.534565
0:  12.434875 12.695044 13.246677 13.781227 11.924562 12.090307]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.295105 25.568989 25.998344 26.304562 26.341522 26.114315 26.027027
0:  25.901783 25.72514  25.449146 24.88604  24.124668 23.577251 23.298489
0:  23.610703 24.268387 25.04818  25.643837 22.13321  22.24799 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.750618 18.519594 18.306135 18.081322 17.759958 17.256481 17.018356
0:  16.647968 16.290907 15.774828 14.951649 14.143503 13.497076 13.180505
0:  13.290163 13.638752 13.984217 14.150692 11.611741 11.184109]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.026493  18.077494  18.11159   18.192778  18.180082  17.997103
0:  17.611351  17.30229   16.745527  16.36579   15.951609  15.333048
0:  14.656102  13.916276  13.301866  13.001133  13.036286  13.225085
0:  10.5683155 10.332643 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.506887 20.561245 20.679125 20.774725 20.74357  20.483335 20.501167
0:  20.429733 20.258425 19.976395 19.2943   18.47013  17.62236  17.074373
0:  16.936365 17.20975  17.686966 18.05576  15.661575 15.398499]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.236247  4.283414  4.595957  5.129405  5.630264  5.962874  6.0718412
0:  6.0855637 5.6977987 5.301689  4.7553363 3.9526901 3.2830071 2.6596274
0:  2.3634996 2.453127  2.822928  3.2332575 2.1300764 2.1208467]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.132668 28.844727 28.562527 28.24747  27.914242 27.423641 27.282272
0:  26.998798 26.69595  26.331093 25.59858  24.934235 24.405437 24.27671
0:  24.702427 25.458237 26.342884 27.184841 28.230125 28.444153]
0: validation loss for strategy=forecast at epoch 16 : 0.34325677156448364
0: validation loss for velocity_u : 0.15584351122379303
0: validation loss for velocity_v : 0.24457986652851105
0: validation loss for specific_humidity : 0.1774008423089981
0: validation loss for velocity_z : 0.5844117403030396
0: validation loss for temperature : 0.11406586319208145
0: validation loss for total_precip : 0.7832388877868652
0: 17 : 19:05:55 :: batch_size = 96, lr = 1.3809311136779726e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 17, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1874, -1.2011, -1.2143, -1.2265, -1.2381, -1.2490, -1.2589, -1.2683, -1.2770, -1.2856, -1.2940, -1.3028,
0:         -1.3117, -1.3209, -1.3305, -1.3403, -1.3502, -1.3601, -1.1132, -1.1253, -1.1367, -1.1472, -1.1573, -1.1669,
0:         -1.1760], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3021, 0.2956, 0.2900, 0.2856, 0.2822, 0.2800, 0.2796, 0.2814, 0.2860, 0.2936, 0.3048, 0.3190, 0.3362, 0.3565,
0:         0.3792, 0.4039, 0.4307, 0.4590, 0.2878, 0.2834, 0.2802, 0.2780, 0.2769, 0.2769, 0.2782], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6960, -0.7098, -0.7236, -0.7354, -0.7472, -0.7496, -0.7520, -0.7595, -0.7673, -0.7688, -0.7700, -0.7708,
0:         -0.7716, -0.7721, -0.7726, -0.7731, -0.7736, -0.7734, -0.6724, -0.6861, -0.6997, -0.7102, -0.7174, -0.7298,
0:         -0.7397], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9466, -0.9863, -1.0193, -1.0436, -1.0568, -1.0590, -1.0436, -1.0149, -0.9686, -0.9069, -0.8342, -0.7504,
0:         -0.6600, -0.5609, -0.4595, -0.3581, -0.2589, -0.1685, -0.8342, -0.8540, -0.8783, -0.9047, -0.9268, -0.9422,
0:         -0.9444], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0333,  0.0159,  0.0647,  0.1130,  0.1598,  0.2047,  0.2478,  0.2880,  0.3263,  0.3619,  0.3954,  0.4263,
0:          0.4553,  0.4822,  0.5069,  0.5297,  0.5511,  0.5711,  0.5896,  0.6071,  0.6232,  0.6383,  0.6518,  0.6643,
0:          0.6748], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2256, -0.2290, -0.2324, -0.2380, -0.2437, -0.2437, -0.2437, -0.2437, -0.2425, -0.2358, -0.2391, -0.2425,
0:         -0.2437, -0.2448, -0.2448, -0.2437, -0.2437, -0.2437, -0.2437, -0.2437, -0.2437, -0.2448, -0.2448, -0.2448,
0:         -0.2448], device='cuda:0')
0: [DEBUG] Epoch 17, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2380, -0.2380,     nan,     nan,     nan,     nan,     nan, -0.2403,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2369,     nan,     nan,     nan,     nan,     nan, -0.2358, -0.2346,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2346,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2312,     nan,     nan,     nan, -0.2335,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2369, -0.2301,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2403,     nan,     nan,     nan, -0.2391,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2380,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2369,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2346,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2335,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2312,     nan,     nan,     nan,
0:             nan, -0.2425,     nan, -0.2346,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2391,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2414,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2358, -0.2358,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 17, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9340, -0.9333, -0.9190, -0.9020, -0.8933, -0.8962, -0.8816, -0.8678, -0.8626, -0.8662, -0.8976, -0.9397,
0:         -0.9666, -0.9613, -0.9201, -0.8536, -0.7897, -0.7541, -0.8479, -0.8696, -0.8741, -0.8796, -0.8971, -0.9055,
0:         -0.8971], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1091, -0.0584, -0.0485, -0.0668, -0.0962, -0.1144, -0.1214, -0.1020, -0.0645, -0.0239,  0.0190,  0.0565,
0:          0.0763,  0.0711,  0.0601,  0.0739,  0.1170,  0.1843, -0.1552, -0.0987, -0.0767, -0.0933, -0.1195, -0.1436,
0:         -0.1402], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8643, -0.8559, -0.8453, -0.8352, -0.8250, -0.8163, -0.8175, -0.8192, -0.8204, -0.8211, -0.8200, -0.8170,
0:         -0.8094, -0.7997, -0.7977, -0.7993, -0.8102, -0.8144, -0.8524, -0.8339, -0.8157, -0.8003, -0.7945, -0.7955,
0:         -0.7992], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1982, -0.1569, -0.1041, -0.2248, -0.3222, -0.2405, -0.1828, -0.1886, -0.1470, -0.1403, -0.1904, -0.1435,
0:         -0.0299, -0.0509, -0.1294, -0.1100, -0.0867, -0.1027, -0.1933, -0.2955, -0.2891, -0.3533, -0.3859, -0.2925,
0:         -0.2238], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.6975, -0.7268, -0.7217, -0.7018, -0.6739, -0.6643, -0.6645, -0.6718, -0.6776, -0.6829, -0.6939, -0.7082,
0:         -0.7165, -0.7046, -0.6874, -0.6731, -0.6761, -0.6823, -0.6751, -0.6446, -0.6063, -0.5753, -0.5636, -0.5691,
0:         -0.5792], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1924, -0.1951, -0.2044, -0.1958, -0.1896, -0.1880, -0.1916, -0.1932, -0.1963, -0.2100, -0.2028, -0.1979,
0:         -0.1948, -0.1888, -0.1909, -0.1981, -0.1957, -0.2024, -0.2159, -0.2061, -0.2038, -0.1957, -0.1884, -0.1941,
0:         -0.2018], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23460324108600616; velocity_v: 0.29301178455352783; specific_humidity: 0.18782897293567657; velocity_z: 0.5504607558250427; temperature: 0.18694376945495605; total_precip: 0.6072069406509399; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22557130455970764; velocity_v: 0.2772723436355591; specific_humidity: 0.22227518260478973; velocity_z: 0.5852460861206055; temperature: 0.17213600873947144; total_precip: 0.8032172322273254; 
0: epoch: 17 [1/5 (20%)]	Loss: 0.70521 : 0.32667 :: 0.20744 (2.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17618496716022491; velocity_v: 0.26965251564979553; specific_humidity: 0.22985225915908813; velocity_z: 0.5700121521949768; temperature: 0.16362814605236053; total_precip: 0.6827490925788879; 
0: epoch: 17 [2/5 (40%)]	Loss: 0.68275 : 0.31313 :: 0.19985 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.215253084897995; velocity_v: 0.28409889340400696; specific_humidity: 0.2224641591310501; velocity_z: 0.5441274046897888; temperature: 0.18590393662452698; total_precip: 0.6536531448364258; 
0: epoch: 17 [3/5 (60%)]	Loss: 0.65365 : 0.31569 :: 0.20696 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2295076996088028; velocity_v: 0.2845706641674042; specific_humidity: 0.20516952872276306; velocity_z: 0.5247015953063965; temperature: 0.16430053114891052; total_precip: 0.39583495259284973; 
0: epoch: 17 [4/5 (80%)]	Loss: 0.39583 : 0.26603 :: 0.20652 (15.88 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0.]
0: Target values (first 200):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0.]
0: Prediction values (first 20):
0: [15.482887 15.728413 16.140461 16.570127 16.782366 16.731325 16.622429
0:  16.519562 16.279345 16.166798 15.941862 15.441017 14.982933 14.524407
0:  14.387579 14.65206  15.165903 15.659903 14.142006 14.183067]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -0.895, max = 1.320, mean = 0.099
0:          sample (first 20): tensor([0.7100, 0.7305, 0.7650, 0.8009, 0.8187, 0.8144, 0.8053, 0.7967, 0.7766, 0.7672, 0.7484, 0.7065, 0.6681, 0.6298,
0:         0.6183, 0.6405, 0.6834, 0.7248, 0.6921, 0.6849])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.31269  17.417381 17.731112 18.063389 18.156048 17.96143  17.829937
0:  17.73676  17.654652 17.640266 17.465223 17.040096 16.674809 16.388927
0:  16.47914  16.943193 17.599571 18.17144  16.376095 16.064169]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.05530071 -0.1778202  -0.08170366  0.1650753   0.34492636  0.35625505
0:   0.38048983  0.33790445  0.16197538 -0.01503611 -0.36454487 -0.8696871
0:  -1.2743506  -1.4663129  -1.351376   -0.95264435 -0.534379   -0.27719784
0:  -1.6646829  -1.617454  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.991765  11.981912  12.160751  12.42073   12.607901  12.639691
0:  12.741651  12.778448  12.682386  12.565101  12.232713  11.739901
0:  11.319696  11.065018  11.1591835 11.54586   12.020151  12.353926
0:  10.747354  10.761864 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4375486 2.2235336 2.2736082 2.6607933 3.1067681 3.4537354 3.530497
0:  3.5849273 3.2808928 3.0563598 2.7828321 2.2733235 1.8143377 1.3802395
0:  1.1901679 1.4453244 2.0727277 2.8080678 2.032711  1.9794955]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8956561  -0.8682642  -0.5507059   0.02539158  0.58404255  0.98778486
0:   1.1553764   1.2393718   0.9063072   0.57543564  0.1352768  -0.5812564
0:  -1.1201162  -1.5895724  -1.6947536  -1.3481402  -0.66822004  0.12528372
0:  -0.7063222  -0.48521233]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.962969  8.658978  8.611312  8.666842  8.627584  8.371341  8.464071
0:  8.453554  8.500228  8.461919  8.062909  7.4715395 6.952973  6.8148932
0:  7.1269045 7.8867226 8.702882  9.307448  7.382902  7.196564 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6724753 -5.7255893 -5.3989425 -4.8084846 -4.2439227 -3.8706756
0:  -3.6851745 -3.6192832 -3.8601193 -4.085276  -4.412826  -5.0136104
0:  -5.4452977 -5.798942  -5.866092  -5.563391  -5.1326966 -4.672131
0:  -5.3970256 -5.453222 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.188898  -9.12081   -8.668526  -7.8370633 -6.947868  -6.2080245
0:  -5.866426  -5.640041  -5.8951774 -6.168628  -6.5616555 -7.256186
0:  -7.870213  -8.470057  -8.800789  -8.681038  -8.185566  -7.4974523
0:  -8.247856  -8.279306 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.450197  9.551652  9.979739 10.374043 10.58324  10.491346 10.485864
0:  10.416123 10.374592 10.323976 10.123399  9.684559  9.506102  9.540895
0:  10.106798 11.13909  12.395804 13.559891 11.364393 11.510574]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.984726 28.664291 28.063847 27.194614 26.331474 25.392498 25.35939
0:  25.36893  25.5298   25.50404  24.80125  24.260128 23.72831  23.861992
0:  24.616215 25.63718  26.488365 26.817844 27.23869  27.210697]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.48451    9.698614  10.07585   10.464707  10.686802  10.678076
0:  10.701978  10.687363  10.564737  10.441484  10.1513     9.645513
0:   9.2074795  8.938335   9.001583   9.408126   9.971825  10.430768
0:   8.23728    8.308199 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.961178  8.001466  8.263631  8.61898   8.837197  8.794884  8.66302
0:  8.325632  7.79845   7.2448273 6.586591  5.8446937 5.3787336 5.266323
0:  5.6214724 6.40543   7.317792  8.140353  7.7652187 7.785015 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.952083  -6.0139146 -5.7375283 -5.117065  -4.502316  -4.039104
0:  -3.7407012 -3.5428739 -3.7265315 -3.9012465 -4.2589974 -4.893954
0:  -5.403091  -5.7587266 -5.747313  -5.3073983 -4.7187457 -4.122173
0:  -4.1099725 -4.1369596]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.015182 31.605474 32.004707 32.064224 31.831741 31.382385 31.062723
0:  30.734982 30.363375 29.849829 29.083714 28.26669  27.583105 27.228884
0:  27.451176 27.921932 28.43457  28.775906 27.088453 27.23973 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4628096  -2.3113937  -1.8040991  -1.0240769  -0.32410812  0.1247797
0:   0.5200958   0.73018456  0.7049203   0.70785236  0.5786667   0.32573986
0:   0.23016071  0.34990597  0.77398777  1.4813824   2.184713    2.7131584
0:   1.8126707   2.1376204 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.2068853 6.1323853 6.2105937 6.2231793 6.0507717 5.6867914 5.582571
0:  5.4928846 5.4012747 5.3392525 4.9991126 4.503104  4.022901  3.9320672
0:  4.203169  4.780306  5.2781916 5.5445986 3.8488152 3.8503296]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.116172   7.934785   7.8598804  7.842276   7.693197   7.337262
0:  6.896824   6.3510237  5.534348   4.7917976  3.9269543  2.8791413
0:  1.98913    1.2803087  0.9303651  0.9399872  1.1223526  1.3929567
0:  0.33478403 0.64050007]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.661032   -9.71904    -9.378233   -8.699524   -8.01347    -7.5572696
0:   -7.46296    -7.5420747  -8.041334   -8.485525   -8.977629   -9.630686
0:  -10.056311  -10.373657  -10.40839   -10.103062   -9.622488   -9.124329
0:   -9.528019   -9.577663 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.174389 18.872057 18.441925 17.929897 17.45374  16.841455 17.103231
0:  17.299374 17.658108 17.756634 17.18071  16.64104  16.095345 16.171886
0:  16.891582 18.02398  19.129642 19.767166 19.675896 19.561527]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.864977 10.802214 10.97198  11.25695  11.454786 11.431579 11.489864
0:  11.50209  11.38105  11.263911 10.916321 10.320603  9.755886  9.301334
0:   9.159982  9.390896  9.796569 10.104373  8.193445  7.981251]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.460581  14.466383  14.595552  14.675166  14.586468  14.281683
0:  14.251886  14.202752  14.087002  13.9389715 13.429007  12.736578
0:  12.015005  11.64336   11.659892  12.033549  12.387467  12.539715
0:  11.277743  11.329979 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.76890993  0.6726432   0.7816453   1.0367699   1.2429142   1.2815733
0:   1.2620325   1.1707416   0.79616785  0.39741182 -0.18609047 -0.9849067
0:  -1.6517797  -2.1766782  -2.3876967  -2.2716384  -2.040954   -1.8402324
0:  -3.2418494  -3.0841947 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0872102  -0.80215883 -0.3425727   0.23797703  0.7506089   1.1455989
0:   1.3548889   1.4815865   1.2409096   0.93078756  0.49124718 -0.07454729
0:  -0.5151744  -0.81938934 -0.7760925  -0.4974017  -0.07495451  0.33367872
0:  -1.0509973  -0.7983208 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.8850474   1.7833514   1.8400273   2.0416136   2.099391    1.9740143
0:   1.8036757   1.623693    1.2531133   0.94231176  0.54089785 -0.04488993
0:  -0.48527908 -0.7636981  -0.7038808  -0.27105093  0.3178401   0.8584366
0:  -0.5814576  -0.5631013 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4719491  -1.6061273  -1.2620173  -0.5060067   0.20519686  0.7059965
0:   0.89120054  0.7441869  -0.04961729 -1.1460576  -2.6651158  -4.551192
0:  -6.2148585  -7.439201   -7.8663898  -7.400086   -6.4400096  -5.314029
0:  -3.3976731  -3.3209214 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.0396376 2.1540499 2.4506745 2.8035486 2.9969137 2.9952958 2.9867194
0:  2.9558659 2.8145354 2.7175717 2.5066557 2.1355119 1.8507438 1.7478433
0:  1.9947705 2.6001792 3.3841136 4.0981474 2.8932772 3.286912 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.000885 14.819733 14.687308 14.512512 14.240095 13.732647 13.574916
0:  13.331911 13.154432 12.87761  12.254683 11.581268 10.982822 10.810646
0:  11.12679  11.871113 12.71783  13.397245 12.526977 12.558064]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.504504    4.5525174   4.755842    5.009252    5.029295    4.756685
0:   4.28012     3.7407553   2.9491353   2.2367706   1.4846501   0.56357956
0:  -0.1881156  -0.73769474 -0.8831301  -0.51533794  0.22930527  1.0271888
0:  -0.13901043 -0.21014547]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.355656    6.4546075   6.7669086   7.071803    6.98533     6.439763
0:   5.658206    4.9052076   4.0585036   3.3962915   2.7248516   1.7549348
0:   0.8981471   0.15977144 -0.11625051  0.35025215  1.3878989   2.64255
0:   1.7051487   1.7834535 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6050153 -5.754803  -5.419     -4.865107  -4.3891606 -4.1455407
0:  -4.0508647 -4.210069  -4.726007  -5.2075496 -5.870997  -6.711268
0:  -7.3569903 -7.730745  -7.774736  -7.4851947 -7.2299323 -7.10016
0:  -8.655497  -8.774683 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.7120647 6.917171  7.2615314 7.6034904 7.799646  7.7908874 7.9210806
0:  8.014276  8.010665  7.9906917 7.728172  7.353848  7.0364323 6.951546
0:  7.2046256 7.7224517 8.232696  8.565351  7.3898296 7.6535115]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.180592 19.480848 19.873905 20.321568 20.652893 20.775265 20.755913
0:  20.714203 20.52201  20.439327 20.364918 20.104324 20.003204 19.885143
0:  20.057777 20.554909 21.299347 21.89191  20.500294 20.771757]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6497412  -1.6593266  -1.3394232  -0.782248   -0.27534914 -0.01166105
0:  -0.0247035  -0.15930986 -0.6733079  -1.1100745  -1.61307    -2.3641262
0:  -2.97289    -3.531227   -3.831255   -3.7225952  -3.3600254  -2.9480438
0:  -4.4013596  -4.584367  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.793639  8.757065  8.944339  9.247411  9.32832   9.117472  8.718937
0:  8.376536  7.8228803 7.348205  6.7217402 5.7011933 4.675788  3.698505
0:  3.142053  3.245288  3.901411  4.713496  3.6032078 3.5733328]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.831827 28.988071 29.071228 28.899654 28.452585 27.79591  27.527061
0:  27.20767  26.769417 26.062634 24.80775  23.462116 22.367893 21.959167
0:  22.334158 23.092876 23.95781  24.431463 23.317438 23.313364]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.4957304   1.4409275   1.6269231   2.0088382   2.2857637   2.3561692
0:   2.1017852   1.745676    0.98814106  0.20958471 -0.68922615 -1.8488965
0:  -2.8689218  -3.7770638  -4.270551   -4.227772   -3.775724   -3.1644702
0:  -3.7811484  -3.8900847 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.9946203 1.9973311 2.1728437 2.432548  2.656621  2.735459  2.9374647
0:  3.098538  3.1767266 3.2174118 3.0690157 2.827051  2.6960588 2.7827575
0:  3.1481154 3.7588208 4.370025  4.839812  2.8755236 2.9893644]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.023063  10.104032  10.264153  10.510417  10.61377   10.579451
0:  10.40212   10.266639   9.906048   9.651908   9.369765   8.930111
0:   8.5186205  8.1289015  8.014652   8.239201   8.738857   9.334534
0:   8.215141   8.112154 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.083738  9.356444  9.826223 10.270072 10.503536 10.494669 10.51383
0:  10.530802 10.476824 10.430437 10.162566  9.672398  9.218184  8.89258
0:   8.938252  9.35828   9.946782 10.50666   9.003099  9.310951]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.162649   -8.31628    -8.108432   -7.5636644  -6.8964787  -6.286071
0:  -5.5572968  -4.8996353  -4.4926453  -4.1910286  -4.106536   -4.1981792
0:  -4.006794   -3.5407753  -2.6253815  -1.4118433  -0.22827768  0.68717766
0:  -0.49088478 -0.29174995]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5968404 1.4869313 1.6258159 1.9038577 2.0646877 2.0224085 2.0496101
0:  2.0411363 1.9689884 1.9038925 1.7457166 1.4399242 1.3395658 1.5304837
0:  2.0994616 3.0597157 4.0729084 4.888131  3.641041  3.7564788]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.2433558 -6.819872  -6.9895916 -6.683628  -6.1684227 -5.628273
0:  -5.063202  -4.535512  -4.352577  -4.3105526 -4.534317  -5.1470995
0:  -5.6193075 -6.0190353 -6.17214   -5.975197  -5.6955996 -5.4254704
0:  -6.866725  -7.094658 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.836235  -6.9555807 -6.7238493 -6.105872  -5.4069653 -4.879443
0:  -4.75068   -4.7724276 -5.2915306 -5.793701  -6.343295  -7.178006
0:  -7.816552  -8.438224  -8.734246  -8.528745  -7.9313316 -7.1232595
0:  -7.4613028 -7.720558 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2081995   1.2968273   1.6402726   2.1019244   2.3953643   2.4409356
0:   2.4393492   2.3793197   2.1395774   1.8859692   1.4644766   0.8037691
0:   0.21767616 -0.18789244 -0.29118872 -0.04201651  0.31798124  0.58008957
0:  -1.0009322  -1.062717  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.031279  -4.9619136 -4.4984126 -3.6846175 -2.871469  -2.2630582
0:  -1.8578658 -1.6348395 -1.8088336 -1.9514546 -2.2109456 -2.7336078
0:  -3.054523  -3.310326  -3.2731986 -2.8514132 -2.2807794 -1.6918597
0:  -2.5659385 -2.4890943]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.222473  -8.3048    -8.008699  -7.318439  -6.576793  -6.042339
0:  -5.8735957 -5.827683  -6.2191653 -6.587273  -7.0262475 -7.7328897
0:  -8.319462  -8.886396  -9.171673  -9.02738   -8.581192  -8.004053
0:  -7.894954  -7.90935  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.610416 11.777282 12.09058  12.405355 12.626072 12.727193 12.969418
0:  13.170397 13.286541 13.340857 13.212311 12.940449 12.870569 12.991243
0:  13.465439 14.183167 14.9412   15.537685 13.316129 13.563158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.7400527  -0.71418667 -0.4061613   0.1595912   0.7337127   1.1870084
0:   1.5237436   1.7964592   1.7643442   1.7201519   1.5518789   1.1318283
0:   0.8131738   0.53116703  0.48903894  0.76199055  1.2062654   1.6131516
0:  -0.01403236 -0.17937803]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.3612695 1.5306578 1.9988799 2.7225509 3.4545107 4.086502  4.567563
0:  4.9670544 5.037616  5.097022  5.0414176 4.7157583 4.492     4.3021708
0:  4.393243  4.904702  5.6543994 6.4153075 5.321158  5.2927876]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.90725946 1.0247798  1.3223367  1.7943475  2.2065394  2.4836423
0:  2.7591963  2.9767962  2.9739926  2.971457   2.7667568  2.2977006
0:  1.8940465  1.5875587  1.610889   2.0478303  2.7113085  3.3774743
0:  2.8105557  3.3497174 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.9348235  -1.1267066  -1.0553737  -0.7822194  -0.5501685  -0.46572733
0:  -0.47681665 -0.51078653 -0.7732253  -0.98987913 -1.3060527  -1.8693967
0:  -2.2643418  -2.5440478  -2.5304713  -2.094458   -1.4335608  -0.78037405
0:  -1.5742583  -1.5168204 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.7126517 -7.809933  -7.5715394 -6.901811  -6.1335273 -5.5423007
0:  -5.2839465 -5.2617564 -5.7571206 -6.317224  -7.038817  -7.9669976
0:  -8.727346  -9.296446  -9.481482  -9.22127   -8.663099  -8.003149
0:  -8.628395  -8.982494 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.63793   11.476591  11.282576  10.947718  10.49753    9.931587
0:   9.879279   9.894871   9.965476   9.955065   9.490267   8.930507
0:   8.336565   8.194496   8.436668   8.983963   9.434763   9.608777
0:   8.7054205  8.50387  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.784752   4.7493377  4.8846474  5.0725346  5.1937943  5.115244
0:  5.233323   5.2505474  5.1058245  4.7910037  4.086459   3.2042418
0:  2.3704062  1.9577017  1.9346795  2.247251   2.569375   2.670165
0:  1.0379419  0.82157516]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.399704    2.4889314   2.8113985   3.4010947   3.8938644   4.2048817
0:   4.194278    4.230901    4.011079    3.9239018   3.769589    3.2594619
0:   2.5909038   1.7334266   0.9864092   0.6055355   0.6731272   0.98636675
0:  -0.30950785 -0.8479576 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.2098927  6.38966    6.9376335  7.682695   8.372918   8.921686
0:   9.627915  10.288614  10.859982  11.356804  11.548462  11.441717
0:  11.339144  11.391228  11.705636  12.29282   12.868385  13.170263
0:  11.436537  11.12368  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.1052904 5.50999   6.2676587 7.1822915 7.743797  7.8382587 7.7178226
0:  7.3535976 6.6885233 5.8791766 4.792785  3.4521255 2.4125605 1.9025578
0:  2.2944398 3.5035887 5.1096535 6.5526476 5.1439447 5.347958 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.502617 24.082695 24.761364 25.36778  25.650055 25.478212 25.329027
0:  24.91811  24.314344 23.535627 22.388859 21.07048  19.818674 18.86607
0:  18.347149 18.23639  18.344925 18.369686 16.599638 16.434   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.24325466 -0.23719978  0.07268333  0.60905695  1.0769567   1.3554807
0:   1.5189004   1.6050463   1.4498429   1.3083091   1.0268512   0.54372835
0:   0.17135859 -0.1107316  -0.12329102  0.21477413  0.701488    1.2140512
0:   0.92469835  1.0537357 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.188057  6.220518  6.361292  6.430372  6.3648305 6.1144586 6.242778
0:  6.3716245 6.454316  6.433607  5.9573812 5.361655  4.718506  4.59878
0:  4.9493666 5.7032137 6.4279327 6.91294   5.767333  5.7368393]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2103016 3.4781733 3.941385  4.414008  4.634979  4.5432386 4.362071
0:  4.0236483 3.5658727 3.2972221 3.0702357 2.90558   2.9799213 3.2941248
0:  3.8676355 4.6136622 5.266852  5.6862273 3.565544  3.8049457]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.20933962 -0.07711267  0.35057974  0.9499836   1.4674106   1.8017342
0:   2.1098425   2.3061965   2.304543    2.2409163   2.0100398   1.5609808
0:   1.2578506   1.1501417   1.3878965   1.9443569   2.535365    2.974744
0:   2.2200882   2.363155  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.897015  14.297277  14.917585  15.506872  15.793194  15.684444
0:  15.541231  15.3267975 15.017342  14.680531  14.185467  13.531448
0:  13.047413  12.859474  13.222887  14.051897  15.145397  16.168907
0:  14.214469  14.143074 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.593523   3.349562   3.45559    3.8120031  4.078545   4.048241
0:  3.9166799  3.6003566  3.0764537  2.5603812  1.9553411  1.2071509
0:  0.6181679  0.36831474 0.53609467 1.128984   1.8471534  2.4245865
0:  0.6733899  0.4763379 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.316639 17.568525 17.93834  18.330524 18.460264 18.351744 18.25554
0:  18.169298 17.978535 17.847572 17.62974  17.140066 16.813267 16.605536
0:  16.789232 17.382076 18.10435  18.71054  16.331797 16.21695 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.82071  17.775383 17.594402 17.266891 16.879105 16.327156 16.477646
0:  16.578466 16.86741  17.026844 16.68584  16.456013 16.27018  16.673769
0:  17.603943 18.873528 20.134289 20.952944 21.800858 22.06284 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0615811  -1.7398372  -1.3794389  -1.0649314  -0.7632761  -0.49545336
0:   0.25085545  1.0025067   1.7026029   2.3785615   2.766491    3.1587203
0:   3.7450163   4.8347206   6.208827    7.683831    8.814368    9.427121
0:   9.913814   10.403081  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.5054035 12.468066  12.504302  12.44772   12.293089  11.931496
0:  12.068744  12.129961  12.157576  12.000747  11.285574  10.438165
0:   9.539949   9.173617   9.287722   9.832411  10.3543625 10.599195
0:   9.082145   8.992832 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.9999611e+00  2.7475424e+00  2.7682965e+00  2.9895577e+00
0:   3.2168489e+00  3.2305198e+00  3.1730969e+00  2.9081078e+00
0:   2.3119125e+00  1.7422001e+00  1.0400987e+00  2.5177908e-01
0:  -3.4032393e-01 -6.8451595e-01 -7.0550632e-01 -4.4115591e-01
0:  -1.4754581e-01 -6.7043304e-04 -1.5215044e+00 -1.7978649e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.121099  -9.205065  -8.799179  -7.8887477 -6.741983  -5.646609
0:  -4.8001513 -4.1351886 -4.03361   -4.105002  -4.4816737 -5.3210196
0:  -6.1499457 -6.995279  -7.483129  -7.423529  -6.9038234 -6.1208854
0:  -3.7306066 -3.0311446]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.729374  -7.0508204 -7.076313  -6.725936  -6.305441  -5.9894085
0:  -6.0870333 -6.2215414 -6.7690897 -7.1951246 -7.5674143 -8.188732
0:  -8.663251  -9.127703  -9.288903  -8.890807  -7.98355   -6.8050404
0:  -6.20041   -6.136462 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.190346 32.881153 32.441418 31.88276  31.299267 30.55532  30.62122
0:  30.564068 30.667088 30.418781 29.49255  28.698124 28.004173 28.082203
0:  28.889748 30.017284 31.035465 31.472878 30.298584 30.19101 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.7744007  6.93649    7.324661   7.8124447  8.240274   8.4794235
0:   9.024788   9.47739    9.915059  10.336519  10.425241  10.334391
0:  10.24318   10.406897  10.880997  11.624079  12.33927   12.742378
0:  11.5029125 11.576988 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5968175  -1.4931006  -1.1253486  -0.46167564  0.20749807  0.7545414
0:   0.9609237   1.1637788   1.0024142   0.9526925   0.90037584  0.53384304
0:   0.21906614 -0.17024946 -0.315938    0.01075172  0.82765245  1.8619342
0:   1.809155    1.9403663 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.239492  7.3427415 7.657749  8.133959  8.493758  8.66662   8.822836
0:  8.939156  8.818962  8.734604  8.463663  7.9335804 7.4790363 7.048293
0:  6.9056168 7.1185937 7.4952445 7.8448324 6.286493  6.338265 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6642504  -1.5841522  -1.1724863  -0.62817526 -0.16818094  0.06017399
0:   0.4155116   0.617002    0.5938978   0.5140424   0.10434914 -0.53536224
0:  -1.1200256  -1.5303125  -1.6151123  -1.4614744  -1.3483934  -1.3924999
0:  -2.53199    -2.4154968 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.674955  -13.786932  -13.461864  -12.715134  -11.905224  -11.345295
0:  -11.176355  -11.298546  -11.950317  -12.598661  -13.32398   -14.230341
0:  -14.835249  -15.288208  -15.39303   -15.082159  -14.563925  -13.929108
0:  -14.0424385 -14.059124 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.3643675 12.755032  13.38409   14.110016  14.618414  14.792813
0:  14.864771  14.727886  14.449269  14.134684  13.66554   13.006577
0:  12.449387  12.131959  12.251192  12.76358   13.522763  14.127249
0:  11.872376  11.933132 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.366398  15.333914  15.216852  14.878092  14.477598  13.9739685
0:  14.148443  14.304094  14.619078  14.801571  14.512257  14.240854
0:  13.978493  14.297651  15.136553  16.318066  17.427578  18.094692
0:  18.112638  18.240602 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.363808  8.466711  8.740216  9.067287  9.272106  9.361381  9.702684
0:  10.031801 10.326605 10.560921 10.456519 10.182262  9.964096 10.106405
0:  10.650013 11.519337 12.333412 12.876324 11.622591 11.827002]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.45876  23.770031 24.081417 24.28946  24.381645 24.289608 24.471718
0:  24.566975 24.664906 24.707264 24.48748  24.213383 24.089073 24.273983
0:  24.814938 25.609093 26.447563 27.130932 24.89058  25.123318]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.6088505 -9.762072  -9.501892  -8.77194   -7.93133   -7.223905
0:  -6.8542237 -6.5820003 -6.7135854 -6.816485  -6.9656615 -7.393392
0:  -7.7560134 -8.064405  -8.151787  -7.7690034 -7.0352454 -6.1318574
0:  -5.6316953 -5.745627 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.839937  9.584393  9.310848  8.94269   8.584205  8.212181  8.368055
0:   8.550911  8.783422  8.84866   8.510354  8.095167  7.781809  8.02099
0:   8.828714 10.14131  11.551016 12.795353 12.932354 13.176137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.260044   5.9439006  6.9287095  8.030745   9.000438   9.795175
0:  10.720249  11.655189  12.493887  13.31221   13.89788   14.289606
0:  14.764721  15.4678955 16.611391  18.065546  19.634954  20.914438
0:  20.32564   21.143196 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.4326105 6.469248  6.660102  7.0844417 7.4285164 7.5832424 7.4066496
0:  7.137394  6.4235406 5.740013  4.96783   3.9609218 3.0646718 2.3268228
0:  2.0482917 2.4077146 3.3292828 4.455026  3.708717  3.4627934]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [42.374847 42.962418 43.234127 43.30887  43.19376  42.822285 43.152622
0:  43.33526  43.542255 43.393833 42.60293  41.852783 41.224304 41.206318
0:  41.763474 42.507698 43.043716 42.920933 40.17251  40.21566 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.400578  -7.45439   -7.2029233 -6.608274  -5.957639  -5.4620376
0:  -5.295509  -5.282597  -5.709395  -6.111343  -6.5875974 -7.2247214
0:  -7.711203  -8.051729  -8.107651  -7.7874913 -7.244389  -6.6560836
0:  -7.3230414 -7.3953757]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.7007675 12.536066  12.4505005 12.434685  12.411948  12.2068615
0:  12.439754  12.515547  12.692318  12.686984  12.250293  11.73602
0:  11.311878  11.302749  11.753206  12.577044  13.456245  14.202185
0:  13.254938  13.08193  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.160542   -9.913511   -9.215988   -8.06403    -6.8808656  -5.9860244
0:   -5.535414   -5.4629006  -5.9397616  -6.4215827  -6.9594398  -7.5846334
0:   -7.9557815  -8.142517   -7.9956813  -7.4154963  -6.6089664  -5.752234
0:   -5.2307105  -5.0222535]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.937214  -10.283056  -10.194187   -9.703058   -9.070051   -8.528408
0:   -8.055735   -7.7847857  -7.9517045  -8.204075   -8.658949   -9.291616
0:   -9.613993   -9.661327   -9.296371   -8.646114   -8.051662   -7.6449685
0:   -9.222912   -9.441029 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.674213 13.99546  14.539028 15.143112 15.557904 15.643559 15.904595
0:  16.020754 16.03105  15.969    15.567247 14.991065 14.48141  14.230257
0:  14.364874 14.866964 15.494762 15.988055 13.785763 13.947548]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.533445  14.810043  15.279991  15.761366  15.998366  15.915685
0:  15.975933  15.9334545 15.854341  15.778173  15.431629  14.944165
0:  14.556236  14.431451  14.652457  15.163393  15.669624  15.949409
0:  12.078852  11.874814 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0553775  -1.0993199  -0.82547903 -0.25521946  0.3326125   0.76890945
0:   0.9220743   1.0451102   0.79802704  0.6122055   0.3386016  -0.2400155
0:  -0.76637983 -1.2975898  -1.5299926  -1.2802849  -0.6256628   0.15583467
0:  -0.5932698  -0.76480436]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.195408 35.23093  35.085575 34.7549   34.411137 33.893078 34.14282
0:  34.30981  34.59928  34.71912  34.218426 33.768997 33.4338   33.57885
0:  34.25584  35.237106 36.179146 36.745857 35.375866 35.555878]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.468212   7.2233734  7.1530013  7.189769   7.130815   6.912909
0:  6.6960382  6.464619   6.065223   5.642537   5.087866   4.308523
0:  3.6054335  3.0440137  2.7454638  2.7818477  3.0000389  3.19588
0:  0.8909645  0.36928225]
0: validation loss for strategy=forecast at epoch 17 : 0.31672433018684387
0: validation loss for velocity_u : 0.13412398099899292
0: validation loss for velocity_v : 0.20778387784957886
0: validation loss for specific_humidity : 0.18329887092113495
0: validation loss for velocity_z : 0.46117737889289856
0: validation loss for temperature : 0.11352929472923279
0: validation loss for total_precip : 0.8004326820373535
0: 18 : 19:09:59 :: batch_size = 96, lr = 1.3472498670029002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 18, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9366, -0.8937, -0.8488, -0.8057, -0.7571, -0.6960, -0.6296, -0.5651, -0.5052, -0.4564, -0.4256, -0.4142,
0:         -0.4222, -0.4479, -0.4829, -0.5149, -0.5335, -0.5406, -0.9383, -0.9041, -0.8656, -0.8265, -0.7821, -0.7315,
0:         -0.6795], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5855, 0.5403, 0.4854, 0.4205, 0.3439, 0.2695, 0.2258, 0.2268, 0.2630, 0.3142, 0.3633, 0.4001, 0.4254, 0.4414,
0:         0.4464, 0.4425, 0.4304, 0.4060, 0.6387, 0.6121, 0.5825, 0.5457, 0.4923, 0.4257, 0.3638], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1568, 1.1959, 1.2261, 1.2797, 1.3131, 1.3747, 1.3965, 1.4176, 1.4349, 1.4294, 1.4260, 1.4365, 1.4202, 1.3987,
0:         1.3709, 1.4158, 1.4298, 1.4329, 1.0893, 1.0844, 1.1076, 1.1486, 1.1728, 1.2366, 1.2678], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3782, -0.3803, -0.2042, -0.1150, -0.1911, -0.2847, -0.3434, -0.4086, -0.3977, -0.2042,  0.1590,  0.5504,
0:          0.7700,  0.7526,  0.6526,  0.4873,  0.1176, -0.2325, -0.0911, -0.1063, -0.0063,  0.0698,  0.1002,  0.1329,
0:          0.1133], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.2972, 0.2795, 0.2533, 0.2412, 0.2504, 0.2682, 0.2893, 0.3220, 0.3564, 0.3710, 0.3674, 0.3750, 0.4064, 0.4384,
0:         0.4525, 0.4570, 0.4676, 0.4850, 0.5049, 0.5317, 0.5623, 0.5821, 0.5830, 0.5667, 0.5341], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 1.1168,  0.9007,  0.8656,  0.6246,  1.0376,  1.6543,  1.4721,  1.0172,  1.8025,  0.6664,  0.6608,  0.6415,
0:          0.3643,  0.1708,  0.6087,  0.5126,  0.8418,  1.6916,  0.2036,  0.1980,  0.1889,  0.0147, -0.1799, -0.2037,
0:         -0.0453], device='cuda:0')
0: [DEBUG] Epoch 18, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([        nan,         nan,         nan,         nan,         nan, -2.4218e-01,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan, -2.4671e-01,         nan, -2.2860e-01,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,  1.6065e-01, -3.2304e-05,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:         -2.3992e-01,         nan,         nan,         nan,         nan, -2.4444e-01,         nan,         nan,
0:                 nan, -1.9013e-01, -1.8560e-01,         nan,         nan,         nan,         nan,         nan,
0:                 nan, -5.4346e-02,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  4.6616e-01,
0:                 nan,         nan, -1.7089e-01,         nan, -2.4558e-01,         nan,         nan,         nan,
0:                 nan,         nan,         nan, -2.4218e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan, -2.4671e-01,
0:          2.9770e+00,         nan,         nan,  7.7733e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,  1.0520e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan, -2.1050e-01,         nan,  5.5895e-01,         nan,         nan,  5.9063e-01,         nan,
0:                 nan,         nan, -2.3426e-01,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan, -2.4105e-01,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan])
0: [DEBUG] Epoch 18, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5745, -0.5712, -0.5473, -0.5056, -0.4654, -0.4315, -0.3849, -0.3360, -0.3027, -0.2813, -0.2859, -0.3175,
0:         -0.3347, -0.3308, -0.2847, -0.2049, -0.1110, -0.0371, -0.4911, -0.5148, -0.5060, -0.4847, -0.4712, -0.4425,
0:         -0.4055], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5718, 0.5781, 0.5489, 0.5144, 0.4763, 0.4421, 0.4056, 0.3651, 0.3246, 0.2943, 0.2680, 0.2472, 0.2213, 0.1753,
0:         0.1274, 0.1136, 0.1405, 0.1932, 0.5795, 0.5944, 0.5776, 0.5436, 0.5130, 0.4864, 0.4556], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1586,  0.1317,  0.0771,  0.0235, -0.0399, -0.0964, -0.1393, -0.1614, -0.1626, -0.1414, -0.0940, -0.0367,
0:          0.0469,  0.1440,  0.2155,  0.3014,  0.3620,  0.4339,  0.1528,  0.1147,  0.0678,  0.0067, -0.0513, -0.1134,
0:         -0.1516], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0906,  0.2359,  0.2464,  0.3364,  0.1702,  0.0103,  0.0496,  0.0011,  0.0304,  0.1081,  0.0706,  0.0458,
0:          0.1241,  0.2095,  0.1582,  0.2202,  0.4240,  0.4212,  0.1059,  0.1176, -0.0448,  0.0216, -0.0464, -0.1215,
0:         -0.1147], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0898, -0.1023, -0.1041, -0.0994, -0.0834, -0.0671, -0.0554, -0.0509, -0.0431, -0.0274, -0.0043,  0.0209,
0:          0.0457,  0.0658,  0.0843,  0.0999,  0.1089,  0.1120,  0.1110,  0.1063,  0.1038,  0.1019,  0.0990,  0.0887,
0:          0.0600], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1418, -0.1509, -0.1571, -0.1615, -0.1611, -0.1640, -0.1713, -0.1724, -0.1801, -0.1626, -0.1592, -0.1598,
0:         -0.1612, -0.1620, -0.1668, -0.1753, -0.1782, -0.1835, -0.1707, -0.1653, -0.1644, -0.1617, -0.1629, -0.1681,
0:         -0.1769], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2182474136352539; velocity_v: 0.2937551438808441; specific_humidity: 0.1983259618282318; velocity_z: 0.5670069456100464; temperature: 0.16243787109851837; total_precip: 0.5940978527069092; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21408405900001526; velocity_v: 0.29554370045661926; specific_humidity: 0.21189089119434357; velocity_z: 0.6409178972244263; temperature: 0.1516832411289215; total_precip: 0.7066687941551208; 
0: epoch: 18 [1/5 (20%)]	Loss: 0.65038 : 0.31946 :: 0.20816 (2.64 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21888966858386993; velocity_v: 0.2809067964553833; specific_humidity: 0.17849089205265045; velocity_z: 0.6899424195289612; temperature: 0.15099436044692993; total_precip: 0.9249017834663391; 
0: epoch: 18 [2/5 (40%)]	Loss: 0.92490 : 0.37269 :: 0.20808 (15.86 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19059374928474426; velocity_v: 0.29444620013237; specific_humidity: 0.19322775304317474; velocity_z: 0.5505320429801941; temperature: 0.16002897918224335; total_precip: 0.7876737713813782; 
0: epoch: 18 [3/5 (60%)]	Loss: 0.78767 : 0.32885 :: 0.20342 (15.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20645155012607574; velocity_v: 0.2697559893131256; specific_humidity: 0.21365024149417877; velocity_z: 0.6261196136474609; temperature: 0.16180232167243958; total_precip: 0.857964277267456; 
0: epoch: 18 [4/5 (80%)]	Loss: 0.85796 : 0.35454 :: 0.20771 (15.91 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [9.53674316e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 2.38418579e-06 8.10623169e-06
0:  2.67028809e-05 4.52995300e-05 8.96453857e-05 1.00135803e-04
0:  8.10623169e-05 2.24113464e-05 7.62939453e-06 1.43051147e-06
0:  1.04904175e-05 4.38690149e-05 7.20024109e-05 9.82284546e-05
0:  1.03473663e-04 7.34329224e-05 8.58306885e-05 7.91549683e-05
0:  6.24656677e-05 2.76565552e-05 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 3.67164612e-05 1.13487244e-04 1.31130219e-04
0:  1.86920166e-04 1.58786774e-04 2.12669373e-04 2.05993652e-04
0:  1.23023987e-04 7.00950623e-05 1.33514404e-05 4.29153442e-06
0:  6.67572021e-06 3.05175781e-05 4.86373901e-05 5.62667847e-05
0:  5.43594360e-05 4.38690149e-05 6.81877136e-05 8.77380371e-05
0:  5.43594360e-05 3.05175781e-05 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Target values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 5.24520874e-06
0:  9.05990601e-06 8.10623169e-06 7.62939453e-06 8.58306885e-06
0:  8.58306885e-06 1.28746033e-05 1.66893005e-05 1.95503235e-05
0:  2.09808350e-05 2.52723694e-05 1.90734863e-05 1.14440918e-05
0:  3.81469727e-06 1.43051147e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 9.53674316e-07 1.90734863e-06
0:  4.29153442e-06 1.04904175e-05 2.05039978e-05 4.29153442e-05
0:  8.15391541e-05 1.53064728e-04 2.88486481e-04 4.10079956e-04
0:  5.38349152e-04 6.17027283e-04 5.57899475e-04 5.87463379e-04
0:  7.43865967e-04 8.48770142e-04 6.58988953e-04 4.32491302e-04
0:  4.62055206e-04 5.45024872e-04 4.74452972e-04 3.27110291e-04
0:  1.48773193e-04 1.41620636e-04 1.28269196e-04 8.53538513e-05
0:  3.48091125e-05 2.28881836e-05 1.00135803e-05 7.15255737e-06
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.66893005e-05 2.19345093e-05 2.71797180e-05
0:  1.43051147e-05 2.19345093e-05 1.95503235e-05 2.33650208e-05
0:  1.57356262e-05 6.19888306e-06 5.24520874e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 9.53674316e-07
0:  9.53674316e-07 4.76837158e-07 4.76837158e-07 0.00000000e+00
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 1.43051147e-06
0:  2.86102295e-06 3.81469727e-06 9.53674316e-06 2.33650208e-05
0:  3.19480896e-05 8.05854797e-05 1.53064728e-04 2.24590302e-04
0:  3.05175781e-04 3.68595123e-04 4.12464142e-04 6.95228577e-04]
0: Prediction values (first 20):
0: [36.62822  36.637775 36.483444 36.23879  36.043453 35.757206 36.361492
0:  36.941196 37.761917 38.34738  38.312916 38.173416 38.037197 38.241924
0:  38.865635 39.760376 40.63339  41.162365 40.954636 41.208878]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.192, max = 3.212, mean = 0.787
0:          sample (first 20): tensor([2.5620, 2.5628, 2.5492, 2.5277, 2.5105, 2.4853, 2.5385, 2.5896, 2.6618, 2.7134, 2.7104, 2.6981, 2.6861, 2.7041,
0:         2.7591, 2.8379, 2.9148, 2.9613, 2.5202, 2.5834])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.902113 25.405064 24.949972 24.575844 24.180468 23.474878 23.302057
0:  22.888714 22.644825 22.200438 21.276173 20.36665  19.70041  19.503256
0:  20.017338 21.018486 22.194983 23.130444 24.782728 24.598158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.852413  -8.919861  -8.564224  -7.846145  -7.091248  -6.544502
0:  -6.1083493 -5.8853946 -6.0637937 -6.2001224 -6.5369964 -7.033501
0:  -7.298326  -7.4003377 -7.1509356 -6.620954  -6.1645446 -5.8756194
0:  -5.5242715 -5.2678266]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9224353 -3.9515872 -3.98848   -4.044535  -4.2132716 -4.440842
0:  -4.3808427 -4.1855216 -4.1843    -4.2765307 -4.8286557 -5.528911
0:  -6.2494197 -6.4774375 -6.2769647 -5.8782315 -5.4848504 -5.3089194
0:  -7.762176  -7.854111 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4577479 1.2827907 1.2954335 1.4125648 1.5110664 1.47614   1.6474452
0:  1.7669094 1.8595984 1.9171019 1.7513771 1.4742932 1.2821622 1.4612808
0:  2.001603  2.854372  3.6956458 4.2942753 3.6449218 3.8235266]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.383131   -3.4025393  -3.1452084  -2.48948    -1.699503   -0.9064884
0:  -0.32867384  0.28579998  0.5169053   0.784039    1.0123487   0.9265752
0:   0.88480806  0.767303    0.8656316   1.3534017   2.2377634   3.3204248
0:   4.2588973   4.5053763 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.64455  13.152446 13.950054 15.00689  16.04169  16.904793 17.866165
0:  18.736286 19.456291 20.263134 20.978142 21.675295 22.539867 23.551285
0:  24.827667 26.195206 27.494469 28.513042 27.800266 28.37883 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.6696844 1.6391277 1.875782  2.374538  2.8501444 3.1875136 3.35123
0:  3.5456684 3.4360464 3.3726301 3.1729875 2.5721374 1.9806728 1.322917
0:  0.9624009 1.1257586 1.7416615 2.539804  1.5149112 1.4723063]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8045917  -1.8583302  -1.6015306  -1.0197358  -0.4381671  -0.02444839
0:   0.11850166  0.09184074 -0.35304928 -0.8004756  -1.3518977  -2.101313
0:  -2.6836457  -3.1229548  -3.2254882  -2.88979    -2.3136306  -1.6919265
0:  -1.8721013  -1.802176  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.934721  4.2932568 4.919326  5.5777984 6.069723  6.342338  6.5767136
0:  6.781498  6.81472   6.9059124 6.8514843 6.588193  6.4299464 6.39824
0:  6.6746264 7.2348137 7.836125  8.2703085 6.6649175 7.1285253]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.900955 21.706768 21.46913  21.164711 20.898907 20.440647 20.677828
0:  20.697598 20.79172  20.58066  19.790827 19.010208 18.278896 18.086767
0:  18.420534 19.164886 19.898285 20.353611 19.97321  19.926188]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.199496  12.406633  12.721333  13.038597  13.244781  13.314028
0:  13.654533  13.9983635 14.386637  14.655039  14.671921  14.54867
0:  14.515671  14.696533  15.245504  16.074211  17.029518  17.870403
0:  16.823978  17.19583  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.7453165 10.820706  11.099054  11.530663  11.791254  11.806149
0:  11.637597  11.590347  11.452471  11.549231  11.644507  11.419804
0:  11.040625  10.417843   9.930343   9.809914  10.0973215 10.547613
0:   7.539809   7.2718363]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.35321  24.415491 24.596943 24.708588 24.602028 24.199814 23.911356
0:  23.54149  23.119806 22.716677 22.077408 21.279242 20.602016 20.123621
0:  20.085512 20.435589 20.967346 21.35095  19.00034  19.004427]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.028503  16.06445   16.215063  16.404919  16.504852  16.401852
0:  16.691288  16.877447  17.039413  17.061157  16.655045  16.006897
0:  15.389962  15.104292  15.293277  15.926068  16.751297  17.440762
0:  15.320772  15.1079035]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4430699e+00 -1.4852777e+00 -1.2301469e+00 -6.4414549e-01
0:  -1.0466576e-03  5.0220633e-01  7.0226860e-01  8.2801151e-01
0:   5.1903963e-01  2.9752874e-01  5.0371647e-02 -4.8680019e-01
0:  -8.7989426e-01 -1.3250957e+00 -1.5037656e+00 -1.1349616e+00
0:  -2.9006720e-01  7.8433847e-01  9.5976353e-02  6.3375473e-02]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.8946614 0.9142866 1.2533717 1.9461694 2.6574566 3.273088  3.7474248
0:  4.188979  4.303894  4.4507437 4.4849086 4.17589   3.8980246 3.58372
0:  3.5440428 3.9374418 4.667928  5.52184   4.995753  5.078446 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.13072   -9.149216  -8.831907  -8.170631  -7.444514  -6.842817
0:  -6.329577  -5.945417  -5.969009  -6.0171857 -6.2982583 -6.881861
0:  -7.2607503 -7.571117  -7.5526643 -7.21631   -6.8228755 -6.415993
0:  -7.1590714 -7.1842666]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6134219  -1.0863433  -0.32883596  0.46670675  1.087892    1.4602342
0:   1.5717912   1.5704079   1.2837734   0.9594698   0.5362115  -0.1372261
0:  -0.70096827 -1.144464   -1.3365121  -1.1846452  -0.879189   -0.45320797
0:  -1.9173989  -1.6141534 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2198539 1.3376007 1.7284646 2.3388617 2.903832  3.328756  3.740151
0:  4.0553656 4.0807095 4.013597  3.6607792 3.0438998 2.4944878 2.091166
0:  2.0454516 2.3378    2.7431564 3.0511622 2.0378494 2.2925797]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.47987  15.453395 15.668871 15.975812 16.211971 16.272324 16.527561
0:  16.71071  16.809643 16.841465 16.597887 16.177109 15.847597 15.725281
0:  16.02633  16.660885 17.428381 18.050503 16.07453  16.116323]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2081861  1.4393854  1.9910233  2.811387   3.6732516  4.489396
0:   5.2512503  6.016485   6.5707808  7.149307   7.6530576  7.917655
0:   8.251469   8.569067   9.064036   9.81954   10.716595  11.572534
0:  10.483257  10.771809 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.012533  18.905079  19.02312   19.18589   19.243597  19.11053
0:  19.12684   18.979881  18.631628  18.090464  17.189598  16.000221
0:  14.8967495 14.083826  13.750378  13.84626   14.166489  14.3971
0:  11.575296  11.168093 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.114192  -10.225368   -9.981141   -9.327185   -8.617239   -8.067066
0:   -7.8499184  -7.777912   -8.199282   -8.603459   -9.101355   -9.813838
0:  -10.323638  -10.706874  -10.7373495 -10.325753   -9.638856   -8.866959
0:   -8.72013    -8.636366 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.1332016   0.07411385  0.31353664  0.8723674   1.4473968   1.8658452
0:   2.01581     2.122603    1.8178902   1.5582595   1.2121253   0.5409274
0:  -0.01050091 -0.54005337 -0.72272396 -0.34412146  0.4913411   1.5039916
0:   1.0847678   1.0087452 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1679249e-01 -4.6943712e-01 -3.5871363e-01  4.7683716e-06
0:   3.2124233e-01  4.7433138e-01  3.5756159e-01  2.5790405e-01
0:  -1.3214207e-01 -3.8906670e-01 -6.6127777e-01 -1.2218385e+00
0:  -1.7496939e+00 -2.3124943e+00 -2.5935206e+00 -2.3963332e+00
0:  -1.7683754e+00 -9.8027182e-01 -1.5694308e+00 -1.7393064e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.009794 14.426277 14.968038 15.447918 15.787724 16.05191  16.321552
0:  16.668575 16.878649 17.070488 17.200216 17.201935 17.437077 17.803446
0:  18.578192 19.568298 20.600134 21.482033 19.165012 19.58006 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.618519 21.451265 21.075653 20.44202  19.814354 19.093615 19.174664
0:  19.246002 19.470016 19.471848 18.908459 18.425276 18.016792 18.25761
0:  19.067047 20.244461 21.315372 21.934559 23.350368 23.412037]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.86726  -13.667391 -13.081163 -12.106813 -11.062992 -10.238974
0:   -9.650829  -9.311213  -9.469252  -9.681862 -10.087944 -10.676273
0:  -11.063772 -11.272345 -11.178999 -10.732363 -10.201372  -9.724325
0:  -10.517838 -10.506868]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.016359  8.1371765 8.36636   8.63627   8.751015  8.670778  8.523359
0:  8.333193  7.947994  7.5767856 7.078339  6.3987675 5.8618793 5.479208
0:  5.473605  5.871959  6.4980984 7.0593653 6.5482545 6.8579683]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2967546 3.288251  3.527649  3.8637862 4.0526266 3.9939675 3.9513497
0:  3.8625104 3.7258816 3.6232734 3.4100902 3.0051215 2.6541412 2.5005863
0:  2.6425924 3.1583657 3.8278363 4.446557  2.7079911 2.6027083]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.69103  28.674702 28.445824 27.919369 27.305786 26.504148 26.393671
0:  26.210184 26.079218 25.730097 24.783922 23.951048 23.215572 23.111269
0:  23.60068  24.410067 25.083231 25.351982 25.739414 25.875341]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.9751067 -7.265674  -7.1740737 -6.596356  -5.837315  -5.1335244
0:  -4.8176255 -4.592832  -4.902079  -5.1733565 -5.5394883 -6.219568
0:  -6.8354774 -7.442468  -7.773975  -7.559836  -6.831454  -5.8387218
0:  -5.8124804 -5.866896 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.152969  -11.333302  -11.1031685 -10.4327545  -9.679426   -9.118998
0:   -8.8708725  -8.841885   -9.283897   -9.70237   -10.200775  -10.907223
0:  -11.409912  -11.790476  -11.925434  -11.673756  -11.237404  -10.730591
0:  -11.825682  -12.017128 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8949552 -3.0257688 -2.8340735 -2.3097672 -1.7814069 -1.4223332
0:  -1.3005118 -1.200355  -1.3826804 -1.4628873 -1.588161  -2.0021253
0:  -2.3560243 -2.740399  -2.8755298 -2.5438495 -1.869719  -1.0854459
0:  -1.6927867 -1.8560014]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.584021  9.452372  9.408     9.352278  9.212614  8.979696  9.212421
0:   9.423994  9.710077  9.882218  9.637895  9.274144  9.002841  9.286127
0:  10.088645 11.330828 12.521101 13.362219 12.312622 12.36786 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.1499066 4.900882  4.848025  4.8451037 4.7078753 4.3863688 4.311803
0:  4.2247543 4.1420455 4.059826  3.6893692 3.1696234 2.583767  2.334146
0:  2.4165673 2.7658799 3.0517168 3.1491683 2.1263037 1.7736101]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.789342  4.816905  5.119287  5.5262766 5.778348  5.81485   5.884161
0:  5.8968706 5.8119516 5.7123656 5.449047  4.965987  4.5943565 4.438147
0:  4.5544424 4.999605  5.457086  5.7178006 3.8386354 3.9868085]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.08644962 0.39627123 0.96788025 1.7384777  2.4484348  2.9955168
0:  3.3829453  3.7581732  3.8342834  3.9278157  3.8854573  3.5489984
0:  3.2426696  3.0064802  3.0635586  3.5182018  4.246185   4.9688396
0:  3.5504394  3.6682863 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.072273  -13.410049  -13.403114  -13.028757  -12.617212  -12.291733
0:  -12.14889   -12.0604    -12.245951  -12.503445  -12.9108925 -13.597441
0:  -14.155542  -14.507767  -14.481998  -14.036448  -13.447296  -12.787947
0:  -13.506653  -13.854385 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1319599   1.1047087   1.3823581   1.8286631   2.160029    2.2729115
0:   2.2656786   2.090243    1.6080799   1.1252899   0.4838333  -0.33146763
0:  -0.97544813 -1.4088922  -1.5111995  -1.2273064  -0.87017345 -0.550889
0:  -2.8553958  -3.1088328 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3442335 2.5086477 2.8702934 3.4194596 3.8461168 4.111264  4.127837
0:  4.1514745 3.914026  3.725719  3.5032554 3.061393  2.6891422 2.3233013
0:  2.2651775 2.600586  3.2835276 3.998374  3.0317392 3.0859933]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.232172   -1.2483711  -1.072351   -0.7261696  -0.5087948  -0.5156026
0:  -0.80647135 -1.0764446  -1.7103043  -2.2139664  -2.7765465  -3.624508
0:  -4.3405886  -4.999116   -5.2969484  -5.009676   -4.2496777  -3.2790556
0:  -3.8587208  -3.8445024 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.98436  25.878283 25.520885 24.939651 24.413887 23.740088 23.97115
0:  24.106989 24.425121 24.480991 23.898445 23.455347 23.005165 23.16474
0:  23.83952  24.8565   25.808315 26.256113 26.413202 26.506893]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.215442 14.396824 14.809594 15.21921  15.553932 15.71793  15.903324
0:  16.070269 16.03026  15.995308 15.849524 15.508995 15.364227 15.286894
0:  15.480337 15.87454  16.304733 16.581741 14.258063 14.481207]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.55321  35.431404 34.84034  34.025536 33.202717 32.161037 32.140156
0:  32.01133  32.07466  31.838516 30.876644 30.197693 29.570568 29.682224
0:  30.405891 31.411674 32.24577  32.446075 34.11312  34.185257]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.361727 17.988167 17.804445 17.690912 17.55441  17.173912 17.302643
0:  17.34216  17.558685 17.681684 17.337332 16.920387 16.455568 16.347664
0:  16.614065 17.196    17.753935 18.107811 17.186792 17.15894 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.851547  16.683268  16.551739  16.453478  16.383242  16.154165
0:  16.580225  16.811687  17.083473  17.051527  16.364552  15.5662155
0:  14.749634  14.423725  14.611704  15.194698  15.775248  16.098717
0:  14.1299095 13.812376 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9579663 -3.9435668 -3.6917987 -3.25168   -2.8397765 -2.554421
0:  -2.4209304 -2.275473  -2.4090705 -2.5056643 -2.7084746 -3.202273
0:  -3.6638436 -4.1181636 -4.3459926 -4.1379247 -3.6254086 -2.9643207
0:  -2.8512883 -2.6228166]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -5.7008543  -6.14277    -6.191784   -5.7758617  -5.278413   -4.8858356
0:   -4.9424105  -5.060523   -5.642972   -6.1513205  -6.709198   -7.6681767
0:   -8.562544   -9.500294  -10.098807  -10.010385   -9.260109   -8.136818
0:   -8.900699   -9.312325 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.077894 16.153376 16.25652  16.312296 16.26131  16.092806 16.255835
0:  16.395823 16.531195 16.554564 16.217121 15.803652 15.487303 15.605595
0:  16.218498 17.09576  17.99069  18.583744 16.603872 16.709602]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.18042  26.474026 26.906422 27.267681 27.550499 27.705235 28.33627
0:  28.95773  29.642189 30.149902 30.158577 29.91364  29.699314 29.578568
0:  29.831463 30.234241 30.57515  30.64021  26.001526 25.824947]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.303222  10.148935  10.157539  10.162034   9.980032   9.581377
0:   9.278205   8.911394   8.493341   8.085926   7.462321   6.728066
0:   6.1145964  5.872248   6.0859046  6.655461   7.26995    7.7245026
0:   6.7467337  6.8577404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.71249866 -0.75837374 -0.5565977  -0.2056055   0.02692699  0.07573462
0:   0.21250057  0.3796687   0.4128194   0.4481058   0.2583084  -0.20193338
0:  -0.63755035 -0.8603945  -0.72792864 -0.19777155  0.4655199   0.97303295
0:  -1.1373692  -1.1020865 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.869564 39.95099  39.87027  39.599155 39.223454 38.65635  38.726727
0:  38.67534  38.716034 38.600407 37.989662 37.40954  36.998993 37.072292
0:  37.716217 38.636658 39.537106 40.044483 39.124813 39.359722]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.5469756  -0.04315424 -0.53774023 -0.67457724 -0.6924367  -0.533792
0:   0.12902212  1.016757    1.8982053   2.6719556   3.196187    3.4341335
0:   3.7785113   4.5186825   5.744519    7.2967424   8.823451    9.87808
0:   8.214355    8.182635  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.624897 28.635363 28.794275 28.99441  29.115297 29.018892 29.15825
0:  29.11591  29.043554 28.861761 28.305742 27.595364 27.036163 26.71526
0:  26.873013 27.291306 27.697678 27.996382 24.99632  24.57067 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.486829   3.7247815  4.0105944  4.3375034  4.630846   4.806012
0:   5.5017376  6.2286744  6.987528   7.69559    7.9049025  7.9936624
0:   7.843542   8.134088   8.760276   9.497895   9.952892   9.944639
0:  11.657153  12.234118 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7884836  -2.6857128  -2.2706432  -1.6212983  -1.025218   -0.6372404
0:  -0.47870302 -0.44225454 -0.76567364 -1.0851493  -1.5127435  -2.173932
0:  -2.6772861  -3.1371784  -3.3183537  -3.1470947  -2.821741   -2.4973984
0:  -3.7777157  -3.7693043 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.862334 15.811176 15.93563  16.092087 16.135117 15.958317 16.03371
0:  16.039831 16.020657 15.993203 15.653351 15.136287 14.66209  14.45528
0:  14.642383 15.205974 15.86823  16.38126  14.221333 14.151611]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.16295    -1.0999446  -0.6400447   0.03500795  0.645803    1.082933
0:   1.5606303   1.885725    1.9648681   2.0034628   1.7797856   1.3314505
0:   0.9750924   0.8823414   1.1209474   1.6739287   2.1948915   2.5533373
0:   1.6623707   1.7213502 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.1489825 -7.3504376 -7.169721  -6.662503  -6.151337  -5.816175
0:  -5.6981034 -5.7487416 -6.2007394 -6.6659503 -7.2419066 -8.042276
0:  -8.538809  -8.828117  -8.725658  -8.138025  -7.4026513 -6.6798854
0:  -7.8915772 -7.987181 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.271374 21.194962 21.206154 21.272837 21.31458  21.232115 21.44494
0:  21.617525 21.799038 21.919302 21.749544 21.383465 20.985765 20.67162
0:  20.61672  20.797667 21.121525 21.376915 19.623571 19.578304]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.691338 -18.79525  -18.474129 -17.688694 -16.90945  -16.44394
0:  -16.369652 -16.599407 -17.332535 -18.00272  -18.79198  -19.769455
0:  -20.482655 -20.949844 -20.882969 -20.270905 -19.478977 -18.704258
0:  -17.677662 -17.70628 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.050361   9.261452   9.587054   9.910641  10.110367  10.142879
0:  10.396423  10.6031685 10.7518425 10.871105  10.668306  10.327618
0:  10.0002    10.008217  10.360999  10.999177  11.599373  11.961613
0:   9.878146  10.044599 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3680072  -1.1330032  -0.60440445  0.06292868  0.57502604  0.84569025
0:   1.0680699   1.0829859   0.8315325   0.6143613   0.20898724 -0.34428406
0:  -0.79859686 -1.0383148  -0.99193096 -0.6254325  -0.23728132  0.1152215
0:   0.04960346  0.37208557]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2838154 -3.395317  -3.189518  -2.6685133 -2.1470385 -1.7959614
0:  -1.7748814 -1.8630486 -2.3840704 -2.9116077 -3.5051708 -4.402911
0:  -5.084568  -5.662141  -5.846135  -5.452217  -4.665165  -3.7168112
0:  -4.608118  -4.790455 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.292212  -12.241892  -11.840515  -11.19644   -10.472736   -9.794786
0:   -8.78694    -7.7782435  -6.9690795  -6.278294   -6.003201   -5.9936633
0:   -5.9381623  -5.6205325  -5.018639   -4.190182   -3.4949508  -3.058886
0:   -4.967912   -4.9502664]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.093153   8.295405   8.532126   8.774075   8.93796    9.055413
0:   9.611411  10.19503   10.731922  11.109198  10.9833355 10.692215
0:  10.347912  10.454189  11.003635  11.909889  12.859186  13.542086
0:  13.229766  13.275491 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.706456  -12.9438925 -12.840465  -11.918614  -10.613971   -9.284617
0:   -8.035251   -7.0273705  -6.5602837  -6.1426973  -5.938674   -5.99053
0:   -5.920742   -5.8930864  -5.5719404  -5.059353   -4.3658924  -3.790348
0:   -4.289092   -4.5235815]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.221736    2.0101094   2.0369291   2.2826777   2.3942184   2.2853909
0:   1.8004842   1.3558283   0.66233444  0.16091013 -0.28602648 -1.0582457
0:  -1.8023605  -2.556941   -2.950776   -2.5935755  -1.5109634  -0.04557943
0:  -0.73842955 -0.9319601 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.01252985 -0.06158066  0.2288146   0.7514372   1.2583523   1.593781
0:   2.105733    2.4837265   2.6984482   2.7700996   2.4371338   1.8378448
0:   1.2122831   0.90834713  0.9209719   1.2408652   1.4878926   1.4782491
0:  -1.1175122  -1.4535718 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.253706 28.308054 28.094774 27.577246 26.97773  26.270493 26.490217
0:  26.680853 27.004421 27.047638 26.412313 25.993105 25.717747 26.257704
0:  27.43973  28.863125 30.036951 30.537249 31.608051 31.863396]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.343483  12.3483715 12.439657  12.634183  12.814373  12.835901
0:  13.159716  13.396325  13.649076  13.774235  13.560151  13.211485
0:  12.82564   12.662225  12.82757   13.313917  14.013306  14.634144
0:  13.353488  13.342842 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.801058   7.89843    8.07011    8.33322    8.568274   8.640571
0:   8.9961     9.299604   9.56491    9.7759285  9.767665   9.666288
0:   9.688825  10.026615  10.697048  11.696898  12.773674  13.631203
0:  12.335209  12.437012 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.09485  34.37491  34.547455 34.578655 34.540596 34.332733 34.643955
0:  34.82893  35.060375 35.057354 34.57039  34.045612 33.707554 33.717182
0:  34.224075 34.991882 35.823395 36.43655  34.44214  34.70778 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.162669  -5.0625343 -4.6529465 -3.875245  -3.065858  -2.4310856
0:  -2.1255455 -1.9941969 -2.351202  -2.683239  -3.1134024 -3.6784515
0:  -4.032795  -4.2390614 -4.078845  -3.5578032 -2.8737688 -2.2220917
0:  -2.2356358 -2.0965185]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.70310545 -0.86568975 -0.74752903 -0.43032694 -0.14631414 -0.04270506
0:   0.10233212  0.12708569 -0.08225489 -0.30266094 -0.7465129  -1.3520627
0:  -1.8525348  -2.0812993  -1.9850535  -1.5589557  -1.0946593  -0.80010223
0:  -3.202392   -3.1114984 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.77604  19.287579 19.75053  19.97978  20.069313 19.988615 20.607422
0:  21.164219 21.724415 21.949205 21.382544 20.720951 19.968447 19.813583
0:  20.14471  20.781126 21.222858 21.224323 19.703735 19.67231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.028837  19.88933   18.54847   17.06608   15.598193  14.080652
0:  13.622116  13.287302  13.310804  13.2807255 12.771511  12.41757
0:  12.161349  12.725316  13.921755  15.469041  16.771427  17.390732
0:  16.614727  16.520483 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.046562 10.265356 10.738323 11.270508 11.524824 11.424132 11.235945
0:  11.138582 10.931173 10.83728  10.637695 10.100993  9.661783  9.250525
0:   9.222357  9.735082 10.646452 11.652077  9.536442  9.715803]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.378674 23.822334 24.227303 24.568865 24.810879 24.967535 25.560556
0:  26.083155 26.722862 27.237955 27.434814 27.482546 27.647337 28.0252
0:  28.747938 29.622862 30.468027 31.11084  29.434593 29.75983 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.58142  21.505869 21.584867 21.664736 21.648863 21.433737 21.410522
0:  21.321194 21.142597 20.885487 20.313889 19.528656 18.762268 18.203802
0:  18.017103 18.145933 18.342905 18.401947 15.118992 14.910011]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.477617 11.174532 10.916783 10.590857 10.220509  9.701315  9.775715
0:   9.845988 10.010477 10.054515  9.578991  9.012417  8.369213  8.238017
0:   8.606068  9.353255 10.043984 10.362892  9.697221  9.49394 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.317874   5.4318027  5.730942   6.117806   6.4464207  6.653739
0:   7.086662   7.512304   7.8806224  8.160098   8.167064   8.018971
0:   7.9116044  8.040972   8.4999695  9.216422   9.93952   10.496031
0:   9.614645   9.700187 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.355991 14.856994 14.355122 13.857641 13.442852 12.852224 13.057411
0:  13.112511 13.422688 13.559317 13.08777  12.491309 11.766998 11.473053
0:  11.531509 12.032896 12.537981 12.797846 11.158733 10.901554]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.091776  10.232877  10.4817705 10.632271  10.657407  10.487968
0:  10.719012  10.945267  11.181412  11.373821  11.092283  10.658202
0:  10.157001  10.076393  10.349171  10.970461  11.534652  11.787216
0:  10.019984  10.126064 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.959932 16.845587 16.702932 16.48656  16.224981 15.760637 15.883064
0:  15.915468 16.063519 16.000149 15.42042  14.885565 14.387522 14.359535
0:  14.834057 15.599073 16.36883  16.874039 16.609846 16.615126]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.836304  10.964129  11.204613  11.484577  11.665493  11.680443
0:  11.94343   12.125065  12.195509  12.155508  11.800007  11.318029
0:  10.940691  10.915767  11.3094425 12.018128  12.825861  13.386264
0:  12.08144   12.091455 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.300911 15.392183 15.569586 15.718386 15.8386   15.858433 16.098001
0:  16.355265 16.51613  16.659208 16.56333  16.263458 16.066946 16.037716
0:  16.346363 16.909939 17.485434 17.90947  14.771303 14.938345]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.96162    -9.240303   -9.202986   -8.749334   -8.223703   -7.8361745
0:   -7.8588414  -7.9778314  -8.517811   -8.9364605  -9.312641   -9.856682
0:  -10.21813   -10.516718  -10.55961   -10.158619   -9.387869   -8.477041
0:   -8.918373   -9.1473255]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.37347    -9.389935   -8.965284   -8.144724   -7.365456   -6.8781123
0:   -6.759155   -6.9937453  -7.7728744  -8.6394615  -9.636555  -10.869236
0:  -11.80862   -12.565874  -12.867817  -12.680262  -12.209488  -11.605444
0:  -11.335808  -11.320219 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3843503  -2.667235   -2.5949264  -2.1718478  -1.5816436  -1.0316463
0:  -0.48691034 -0.17591572 -0.3655467  -0.6821008  -1.2458262  -2.0235343
0:  -2.4502234  -2.621778   -2.3489451  -1.7144685  -1.0477242  -0.46765995
0:  -0.47093296 -0.18083858]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.007607   8.773773   8.664241   8.558659   8.453484   8.19803
0:   8.612424   8.976745   9.397196   9.627131   9.232851   8.673822
0:   8.008256   7.9070163  8.337202   9.245382  10.170061  10.767473
0:  11.333504  11.331167 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.229326  17.087887  16.728094  16.052332  15.292309  14.4834175
0:  14.487114  14.539276  14.751406  14.78772   14.2583885 13.787294
0:  13.310413  13.526427  14.316202  15.482935  16.555628  17.17487
0:  18.385502  18.481894 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4017758  -3.229772   -2.820167   -2.2399306  -1.7083259  -1.3651614
0:  -0.9764643  -0.68009377 -0.4935832  -0.28290987 -0.23167944 -0.2793002
0:  -0.21125174  0.06597853  0.5913482   1.3351245   2.0190969   2.5270648
0:   0.7967334   0.81144476]
0: validation loss for strategy=forecast at epoch 18 : 0.28176578879356384
0: validation loss for velocity_u : 0.16659952700138092
0: validation loss for velocity_v : 0.25373074412345886
0: validation loss for specific_humidity : 0.16961103677749634
0: validation loss for velocity_z : 0.5099806189537048
0: validation loss for temperature : 0.11278263479471207
0: validation loss for total_precip : 0.4778902530670166
0: 19 : 19:13:55 :: batch_size = 96, lr = 1.3143901141491711e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 19, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5723, -0.5661, -0.5622, -0.5604, -0.5599, -0.5607, -0.5623, -0.5651, -0.5690, -0.5746, -0.5825, -0.5927,
0:         -0.6054, -0.6204, -0.6370, -0.6546, -0.6724, -0.6897, -0.5204, -0.5139, -0.5096, -0.5070, -0.5062, -0.5070,
0:         -0.5096], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-3.3231, -3.2909, -3.2592, -3.2274, -3.1950, -3.1618, -3.1279, -3.0930, -3.0577, -3.0218, -2.9856, -2.9495,
0:         -2.9133, -2.8774, -2.8416, -2.8065, -2.7725, -2.7392, -3.3685, -3.3363, -3.3039, -3.2704, -3.2349, -3.1971,
0:         -3.1570], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6927, -0.6914, -0.6901, -0.6886, -0.6871, -0.6857, -0.6840, -0.6822, -0.6805, -0.6788, -0.6779, -0.6768,
0:         -0.6757, -0.6747, -0.6739, -0.6733, -0.6727, -0.6720, -0.6987, -0.6975, -0.6963, -0.6951, -0.6934, -0.6914,
0:         -0.6895], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4864, 0.4941, 0.4974, 0.4963, 0.4930, 0.4919, 0.4941, 0.5029, 0.5195, 0.5426, 0.5668, 0.5888, 0.6020, 0.5998,
0:         0.5811, 0.5437, 0.4919, 0.4314, 2.3174, 2.3515, 2.3592, 2.3350, 2.2734, 2.1754, 2.0499], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.9903, -0.9709, -0.9495, -0.9262, -0.9011, -0.8750, -0.8482, -0.8221, -0.7978, -0.7756, -0.7563, -0.7395,
0:         -0.7256, -0.7137, -0.7029, -0.6925, -0.6821, -0.6712, -0.6589, -0.6453, -0.6303, -0.6139, -0.5963, -0.5778,
0:         -0.5592], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0.4854, 0.4991, 0.5139, 0.5287, 0.5435, 0.5515, 0.5424, 0.5344, 0.5264, 0.3214, 0.3419, 0.3624, 0.3807, 0.3909,
0:         0.3989, 0.4080, 0.4160, 0.4080, 0.1233, 0.1415, 0.1597, 0.1768, 0.1962, 0.2155, 0.2349], device='cuda:0')
0: [DEBUG] Epoch 19, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.1512,     nan,     nan,     nan,     nan,     nan, -0.1102,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1478,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1626,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1717,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1136,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1956,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1797,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0590,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1011,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1843,     nan,     nan, -0.2287,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2036,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 19, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-2.7214, -2.7654, -2.7575, -2.7104, -2.6627, -2.6238, -2.6160, -2.6155, -2.6594, -2.7152, -2.7953, -2.9108,
0:         -3.0026, -3.0724, -3.0879, -3.0468, -2.9825, -2.9061, -2.5480, -2.6452, -2.6814, -2.6583, -2.6121, -2.5701,
0:         -2.5321], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4131, 0.4371, 0.4591, 0.4736, 0.4746, 0.4508, 0.3841, 0.3242, 0.3026, 0.3127, 0.3458, 0.3855, 0.3924, 0.3491,
0:         0.2868, 0.2553, 0.2642, 0.2849, 0.1854, 0.2150, 0.2472, 0.2875, 0.3040, 0.2787, 0.2228], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7356, -0.7334, -0.7329, -0.7292, -0.7226, -0.7177, -0.7226, -0.7258, -0.7279, -0.7332, -0.7364, -0.7362,
0:         -0.7264, -0.7153, -0.7077, -0.6983, -0.7039, -0.7047, -0.7238, -0.7173, -0.7142, -0.7057, -0.7030, -0.7072,
0:         -0.7091], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0656, -0.2229, -0.3492, -0.5612, -0.8776, -1.0191, -1.1072, -1.2402, -1.2452, -1.2422, -1.2627, -1.1237,
0:         -0.9203, -0.8221, -0.7339, -0.5077, -0.2847, -0.2319,  0.0271, -0.1674, -0.2642, -0.3787, -0.5977, -0.6569,
0:         -0.6978], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1691, -0.1545, -0.1328, -0.1159, -0.0998, -0.0869, -0.0678, -0.0466, -0.0252, -0.0122, -0.0164, -0.0361,
0:         -0.0605, -0.0801, -0.0941, -0.1053, -0.1122, -0.1124, -0.1051, -0.0958, -0.0956, -0.1092, -0.1357, -0.1704,
0:         -0.2106], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0789, -0.0826, -0.0832, -0.0835, -0.0836, -0.0790, -0.0797, -0.0889, -0.0883, -0.1003, -0.0890, -0.0866,
0:         -0.0801, -0.0813, -0.0817, -0.0917, -0.0908, -0.0985, -0.1145, -0.1067, -0.1016, -0.0989, -0.0945, -0.0988,
0:         -0.1019], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.19854258000850677; velocity_v: 0.24470995366573334; specific_humidity: 0.19976644217967987; velocity_z: 0.48650193214416504; temperature: 0.14174425601959229; total_precip: 0.6093065738677979; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21104201674461365; velocity_v: 0.26242920756340027; specific_humidity: 0.20410756766796112; velocity_z: 0.57842618227005; temperature: 0.18083277344703674; total_precip: 0.5722522735595703; 
0: epoch: 19 [1/5 (20%)]	Loss: 0.59078 : 0.29023 :: 0.20553 (2.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19310958683490753; velocity_v: 0.3134440779685974; specific_humidity: 0.1714038997888565; velocity_z: 0.5683813691139221; temperature: 0.14087218046188354; total_precip: 0.5931188464164734; 
0: epoch: 19 [2/5 (40%)]	Loss: 0.59312 : 0.29618 :: 0.20936 (16.06 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19230037927627563; velocity_v: 0.2889496982097626; specific_humidity: 0.21103867888450623; velocity_z: 0.6831902265548706; temperature: 0.144937202334404; total_precip: 1.2640092372894287; 
0: epoch: 19 [3/5 (60%)]	Loss: 1.26401 : 0.43049 :: 0.20254 (16.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2061544954776764; velocity_v: 0.281807541847229; specific_humidity: 0.20482565462589264; velocity_z: 0.5521510243415833; temperature: 0.141204833984375; total_precip: 0.8382997512817383; 
0: epoch: 19 [4/5 (80%)]	Loss: 0.83830 : 0.33631 :: 0.20869 (16.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [1.23977661e-05 1.71661377e-05 3.29017639e-05 5.57899475e-05
0:  1.65939331e-04 2.35080719e-04 2.35557556e-04 2.33650208e-04
0:  1.64985657e-04 1.43527985e-04 1.34944916e-04 1.16348267e-04
0:  1.16825104e-04 1.12056732e-04 2.95162201e-04 3.40461731e-04
0:  9.05513763e-04 1.32131577e-03 2.11620331e-03 1.44958496e-04
0:  9.53674316e-05 7.94410706e-04 1.72090530e-03 3.03936005e-03
0:  1.73521054e-03 2.69985199e-03 3.67546058e-03 5.76019287e-04
0:  1.06811523e-04 2.08854675e-04 3.18050385e-04 7.29560852e-05
0:  1.32560730e-04 1.63555145e-04 1.22070312e-04 8.72612000e-05
0:  4.14371490e-04 2.15530396e-04 2.01702118e-04 8.72612000e-04
0:  1.50632858e-03 9.81330872e-04 2.92301178e-04 2.74181366e-04
0:  3.52859497e-04 2.90870667e-04 3.47137451e-04 1.04427338e-04
0:  9.48905945e-05 7.43865967e-05 3.67164612e-05 1.43051147e-05
0:  8.58306885e-06 1.62124634e-05 2.38418579e-05 2.62260437e-05
0:  1.04904175e-05 1.00135803e-05 1.09672546e-05 3.48091125e-04
0:  4.03881073e-04 8.77380371e-05 1.57356262e-05 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  9.53674316e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 5.10215759e-05 9.53674316e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.05857849e-04
0:  1.19209290e-04 7.43865967e-05 1.09672546e-05 7.43865967e-05
0:  1.23977661e-05 1.04904175e-05 8.58306885e-06 3.33786011e-06
0:  1.90734863e-06 2.86102295e-06 1.90734863e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  2.24113464e-05 1.43051147e-05 4.19616663e-05 6.19888306e-05
0:  1.67369843e-04 2.08854675e-04 2.09808350e-04 1.78337097e-04
0:  1.23500824e-04 1.03950500e-04 1.01566315e-04 8.29696655e-05
0:  5.67436218e-05 7.58171082e-05 4.03404236e-04 4.48703766e-04
0:  4.44412231e-04 1.12628937e-03 3.52573418e-03 1.59502029e-03
0:  8.11576843e-04 1.09004974e-03 2.14910507e-03 1.91068649e-03
0:  6.07013702e-04 1.55067444e-03 1.45339966e-03 1.86347973e-03
0:  1.96504593e-03 7.81059265e-04 4.44412231e-04 1.07765198e-04
0:  6.43730164e-05 9.53674316e-05 1.92642212e-04 1.40666962e-04
0:  1.78813934e-04 3.00884247e-04 2.12669373e-04 6.64234161e-04
0:  1.60312653e-03 1.28078461e-03 5.91754913e-04 4.32491302e-04
0:  6.77585602e-04 1.36375427e-04 4.05311584e-04 8.48770142e-05
0:  7.67707825e-05 6.77108765e-05 3.33786011e-05 1.14440918e-05
0:  1.23977661e-05 2.05039978e-05 5.57899475e-05 1.27792358e-04
0:  1.81198120e-04 7.67707825e-05 6.81877136e-05 1.03950500e-04
0:  1.59740448e-04 8.20159912e-05 1.23977661e-05 4.76837158e-07
0:  0.00000000e+00 4.76837158e-07 9.53674316e-07 1.43051147e-06
0:  9.53674316e-07 9.53674316e-07 0.00000000e+00 9.53674316e-07
0:  4.76837158e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.14440918e-05 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.43051147e-06 1.07288361e-04
0:  1.55448914e-04 1.02996826e-04 1.66893005e-05 9.48905945e-05]
0: Target values (first 200):
0: [1.77288044e-03 5.25474548e-04 4.70161438e-04 1.23500824e-04
0:  9.53674316e-07 0.00000000e+00 9.05990601e-06 1.04904175e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-06
0:  1.57356262e-05 1.23977661e-05 8.29696655e-05 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.43051147e-06 1.85966492e-05 1.11579895e-04 1.87397003e-04
0:  1.86920166e-04 2.06470490e-04 1.26361847e-04 2.28881836e-04
0:  2.16007233e-04 5.72204590e-06 5.72204590e-06 1.43051147e-05
0:  2.14576721e-05 7.62939453e-06 1.19209290e-05 2.67028809e-05
0:  4.48226929e-05 3.86238062e-05 5.10215759e-05 4.95910645e-05
0:  0.00000000e+00 2.57492065e-05 7.96318054e-05 5.05447388e-05
0:  1.05857849e-04 5.00679016e-05 2.24113464e-05 1.62124634e-05
0:  1.66893005e-05 1.81198120e-05 1.57356262e-05 1.57356262e-05
0:  1.52587891e-05 1.47819519e-05 1.28746033e-05 1.33514404e-05
0:  1.85966492e-05 1.52587891e-05 1.14440918e-05 1.00135803e-05
0:  9.49382724e-04 6.26087189e-04 5.83648682e-04 1.22070312e-04
0:  0.00000000e+00 0.00000000e+00 9.05990601e-06 1.23977661e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-06
0:  1.43051147e-05 1.23977661e-05 3.33786011e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 9.53674316e-07 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.43051147e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.71661377e-05 2.95639038e-05 9.34600830e-05
0:  9.91821289e-05 1.29699707e-04 1.03473663e-04 3.00407410e-05
0:  6.19888306e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  6.19888306e-05 1.43051147e-06 1.57356262e-05 1.47819519e-05
0:  1.90734863e-06 2.81333923e-05 6.86645508e-05 9.53674316e-07]
0: Prediction values (first 20):
0: [-6.914287  -6.976279  -6.6328254 -5.8408084 -4.975121  -4.2727675
0:  -3.900642  -3.7338262 -4.073173  -4.4700885 -5.0264487 -5.8455186
0:  -6.540753  -7.137097  -7.4146714 -7.2358003 -6.7611594 -6.152265
0:  -6.008906  -6.1904416]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.011, max = 0.630, mean = -0.525
0:          sample (first 20): tensor([-1.1636, -1.1687, -1.1400, -1.0738, -1.0013, -0.9426, -0.9115, -0.8975, -0.9259, -0.9591, -1.0056, -1.0742,
0:         -1.1323, -1.1822, -1.2054, -1.1905, -1.1507, -1.0998, -1.1064, -1.1617])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2133245  -2.0271368  -1.5324526  -0.75538826 -0.03555107  0.5073242
0:   0.827518    1.0548878   0.9087963   0.73930025  0.40549135 -0.238451
0:  -0.7598443  -1.227747   -1.3494158  -1.0467935  -0.5037694   0.05732536
0:  -0.79291487 -0.6949358 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.218675  -15.2164545 -14.755973  -13.791875  -12.72766   -11.8616495
0:  -11.403848  -11.234887  -11.678239  -12.204778  -12.901788  -13.843411
0:  -14.549521  -15.128421  -15.34051   -15.119658  -14.675916  -14.114394
0:  -14.924411  -15.273673 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.009434 27.691097 27.399925 27.198114 27.159136 26.981543 27.58443
0:  27.992601 28.504536 28.657032 28.20091  27.820148 27.596441 27.83988
0:  28.570944 29.521837 30.367176 30.810123 29.549479 29.409754]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.491839  12.534142  12.715032  12.90932   12.954489  12.840945
0:  12.873772  12.851437  12.78643   12.656263  12.3061695 11.775461
0:  11.361625  11.246065  11.582042  12.305023  13.184643  13.917333
0:  11.849983  11.894089 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.603918  11.261469  11.055119  10.914935  10.755002  10.463026
0:  10.244534   9.9519415  9.488148   9.125905   8.631125   8.070679
0:   7.6633677  7.424987   7.527969   7.881857   8.370522   8.7434845
0:  10.112951  10.05487  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.43902  30.679571 30.657185 30.271425 29.76066  29.147663 29.35349
0:  29.609509 30.0267   30.185732 29.644457 29.16431  28.679445 28.85232
0:  29.631393 30.683306 31.61358  31.967958 31.72519  32.0965  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2353368  -1.4245677  -1.302774   -0.83563375 -0.32311487  0.0330267
0:   0.07943487  0.06049728 -0.3523507  -0.6896548  -1.0665584  -1.7230754
0:  -2.2905965  -2.871409   -3.201159   -3.0649362  -2.5310845  -1.8056827
0:  -2.7067122  -3.0221639 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -5.929649   -6.0739884  -5.7518306  -5.0529714  -4.403753   -4.1188865
0:   -4.212136   -4.6948085  -5.6880946  -6.6063867  -7.628634   -8.789104
0:   -9.715563  -10.411488  -10.605478  -10.347263   -9.900267   -9.399609
0:   -8.503742   -8.6171   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.866638 13.697649 13.679606 13.672466 13.587895 13.342848 13.388977
0:  13.390356 13.33975  13.21204  12.685073 12.065467 11.458954 11.210579
0:  11.356517 11.778173 12.17687  12.386232 10.41354  10.33284 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.907671   7.2761517  7.856897   8.493      8.872432   8.998295
0:   9.295448   9.592719   9.895241  10.166301  10.136953   9.903124
0:   9.736498   9.882278  10.476284  11.439526  12.532249  13.458806
0:  13.391459  14.01425  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.353266  12.881947  13.593893  14.235014  14.633007  14.837639
0:  15.048199  15.22838   15.286161  15.325455  15.2249775 15.0334835
0:  15.003166  15.15822   15.66363   16.346668  16.97925   17.382872
0:  14.957056  15.440994 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1032434 -3.9285426 -3.4664426 -2.8170314 -2.26656   -1.943315
0:  -1.7260218 -1.5887356 -1.6907663 -1.7403846 -1.9391685 -2.3870769
0:  -2.7611022 -3.0343127 -3.048327  -2.6981006 -2.2267575 -1.820077
0:  -2.61056   -2.4089408]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.226227 23.139055 22.81521  22.211267 21.549736 20.785698 20.915804
0:  21.075607 21.392998 21.461985 20.823238 20.274467 19.660889 19.757053
0:  20.437025 21.413185 22.223814 22.429688 22.467545 22.463871]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.399315  8.844453  9.452122 10.136776 10.666288 10.994178 11.117443
0:  11.179729 11.011699 10.89374  10.764344 10.425333 10.208744 10.033263
0:  10.183014 10.730017 11.581278 12.469448 10.527541 10.78342 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.841914 16.067343 16.41662  16.759855 16.970495 17.007835 17.252914
0:  17.460398 17.611332 17.699959 17.4911   17.12478  16.831326 16.739086
0:  17.018589 17.620296 18.374195 19.009695 17.522833 17.723412]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.150217 24.177095 24.281752 24.363075 24.250631 23.839893 23.63073
0:  23.391403 23.10869  22.72985  22.0706   21.279528 20.63942  20.306438
0:  20.531782 21.152555 21.948889 22.603062 20.607307 20.68732 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.941444  11.1876745 11.627825  12.104837  12.338087  12.261536
0:  12.058659  11.939684  11.690281  11.552248  11.33538   10.804826
0:  10.263183   9.673411   9.323214   9.413796   9.874338  10.409709
0:   7.439155   7.26441  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.700421  3.5383167 3.6137881 3.8917282 4.0929317 4.123453  4.1282873
0:  4.1302204 3.9151626 3.7614138 3.4651928 2.907724  2.3694234 1.8951068
0:  1.6714067 1.8678722 2.342023  2.8615975 1.0461826 0.9269705]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.21922112 -0.14725685  0.34440422  1.1740766   2.0953193   2.866132
0:   3.5923553   4.104946    4.1998672   4.2219114   3.9836938   3.5355616
0:   3.2490156   3.042117    3.1162946   3.347978    3.5477848   3.6103122
0:   1.8835244   1.8501296 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.37398  33.854084 34.21091  34.294422 34.086906 33.381195 32.99184
0:  32.203064 31.272243 30.06562  28.388786 26.940014 25.96091  25.783203
0:  26.495453 27.772861 29.248123 30.534042 32.760483 33.102657]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.750105 18.590244 18.35525  18.012264 17.620192 17.032785 17.1863
0:  17.282686 17.50252  17.5153   16.890732 16.270082 15.588692 15.476116
0:  15.911404 16.722836 17.542667 17.987642 17.84219  17.728374]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.60740423  0.4462881   0.51599693  0.8681512   1.2439651   1.5140905
0:   1.5270376   1.458632    0.9880533   0.56191206  0.0473485  -0.6738162
0:  -1.2705879  -1.8052354  -2.0345216  -1.8599515  -1.3536763  -0.71255636
0:  -1.151433   -1.1428852 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.749989  3.9323454 4.317795  4.8686266 5.3621674 5.6985693 5.8943644
0:  6.020462  5.858112  5.7464943 5.523938  5.120187  4.7797165 4.4733872
0:  4.420948  4.6732354 5.1002684 5.5207253 4.9530616 5.164276 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2163162 2.2398057 2.5576587 3.024863  3.4722528 3.7757447 4.1189394
0:  4.358289  4.3964386 4.380108  4.1679106 3.7581494 3.5080256 3.4510372
0:  3.6456988 4.1148357 4.5828023 4.896327  1.395225  1.2526269]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0389714 -4.264191  -4.1710687 -3.7195039 -3.189672  -2.8330512
0:  -2.7118182 -2.6608567 -3.0365539 -3.3971953 -3.9033275 -4.7101674
0:  -5.391825  -6.0664525 -6.374964  -6.3087077 -5.978661  -5.605575
0:  -6.3300815 -6.5375876]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.441331  4.6895075 5.219505  5.8665547 6.3667264 6.5632763 6.8066797
0:  6.8445387 6.734084  6.5833964 6.195291  5.691838  5.299618  5.2038774
0:  5.4916954 6.110603  6.7770824 7.2450433 5.487585  5.2742343]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.745943 20.098455 20.665869 21.19086  21.404041 21.186514 21.096815
0:  20.928022 20.766035 20.621214 20.289883 19.78733  19.422588 19.252645
0:  19.484072 20.08009  20.856026 21.49024  18.207779 18.251427]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.160085  -4.112245  -3.7550635 -3.1141458 -2.4117198 -1.8416381
0:  -1.4427066 -1.1647096 -1.3284097 -1.5495391 -1.9221206 -2.5492606
0:  -2.959311  -3.3027253 -3.3263202 -3.0539346 -2.6726422 -2.2901444
0:  -3.6834083 -3.6293244]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.851902 16.9099   17.137348 17.315056 17.23219  16.863743 16.653467
0:  16.488232 16.390438 16.345585 16.10571  15.652399 15.250711 14.93798
0:  14.949356 15.253064 15.709214 16.069893 12.278697 12.47079 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.372936 29.269213 29.023197 28.683949 28.375523 27.849087 28.104101
0:  28.121174 28.220482 28.092522 27.377174 26.715929 26.233782 26.317757
0:  27.0842   28.300137 29.5333   30.414003 30.4219   30.32624 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.204565 18.33665  18.764051 19.268894 19.666973 19.824455 20.042091
0:  20.20916  20.245872 20.382973 20.284363 19.91322  19.403938 18.808092
0:  18.252125 17.713028 17.050636 16.230476 10.410305  9.8692  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.6512165 12.306126  12.163891  12.032814  11.818592  11.4069395
0:  11.1822605 10.864373  10.555198  10.189634   9.58524    8.855665
0:   8.2282715  7.9742775  8.190585   8.793863   9.493985  10.062984
0:   8.495602   8.236481 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8347917  -1.8465104  -1.5475745  -0.9589443  -0.36313868  0.12734413
0:   0.3234825   0.47478676  0.24911213  0.05123615 -0.24479342 -0.84641504
0:  -1.4072309  -1.9566197  -2.188725   -1.8776946  -1.1259298  -0.14045191
0:  -1.1030736  -1.2224064 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1638703  -2.287087   -2.1130385  -1.5704141  -1.0157056  -0.620842
0:  -0.54095316 -0.50641394 -0.8556442  -1.1388888  -1.4760795  -2.0973096
0:  -2.6456246  -3.1701903  -3.4308314  -3.2267947  -2.6478696  -1.9415498
0:  -2.4754014  -2.645434  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.651394  14.503155  14.3796215 14.281317  14.18911   13.874274
0:  14.185941  14.300224  14.514508  14.535993  14.008414  13.537167
0:  13.122965  13.291363  13.941927  14.948899  15.941601  16.531012
0:  15.712792  15.671434 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.70308  31.470922 30.945147 30.201601 29.574327 28.777012 28.887398
0:  28.815903 28.868544 28.668667 27.833803 27.175226 26.69287  26.82477
0:  27.564844 28.675581 29.765339 30.548336 30.738205 30.778597]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.435066   3.941598   3.5812912  3.3485072  3.0526996  2.6453924
0:   2.3268886  1.9212551  1.2823339  0.5279727 -0.550673  -1.8779273
0:  -3.0798726 -3.9618125 -4.4393134 -4.508241  -4.477265  -4.449922
0:  -6.738813  -6.964351 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.18057203  0.37136698  0.8153949   1.5092392   2.1067038   2.473719
0:   2.5098715   2.4495335   2.0373182   1.7122355   1.3482738   0.7099881
0:   0.18066692 -0.3517394  -0.56539106 -0.27398634  0.40119934  1.2231383
0:  -0.21987247 -0.2906475 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.380362 20.481993 20.450972 20.255146 20.061775 19.79689  20.347212
0:  20.857412 21.464249 21.776913 21.403976 21.02651  20.547783 20.712257
0:  21.359348 22.338118 23.245115 23.704653 25.661383 25.92956 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.9422603 2.162261  2.6328444 3.215779  3.729346  3.9747398 4.130411
0:  4.149918  3.9134338 3.678182  3.325586  2.8586812 2.5490313 2.4328232
0:  2.589017  2.9659739 3.3047972 3.4431214 2.4669967 2.4079285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4574142 2.3837986 2.5578895 2.7963557 3.0227163 3.095447  3.5515952
0:  3.893269  4.2555556 4.4882793 4.3213506 3.9668252 3.664471  3.7970881
0:  4.3208675 5.310046  6.301166  7.0778375 5.564624  5.578051 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.51086    -8.337847   -7.679235   -6.518186   -5.1136317  -3.6608853
0:  -2.243713   -0.7938051   0.35989475  1.4926062   2.424208    2.8472567
0:   3.1304915   3.1449416   3.1252444   3.2859588   3.5307636   3.729497
0:   0.16693783  0.04758215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.238135 11.250741 11.458116 11.701773 11.866226 11.826727 12.043538
0:  12.138239 12.266268 12.351295 12.185392 11.908789 11.711111 11.736998
0:  12.134789 12.894152 13.779419 14.54594  13.253258 13.397351]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.87499  20.7441   20.630718 20.558546 20.49957  20.277683 20.7077
0:  20.999485 21.348646 21.440506 21.008463 20.562634 20.211401 20.373682
0:  21.090605 22.200321 23.422443 24.30578  24.400314 24.586426]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.261003 -18.461721 -18.193945 -17.431568 -16.576607 -15.939512
0:  -15.794752 -15.932386 -16.626501 -17.304432 -18.046427 -18.991114
0:  -19.640404 -20.159355 -20.280582 -19.88155  -19.182545 -18.34318
0:  -18.626287 -18.887066]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.069189  11.483622  11.98666   12.372774  12.594253  12.607297
0:  13.044304  13.49646   13.986536  14.465128  14.5397005 14.525633
0:  14.480761  14.914765  15.712379  16.791084  17.753153  18.30291
0:  17.183247  17.535946 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.632383   9.829794  10.165159  10.52784   10.727373  10.720524
0:  10.69733   10.696268  10.595891  10.538478  10.3631115  9.937346
0:   9.554504   9.2716     9.32325    9.777821  10.520378  11.298731
0:   9.443753   9.68276  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.650572  9.359831  9.176627  9.017817  8.858712  8.526486  8.67395
0:  8.707472  8.823385  8.8145275 8.34778   7.795861  7.2451873 7.127478
0:  7.423419  8.093946  8.761416  9.188922  7.9962683 7.9011307]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.7465386 5.5842614 5.6366534 5.790475  5.861985  5.73049   5.9395866
0:  6.0893736 6.25031   6.313983  5.9291825 5.374658  4.6862707 4.3229923
0:  4.2839823 4.5029325 4.655786  4.526699  2.1590087 1.6584358]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.468092  5.769691  6.229502  6.694381  7.10003   7.420867  8.215185
0:   9.00517   9.859146 10.598948 10.905804 11.02908  11.123822 11.64821
0:  12.54607  13.751467 14.843863 15.609541 14.910162 15.00634 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1223912   0.73484325  0.5680623   0.68868685  0.7851262   0.7193761
0:   0.34264135 -0.0654974  -0.7755399  -1.3679261  -1.9376397  -2.7322245
0:  -3.40899    -4.073513   -4.471217   -4.439883   -4.04539    -3.5235634
0:  -4.4451694  -4.674636  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.225582 20.284498 20.363691 20.34875  20.233892 20.043491 20.225874
0:  20.432034 20.640934 20.694017 20.34088  19.864458 19.474707 19.525286
0:  20.03764  20.821932 21.57043  22.082388 19.843363 20.040133]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.131234   9.925181   9.918627  10.041898  10.0301075  9.794506
0:   9.622186   9.40732    9.040992   8.771071   8.348586   7.744831
0:   7.233765   6.9585686  7.0607023  7.573803   8.243897   8.793174
0:   7.2613783  7.2530413]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0447173 -3.0993228 -2.8884158 -2.395266  -1.8782554 -1.4553895
0:  -1.3372035 -1.1946349 -1.4657874 -1.6854005 -1.9769053 -2.6445994
0:  -3.226418  -3.881019  -4.212587  -4.0090985 -3.3032107 -2.3556256
0:  -2.716372  -2.6494575]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.282442  10.362196  10.641593  10.938074  11.080353  11.059
0:  11.130266  11.153448  11.11578   11.095469  10.866903  10.483005
0:  10.1358795 10.011911  10.254394  10.805334  11.460784  11.999754
0:  10.398594  10.512993 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.29236   13.244196  13.346256  13.535247  13.627259  13.587669
0:  13.51854   13.517094  13.35451   13.206422  12.943018  12.430997
0:  12.03751   11.733868  11.789354  12.208165  12.8616705 13.437901
0:  11.55405   11.368172 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.109003  -8.168657  -7.868943  -7.203247  -6.4921713 -5.9253197
0:  -5.6405487 -5.453171  -5.672942  -5.876438  -6.1722813 -6.766692
0:  -7.218896  -7.674658  -7.856537  -7.6165915 -7.1227884 -6.5503387
0:  -7.270854  -7.3960276]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.437037   -1.2368455  -0.7010226   0.1871934   1.1226268   1.9174266
0:   2.4324832   2.813746    2.7045884   2.5572839   2.2134752   1.5966201
0:   1.0477033   0.582222    0.45314646  0.77410936  1.4185548   2.1288447
0:   0.89694405  0.76142883]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.336193  -12.347836  -11.939844  -11.1096115 -10.028545   -8.925516
0:   -7.635864   -6.4270473  -5.4797797  -4.619785   -3.9887547  -3.5756898
0:   -3.0482059  -2.372519   -1.5244589  -0.5473833   0.2480402   0.8435316
0:   -2.2663856  -2.2881417]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.140066 19.992847 19.985254 19.954233 19.77889  19.312359 19.08174
0:  18.884176 18.738102 18.569359 18.161139 17.639072 17.190924 17.056692
0:  17.445536 18.21832  19.114683 19.822502 18.803877 18.69476 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.7249   28.737232 28.670387 28.305798 27.545563 26.443836 25.281746
0:  24.138718 22.900597 21.703426 20.34308  18.784706 17.381062 16.17377
0:  15.449252 15.171659 15.294506 15.486364 11.827518 11.427679]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.560886 -11.892221 -11.872702 -11.502062 -11.111048 -10.997591
0:  -11.003941 -11.175896 -11.69614  -12.282877 -13.125675 -14.264856
0:  -15.234602 -15.999855 -16.341934 -16.229403 -15.95661  -15.715456
0:  -17.348955 -18.00073 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.521617  8.5811615 8.839804  9.283274  9.636069  9.797385  9.740923
0:  9.6542635 9.32384   9.111852  8.874769  8.401656  7.9630275 7.484058
0:  7.2652807 7.4809613 8.05097   8.694653  6.9443107 6.7973943]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.563073   7.722642   7.979194   8.318787   8.611268   8.792179
0:   9.007409   9.231789   9.3031225  9.405333   9.386857   9.162647
0:   8.982236   8.864278   8.984059   9.397436  10.026609  10.601858
0:   8.721828   8.824973 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5691752 -5.7835937 -5.600096  -5.162805  -4.7663665 -4.584579
0:  -4.4513345 -4.4629436 -4.680077  -4.9146886 -5.310329  -5.953669
0:  -6.4596925 -6.739783  -6.7281833 -6.2585273 -5.653913  -5.100508
0:  -6.56375   -6.750032 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.606695   -9.786676   -9.488077   -8.777405   -8.050444   -7.5908694
0:   -7.3961387  -7.5610538  -8.247192   -8.917992   -9.728842  -10.634951
0:  -11.241928  -11.577709  -11.536686  -11.160652  -10.759542  -10.408436
0:  -10.854486  -10.738691 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.619367  -5.6835113 -5.4165635 -4.84438   -4.2768826 -3.8459077
0:  -3.7048306 -3.5956998 -3.82789   -3.9699368 -4.1472907 -4.6034875
0:  -4.9348054 -5.2765384 -5.330051  -4.884316  -4.0644565 -3.1165547
0:  -3.633864  -3.5232491]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.160302 12.566824 12.119656 11.734026 11.372528 10.830904 10.806181
0:  10.650333 10.640869 10.457146  9.781218  9.017486  8.33423   8.128821
0:   8.44085   9.215726 10.114418 10.883866  9.610879  9.307987]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9518046 -2.131527  -2.1396065 -1.9543276 -1.7715588 -1.7044506
0:  -1.6172113 -1.5984764 -1.7423387 -1.9492269 -2.3278494 -2.823288
0:  -3.2145786 -3.4043074 -3.28082   -2.897294  -2.5044355 -2.176159
0:  -3.8277178 -3.9675226]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.413603 23.149677 22.841753 22.471369 22.026041 21.363455 21.24864
0:  21.035984 20.878174 20.591436 19.828901 19.00571  18.182898 17.860031
0:  18.06601  18.732958 19.574245 20.182804 20.089012 20.089497]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.35121   15.222067  15.0758095 14.862968  14.679989  14.340406
0:  14.633129  14.841581  15.19578   15.3697815 15.046345  14.724545
0:  14.357819  14.467361  15.012028  15.923346  16.826439  17.430096
0:  17.304667  17.392769 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.4748168   1.2831717   1.3259182   1.4681816   1.5323243   1.5124788
0:   1.8294516   2.11757     2.2455268   2.2414632   1.7701602   0.94547176
0:   0.17675877 -0.17904949 -0.00634146  0.48447275  0.9000282   1.0442271
0:   1.1518645   1.3168488 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6202722 -2.7344317 -2.5819802 -2.1393542 -1.7070479 -1.462904
0:  -1.5470896 -1.8293271 -2.557405  -3.2410884 -3.9634633 -4.833176
0:  -5.4796176 -6.0045924 -6.189588  -6.010117  -5.5754943 -5.0324483
0:  -5.646459  -5.6449704]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2524166  -1.3396506  -1.1167779  -0.6792512  -0.28188848 -0.04565859
0:   0.14568138  0.26896954  0.17481804  0.08351088 -0.15914011 -0.6374898
0:  -0.9582982  -1.1065555  -0.9329591  -0.3765893   0.2820654   0.86327934
0:  -0.27643538 -0.16802454]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.322683 16.754698 17.340528 17.832882 18.187414 18.469856 18.917482
0:  19.506752 20.056606 20.556784 20.893232 21.070324 21.477034 22.087639
0:  23.168833 24.449762 25.74699  26.85206  24.266632 24.877596]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2891288  -2.4152389  -2.2432332  -1.7618289  -1.2846818  -0.95294285
0:  -0.96707964 -0.9672365  -1.2932973  -1.5306687  -1.7987037  -2.3793454
0:  -2.949903   -3.5691085  -3.891192   -3.7240138  -3.122808   -2.306707
0:  -3.0397162  -3.214149  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.775446 23.96599  24.168999 24.308144 24.254002 23.827036 23.714977
0:  23.387636 23.023615 22.53988  21.68475  20.789898 20.08387  19.861664
0:  20.141916 20.84791  21.659649 22.315142 21.540062 21.652384]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.4187188 -0.7579446 -0.886261  -0.8150134 -0.7682004 -0.8442569
0:  -1.0549703 -1.2679086 -1.7636757 -2.2062397 -2.7484298 -3.516996
0:  -4.172708  -4.7217793 -4.9841504 -4.877545  -4.5862203 -4.291179
0:  -4.9846425 -5.2625623]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.65778  27.717136 27.869335 27.983994 27.941002 27.545002 27.554409
0:  27.467087 27.57901  27.742046 27.668308 27.577717 27.674297 28.171661
0:  29.202213 30.542416 31.937698 32.932617 30.489784 30.47065 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6608448  -2.5514975  -2.138998   -1.4358435  -0.7355118  -0.17011118
0:   0.10498953  0.38331413  0.31819725  0.34247684  0.31787872 -0.02904654
0:  -0.33349323 -0.7011075  -0.8410797  -0.49801826  0.26280832  1.1991277
0:   0.38093662  0.3469224 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.36057854  0.1831913   0.23950481  0.31642103  0.3030548   0.1588211
0:   0.5465522   0.9279847   1.2556977   1.2364407   0.40491104 -0.7792201
0:  -2.2132611  -3.212048   -3.837545   -4.288894   -5.0236545  -6.036588
0:  -8.419548   -9.173819  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.735466   5.9718747  6.473967   7.064511   7.5173593  7.807954
0:   8.144262   8.583052   8.910383   9.244423   9.3991375  9.219814
0:   9.05057    8.958248   9.157398   9.735536  10.470668  11.105711
0:   9.728029  10.034493 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.501956   0.56170225 0.85839224 1.4101648  1.9984407  2.4522178
0:  2.677164   2.8291152  2.6006453  2.4037225  2.1326911  1.6226826
0:  1.2301598  0.8706598  0.7489939  0.95692635 1.3515677  1.7804494
0:  0.76028156 0.77212095]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.371395 25.265423 25.007433 24.501894 23.936619 23.307274 23.525644
0:  23.861946 24.353731 24.651388 24.30558  23.964134 23.481297 23.52336
0:  24.013206 24.700483 25.143349 25.011425 24.206993 23.998745]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.24854612  0.02975035  0.06751871  0.41761065  0.74685335  0.9017415
0:   0.79055834  0.64850473  0.17536163 -0.19892168 -0.62632465 -1.3517823
0:  -2.010674   -2.7064977  -3.148005   -3.088861   -2.5949378  -1.9066739
0:  -3.1577725  -3.6409297 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.874241 11.877792 12.073714 12.453655 12.809517 12.951233 13.580519
0:  14.013398 14.315786 14.316795 13.70203  12.917342 12.10404  11.793629
0:  11.967086 12.597733 13.242748 13.621981 10.896812 10.588711]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.130206  3.2665117 3.6200807 4.17873   4.677194  4.9984922 5.051999
0:  5.037922  4.6634936 4.366428  4.041658  3.5044336 3.0769298 2.6590774
0:  2.4910295 2.68852   3.1615531 3.7136877 2.6676161 2.7188735]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.331034 -11.906721 -11.926929 -11.226282 -10.229815  -9.269903
0:   -8.655113  -8.244154  -8.337961  -8.476652  -8.707655  -9.232965
0:   -9.639755 -10.068748 -10.293358 -10.057341  -9.482485  -8.725689
0:   -8.996342  -9.11742 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.3935714 5.4920034 5.702272  5.897689  5.958653  5.814149  5.8546095
0:  5.8616543 5.7276216 5.5851135 5.228404  4.664381  4.28538   4.2211475
0:  4.598698  5.3519554 6.236324  6.956981  4.945159  4.891788 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.430473 16.934711 17.513199 18.102516 18.506657 18.661346 19.036552
0:  19.356083 19.723991 20.066578 20.249126 20.255138 20.39877  20.704771
0:  21.252453 21.98685  22.719341 23.2183   20.147331 20.259651]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.35011  26.826263 27.379917 27.840122 28.065037 28.122913 28.60926
0:  29.114052 29.707489 30.222675 30.353485 30.382248 30.540867 30.983585
0:  31.842476 32.880154 33.868523 34.55134  32.377563 32.7214  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.443895 31.835445 31.98244  31.806469 31.309208 30.420637 29.881908
0:  29.122208 28.267567 27.267748 25.77192  24.345728 23.08801  22.392324
0:  22.341671 22.76325  23.492363 24.122444 25.63758  26.029285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.657534  13.801884  14.067448  14.379825  14.595758  14.675501
0:  14.8519535 14.949917  14.896629  14.711046  14.221912  13.529114
0:  12.867076  12.430668  12.377132  12.677653  13.11697   13.481458
0:  11.830986  11.920504 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2849736  -1.094933   -0.6843858  -0.11521578  0.3644681   0.6635852
0:   0.7796321   0.7603326   0.38991642  0.04063797 -0.38722944 -0.9704428
0:  -1.3497686  -1.5622506  -1.415576   -0.8902426  -0.17331791  0.55037785
0:   0.7495179   1.005961  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.7741599  -0.86745024 -0.688354   -0.33423042 -0.0201292   0.13631535
0:   0.38898325  0.58386517  0.62950754  0.6552539   0.47934532  0.10893297
0:  -0.13165236 -0.14193201  0.12517977  0.7242789   1.3205299   1.7523756
0:  -0.18356562 -0.38154364]
0: validation loss for strategy=forecast at epoch 19 : 0.2943618595600128
0: validation loss for velocity_u : 0.14526031911373138
0: validation loss for velocity_v : 0.23880036175251007
0: validation loss for specific_humidity : 0.15721212327480316
0: validation loss for velocity_z : 0.49473270773887634
0: validation loss for temperature : 0.11470410227775574
0: validation loss for total_precip : 0.6154613494873047
0: 20 : 19:17:55 :: batch_size = 96, lr = 1.2823318186821183e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 20, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4540, -0.4444, -0.4277, -0.4042, -0.3743, -0.3391, -0.3006, -0.2617, -0.2239, -0.1874, -0.1522, -0.1194,
0:         -0.0892, -0.0613, -0.0350, -0.0100,  0.0150,  0.0416, -0.4970, -0.4911, -0.4800, -0.4621, -0.4374, -0.4068,
0:         -0.3717], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1891, -0.2079, -0.2233, -0.2326, -0.2351, -0.2307, -0.2191, -0.2010, -0.1763, -0.1459, -0.1113, -0.0737,
0:         -0.0343,  0.0056,  0.0453,  0.0829,  0.1179,  0.1498, -0.0750, -0.0720, -0.0710, -0.0693, -0.0657, -0.0589,
0:         -0.0469], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5040, -0.5059, -0.5103, -0.5193, -0.5125, -0.4958, -0.5070, -0.5193, -0.5083, -0.5161, -0.5338, -0.5512,
0:         -0.5692, -0.5865, -0.5867, -0.5709, -0.5492, -0.5420, -0.4755, -0.4759, -0.4823, -0.4845, -0.4905, -0.4978,
0:         -0.5041], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5478, 0.5812, 0.6214, 0.6548, 0.6303, 0.5679, 0.5366, 0.5500, 0.5612, 0.5500, 0.5456, 0.5567, 0.5545, 0.5322,
0:         0.5121, 0.4898, 0.4429, 0.4162, 0.3872, 0.4318, 0.4831, 0.5277, 0.5545, 0.5723, 0.6102], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.7278, -0.7220, -0.7137, -0.7039, -0.6932, -0.6833, -0.6737, -0.6639, -0.6530, -0.6401, -0.6253, -0.6087,
0:         -0.5911, -0.5720, -0.5523, -0.5334, -0.5176, -0.5039, -0.4908, -0.4768, -0.4611, -0.4411, -0.4149, -0.3809,
0:         -0.3402], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1923, -0.1807, -0.1831, -0.1900, -0.2016, -0.2086, -0.2063, -0.1807, -0.1807, -0.1923, -0.1645, -0.1668,
0:         -0.1714, -0.1505, -0.1296, -0.1552, -0.1320, -0.1877, -0.0716, -0.0507, -0.0390, -0.0367, -0.0251,  0.0051,
0:         -0.0205], device='cuda:0')
0: [DEBUG] Epoch 20, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.1900,     nan,     nan,     nan,     nan,     nan,     nan, -0.0785,     nan,     nan,     nan, -0.1273,
0:             nan,     nan,     nan, -0.0414,     nan,     nan,     nan,     nan,     nan, -0.1320,     nan,     nan,
0:         -0.1134,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan, -0.2132,     nan,
0:             nan,     nan, -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1854,     nan,     nan, -0.1296,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0623,     nan,     nan,     nan, -0.2179,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,
0:             nan,     nan,     nan, -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.0353,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1482, -0.1227,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1552,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0808,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1784,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan, -0.2481,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 20, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0028, 1.0235, 1.0553, 1.0872, 1.1096, 1.1118, 1.1463, 1.1779, 1.2106, 1.2434, 1.2431, 1.2298, 1.2064, 1.2035,
0:         1.2237, 1.2675, 1.3146, 1.3421, 1.0192, 1.0494, 1.0902, 1.1201, 1.1176, 1.1190, 1.1312], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2804, -1.2430, -1.2268, -1.2402, -1.2969, -1.4047, -1.5624, -1.7089, -1.7803, -1.8009, -1.7624, -1.7119,
0:         -1.7089, -1.7503, -1.8169, -1.8427, -1.8060, -1.7077, -1.3729, -1.3220, -1.2736, -1.2559, -1.2863, -1.4001,
0:         -1.5351], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7368, -0.7416, -0.7414, -0.7291, -0.7154, -0.7066, -0.7047, -0.7071, -0.7128, -0.7221, -0.7288, -0.7302,
0:         -0.7204, -0.7037, -0.6906, -0.6808, -0.6798, -0.6798, -0.7352, -0.7409, -0.7381, -0.7259, -0.7141, -0.7050,
0:         -0.6990], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3323, 0.5113, 0.5979, 0.5287, 0.3867, 0.4416, 0.5612, 0.5491, 0.5834, 0.7179, 0.8340, 0.9404, 1.1005, 1.2222,
0:         1.3123, 1.4913, 1.6674, 1.6386, 0.4948, 0.4197, 0.3547, 0.3325, 0.2962, 0.4273, 0.5712], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.5546, -1.6552, -1.6816, -1.6586, -1.6210, -1.6195, -1.6475, -1.6893, -1.7175, -1.7353, -1.7574, -1.7757,
0:         -1.7859, -1.7623, -1.7210, -1.6822, -1.6759, -1.6944, -1.7127, -1.7153, -1.7072, -1.7042, -1.7110, -1.7206,
0:         -1.7010], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1982, -0.2063, -0.2107, -0.2131, -0.2072, -0.2072, -0.2063, -0.2089, -0.2066, -0.2180, -0.2104, -0.2121,
0:         -0.2102, -0.2059, -0.2032, -0.2104, -0.2082, -0.2152, -0.2201, -0.2164, -0.2140, -0.2084, -0.2064, -0.2021,
0:         -0.2123], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.18138110637664795; velocity_v: 0.27691683173179626; specific_humidity: 0.19898034632205963; velocity_z: 0.5038612484931946; temperature: 0.14458529651165009; total_precip: 0.5277725458145142; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19431737065315247; velocity_v: 0.28005680441856384; specific_humidity: 0.1992463618516922; velocity_z: 0.5379647016525269; temperature: 0.1637713462114334; total_precip: 0.7176797986030579; 
0: epoch: 20 [1/5 (20%)]	Loss: 0.62273 : 0.29323 :: 0.20514 (2.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19597496092319489; velocity_v: 0.2852805256843567; specific_humidity: 0.22750428318977356; velocity_z: 0.6142969727516174; temperature: 0.15966227650642395; total_precip: 0.7772096395492554; 
0: epoch: 20 [2/5 (40%)]	Loss: 0.77721 : 0.34121 :: 0.20666 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1945643275976181; velocity_v: 0.28519222140312195; specific_humidity: 0.19142912328243256; velocity_z: 0.5991288423538208; temperature: 0.14300547540187836; total_precip: 0.8039149045944214; 
0: epoch: 20 [3/5 (60%)]	Loss: 0.80391 : 0.33583 :: 0.20609 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20847201347351074; velocity_v: 0.3245823383331299; specific_humidity: 0.1992764174938202; velocity_z: 0.7571988701820374; temperature: 0.19888703525066376; total_precip: 1.1916677951812744; 
0: epoch: 20 [4/5 (80%)]	Loss: 1.19167 : 0.44428 :: 0.20700 (16.02 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [5.2452087e-05 7.8678131e-05 8.7261200e-05 7.9631805e-05 3.7193298e-05
0:  2.0503998e-05 1.4305115e-05 1.5258789e-05 7.1525574e-06 8.5830688e-06
0:  9.5367432e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 9.5367432e-07 1.9073486e-06 3.3378601e-06 7.6293945e-06
0:  1.8596649e-05 4.0531158e-05 4.2915344e-05 8.4877014e-05 1.4066696e-04
0:  1.3208389e-04 5.7220459e-05 3.8146973e-05 1.4305115e-05 2.8133392e-05
0:  4.3392181e-05 5.2928925e-05 4.3869019e-05 3.2424927e-05 1.7642975e-05
0:  5.7220459e-06 1.4305115e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 9.5367432e-07 9.5367432e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 4.7683716e-07 4.7683716e-06 6.1988831e-06
0:  5.7220459e-06 3.8146973e-06 2.8610229e-06 0.0000000e+00 4.7683716e-07
0:  4.7683716e-07 0.0000000e+00 0.0000000e+00 4.7683716e-07 0.0000000e+00
0:  4.7683716e-07 3.3378601e-06 7.1525574e-06 9.0599060e-06 8.5830688e-06
0:  3.8146973e-06 9.5367432e-07 9.5367432e-07 9.5367432e-07 1.4305115e-06
0:  1.0013580e-05 2.0980835e-05 2.9087067e-05 2.2888184e-05 2.3841858e-06
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07 2.8610229e-06
0:  3.3378601e-06 1.4305115e-06 1.4305115e-06 5.7220459e-06 6.6757202e-06
0:  5.2452087e-06 7.6293945e-06 7.1525574e-06 2.9087067e-05 2.6702881e-05
0:  2.2888184e-05 2.0503998e-05 1.8596649e-05 2.2411346e-05 3.5285950e-05
0:  2.3841858e-05 2.4795532e-05 3.4332275e-05 3.9577484e-05 6.4849854e-05
0:  8.1062317e-05 7.3432922e-05 1.0204315e-04 9.2506409e-05 1.7786026e-04
0:  1.8978119e-04 1.2540817e-04 8.3446503e-05 5.0067902e-05 4.4822693e-05
0:  4.1484833e-05 2.4318695e-05 1.2397766e-05 3.3378601e-06 2.3841858e-06
0:  4.7683716e-07 4.7683716e-07 0.0000000e+00 0.0000000e+00 4.7683716e-07
0:  1.9073486e-06 2.8610229e-06 4.2915344e-06 1.9073486e-05 2.6226044e-05
0:  4.5299530e-05 9.2506409e-05 1.4257431e-04 1.4543533e-04 9.7274780e-05
0:  5.7697296e-05 1.5735626e-05 1.7642975e-05 2.2888184e-05 4.1007996e-05
0:  3.9100647e-05 3.0517578e-05 2.4318695e-05 1.0013580e-05 5.2452087e-06
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  1.4305115e-06 1.9073486e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 4.7683716e-07 4.7683716e-07 4.7683716e-07
0:  2.3841858e-06 7.6293945e-06 2.1934509e-05 2.0503998e-05 1.6689301e-05
0:  8.5830688e-06 5.7220459e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 9.5367432e-07 2.3841858e-06 3.3378601e-06 2.8610229e-06
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3378601e-06 2.8610229e-06
0:  1.4305115e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07]
0: Target values (first 200):
0: [1.11579895e-04 7.00950623e-05 8.63075256e-05 1.33514404e-04
0:  1.74045563e-04 2.73704529e-04 4.89711761e-04 6.14166260e-04
0:  4.97341156e-04 3.46660614e-04 1.36852264e-04 4.29153442e-05
0:  1.62124634e-05 4.76837158e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 2.38418579e-06
0:  4.76837158e-06 2.86102295e-05 1.90734863e-05 9.53674316e-07
0:  1.90734863e-06 2.86102295e-06 6.67572021e-06 2.19345093e-05
0:  2.76565552e-05 3.24249268e-05 3.57627869e-05 2.86102295e-05
0:  2.67028809e-05 2.38418579e-05 8.58306885e-06 3.81469727e-06
0:  1.90734863e-06 1.90734863e-06 7.62939453e-06 1.71661377e-05
0:  3.52859497e-05 2.86102295e-05 4.48226929e-05 5.53131104e-05
0:  3.43322754e-05 9.53674316e-06 4.76837158e-07 9.53674316e-07
0:  1.43051147e-06 5.24520874e-06 1.19209290e-05 1.04904175e-05
0:  3.33786011e-06 3.33786011e-06 3.81469727e-06 4.29153442e-06
0:  1.23977661e-05 2.28881836e-05 3.81469727e-05 4.00543213e-05
0:  3.19480896e-05 2.14576721e-05 9.53674316e-06 5.24520874e-06
0:  2.38418579e-06 1.90734863e-06 2.86102295e-06 3.81469727e-06
0:  1.00135803e-05 1.09672546e-05 9.53674316e-06 1.23977661e-05
0:  1.38282776e-05 9.53674316e-06 3.81469727e-06 2.38418579e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 1.90734863e-06 6.19888306e-06
0:  2.86102295e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 3.81469727e-06 3.81469727e-06
0:  6.19888306e-06 1.43051147e-05 1.19209290e-05 4.76837158e-07
0:  9.53674316e-07 9.53674316e-07 4.76837158e-07 0.00000000e+00
0:  2.16484070e-04 2.26497650e-04 2.02178955e-04 1.70230865e-04
0:  2.67028809e-04 4.05311584e-04 6.46114349e-04 8.24451447e-04
0:  6.21795654e-04 3.02791595e-04 2.76565552e-05 2.43186951e-05
0:  5.72204590e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.72204590e-06
0:  8.58306885e-06 7.62939453e-05 8.63075256e-05 5.48362732e-05
0:  1.09672546e-05 1.19209290e-05 7.62939453e-06 2.38418579e-05
0:  5.10215759e-05 5.86509705e-05 4.81605530e-05 8.39233398e-05
0:  5.38825989e-05 2.43186951e-05 4.29153442e-06 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 2.19345093e-05
0:  8.10623169e-06 2.33650208e-05 2.67028809e-05 3.62396240e-05
0:  3.76701355e-05 8.10623169e-06 9.53674316e-07 9.53674316e-07
0:  3.33786011e-06 6.19888306e-06 2.43186951e-05 3.86238098e-05
0:  2.71797180e-05 9.53674316e-06 1.33514404e-05 8.58306885e-06
0:  3.29017639e-05 3.05175781e-05 3.14712524e-05 1.90734863e-05
0:  1.85966492e-05 1.38282776e-05 7.15255737e-06 5.72204590e-06
0:  3.81469727e-06 1.43051147e-06 3.81469727e-06 1.23977661e-05
0:  1.85966492e-05 2.57492065e-05 2.14576721e-05 2.00271606e-05
0:  2.62260437e-05 2.76565552e-05 1.28746033e-05 9.05990601e-06
0:  8.10623169e-06 7.62939453e-06 3.81469727e-06 1.43051147e-06
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [-12.395368 -12.539954 -12.312238 -11.669154 -10.9232   -10.297578
0:   -9.911979  -9.617045  -9.736716  -9.867156 -10.09043  -10.562985
0:  -10.897962 -11.16616  -11.17472  -10.870217 -10.420678  -9.964757
0:  -10.614085 -10.660985]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.635, max = 1.016, mean = -0.283
0:          sample (first 20): tensor([-1.5884, -1.6002, -1.5817, -1.5295, -1.4689, -1.4181, -1.3868, -1.3628, -1.3726, -1.3831, -1.4013, -1.4396,
0:         -1.4668, -1.4886, -1.4893, -1.4646, -1.4281, -1.3911, -1.5536, -1.6152])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8176723  -1.8344359  -1.5635829  -1.0877938  -0.64219713 -0.38636065
0:  -0.16671991 -0.04465866 -0.1586194  -0.2959752  -0.5905533  -1.1258817
0:  -1.551518   -1.8782144  -2.0034928  -1.7444501  -1.3263974  -0.8797827
0:  -2.7799468  -2.7464347 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.54058075  0.96101713  1.8350415   3.1048756   4.33053     5.2957606
0:   5.87698     5.861339    5.013457    3.717901    1.976599   -0.02102566
0:  -1.7587819  -3.1343594  -3.7982693  -3.7796783  -3.3737097  -2.7927303
0:  -2.3693     -1.8834763 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0265465  -2.7885246  -2.2281075  -1.3320966  -0.3985958   0.40886307
0:   0.9924426   1.4804931   1.5669675   1.6445446   1.5909891   1.2700915
0:   0.9729295   0.669549    0.56069326  0.7997775   1.3004022   1.9037685
0:   0.62258196  0.86001873]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.96976   5.9961653 6.2323728 6.6708565 7.0649066 7.2895246 7.4141674
0:  7.390775  6.9515123 6.5690413 6.0031013 5.2622585 4.648164  4.0980663
0:  3.8983746 4.049033  4.367298  4.6692634 3.7164278 3.7038922]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.086918 38.46705  38.836903 39.196293 39.405567 39.534664 40.393463
0:  41.252605 42.141014 42.81424  42.935806 42.91255  42.965878 43.5156
0:  44.481953 45.508923 46.288807 46.3576   44.99132  45.493538]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.745092 22.39192  21.792822 20.952219 20.107206 19.09717  19.063911
0:  19.052446 19.231924 19.125896 18.236113 17.470129 16.628265 16.484692
0:  16.919231 17.650928 18.159351 18.088737 19.32433  19.207996]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.434023 17.461246 17.28563  16.841381 16.360397 15.825781 16.060732
0:  16.336088 16.750969 16.966301 16.649982 16.396809 16.16735  16.574076
0:  17.513338 18.767696 19.934265 20.669071 21.501925 21.786829]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4289103 4.5511646 4.8379946 5.1498547 5.360961  5.4441266 5.618125
0:  5.7573104 5.729706  5.6603317 5.3415494 4.844904  4.4542246 4.2711906
0:  4.447504  4.9013853 5.432663  5.8496003 4.659994  4.9002132]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.148354 11.325366 11.627655 11.95203  12.137106 12.109522 12.244614
0:  12.266225 12.167429 11.974775 11.492397 10.808943 10.168013  9.779787
0:   9.701551  9.924954 10.214835 10.274478  9.014495  9.114086]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.179089 18.185688 18.067833 17.704153 17.341362 16.852013 17.234798
0:  17.586338 18.12178  18.42998  18.094147 17.787462 17.418657 17.67749
0:  18.462217 19.648706 20.751556 21.402365 22.082558 22.275158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.967081  14.984705  15.031618  15.0921    15.053915  14.882124
0:  15.071789  15.223492  15.3653965 15.391037  14.980274  14.416441
0:  13.763645  13.453863  13.520714  13.869087  14.242548  14.312214
0:  12.20389   12.051463 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.723489  4.8684106 5.1564083 5.6201687 6.0745726 6.427413  6.71005
0:  7.0255895 7.099402  7.231291  7.2361608 7.0051603 6.7797303 6.570192
0:  6.569606  6.8848047 7.413701  7.946159  6.3796077 6.4467955]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.917997 26.885702 26.937012 26.954355 26.713339 26.068258 25.481913
0:  24.641832 23.670216 22.534721 21.111282 19.631664 18.423065 17.63015
0:  17.457767 17.746313 18.265854 18.671595 17.203066 16.922447]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.598509 31.643711 31.560122 31.310234 31.077106 30.656153 31.077242
0:  31.349382 31.655226 31.621363 30.79942  30.020031 29.213472 29.026234
0:  29.438162 30.126425 30.69578  30.687153 28.09508  28.021765]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.88908  19.191866 19.432213 19.517475 19.481361 19.379612 20.135113
0:  20.925903 21.790882 22.37885  22.186455 21.853292 21.362164 21.50636
0:  22.091349 22.931007 23.47387  23.403706 21.3816   21.322098]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.3810024   5.7231197   6.3499703   7.226136    7.9689226   8.349301
0:   8.277604    7.7678213   6.667113    5.4410067   4.071029    2.615913
0:   1.344306    0.28662205 -0.34733772 -0.5335946  -0.44560003 -0.22878027
0:  -1.5217743  -1.439137  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.7072825 -3.6268287 -3.2953458 -2.7000804 -2.2197728 -2.0046487
0:  -2.1880918 -2.518083  -3.2885203 -3.9628625 -4.6612296 -5.5870843
0:  -6.3648562 -7.11389   -7.5609345 -7.5295615 -7.1097035 -6.466439
0:  -7.753427  -7.7237763]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.1313972 5.3666058 5.7806554 6.3551555 6.802444  7.0572643 7.2551684
0:  7.504432  7.5036936 7.556967  7.4515467 7.0685997 6.7407303 6.470465
0:  6.5154834 6.971748  7.6754875 8.305069  7.097259  7.1366563]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.647177  14.709327  14.839201  14.843417  14.639114  14.271256
0:  14.320032  14.616207  15.187811  15.7645645 16.119535  16.2135
0:  16.467997  16.882677  17.777945  19.062134  20.54691   21.90899
0:  20.840841  21.391668 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.52191  12.325708 12.165169 11.857843 11.515446 11.051337 11.330751
0:  11.648634 12.099641 12.385428 12.068069 11.730782 11.274517 11.397028
0:  11.988822 12.888941 13.632107 13.927097 14.502205 14.557583]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.629328 22.48574  22.18905  21.737837 21.289835 20.642887 20.661678
0:  20.548904 20.590904 20.491137 19.889875 19.377611 18.923264 18.961258
0:  19.502117 20.339996 21.153488 21.650557 20.921963 20.716223]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2775898 -2.9803858 -2.434536  -1.7549543 -1.2554932 -1.0747313
0:  -1.0763187 -1.2445211 -1.7230511 -2.1976652 -2.7639866 -3.4973164
0:  -3.9967995 -4.259717  -4.1499653 -3.6529236 -3.0545368 -2.5348368
0:  -3.513856  -3.321679 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.7856503  6.137429   6.840873   7.817837   8.839565   9.794714
0:  10.712652  11.343555  11.458452  11.332589  10.804157   9.945577
0:   9.082807   8.329693   7.809613   7.6232347  7.6303005  7.7479515
0:   4.75943    4.6744943]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.828974  14.007145  14.395905  14.831458  15.138619  15.229597
0:  15.458712  15.585785  15.62453   15.618377  15.373879  14.990409
0:  14.753428  14.766233  15.171629  15.904891  16.733614  17.376331
0:  14.805475  15.0636835]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.5058594 -4.8462505 -4.887187  -4.5570154 -4.126216  -3.767418
0:  -3.6342611 -3.5736122 -3.952714  -4.2854223 -4.693886  -5.2499886
0:  -5.589305  -5.8115225 -5.6950827 -5.225897  -4.550845  -3.8736868
0:  -3.6622114 -3.7270775]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.3408494 -6.6825137 -6.624946  -6.15583   -5.5804133 -5.099406
0:  -4.9866605 -4.908834  -5.247582  -5.504642  -5.801763  -6.430617
0:  -7.01306   -7.6625247 -8.058342  -7.957909  -7.4269013 -6.705504
0:  -7.4735675 -7.868072 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.883997  -9.171601  -9.074545  -8.577039  -7.967024  -7.404558
0:  -6.7607927 -6.2356625 -5.9989033 -5.854331  -5.9608383 -6.248887
0:  -6.3670454 -6.2779236 -5.8712053 -5.2506514 -4.7623196 -4.462119
0:  -4.5470037 -4.3960857]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.9865956 2.1714826 2.5744195 3.1553721 3.7017617 4.1567717 4.507291
0:  4.8908424 5.0131783 5.131454  5.1079936 4.790105  4.502736  4.2055283
0:  4.1825614 4.5044413 5.0528126 5.625561  4.663847  4.9097586]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.778022  11.070805  11.59465   12.161324  12.549824  12.650475
0:  12.929269  13.065738  13.109098  13.110167  12.81336   12.3182955
0:  11.937025  11.910345  12.277892  13.047178  13.877239  14.46593
0:  13.5643635 13.814257 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.573679  8.195168  7.9997373 8.025161  7.910364  7.5976787 7.0075316
0:  6.474478  5.7386303 5.2057853 4.70402   3.9284365 3.143443  2.2787356
0:  1.6742644 1.5980344 2.057251  2.7556515 1.8570747 1.5368872]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.402384 15.816494 16.152876 16.349358 16.376661 16.236616 16.608166
0:  16.90271  17.172098 17.25352  16.788683 16.245865 15.642738 15.560167
0:  15.933916 16.592077 17.222725 17.503149 16.0432   15.824148]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.446588   -7.726707   -7.6507936  -7.1447883  -6.5702415  -6.152498
0:   -6.0456243  -6.0865235  -6.535419   -6.926246   -7.389132   -8.083088
0:   -8.651052   -9.213297   -9.554964   -9.541431   -9.260525   -8.863335
0:  -10.072781  -10.29336  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.227242   -2.0432258  -1.6398063  -1.0658717  -0.5235605  -0.188797
0:  -0.01940966  0.01623821 -0.2995839  -0.6016121  -1.000402   -1.6355333
0:  -2.0900798  -2.4882684  -2.6605325  -2.5140443  -2.2823539  -1.9974685
0:  -2.895763   -2.7668786 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.42304  12.336164 12.41393  12.527872 12.50223  12.271944 12.176174
0:  12.095947 11.994877 11.884344 11.573416 11.061028 10.532265 10.142405
0:  10.059206 10.320528 10.780558 11.130835  8.377937  8.303369]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.634367 18.738955 18.843414 18.877005 18.742983 18.407719 18.41707
0:  18.398243 18.449078 18.434002 18.06235  17.580294 17.151814 16.949957
0:  17.165546 17.694197 18.40938  19.05888  17.872595 17.986883]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.144386 22.151245 22.21994  22.292168 22.342716 22.164654 22.492405
0:  22.668568 22.935734 23.070324 22.788395 22.433922 22.209114 22.27628
0:  22.75911  23.59599  24.575523 25.350983 24.218407 24.274292]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7773445 2.90653   3.2670572 3.8006382 4.2718735 4.5812926 4.8039885
0:  5.0079494 4.974769  4.968267  4.837693  4.4211545 4.035266  3.6576462
0:  3.5485008 3.8462803 4.432062  5.098833  3.4033277 3.3190765]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.1782417   4.1336255   4.3907366   4.9075785   5.305307    5.379424
0:   5.029631    4.550966    3.709769    2.9911785   2.281189    1.3068662
0:   0.4227891  -0.4218607  -0.9102664  -0.703032    0.11069822  1.2105169
0:   0.37500906  0.3476138 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.651598  10.019463  10.5212755 11.050837  11.398891  11.51921
0:  11.488243  11.445332  11.185427  10.938105  10.570463   9.9413395
0:   9.364725   8.884168   8.787513   9.159881   9.897861  10.726013
0:   9.298965   9.551017 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.802977  8.501781  8.34602   8.301732  8.1931095 7.89634   8.018252
0:  8.080563  8.156706  8.114519  7.6653647 7.1095057 6.543434  6.4421854
0:  6.7696605 7.4341393 8.108023  8.484196  6.93657   6.616321 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.4156528 -6.6409373 -6.4858108 -5.9129214 -5.1956306 -4.6114974
0:  -4.3033195 -4.1275935 -4.379764  -4.614784  -4.8718762 -5.361283
0:  -5.6373124 -5.8477674 -5.760674  -5.321366  -4.7271705 -4.1342206
0:  -4.5794206 -4.6777225]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.868046 36.828957 36.45389  35.916378 35.398952 34.76265  35.135517
0:  35.442898 35.926346 36.021046 35.346024 34.77551  34.22478  34.34691
0:  35.099712 36.06658  36.861343 36.925213 37.110363 37.2968  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.025474 12.134521 12.169998 12.12611  11.908926 11.493499 11.289027
0:  11.09436  10.815837 10.508099  9.983822  9.383163  8.86837   8.685264
0:   8.879814  9.397818 10.020046 10.558432  9.544626  9.801409]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.9059782 2.7746234 2.9095757 3.2406774 3.5255427 3.645558  3.7640896
0:  3.7849567 3.5724542 3.3598068 2.9998307 2.480154  2.1191225 1.9782486
0:  2.1548648 2.6613183 3.2079835 3.6021855 2.4565558 2.363398 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.6863384 -7.615383  -7.1585865 -6.3983555 -5.723895  -5.277398
0:  -4.923232  -4.7475986 -4.818262  -4.9275    -5.2462683 -5.7955313
0:  -6.2081375 -6.369709  -6.1878576 -5.7614765 -5.4514685 -5.3692665
0:  -6.2133865 -6.2926188]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.1922874  4.6769195  5.3695383  6.174647   6.8163624  7.242941
0:   7.499042   7.738125   7.736803   7.824469   7.8489075  7.608232
0:   7.466348   7.357129   7.5911226  8.282632   9.25285   10.191397
0:   9.185116   9.5353365]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.325647  13.059736  12.691467  12.158041  11.562939  10.872553
0:  10.894369  10.941948  11.2132635 11.409223  11.130753  10.850316
0:  10.545784  10.8986225 11.803029  13.162119  14.488804  15.44591
0:  16.171274  16.355625 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5909736 3.7818182 4.2063103 4.8523016 5.4844613 5.966981  6.238253
0:  6.4453773 6.3255506 6.2449164 6.107415  5.758014  5.4944983 5.25246
0:  5.2525415 5.563481  6.0678396 6.5732737 5.5393686 5.7312274]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.70799065  0.624743    0.79355955  1.1565509   1.4575429   1.5610008
0:   1.622942    1.5603151   1.2248812   0.8542452   0.35905933 -0.27962637
0:  -0.7054701  -0.84344625 -0.63553286 -0.08136892  0.4788561   0.817647
0:  -1.390204   -1.6681266 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8915153 -4.21631   -4.2255063 -3.837852  -3.3745666 -3.0042558
0:  -3.011516  -3.0413146 -3.4860673 -3.8260274 -4.178678  -4.8415523
0:  -5.4420333 -6.0684743 -6.4051003 -6.1831136 -5.4073462 -4.337501
0:  -3.6893916 -3.8200583]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.508284  8.577052  8.825377  9.123102  9.222052  9.025219  8.77046
0:  8.379435  7.8032646 7.229808  6.5138116 5.627846  4.890051  4.3621025
0:  4.2099733 4.442598  4.8902287 5.257445  3.5714047 3.5658822]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.712149 18.392607 18.182774 17.943819 17.569807 17.030783 16.845602
0:  16.695951 16.64525  16.543056 16.141632 15.591242 15.083285 14.817476
0:  15.01925  15.630268 16.4783   17.276028 16.121855 16.21095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2059112 -2.598309  -2.6521153 -2.3262525 -1.8851771 -1.5345359
0:  -1.3477373 -1.2236443 -1.5158858 -1.8075895 -2.200397  -2.8709388
0:  -3.3769174 -3.8307881 -4.013881  -3.7778163 -3.318997  -2.8454165
0:  -3.7590165 -4.074348 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.98804    9.118582   9.388347   9.603956   9.677462   9.583971
0:   9.809109  10.094912  10.364     10.60209   10.484808  10.237175
0:  10.0199995 10.22633   10.835619  11.731457  12.5700655 13.06252
0:  12.293966  12.430006 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.685825 27.36759  26.963121 26.465185 25.997696 25.407099 25.5304
0:  25.588959 25.764654 25.728273 25.05282  24.547365 24.099398 24.343872
0:  25.203112 26.366478 27.41745  28.05072  29.227764 29.196215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.951798   -7.860513   -7.347179   -6.5336986  -5.588983   -4.7398753
0:  -3.6282687  -2.6546597  -1.7703605  -1.0174527  -0.57435083 -0.33316422
0:  -0.07987499  0.46176147  1.2769923   2.381853    3.3504148   3.9992466
0:   3.4257426   3.5293994 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.5723495 4.743637  5.1392035 5.599803  6.003831  6.2431626 6.5759315
0:  6.826546  6.9521565 7.046794  6.964333  6.7801104 6.774096  6.9590635
0:  7.454908  8.173156  8.917187  9.5286455 7.500368  7.7817316]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.7909274 5.3611383 5.160779  5.131503  5.0683303 4.846616  4.8021812
0:  4.6817856 4.4814825 4.1894703 3.6501725 2.990411  2.44446   2.2897162
0:  2.6203923 3.3457446 4.1635447 4.792404  3.049301  3.0166514]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.7621083 1.5312939 1.6334901 2.0027337 2.3840704 2.6532536 2.935668
0:  3.252924  3.4753325 3.8013043 4.040714  4.0584397 4.131382  4.303412
0:  4.7312903 5.502138  6.403207  7.1145177 6.4545    6.330947 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.4689665  -9.474549   -9.316724   -9.209666   -9.364511   -9.911609
0:  -10.742818  -11.81462   -13.296037  -14.763343  -16.337715  -17.905762
0:  -19.053139  -19.614876  -19.570862  -18.909729  -17.988052  -17.098042
0:  -21.440987  -21.763477 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.091957  7.889749  7.848431  7.821826  7.68597   7.34006   7.29991
0:  7.1866374 7.1273174 6.97383   6.485425  5.8557625 5.26449   5.0401998
0:  5.298797  6.0311804 6.9306965 7.7440076 7.172862  7.1092234]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.686811  -4.737492  -4.4930577 -3.9243913 -3.312365  -2.8834052
0:  -2.7405543 -2.7607093 -3.2017474 -3.6039767 -4.047171  -4.6725373
0:  -5.128021  -5.5022407 -5.628694  -5.3994594 -4.9487615 -4.408693
0:  -3.7759128 -3.3857503]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.856468 29.289108 28.671127 27.976292 27.302307 26.474361 26.351345
0:  26.17955  26.264194 26.27893  25.897827 25.583649 25.396202 25.755512
0:  26.61959  27.77975  28.876541 29.465813 26.472786 26.286   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.951154  -7.0831494 -6.8277287 -6.171785  -5.4697556 -4.9428134
0:  -4.79299   -4.7558036 -5.1484504 -5.4936433 -5.90021   -6.621906
0:  -7.2067046 -7.8154645 -8.141146  -7.981828  -7.469684  -6.780891
0:  -7.743851  -7.893865 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.120033 31.047253 30.820179 30.493649 30.17223  29.702225 29.928253
0:  30.031864 30.228695 30.20589  29.592575 29.097973 28.64721  28.730118
0:  29.29652  30.089813 30.754982 30.96939  30.203423 30.252777]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.830832  9.107406  9.607126 10.137423 10.519538 10.727394 10.906467
0:  11.068226 11.132858 11.179743 11.101527 10.810898 10.67228  10.641577
0:  10.953565 11.557786 12.287554 12.871159 10.998671 11.226549]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.919645 12.985769 13.099538 13.125687 12.964721 12.61937  12.446136
0:  12.250116 11.978905 11.677204 11.097763 10.401087  9.750596  9.407995
0:   9.444163  9.802547 10.225665 10.503124  8.364565  8.555038]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-23.967598 -23.596449 -22.639023 -21.329191 -19.953264 -18.925707
0:  -17.8362   -17.236803 -17.022327 -16.893456 -17.171576 -17.717766
0:  -17.948072 -17.824741 -17.19935  -16.306578 -15.702007 -15.627205
0:  -15.93219  -15.53923 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.635885  -11.017656  -11.0073395 -10.552797   -9.849731   -9.231274
0:   -8.509235   -8.024357   -7.863172   -7.7920766  -7.9679303  -8.342499
0:   -8.569069   -8.502352   -8.108316   -7.537791   -7.1016994  -6.960159
0:   -9.450037   -9.262777 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.0986223   3.369634    2.748362    2.2427516   1.7868762   1.2127204
0:   0.96401596  0.6697707   0.24854851 -0.23985815 -1.0811224  -2.0973291
0:  -3.0471077  -3.5757446  -3.7506762  -3.5709324  -3.3491745  -3.3199787
0:  -3.915619   -4.5846205 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.662542 15.724495 15.946381 16.2486   16.397154 16.278446 16.350307
0:  16.322987 16.241085 16.093502 15.591806 14.931974 14.27022  13.825748
0:  13.711926 13.916988 14.196669 14.314673 11.657656 11.235291]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.2450466 -5.3572645 -5.064681  -4.3294735 -3.5130901 -2.8113651
0:  -2.418765  -2.067048  -2.1393127 -2.1499186 -2.280445  -2.802188
0:  -3.310956  -3.971202  -4.442321  -4.50401   -4.2185764 -3.7579927
0:  -5.206876  -5.4558134]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.326068  5.474169  5.7513714 6.1905756 6.5701723 6.79626   6.8598704
0:  6.8002996 6.3826528 5.9365773 5.3512006 4.603266  3.992526  3.491974
0:  3.2334728 3.2204587 3.2950397 3.2835674 1.4375544 1.2789502]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.512978 22.504093 22.706722 22.953732 23.119194 23.122284 23.311398
0:  23.45275  23.613373 23.735882 23.6214   23.396059 23.280172 23.403996
0:  23.937262 24.717718 25.503082 26.03137  22.718159 22.628592]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.663431  17.533707  17.572056  17.595135  17.50948   17.345434
0:  17.261791  17.14418   16.853456  16.537777  15.91379   15.148043
0:  14.4815645 14.023334  13.985552  14.214352  14.500747  14.67598
0:  11.619114  11.474493 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.69556   11.762516  11.874371  11.877283  11.6773205 11.257229
0:  10.936527  10.687535  10.42127   10.26637   10.029474   9.693285
0:   9.530926   9.611861  10.103812  10.888214  11.710672  12.3085575
0:  12.035422  12.245787 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8198466  -2.0183473  -1.9546199  -1.5591488  -1.1602287  -0.94910145
0:  -1.079462   -1.3677063  -2.0546203  -2.6816525  -3.2717614  -3.9188142
0:  -4.3254037  -4.5539985  -4.470185   -3.9923806  -3.2909017  -2.5521207
0:  -2.104898   -2.1332402 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.636957 17.31965  17.076874 16.893017 16.712578 16.326094 16.47574
0:  16.480469 16.541605 16.435741 15.814592 15.217215 14.621391 14.551672
0:  14.958639 15.792759 16.636595 17.20772  16.977512 16.89266 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7465034 -5.9790254 -5.837357  -5.4178567 -4.9830375 -4.742042
0:  -4.6779704 -4.7521443 -5.06377   -5.233937  -5.3672442 -5.5881963
0:  -5.576223  -5.473994  -5.219852  -4.7474446 -4.273668  -3.9197145
0:  -5.6937127 -5.5948157]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.817282 23.806303 23.888018 23.994139 24.113277 24.089554 24.519552
0:  24.841545 25.214054 25.455173 25.327776 25.12339  25.000027 25.196384
0:  25.790245 26.651205 27.62977  28.408436 27.291634 27.662464]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.043983 15.984856 15.94387  15.643871 15.123536 14.434676 14.241792
0:  14.100001 14.058079 13.987265 13.491392 12.926844 12.406668 12.457943
0:  13.036702 14.02436  15.009014 15.72912  15.038257 15.343956]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.227545 29.573975 29.925152 30.115559 30.120687 29.905792 30.095463
0:  30.232883 30.36189  30.371067 30.051538 29.591215 29.228802 29.209568
0:  29.58941  30.175173 30.762169 30.989084 27.046492 27.165754]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.490343 22.319847 22.42747  22.631985 22.732769 22.592209 22.241898
0:  21.850698 21.262585 20.82506  20.274887 19.548828 18.879482 18.19326
0:  17.79078  17.68322  17.789568 18.070724 16.380938 15.717894]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.937742 18.902819 18.767155 18.421629 18.011492 17.391249 17.500683
0:  17.518675 17.695555 17.70022  17.136848 16.575644 16.007887 15.985577
0:  16.471798 17.38242  18.306208 18.886295 18.55924  18.652006]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.315283 14.168397 13.909843 13.38722  12.733367 11.986334 11.974598
0:  12.073099 12.400354 12.653481 12.3976   12.168476 11.956532 12.514778
0:  13.687757 15.220108 16.576826 17.29881  17.50566  18.03429 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.854088  10.1215515 10.540817  10.989908  11.282682  11.392813
0:  11.45743   11.526823  11.411674  11.237259  10.905954  10.3760195
0:   9.923707   9.626211   9.709707  10.211362  10.987154  11.753914
0:  10.162006  10.589687 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.956757  19.13013   19.496567  19.895363  20.109617  20.02917
0:  19.985514  19.777914  19.389952  18.886576  18.079037  17.039362
0:  16.082605  15.351917  15.090166  15.250483  15.586935  15.798075
0:  13.2308235 12.9575405]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.877167 34.331875 34.56988  34.511715 34.174984 33.599083 33.25305
0:  32.93722  32.696568 32.442158 31.920567 31.344254 30.979757 30.95408
0:  31.511734 32.36596  33.29258  34.004433 31.086279 31.418741]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.9153652   0.97077703  1.3012171   1.891613    2.4132752   2.7473862
0:   3.024422    3.157561    3.001317    2.7911332   2.3708985   1.695157
0:   1.1343784   0.6614227   0.4540577   0.56529236  0.764801    0.90945625
0:  -0.5730696  -0.63383865]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6505284  -3.388454   -2.7899384  -1.8566203  -0.91171217 -0.1303277
0:   0.4478655   0.90423775  1.0544853   1.2139077   1.3198333   1.2111506
0:   1.2014627   1.240469    1.5557113   2.1687536   2.9815311   3.8263342
0:   4.179706    4.605337  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7273974 -4.806332  -4.595865  -4.0857863 -3.6223931 -3.3701673
0:  -3.3666253 -3.4271293 -3.856484  -4.2116027 -4.641131  -5.2965426
0:  -5.81039   -6.2493453 -6.404145  -6.1274905 -5.601258  -4.9886003
0:  -5.312384  -5.6288695]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.116985 18.394691 18.747322 19.059187 19.232662 19.284168 19.651796
0:  20.011074 20.354506 20.598026 20.454342 20.164318 19.949404 20.075022
0:  20.6691   21.599499 22.601566 23.418663 21.29877  21.550653]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.628194  -8.636972  -8.2495985 -7.4288692 -6.531642  -5.741293
0:  -5.142495  -4.636324  -4.534301  -4.4736176 -4.5158873 -4.885558
0:  -5.1138706 -5.272681  -5.103944  -4.523201  -3.6982517 -2.8615074
0:  -2.2749825 -2.094781 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.060612 19.947182 19.930048 19.891453 19.68483  19.293858 19.110191
0:  18.808655 18.345947 17.72621  16.692822 15.389196 14.060329 12.948007
0:  12.236019 11.879159 11.660979 11.428534  9.897579  9.925512]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.96593  29.256836 29.46643  29.507622 29.481853 29.35263  30.029545
0:  30.716606 31.529003 32.15878  32.164635 32.193275 32.347084 33.032017
0:  34.291424 35.772976 37.10119  37.868813 37.353817 37.1604  ]
0: validation loss for strategy=forecast at epoch 20 : 0.2810095548629761
0: validation loss for velocity_u : 0.1533784121274948
0: validation loss for velocity_v : 0.24068738520145416
0: validation loss for specific_humidity : 0.1835203915834427
0: validation loss for velocity_z : 0.47752001881599426
0: validation loss for temperature : 0.11506760865449905
0: validation loss for total_precip : 0.5158830285072327
0: 21 : 19:21:58 :: batch_size = 96, lr = 1.2510554328606033e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 21, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0591,  0.0374,  0.0092, -0.0240, -0.0588, -0.0918, -0.1197, -0.1415, -0.1572, -0.1679, -0.1759, -0.1827,
0:         -0.1899, -0.1972, -0.2041, -0.2098, -0.2150, -0.2207,  0.0342,  0.0064, -0.0258, -0.0602, -0.0933, -0.1222,
0:         -0.1449], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3982, 0.3570, 0.3045, 0.2480, 0.1985, 0.1659, 0.1551, 0.1654, 0.1925, 0.2317, 0.2806, 0.3384, 0.4053, 0.4819,
0:         0.5686, 0.6643, 0.7649, 0.8645, 0.3199, 0.2663, 0.2083, 0.1556, 0.1167, 0.0983, 0.1017], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5875, -0.5539, -0.5058, -0.4602, -0.4072, -0.3712, -0.3606, -0.3484, -0.3532, -0.3594, -0.3751, -0.3905,
0:         -0.4269, -0.4633, -0.5006, -0.5369, -0.5609, -0.5804, -0.5694, -0.5140, -0.4608, -0.4181, -0.3756, -0.3548,
0:         -0.3546], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0551, -0.0865, -0.1225, -0.1539, -0.1742, -0.1831, -0.1809, -0.1786, -0.1742, -0.1652, -0.1517, -0.1427,
0:         -0.1404, -0.1315, -0.0978, -0.0281,  0.0573,  0.1337, -0.0955, -0.1157, -0.1427, -0.1697, -0.1876, -0.2034,
0:         -0.2146], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.5061, 1.4744, 1.4432, 1.4172, 1.4016, 1.3998, 1.4126, 1.4377, 1.4714, 1.5095, 1.5483, 1.5857, 1.6201, 1.6515,
0:         1.6801, 1.7058, 1.7288, 1.7490, 1.7664, 1.7806, 1.7907, 1.7943, 1.7886, 1.7715, 1.7420], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2188, -0.2279, -0.2324, -0.2347, -0.2324, -0.2324, -0.2347, -0.2347, -0.2324, -0.2301, -0.2301, -0.2324,
0:         -0.2324, -0.2324, -0.2324, -0.2324, -0.2324, -0.2324, -0.2347, -0.2347, -0.2324, -0.2324, -0.2324, -0.2324,
0:         -0.2324], device='cuda:0')
0: [DEBUG] Epoch 21, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.1893,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1961,     nan,
0:             nan, -0.1904, -0.1234,     nan,     nan, -0.1847, -0.1847,     nan,     nan,     nan,     nan,     nan,
0:         -0.1927,     nan,     nan,     nan,     nan,     nan,     nan, -0.1631,     nan,     nan,     nan,     nan,
0:             nan, -0.0416,     nan,     nan,     nan,     nan,     nan,     nan, -0.1018,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1893, -0.2086,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2233,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2256,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2029,     nan,     nan, -0.1836,     nan, -0.1938,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0541,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2006,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2188,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1961,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2256,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 21, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4651, -0.4472, -0.4086, -0.3496, -0.2957, -0.2582, -0.2380, -0.2264, -0.2335, -0.2377, -0.2524, -0.2892,
0:         -0.3193, -0.3551, -0.3729, -0.3629, -0.3308, -0.2857, -0.4755, -0.5037, -0.4930, -0.4574, -0.4180, -0.3785,
0:         -0.3495], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1330,  0.1630,  0.1602,  0.1365,  0.1027,  0.0671,  0.0355,  0.0235,  0.0380,  0.0543,  0.0836,  0.1048,
0:          0.0921,  0.0471, -0.0100, -0.0339, -0.0075,  0.0586,  0.1355,  0.1855,  0.2009,  0.1923,  0.1630,  0.1191,
0:          0.0874], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5640, -0.5636, -0.5604, -0.5566, -0.5485, -0.5399, -0.5447, -0.5501, -0.5584, -0.5663, -0.5744, -0.5818,
0:         -0.5812, -0.5776, -0.5824, -0.5810, -0.5890, -0.5808, -0.5629, -0.5588, -0.5511, -0.5386, -0.5314, -0.5316,
0:         -0.5316], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2894, 0.3808, 0.5217, 0.3063, 0.0803, 0.2143, 0.2503, 0.1811, 0.2032, 0.1448, 0.0832, 0.1788, 0.2673, 0.1542,
0:         0.0183, 0.0789, 0.1557, 0.0997, 0.2635, 0.1582, 0.2265, 0.1192, 0.0190, 0.1565, 0.1885], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.0774, 1.0587, 1.0242, 0.9866, 0.9601, 0.9478, 0.9371, 0.9156, 0.8843, 0.8515, 0.8209, 0.7966, 0.7706, 0.7409,
0:         0.7042, 0.6673, 0.6346, 0.6128, 0.5973, 0.5858, 0.5825, 0.5855, 0.5959, 0.6057, 0.6075], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1598, -0.1641, -0.1677, -0.1689, -0.1684, -0.1676, -0.1692, -0.1715, -0.1706, -0.1750, -0.1678, -0.1661,
0:         -0.1630, -0.1607, -0.1662, -0.1705, -0.1677, -0.1710, -0.1695, -0.1677, -0.1626, -0.1574, -0.1563, -0.1607,
0:         -0.1638], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2095874696969986; velocity_v: 0.30869096517562866; specific_humidity: 0.1831907033920288; velocity_z: 0.6775230765342712; temperature: 0.15298813581466675; total_precip: 0.8687298893928528; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21030454337596893; velocity_v: 0.3453786075115204; specific_humidity: 0.1781446635723114; velocity_z: 0.624136745929718; temperature: 0.1485210359096527; total_precip: 0.768987238407135; 
0: epoch: 21 [1/5 (20%)]	Loss: 0.81886 : 0.35472 :: 0.21156 (2.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18738922476768494; velocity_v: 0.3091657757759094; specific_humidity: 0.19977429509162903; velocity_z: 0.5880087614059448; temperature: 0.15037716925144196; total_precip: 0.7121696472167969; 
0: epoch: 21 [2/5 (40%)]	Loss: 0.71217 : 0.32307 :: 0.20386 (16.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19377148151397705; velocity_v: 0.31291332840919495; specific_humidity: 0.17793753743171692; velocity_z: 0.6683146953582764; temperature: 0.1468660533428192; total_precip: 0.8724042177200317; 
0: epoch: 21 [3/5 (60%)]	Loss: 0.87240 : 0.36154 :: 0.20723 (16.38 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2006538361310959; velocity_v: 0.3013099133968353; specific_humidity: 0.20224283635616302; velocity_z: 0.6191808581352234; temperature: 0.15397386252880096; total_precip: 0.6837035417556763; 
0: epoch: 21 [4/5 (80%)]	Loss: 0.68370 : 0.32567 :: 0.20462 (16.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 1.43051147e-06 2.38418579e-06 2.38418579e-06
0:  4.29153442e-06 8.58306885e-06 7.62939453e-06 5.72204590e-06
0:  4.29153442e-06 2.86102295e-06 1.90734863e-06 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 9.53674316e-07 4.29153442e-06 5.24520874e-06
0:  6.67572021e-06 1.66893005e-05 2.52723694e-05 3.76701355e-05
0:  6.10351562e-05 7.24792480e-05 8.34465027e-05 1.00135803e-04
0:  9.05990601e-05 7.20024109e-05 4.57763635e-05 3.81469690e-05
0:  3.14712524e-05 2.09808350e-05 3.05175781e-05 5.62667847e-05
0:  7.62939453e-05 5.34057617e-05 3.91006470e-05 3.43322754e-05
0:  1.85966492e-05 1.00135803e-05 4.76837158e-06 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 1.43051147e-06 2.38418579e-06 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 4.76837158e-07 6.19888306e-06
0:  8.10623169e-06 7.15255737e-06 1.38282776e-05 9.53674316e-06
0:  2.38418579e-06 9.53674316e-07 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 1.43051147e-06 1.90734863e-06
0:  4.29153442e-06 7.62939453e-06 4.29153442e-06 3.33786011e-06
0:  3.33786011e-06 1.90734863e-06 9.53674316e-07 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 1.90734863e-06 4.29153442e-06 4.29153442e-06
0:  2.86102295e-06 3.81469727e-06 1.38282776e-05 2.81333923e-05
0:  5.48362732e-05 6.48498535e-05 6.58035278e-05 6.53266907e-05
0:  5.96046448e-05 5.72204590e-05 4.72068787e-05 4.48226929e-05
0:  4.91142273e-05 4.81605530e-05 3.57627869e-05 2.05039978e-05
0:  2.86102295e-05 3.00407410e-05 4.67300415e-05 5.67436218e-05
0:  2.90870667e-05 1.09672546e-05 7.15255737e-06 5.72204590e-06
0:  2.86102295e-06 4.76837158e-07 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.90734863e-06 2.38418579e-06 3.81469727e-06 1.14440918e-05
0:  1.28746033e-05 6.67572021e-06 1.04904175e-05 8.58306885e-06
0:  7.15255737e-06 1.47819519e-05 1.19209290e-05 3.33786011e-06]
0: Target values (first 200):
0: [2.19345093e-05 1.76429749e-05 1.23977661e-05 6.67572021e-06
0:  2.86102295e-06 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  1.43051147e-06 9.53674316e-07 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 4.76837158e-07 4.76837158e-07
0:  1.43051147e-06 1.90734863e-06 1.90734863e-06 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.43051147e-06 3.33786011e-06 5.72204590e-06 1.04904175e-05
0:  5.24520874e-06 1.04904175e-05 6.19888306e-06 2.38418579e-06
0:  9.53674316e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  1.43051147e-06 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 2.38418579e-06 3.33786011e-06 5.24520874e-06
0:  3.81469727e-06 3.81469727e-06 4.29153442e-06 3.33786011e-06
0:  1.90734863e-05 1.52587891e-05 2.33650208e-05 1.95503235e-05
0:  1.38282776e-05 4.29153442e-06 1.43051147e-06 1.43051147e-06
0:  1.90734863e-06 2.38418579e-06 2.38418579e-06 1.90734863e-06
0:  1.90734863e-06 2.38418579e-06 2.86102295e-06 1.43051147e-06
0:  9.53674316e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 3.81469727e-06 3.33786011e-06
0:  4.29153442e-06 6.67572021e-06 9.05990601e-06 7.15255737e-06
0:  6.19888306e-06 6.67572021e-06 7.15255737e-06 2.86102295e-06
0:  9.53674316e-07 4.76837158e-07 0.00000000e+00 3.33786011e-06
0:  4.29153442e-06 4.76837158e-06 1.90734863e-06 9.53674316e-07
0:  4.76837158e-07 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [19.850605 19.70969  19.4117   18.942633 18.409918 17.68129  17.787888
0:  17.86032  18.049643 17.986898 17.153399 16.372114 15.410566 15.087299
0:  15.273546 15.733189 16.013754 15.771788 15.698557 15.577852]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -0.219, max = 3.317, mean = 1.399
0:          sample (first 20): tensor([1.0525, 1.0409, 1.0163, 0.9775, 0.9335, 0.8733, 0.8821, 0.8881, 0.9037, 0.8985, 0.8297, 0.7651, 0.6857, 0.6589,
0:         0.6743, 0.7123, 0.7355, 0.7155, 1.0658, 1.1145])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5175111  3.3028226  3.3714075  3.7101471  4.037944   4.150424
0:  4.328805   4.33057    4.065733   3.7244363  3.0865598  2.2731495
0:  1.5798955  1.2141347  1.2622666  1.6282701  1.9887462  2.136466
0:  0.67955446 0.5358782 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.544494  -7.598557  -7.2904515 -6.5795503 -5.7820363 -5.1324143
0:  -4.7012134 -4.449121  -4.6518455 -4.9034305 -5.323791  -6.026809
0:  -6.5834594 -7.1084256 -7.3845515 -7.2906938 -7.011332  -6.653419
0:  -8.130114  -8.171925 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.962112  17.946604  17.948004  17.84648   17.501308  16.86138
0:  16.331627  15.780609  15.229399  14.673424  13.882342  12.819706
0:  11.693634  10.738983  10.132212  10.030604  10.255238  10.553175
0:   8.714676   8.8126745]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.37548  13.331991 13.430307 13.54602  13.537897 13.350191 13.271182
0:  13.07911  12.722427 12.276785 11.579531 10.746942 10.091271  9.798337
0:   9.91996  10.345083 10.836157 11.153849  8.282488  8.292678]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.061962 17.825497 17.769627 17.811089 17.78996  17.575119 17.594265
0:  17.579575 17.606195 17.666496 17.494953 17.219303 16.973537 16.965076
0:  17.37534  18.150143 19.109293 19.927086 18.3774   18.37334 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.6922293 -7.854596  -7.6580105 -7.0682764 -6.361267  -5.785475
0:  -5.441736  -5.2680874 -5.578742  -5.9164095 -6.353257  -7.043294
0:  -7.4432898 -7.7835455 -7.7819834 -7.4314795 -6.9306955 -6.3739004
0:  -6.536393  -6.587844 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.814964 19.533663 19.264147 18.903347 18.469156 17.83619  17.765999
0:  17.652422 17.642393 17.546276 16.982058 16.442848 15.906578 15.930464
0:  16.430645 17.298788 18.10172  18.525787 17.990137 17.981585]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -6.1082063  -6.7230287  -7.0104947  -7.0044     -7.0769877  -7.3560977
0:   -7.680386   -8.066845   -8.634237   -9.151155   -9.820326  -10.698521
0:  -11.437115  -11.929129  -12.011284  -11.565592  -10.890854  -10.116865
0:  -11.906485  -12.568863 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0875807 2.985001  3.03885   3.203924  3.3870575 3.4604871 3.9316187
0:  4.3570375 4.8028073 5.192207  5.2264843 5.1683755 5.092018  5.451811
0:  6.1844196 7.2433577 8.206934  8.862676  8.793791  9.017887 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.461185 20.72598  21.055288 21.335217 21.416285 21.247547 21.452719
0:  21.602673 21.775742 21.92092  21.64583  21.28225  20.942297 21.009941
0:  21.49824  22.255617 22.933111 23.272213 21.573366 21.552189]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.3861327  6.4374957  6.5408535  6.5039215  6.3918834  6.183316
0:   6.6519165  7.2190585  7.882547   8.4372635  8.421959   8.306554
0:   8.037607   8.344584   9.0342245  9.979597  10.619188  10.758686
0:  10.263005  10.495131 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.810859  -7.8347554 -7.504603  -6.790297  -5.979614  -5.2852435
0:  -4.8868127 -4.6151404 -4.792151  -4.9680696 -5.2285743 -5.6720796
0:  -5.987045  -6.19387   -6.161343  -5.7422204 -5.109813  -4.435965
0:  -4.8470902 -4.848622 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.231437  -4.2235236 -3.86416   -3.1155572 -2.3221307 -1.6524358
0:  -1.3387103 -1.1417098 -1.3897882 -1.6375461 -1.9294314 -2.5225844
0:  -3.0109105 -3.4421506 -3.568437  -3.179792  -2.3364062 -1.2801824
0:  -1.3953338 -1.3175082]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.941183  7.288784  7.87781   8.542957  9.080038  9.41272   9.69321
0:  9.829934  9.725161  9.566417  9.199049  8.674107  8.270131  8.071878
0:  8.255556  8.749878  9.356594  9.841276  7.8533154 8.110417 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.35151  22.306316 22.133022 21.703934 21.153725 20.408527 20.493776
0:  20.572672 20.776405 20.764465 20.003254 19.258686 18.425968 18.333858
0:  18.865335 19.835182 20.712389 21.095295 21.115538 21.09134 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.241325 31.503952 31.650408 31.694353 31.710312 31.457788 31.948069
0:  32.27607  32.7508   32.92818  32.502502 32.085514 31.718067 31.771105
0:  32.26478  32.993782 33.72485  34.06951  32.593613 32.799988]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9919086  -1.9316659  -1.5767713  -0.9618087  -0.3998127  -0.05345201
0:   0.1078577   0.10071945 -0.25365543 -0.54120255 -0.8979516  -1.4516234
0:  -1.8235135  -2.118587   -2.1444306  -1.8008742  -1.2677903  -0.68030787
0:  -1.5974402  -1.537499  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.772432  -8.614271  -8.0576725 -7.051865  -5.9380555 -4.975223
0:  -4.2440066 -3.677525  -3.535295  -3.451447  -3.483809  -3.7820077
0:  -3.959556  -4.091473  -3.9562716 -3.5296006 -2.942863  -2.368463
0:  -2.0946174 -1.84274  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5407629  -1.6043844  -1.3475695  -0.6364708   0.11488104  0.72838926
0:   1.0808458   1.3384376   1.1482964   0.9687891   0.6042042  -0.07733011
0:  -0.7543354  -1.3746243  -1.6469545  -1.3558531  -0.63426113  0.2594347
0:   0.20894289  0.15487242]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.994455 28.761517 28.248198 27.505287 26.787636 25.957806 26.266113
0:  26.530298 26.98477  27.11674  26.372366 25.79574  25.141697 25.30106
0:  26.124475 27.238367 28.112383 28.219881 29.360374 29.34383 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.23352   8.939772  9.901327 10.955735 11.907482 12.79404  13.687246
0:  14.49375  15.064445 15.559154 15.82249  15.876688 16.01536  16.195885
0:  16.677166 17.248241 17.801874 18.124035 16.302128 16.400665]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.917551  -8.807266  -8.337807  -7.5289454 -6.7017922 -6.0851283
0:  -5.8297677 -5.778333  -6.2178283 -6.6732993 -7.2291865 -7.992187
0:  -8.542648  -8.9450245 -9.020905  -8.662054  -8.084742  -7.4350915
0:  -7.282744  -7.200031 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8047228  -3.6251078  -3.104536   -2.2834902  -1.5079222  -0.90310764
0:  -0.57090235 -0.26995707 -0.29877615 -0.24896717 -0.23613548 -0.5319834
0:  -0.74986887 -0.9977522  -0.9766855  -0.46911144  0.40498114  1.409884
0:   1.1859169   1.3581429 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.8226314  7.3086085  7.9490128  8.563384   8.949163   9.053715
0:   9.144385   9.182936   9.015401   8.819976   8.42377    7.8261924
0:   7.3473625  7.1372285  7.4504237  8.23634    9.164365  10.009083
0:   8.629628   8.883773 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.6156664 6.488886  6.5451555 6.7056055 6.745955  6.571863  6.475655
0:  6.3229866 6.117703  5.9187617 5.5712843 5.07543   4.70654   4.6244526
0:  4.8970385 5.511202  6.211428  6.774595  5.001185  5.0669785]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.368682  -10.273238   -9.790984   -8.902848   -7.9522285  -7.173687
0:   -6.6049566  -6.1793337  -6.1332273  -6.073163   -6.1041718  -6.3122864
0:   -6.374048   -6.3560553  -6.1205463  -5.6125774  -5.0168104  -4.5022507
0:   -4.8130074  -4.5684767]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6115248 2.925796  3.48377   4.1611605 4.708801  5.037478  5.2349715
0:  5.2498817 4.9610343 4.562374  4.0016694 3.2457914 2.7056255 2.415404
0:  2.5821006 3.183748  3.9978032 4.7397227 4.7064643 5.1256466]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.712458 18.602783 18.436928 18.158474 17.884806 17.50588  17.749382
0:  17.883123 18.09223  18.072512 17.578018 17.12405  16.811516 17.162083
0:  18.118492 19.516472 20.98484  22.14427  22.544083 22.948923]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.683128  14.743914  14.899722  15.0627575 15.076711  14.920517
0:  14.553808  14.212566  13.618348  13.077237  12.440218  11.546341
0:  10.693401   9.922152   9.490736   9.454166   9.694346   9.930512
0:   8.262904   8.078894 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.951126  5.8582516 5.9452667 6.240986  6.523666  6.673082  6.7810845
0:  6.858095  6.694211  6.592923  6.3752046 5.9578786 5.6346455 5.372617
0:  5.3440013 5.58209   5.847599  6.0477543 5.1851625 5.2011695]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.84306  34.893486 34.75807  34.59492  34.574463 34.36616  35.221523
0:  35.93852  36.826218 37.28232  36.86325  36.32049  35.70353  35.637917
0:  36.19637  37.06127  37.828156 37.964725 35.336273 35.269268]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.326426 19.952126 19.602962 19.212982 18.708776 18.054005 17.645487
0:  17.168352 16.630196 15.932735 14.860522 13.525024 12.215259 11.203878
0:  10.702046 10.702566 11.051355 11.422188  9.116851  8.976069]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.503836  8.610754  8.969049  9.397388  9.66049   9.605539  9.559059
0:  9.418448  9.188162  9.002911  8.64778   8.090472  7.635766  7.3654776
0:  7.4793644 8.010359  8.700682  9.256332  7.4858894 7.419474 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.6914887   1.5784183   1.7539196   2.1129608   2.4759216   2.6429505
0:   2.7935581   2.7889838   2.5122185   2.1455903   1.5734625   0.85201263
0:   0.24279547 -0.06906748 -0.04009771  0.28231096  0.6084962   0.87576866
0:  -0.30126143 -1.215425  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.054866  9.132269  9.43348   9.808515 10.049471 10.085417 10.204626
0:  10.191661 10.055839  9.94695   9.634029  9.201965  8.860853  8.749004
0:   9.003184  9.580867 10.199459 10.704954  9.209262  9.28148 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.028318    4.867277    4.896725    5.0872793   5.1760235   5.0704093
0:   4.6880937   4.1139717   3.1146522   2.1343246   1.0633211  -0.14425564
0:  -1.1964436  -2.0889611  -2.62013    -2.7142825  -2.504909   -2.1159663
0:  -2.523295   -2.5142121 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.206843  11.38238   11.781475  12.212037  12.434271  12.447608
0:  12.462765  12.4328785 12.238569  12.081347  11.739958  11.235565
0:  10.837101  10.582207  10.728531  11.226696  11.844728  12.327365
0:  10.814486  11.047918 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.10423  21.19491  21.453815 21.714224 21.794077 21.645544 21.56836
0:  21.417358 21.214977 21.084162 20.816408 20.365395 20.088694 19.960552
0:  20.304255 21.089594 22.092148 23.068619 21.336939 21.407078]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.025845  15.913666  15.850508  15.814228  15.763891  15.6087475
0:  15.872971  16.06747   16.305893  16.447412  16.282764  16.081787
0:  16.018902  16.303621  17.017004  17.973614  19.023792  19.80011
0:  18.794771  18.824411 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.607502  8.695075  8.983858  9.408509  9.618015  9.560386  9.2779665
0:  9.0170965 8.562458  8.265955  7.974615  7.3783426 6.8283033 6.2010074
0:  5.854649  6.0240493 6.654695  7.4638815 5.522644  5.4117203]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3106093 -5.4581866 -5.242825  -4.8083377 -4.4005914 -4.18786
0:  -3.872704  -3.6903825 -3.5743527 -3.5035949 -3.6295962 -3.9298396
0:  -4.1237206 -4.014549  -3.5855231 -2.7821507 -1.9673104 -1.2980189
0:  -3.1041198 -3.2304082]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.220848  4.3601213 4.7361937 5.2153583 5.555949  5.6683135 5.877837
0:  5.9674654 5.877175  5.754301  5.3053646 4.6267934 3.9857733 3.5977287
0:  3.5510116 3.781604  4.016141  4.056383  2.6110039 2.6961308]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.596186  11.864845  12.233088  12.438722  12.470199  12.347654
0:  12.656162  12.984362  13.333088  13.579819  13.367321  12.999495
0:  12.5514345 12.514601  12.7964    13.371579  13.853667  14.071924
0:  13.121578  13.30254  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-17.630144 -17.681036 -17.26975  -16.470316 -15.645578 -15.087833
0:  -14.866812 -15.005051 -15.726282 -16.3982   -17.247288 -18.32794
0:  -19.199472 -19.80934  -19.938036 -19.536083 -18.95754  -18.326372
0:  -16.62534  -16.532402]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.890545  -8.757963  -8.290516  -7.539741  -6.8695445 -6.451609
0:  -6.285765  -6.3286157 -6.70948   -7.0608897 -7.4857364 -8.004808
0:  -8.312908  -8.409861  -8.19206   -7.7055736 -7.228148  -6.885985
0:  -7.170284  -6.8672013]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.416391 18.07874  17.70043  17.249147 16.823015 16.26808  16.4596
0:  16.608822 16.886534 16.996101 16.52787  16.058317 15.551435 15.647066
0:  16.313238 17.368027 18.437222 19.080832 18.933277 18.786364]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.7881246   3.6093574   3.6201675   3.8448236   4.0260468   3.96867
0:   3.6378825   3.2187057   2.4432      1.8358579   1.258522    0.55931234
0:  -0.01452351 -0.4897647  -0.7054744  -0.4930544   0.00677061  0.55064964
0:   0.05419922  0.04669905]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.372639  12.424442  12.694729  13.009572  13.174202  13.110271
0:  13.085745  13.0184555 12.899353  12.82579   12.64872   12.351927
0:  12.165304  12.215227  12.66042   13.425383  14.311251  15.083416
0:  13.256773  13.610395 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.7969527 5.82372   6.053489  6.4633255 6.7607164 6.861991  6.8330626
0:  6.8047366 6.5282497 6.3070717 5.964449  5.37909   4.827663  4.321422
0:  4.1409435 4.376158  4.897915  5.453335  4.0431633 4.0818577]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.964309   8.226874   8.716927   9.400036   9.951582  10.253204
0:  10.349516  10.289337   9.863804   9.453559   8.877815   7.968564
0:   7.06346    6.168107   5.590297   5.499508   5.8399305  6.3782988
0:   4.306467   4.3573265]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8531721 1.9223752 2.2421756 2.805585  3.4062102 3.872847  4.1864567
0:  4.4317575 4.3241987 4.239069  4.0643897 3.6517575 3.349182  3.0908864
0:  3.097902  3.4656467 4.053789  4.735014  3.67098   3.6465821]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.349368  -5.320999  -4.9339705 -4.098361  -3.1856737 -2.416833
0:  -1.9500399 -1.621378  -1.7526588 -1.9076977 -2.1682634 -2.7219276
0:  -3.1782837 -3.635151  -3.8635235 -3.6937656 -3.257328  -2.670106
0:  -2.7228594 -2.7543612]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.227562 30.464724 30.669659 30.695305 30.603527 30.232946 30.382051
0:  30.368267 30.34079  30.113148 29.327665 28.526382 27.815424 27.613506
0:  27.98833  28.752913 29.621292 30.26689  29.528166 29.777565]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.355669 12.469881 12.773248 13.147963 13.457441 13.617014 14.005922
0:  14.329287 14.53373  14.697135 14.542028 14.260296 14.031596 14.117718
0:  14.550274 15.22661  15.84934  16.240572 13.652109 13.613633]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.340885 19.124601 18.822447 18.3101   17.623396 16.587402 15.938089
0:  15.094156 14.29043  13.39991  12.223539 11.301771 10.634543 10.705698
0:  11.447633 12.623591 13.844632 14.726738 15.361849 15.810625]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.5357895 15.790385  16.282803  16.84032   17.110205  17.081823
0:  16.884253  16.713802  16.392479  16.11932   15.69317   14.939358
0:  14.189443  13.476393  13.182007  13.362447  13.87372   14.420134
0:  11.568432  11.591809 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.783491  6.135453  6.7148147 7.317759  7.7027783 7.8273034 8.003912
0:  8.1293125 8.160585  8.20923   8.063344  7.748356  7.5188766 7.5196
0:  7.907826  8.605134  9.335762  9.89851   8.706618  9.126355 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.856295 27.826447 27.649302 27.332066 27.079487 26.739002 27.213928
0:  27.586962 28.009745 28.169024 27.634409 27.234947 26.920675 27.276388
0:  28.233826 29.46952  30.625183 31.35314  33.497963 33.892853]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.0992217 -5.113816  -4.8531833 -4.2877126 -3.7017112 -3.2172208
0:  -2.8263383 -2.535152  -2.5875888 -2.688311  -2.9644332 -3.5284781
0:  -3.9857335 -4.3632894 -4.4537187 -4.156794  -3.6954618 -3.185989
0:  -3.7639394 -3.5292697]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.514695  12.198055  11.85993   11.41552   10.780292   9.920029
0:   9.409738   8.877483   8.395502   7.893371   7.0716944  6.276978
0:   5.5131235  5.2934146  5.531297   6.0686746  6.530836   6.6771555
0:   4.152153   3.8482401]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.34077  16.683601 17.227222 17.850004 18.45832  18.896416 19.457674
0:  19.866192 20.114353 20.17232  19.921343 19.393595 18.95566  18.711245
0:  18.79461  19.175262 19.727402 20.1779   16.395267 15.979998]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.710728   -9.988247   -9.8887615  -9.492937   -9.145544   -9.040722
0:   -9.036369   -9.243298   -9.729738  -10.193068  -10.837002  -11.535063
0:  -12.053278  -12.197159  -11.995747  -11.59046   -11.345501  -11.422188
0:  -12.678221  -13.089678 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.743172  9.062892  9.619793 10.223699 10.700897 11.03364  11.541808
0:  11.991097 12.37673  12.669056 12.601585 12.404778 12.216468 12.345787
0:  12.876932 13.688936 14.527402 15.168482 13.151346 13.478208]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.7022824   3.8363428   4.2429404   4.802676    5.2673073   5.527502
0:   5.7262926   5.802319    5.5396485   5.0909977   4.359789    3.324884
0:   2.4739828   1.8936605   1.7453046   2.0185237   2.3603582   2.5158472
0:  -0.20984936 -0.6877551 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0905466 3.2578294 3.6391065 4.20545   4.6578665 4.9345517 5.0067854
0:  5.1096234 4.9676256 4.8985395 4.723983  4.275508  3.8347926 3.4376628
0:  3.3836508 3.798284  4.5923166 5.4049134 4.3215237 4.562657 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.442872   3.1195939  3.9567883  4.897289   5.680898   6.2620287
0:   6.7487216  7.1757965  7.3288727  7.421625   7.3214     6.9796963
0:   6.723951   6.648467   7.0405807  7.930076   9.091881  10.226042
0:   9.246482   9.794043 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1396632 2.242931  2.5853086 3.05074   3.4575226 3.736809  4.0974913
0:  4.3903933 4.493951  4.5387974 4.350855  3.9704256 3.7367318 3.7405019
0:  4.010674  4.440789  4.738811  4.728341  2.5320988 2.243259 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.019045 21.812275 22.987572 24.160843 25.014147 25.268002 25.295448
0:  24.96668  24.303137 23.382013 22.142548 20.823645 19.903395 19.52048
0:  20.004755 21.107082 22.47835  23.70494  22.074898 22.284122]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.846277 27.032331 27.321922 27.598564 27.763975 27.748951 28.074759
0:  28.315575 28.67684  28.861618 28.622824 28.299812 28.169468 28.282942
0:  28.92816  29.762953 30.62383  31.333014 28.63105  28.856085]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3801074  -1.2367377  -0.76940536 -0.0764904   0.53850794  0.94651794
0:   1.1747422   1.158114    0.7201905   0.26165628 -0.3551178  -1.0913911
0:  -1.621736   -1.9415069  -1.9252505  -1.584537   -1.1720557  -0.8032718
0:  -1.791307   -1.7464652 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.254605  4.282409  4.5077915 4.9795046 5.4505963 5.757478  5.7609572
0:  5.657012  5.1343822 4.684801  4.1818457 3.4536357 2.800097  2.1343093
0:  1.7146068 1.7256069 2.149613  2.7860775 1.2567482 1.119112 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.40957403 -0.3689828  -0.02649784  0.50557613  0.9575014   1.2436132
0:   1.6037884   1.8569508   1.9467211   2.0011685   1.804903    1.3910756
0:   1.0796814   1.0436945   1.3417988   1.9683199   2.584741    3.0044675
0:   1.5376663   1.6992717 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.307587 14.134568 14.063799 14.017794 13.997404 13.913266 14.487097
0:  15.01062  15.623552 15.991604 15.79813  15.617239 15.508034 16.007256
0:  17.066406 18.45621  19.76721  20.633823 20.39445  20.25443 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.1675925 -6.106298  -5.7635016 -5.1478086 -4.5173774 -4.019277
0:  -3.71376   -3.521668  -3.715427  -3.9050937 -4.1837106 -4.7014565
0:  -5.0973773 -5.410379  -5.479406  -5.1766715 -4.680493  -4.0780635
0:  -3.667173  -3.5700011]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.344164 24.616833 24.8844   25.07918  25.191357 25.153496 25.508999
0:  25.779453 26.040026 26.172676 25.897196 25.54048  25.251415 25.294468
0:  25.74809  26.451357 27.235138 27.747795 26.047003 26.226927]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [44.93832  45.279404 45.650608 45.927612 46.04707  46.085426 46.658356
0:  47.16492  47.637978 47.91938  47.617596 47.183548 46.92512  47.043095
0:  47.68121  48.406654 48.987392 48.98875  44.379925 44.254982]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.600198  12.619545  12.853357  13.106941  13.283002  13.26066
0:  13.438772  13.463671  13.401941  13.349863  13.02958   12.634708
0:  12.304144  12.2698345 12.592815  13.247498  13.946396  14.500533
0:  12.821402  12.822977 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.089666  -8.164991  -7.8978996 -7.2310905 -6.4882336 -5.8959093
0:  -5.7001305 -5.6431475 -6.0613813 -6.431671  -6.8175592 -7.466506
0:  -7.964177  -8.464119  -8.697761  -8.487184  -7.9067135 -7.0986624
0:  -7.488366  -7.6933727]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.268482  15.3908    15.62657   15.856974  15.913225  15.80332
0:  15.824005  15.846283  15.873753  15.950478  15.9251995 15.864475
0:  15.976376  16.369581  17.108383  17.987486  18.748041  19.206282
0:  15.93508   15.915697 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.409018 23.084621 22.715162 22.388903 22.272678 21.965258 22.503508
0:  22.78504  23.214886 23.380898 22.94619  22.658945 22.482584 22.818905
0:  23.686806 24.905926 26.092173 26.849335 27.029913 27.123041]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.355841  -5.226891  -4.848726  -4.330851  -3.881082  -3.6853309
0:  -3.4316325 -3.3517404 -3.446772  -3.5716052 -3.9147372 -4.442471
0:  -4.871119  -4.985871  -4.7437243 -4.2291417 -3.7193227 -3.3722844
0:  -3.8157125 -3.3676133]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.652693 18.629944 18.5738   18.437347 18.306461 18.032566 18.39428
0:  18.7052   19.12072  19.387468 19.160336 18.917088 18.647964 18.770845
0:  19.299557 20.137024 21.022335 21.631699 21.378407 21.465046]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.318489  -12.424316  -12.1694145 -11.537758  -10.872826  -10.436043
0:  -10.362774  -10.45837   -10.967852  -11.36245   -11.711657  -12.199127
0:  -12.384897  -12.461124  -12.245344  -11.617235  -10.868265  -10.121126
0:  -11.651405  -11.829718 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.487211   9.540665   9.637877   9.688731   9.6514845  9.398585
0:   9.649946   9.822086  10.108106  10.252908   9.908003   9.522126
0:   9.144375   9.320535  10.008041  11.146876  12.317422  13.189257
0:  12.522054  12.748129 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.08307  10.163071 10.398099 10.720545 10.959906 11.030729 11.393063
0:  11.748556 12.073286 12.340136 12.25024  11.980131 11.708703 11.713026
0:  12.091459 12.772961 13.486225 13.958553 12.578166 12.681774]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.64808    8.6385145  8.980299   9.608336  10.212903  10.682208
0:  11.1928425 11.658489  11.876205  12.123223  12.179026  12.020073
0:  11.963242  11.957538  12.25097   12.816383  13.438686  13.897019
0:  12.976784  13.081621 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.0936713 4.9771543 5.1553783 5.4226    5.534322  5.396636  5.481366
0:  5.5250654 5.592745  5.6753674 5.4965158 5.0917087 4.7180595 4.67487
0:  4.9292636 5.54648   6.110899  6.404179  3.6640801 3.4703465]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3863525 2.5992975 3.0977778 3.8271143 4.5545096 5.061906  5.452828
0:  5.694013  5.5790114 5.4638405 5.216916  4.7854147 4.5595236 4.414403
0:  4.6004553 5.003258  5.3991137 5.655118  5.023612  5.295178 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3436885 2.5965638 3.1556811 3.828874  4.3075895 4.5181446 4.7031574
0:  4.960783  5.1258817 5.3963833 5.564965  5.408116  5.271761  5.173274
0:  5.328407  5.916226  6.7379413 7.5254583 5.6659164 5.9261174]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.637779  9.651609  9.832681  9.977213  9.96622   9.67215   9.664782
0:   9.546445  9.596294  9.682997  9.508252  9.212175  9.00169   9.032091
0:   9.500751 10.344527 11.314655 12.151041 10.26132  10.376107]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.8581257 -4.982995  -4.80175   -4.280937  -3.7120438 -3.197257
0:  -2.7113562 -2.2578583 -2.1207676 -2.0621223 -2.2527122 -2.7681541
0:  -3.2550898 -3.6471272 -3.783123  -3.5889568 -3.2528992 -2.9250336
0:  -4.046175  -4.004322 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.4992704   1.6256952   2.0328665   2.6519587   3.1051762   3.2266026
0:   2.996615    2.5874116   1.8293586   1.126606    0.4098382  -0.5122566
0:  -1.2134786  -1.768229   -1.9356141  -1.5758119  -0.9086375  -0.16832304
0:  -1.1537924  -1.1514282 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.695687   8.555879   8.472605   8.412871   8.281985   7.995716
0:   8.215413   8.42627    8.7206955  8.931864   8.705259   8.362607
0:   7.9149218  7.9226427  8.360352   9.184041  10.062668  10.633255
0:   9.72451    9.740726 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.4973645 7.370796  7.4907265 7.7238064 7.817799  7.6771846 7.6275964
0:  7.541717  7.32201   7.0739217 6.5447416 5.7404203 4.9377885 4.3533096
0:  4.160364  4.4035263 4.859247  5.266987  3.5248458 3.5581925]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.9913697 -6.3143373 -6.499603  -6.632584  -6.798141  -7.1418805
0:  -7.0354953 -6.972592  -6.939772  -6.9930186 -7.504581  -8.186213
0:  -8.884924  -9.031862  -8.775457  -8.210173  -7.792995  -7.7406707
0:  -8.800055  -9.174688 ]
0: validation loss for strategy=forecast at epoch 21 : 0.3299257755279541
0: validation loss for velocity_u : 0.14068754017353058
0: validation loss for velocity_v : 0.2383221536874771
0: validation loss for specific_humidity : 0.164612278342247
0: validation loss for velocity_z : 0.5641071796417236
0: validation loss for temperature : 0.11259216070175171
0: validation loss for total_precip : 0.7592337131500244
0: 22 : 19:25:54 :: batch_size = 96, lr = 1.2205418857176618e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 22, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9084, -0.9048, -0.9015, -0.8987, -0.8971, -0.8979, -0.9016, -0.9062, -0.9056, -0.8976, -0.8825, -0.8628,
0:         -0.8402, -0.8165, -0.7939, -0.7749, -0.7600, -0.7459, -0.9172, -0.9182, -0.9192, -0.9180, -0.9161, -0.9162,
0:         -0.9188], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2314, -0.2312, -0.2255, -0.2181, -0.2098, -0.2013, -0.1948, -0.1902, -0.1863, -0.1815, -0.1743, -0.1664,
0:         -0.1597, -0.1529, -0.1462, -0.1394, -0.1335, -0.1309, -0.2020, -0.2002, -0.1950, -0.1896, -0.1826, -0.1752,
0:         -0.1686], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3753, -0.3781, -0.3810, -0.3864, -0.3916, -0.3987, -0.4037, -0.4074, -0.4088, -0.4080, -0.4079, -0.4076,
0:         -0.4076, -0.4078, -0.4067, -0.4028, -0.3965, -0.3863, -0.3689, -0.3744, -0.3822, -0.3892, -0.3979, -0.4065,
0:         -0.4143], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2527, 0.2150, 0.1151, 0.0485, 0.0175, 0.0530, 0.1062, 0.1573, 0.2616, 0.3415, 0.4103, 0.4481, 0.4725, 0.4880,
0:         0.4791, 0.4059, 0.2905, 0.2061, 0.3193, 0.2927, 0.2039, 0.1329, 0.0996, 0.1018, 0.1395], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.9570,  0.9662,  0.9844,  0.9984,  1.0096,  1.0167,  1.0045,  0.9767,  0.9274,  0.8660,  0.8011,  0.7359,
0:          0.6719,  0.6068,  0.5493,  0.4858,  0.4190,  0.3470,  0.2686,  0.1942,  0.1183,  0.0389, -0.0415, -0.1130,
0:         -0.1673], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2592, -0.2592, -0.2592, -0.2592, -0.2592, -0.2592, -0.2580, -0.2545, -0.2592, -0.2592, -0.2592, -0.2592,
0:         -0.2592, -0.2568, -0.2568, -0.2533, -0.2486, -0.2580, -0.2521, -0.2545, -0.2556, -0.2568, -0.2428, -0.2018,
0:         -0.1585], device='cuda:0')
0: [DEBUG] Epoch 22, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2486, -0.2322,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2803,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2311,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2357,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2592,     nan,     nan, -0.2322,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1493, -0.1176,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0579,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2592,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2381, -0.2287,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2334,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.2698,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.0147,     nan,     nan,     nan, -0.2592,     nan,     nan,     nan,
0:             nan,     nan, -0.1889,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 22, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9344, -0.9407, -0.9209, -0.8763, -0.8244, -0.7756, -0.7417, -0.7081, -0.7138, -0.7255, -0.7543, -0.8180,
0:         -0.8761, -0.9281, -0.9463, -0.9205, -0.8607, -0.7896, -0.8878, -0.9417, -0.9617, -0.9493, -0.9112, -0.8570,
0:         -0.8037], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3846, 0.3920, 0.3737, 0.3473, 0.3268, 0.3193, 0.3147, 0.3208, 0.3280, 0.3278, 0.3318, 0.3341, 0.3192, 0.2865,
0:         0.2475, 0.2235, 0.2252, 0.2553, 0.3911, 0.4077, 0.3938, 0.3715, 0.3510, 0.3367, 0.3320], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8711, -0.8714, -0.8687, -0.8591, -0.8449, -0.8363, -0.8345, -0.8369, -0.8404, -0.8486, -0.8535, -0.8514,
0:         -0.8394, -0.8215, -0.8080, -0.7956, -0.7981, -0.8011, -0.8658, -0.8597, -0.8489, -0.8335, -0.8208, -0.8140,
0:         -0.8126], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2288, -0.1875, -0.0272, -0.0597, -0.0926,  0.0546,  0.1378,  0.2130,  0.2152,  0.1327,  0.1248,  0.1349,
0:          0.1978,  0.2050,  0.1614,  0.2717,  0.3550,  0.3212, -0.1737, -0.2857, -0.2169, -0.2238, -0.1929, -0.0353,
0:          0.0211], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.3575, 1.3683, 1.3464, 1.2985, 1.2469, 1.2073, 1.1747, 1.1355, 1.0894, 1.0408, 0.9916, 0.9484, 0.9090, 0.8764,
0:         0.8505, 0.8313, 0.8271, 0.8356, 0.8503, 0.8651, 0.8735, 0.8667, 0.8388, 0.7849, 0.7059], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1796, -0.1833, -0.1864, -0.1926, -0.1895, -0.1862, -0.1868, -0.1887, -0.1877, -0.1918, -0.1872, -0.1891,
0:         -0.1908, -0.1899, -0.1900, -0.1945, -0.1962, -0.2008, -0.1868, -0.1869, -0.1900, -0.1914, -0.1944, -0.1977,
0:         -0.2054], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23945143818855286; velocity_v: 0.3013915419578552; specific_humidity: 0.194259911775589; velocity_z: 0.7476286292076111; temperature: 0.17126666009426117; total_precip: 0.8197242617607117; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1787164956331253; velocity_v: 0.31455743312835693; specific_humidity: 0.1743452548980713; velocity_z: 0.5270529985427856; temperature: 0.15076589584350586; total_precip: 0.598736047744751; 
0: epoch: 22 [1/5 (20%)]	Loss: 0.70923 : 0.33348 :: 0.20252 (2.82 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18654990196228027; velocity_v: 0.2993759214878082; specific_humidity: 0.18030081689357758; velocity_z: 0.5691491961479187; temperature: 0.13970687985420227; total_precip: 0.7299937605857849; 
0: epoch: 22 [2/5 (40%)]	Loss: 0.72999 : 0.31684 :: 0.20833 (15.70 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18661199510097504; velocity_v: 0.2805834710597992; specific_humidity: 0.1853322833776474; velocity_z: 0.5667036175727844; temperature: 0.15708306431770325; total_precip: 0.5721054077148438; 
0: epoch: 22 [3/5 (60%)]	Loss: 0.57211 : 0.29089 :: 0.20619 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20173704624176025; velocity_v: 0.2987437844276428; specific_humidity: 0.17087917029857635; velocity_z: 0.6358230113983154; temperature: 0.1479644626379013; total_precip: 0.8132731914520264; 
0: epoch: 22 [4/5 (80%)]	Loss: 0.81327 : 0.34403 :: 0.20883 (15.92 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.43051147e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.72204590e-06
0:  1.90734863e-06 1.90734863e-06 9.53674316e-07 0.00000000e+00
0:  4.76837158e-07 3.33786011e-06 6.67572021e-06 2.14576721e-05
0:  3.81469727e-06 1.43051147e-06 9.53674316e-07 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 1.14440918e-05 2.28881836e-05
0:  1.23977661e-05 1.64031982e-04 2.32219696e-04 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 5.24520874e-06 7.78198242e-04
0:  6.49929047e-04 5.84125519e-04 7.35759735e-04 7.32421875e-04
0:  5.59330045e-04 1.14583969e-03 1.81293476e-03 3.60488892e-04
0:  1.38187408e-03 1.69324875e-03 1.03712094e-03 1.27315521e-04
0:  1.29222870e-04 1.43051147e-06 3.33786011e-06 1.39713287e-04
0:  3.81469727e-06 4.76837158e-07 1.09672546e-05 7.29560852e-05
0:  1.84679031e-03 4.41980362e-03 3.31878662e-03 9.86099243e-04
0:  3.48091125e-05 1.65939331e-04 7.62939453e-05 2.86102295e-05
0:  1.85966492e-05 1.23977661e-05 3.76701355e-05 7.15255737e-05
0:  3.81469727e-05 1.63078308e-04 1.43051147e-04 2.12669373e-04
0:  4.18186188e-04 7.79151917e-04 1.14059448e-03 1.18303299e-03
0:  8.96930695e-04 9.01222229e-04 7.91549683e-04 7.48157501e-04
0:  9.43183899e-04 1.07288361e-03 1.33275986e-03 1.63173676e-03
0:  1.71375275e-03 1.62029266e-03 1.48534775e-03 1.26075745e-03
0:  1.15156174e-03 9.06467438e-04 6.28948212e-04 5.73158264e-04
0:  5.58853149e-04 5.27858734e-04 5.39779663e-04 4.49180603e-04
0:  2.85625458e-04 1.58786774e-04 1.15871429e-04 1.10149384e-04
0:  1.02996826e-04 9.53674316e-05 6.81877136e-05 3.95774841e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 1.43051147e-06 6.19888306e-06 1.14440918e-05
0:  7.15255737e-06 1.43051147e-06 0.00000000e+00 4.76837158e-07
0:  3.33786011e-06 4.29153442e-06 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 2.38418579e-06 9.77516174e-05 2.05039978e-05
0:  3.81469727e-06 5.24520874e-06 6.67572021e-06 1.09672546e-05
0:  2.28881836e-05 0.00000000e+00 2.86102295e-06 3.09944153e-04
0:  4.52041626e-04 8.10146332e-04 7.59124756e-04 2.54631042e-04
0:  3.76224518e-04 3.71932983e-04 4.55379486e-04 1.50156021e-03
0:  1.36232376e-03 7.68661499e-04 1.63078308e-04 0.00000000e+00
0:  9.53674316e-07 4.29153442e-06 3.33786011e-06 1.00135803e-05
0:  3.33786011e-06 0.00000000e+00 0.00000000e+00 3.14712524e-05
0:  3.38554382e-05 1.35803223e-03 3.17144394e-03 2.94208527e-04
0:  2.81333923e-05 4.24385071e-05 1.62124634e-05 1.43051147e-05
0:  8.10623169e-06 1.28746033e-05 4.43458557e-05 1.48296356e-04
0:  2.00271606e-04 2.81810760e-04 1.83105469e-04 3.41892242e-04
0:  5.23090363e-04 7.69615173e-04 7.83920288e-04 7.24315643e-04
0:  6.32286072e-04 6.91890717e-04 7.92026520e-04 9.09328461e-04
0:  1.06525421e-03 1.13725662e-03 1.34420395e-03 1.62029266e-03
0:  1.80578243e-03 2.13289261e-03 1.90973270e-03 1.41525269e-03]
0: Target values (first 200):
0: [9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  3.24249268e-05 2.67028809e-05 5.53131104e-05 1.11579895e-04
0:  7.05718994e-05 6.67572021e-06 0.00000000e+00 5.72204590e-06
0:  1.43051147e-05 1.86920166e-04 2.55584717e-04 1.32560730e-04
0:  4.00543213e-05 0.00000000e+00 0.00000000e+00 1.69754028e-04
0:  5.97000122e-04 3.30924988e-04 1.17301941e-04 1.01089478e-04
0:  7.24792480e-05 8.29696655e-05 8.86917114e-05 7.53402710e-05
0:  4.86373901e-05 4.57763635e-05 4.19616736e-05 2.95639038e-05
0:  2.19345093e-05 7.62939453e-05 2.39372253e-04 3.19480896e-04
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 1.38282776e-04
0:  7.41004944e-04 1.90544117e-03 2.69699097e-03 2.83145905e-03
0:  3.24058533e-03 3.06129456e-03 3.00884247e-03 1.43337250e-03
0:  1.28459930e-03 1.70040131e-03 3.26538086e-03 4.95338440e-03
0:  4.87136841e-03 3.52287292e-03 2.92015076e-03 2.69222260e-03
0:  1.83677673e-03 1.24359131e-03 4.73976135e-04 2.26020813e-04
0:  1.55448914e-04 1.46865845e-04 1.15394592e-04 9.34600830e-05
0:  1.18255615e-04 2.04086304e-04 1.31607056e-04 2.14576721e-04
0:  2.76565552e-04 2.63214111e-04 2.47001648e-04 2.82287598e-04
0:  3.46183777e-04 3.90052795e-04 2.44140625e-04 8.58306885e-05
0:  3.43322754e-05 1.52587891e-05 4.76837158e-06 3.81469727e-06
0:  3.81469727e-06 2.86102295e-06 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  9.53674316e-07 2.86102295e-06 5.72204590e-06 4.76837158e-06
0:  7.62939453e-06 5.72204590e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 2.76565552e-05 4.29153479e-05 4.95910645e-05
0:  3.91006470e-05 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  7.62939453e-06 1.81198120e-05 8.39233398e-05 7.91549683e-05
0:  7.62939453e-06 0.00000000e+00 1.90734863e-06 4.76837158e-06
0:  8.86917114e-05 6.19888306e-05 8.77380371e-05 6.67572021e-05
0:  4.86373901e-05 4.86373901e-05 4.29153479e-05 3.14712524e-05
0:  1.90734863e-05 2.19345093e-05 2.28881836e-05 2.28881836e-05
0:  1.71661377e-05 1.23977661e-05 1.14440918e-04 1.09672546e-04
0:  0.00000000e+00 1.71661377e-05 5.46455383e-04 8.48770142e-04
0:  2.02941895e-03 3.03554535e-03 3.21006775e-03 3.17001343e-03
0:  2.77233124e-03 2.12287903e-03 4.76264954e-03 1.54781342e-03
0:  1.64985657e-03 1.11293793e-03 2.17151642e-03 3.25679779e-03
0:  3.30543518e-03 2.32982635e-03 1.65081024e-03 1.61647797e-03
0:  2.01797485e-03 1.13868713e-03 6.77108765e-04 4.55856323e-04
0:  6.11305237e-04 7.25746155e-04 6.32286072e-04 5.06401062e-04
0:  3.70979309e-04 1.74522400e-04 1.48773193e-04 1.75476074e-04
0:  2.08854675e-04 2.24113464e-04 2.49862671e-04 2.79426575e-04
0:  3.40461731e-04 4.12940979e-04 3.42369080e-04 1.39236450e-04
0:  4.19616736e-05 1.71661377e-05 1.90734863e-06 9.53674316e-07
0:  2.86102295e-06 9.53674316e-07 2.86102295e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [-4.826021  -4.968092  -4.824548  -4.3593683 -3.840887  -3.4756527
0:  -3.3316383 -3.2805214 -3.612628  -3.8877668 -4.1962223 -4.681525
0:  -4.9483376 -5.143791  -5.082654  -4.6637907 -4.0914316 -3.5104847
0:  -4.5309105 -4.530957 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.591, max = 0.783, mean = -0.353
0:          sample (first 20): tensor([-1.0034, -1.0156, -1.0033, -0.9634, -0.9188, -0.8875, -0.8751, -0.8707, -0.8992, -0.9229, -0.9493, -0.9910,
0:         -1.0139, -1.0307, -1.0255, -0.9895, -0.9404, -0.8905, -0.9017, -0.9622])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0801272  -1.2317166  -1.1299453  -0.8537893  -0.50958633 -0.20342255
0:   0.24978065  0.6837387   0.87353516  0.98120594  0.8168502   0.52817154
0:   0.41891718  0.57959175  1.1197944   1.8412275   2.4759426   2.941486
0:   0.8064327   0.649096  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.9287357 8.045758  8.239121  8.42411   8.557085  8.568256  8.8366165
0:  9.089535  9.2620125 9.399662  9.216447  8.916931  8.620836  8.568955
0:  8.83528   9.294958  9.631911  9.77367   8.432246  8.617743 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.807184 14.548374 14.499204 14.610608 14.769365 14.750565 15.209754
0:  15.569382 16.034197 16.391407 16.325737 16.191923 15.971559 16.098846
0:  16.587433 17.375504 18.252575 18.881859 17.196617 17.045055]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.586316 10.391089 10.242634 10.120626 10.005177  9.773384 10.112186
0:  10.38535  10.691107 10.856745 10.536954 10.187329  9.819388  9.992456
0:  10.661535 11.74867  12.912557 13.839537 16.144567 16.3071  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.233727  20.75825   20.238634  19.695318  18.980953  18.012794
0:  16.9124    15.82148   14.56374   13.480343  12.408009  11.272871
0:  10.303693   9.562708   9.301373   9.532973   9.976121  10.43893
0:   9.78461    9.6801405]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.134116 15.285031 15.625922 15.944907 16.07959  15.979363 16.046797
0:  16.079704 16.084822 16.076145 15.810728 15.392928 15.039084 14.881411
0:  15.0917   15.640065 16.30362  16.85316  15.004052 15.175443]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.326944 18.755468 19.390316 19.975079 20.448648 20.823387 21.34698
0:  21.872545 22.28581  22.67578  22.840525 22.89099  23.074005 23.43788
0:  24.192621 25.117167 25.971828 26.597116 23.83599  24.300188]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.595434   8.630985   8.897236   9.205216   9.374211   9.26284
0:   9.509391   9.622253   9.875468  10.089215   9.907094   9.620245
0:   9.320662   9.417809   9.926556  10.827726  11.7464695 12.46977
0:  10.577386  10.605421 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.32084  24.284548 24.204065 24.133207 24.137873 24.020988 24.620657
0:  25.010736 25.450293 25.581474 25.097092 24.706812 24.410742 24.785501
0:  25.701597 26.95596  28.184252 29.002956 29.467049 29.817232]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.23084   9.250063  9.358774  9.517028  9.575916  9.474712  9.271725
0:  8.991725  8.448381  7.9468303 7.4008465 6.681348  6.1479273 5.73744
0:  5.6911006 6.057326  6.749294  7.455212  6.581572  6.3645744]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.1066947 7.2390127 7.5639124 7.957841  8.211334  8.216511  8.479737
0:  8.5679655 8.580806  8.468218  7.9353843 7.2374063 6.5433245 6.3465214
0:  6.641712  7.4365153 8.338937  9.052898  7.917802  8.137284 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.4609833 -6.402686  -6.0136623 -5.2759705 -4.4748964 -3.8024187
0:  -3.4245262 -3.1348934 -3.268447  -3.4043016 -3.606473  -4.0338106
0:  -4.338192  -4.609274  -4.676427  -4.4029007 -3.9253187 -3.4061546
0:  -5.0172434 -5.418187 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.84658  29.709118 29.534367 29.271507 29.033428 28.540348 28.687439
0:  28.614662 28.640137 28.44432  27.800228 27.445759 27.629635 28.914383
0:  31.248604 34.118427 36.876156 38.791363 37.514305 36.961815]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.3345375 12.761314  13.358753  13.922416  14.306993  14.461396
0:  14.749239  15.041119  15.298407  15.520781  15.5432205 15.454552
0:  15.441845  15.637026  16.22685   17.135479  18.158619  19.087458
0:  16.223124  16.60337  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.1482286 1.2860675 1.7729726 2.4341936 3.0305214 3.4139752 3.7286034
0:  3.8441968 3.683351  3.4374678 3.005756  2.3312006 1.8091302 1.5044098
0:  1.486404  1.8092604 2.1569977 2.426301  1.0918212 1.2022376]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [46.348026 46.254906 45.67823  45.01874  44.35022  43.491222 43.84644
0:  44.135643 44.701015 44.68898  43.675144 42.7636   41.748856 41.49786
0:  42.01097  42.63784  42.970695 42.336872 42.13008  42.308514]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.863602  -15.08239   -14.883135  -14.202963  -13.327389  -12.518461
0:  -12.022078  -11.619572  -11.724707  -11.798439  -11.939899  -12.394804
0:  -12.638754  -12.918514  -12.935799  -12.542587  -11.888819  -11.095871
0:  -11.7738285 -12.021553 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.3521433  5.2250404  5.321537   5.4921613  5.408796   5.0037146
0:  4.576104   4.1588125  3.776401   3.5503614  3.2907238  2.8337336
0:  2.3713024  1.9828095  1.8213842  2.0014603  2.3909872  2.769652
0:  0.7444315  0.83654165]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.068623  10.206668  10.51853   10.900888  11.2495365 11.5259285
0:  11.995201  12.378445  12.651847  12.800937  12.658785  12.36125
0:  12.176971  12.224926  12.664654  13.390301  14.213572  14.927374
0:  12.991589  13.271627 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.2969275 -6.9477253 -6.1751804 -5.111335  -4.130129  -3.3992233
0:  -2.7786102 -2.4463434 -2.4698915 -2.5844421 -2.9119005 -3.398673
0:  -3.7184696 -3.799429  -3.5876884 -3.117073  -2.6727037 -2.3436785
0:  -4.0945587 -4.045102 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.724369  -9.647779  -9.211863  -8.5002575 -7.829754  -7.379566
0:  -6.977357  -6.774559  -6.847377  -6.9004583 -7.1219625 -7.479782
0:  -7.6656284 -7.6536655 -7.351247  -6.791013  -6.248351  -5.8445587
0:  -6.8365602 -6.644619 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.77401304 0.7763734  1.0715714  1.597436   2.152678   2.5398521
0:  3.2351766  3.8478777  4.385865   4.8490963  5.0026627  5.0031347
0:  5.0801797  5.521326   6.2614193  7.315321   8.353224   9.224815
0:  8.944257   9.1095085 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.1673694 -6.1691365 -5.8415313 -5.2263074 -4.647948  -4.2439017
0:  -3.9470706 -3.8103023 -4.050939  -4.332319  -4.835749  -5.5998273
0:  -6.1521325 -6.5430484 -6.5225854 -6.1495633 -5.7147927 -5.301739
0:  -6.3324656 -6.298801 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.986722   -1.1809487  -1.0885911  -0.71907616 -0.33709812 -0.07359314
0:   0.10341024  0.21985054  0.01596975 -0.19809103 -0.52300787 -1.0414929
0:  -1.3434243  -1.5215602  -1.4392772  -1.0418572  -0.5717306  -0.14071989
0:  -1.8337197  -1.9320445 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.6349277  -4.4994555  -4.0494013  -3.360489   -2.7075653  -2.252967
0:  -1.8973942  -1.6638403  -1.7193947  -1.7941542  -1.9905682  -2.4352741
0:  -2.6954088  -2.8303547  -2.6908855  -2.1846218  -1.5724277  -0.99731827
0:  -2.7772923  -2.6357155 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.224068 14.304657 14.338845 14.234114 14.095169 13.858224 14.240404
0:  14.608249 15.033754 15.293753 15.057215 14.754662 14.429669 14.614874
0:  15.21095  16.1393   17.049543 17.62415  17.196888 17.398838]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.120876 15.829506 15.728067 15.758619 15.825066 15.751707 16.141928
0:  16.383026 16.551538 16.514719 15.92841  15.202774 14.433014 14.062164
0:  14.169106 14.577433 15.007483 15.206982 11.662522 11.413451]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1960025  -2.2497725  -1.9052739  -1.1487422  -0.344337    0.40304804
0:   0.8916235   1.4681182   1.7508512   2.1546454   2.4612403   2.3318663
0:   2.0125995   1.4367228   0.8557849   0.5959239   0.65421057  0.87686634
0:  -3.859573   -4.947     ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.790879 -10.977521 -10.831943 -10.305305  -9.701131  -9.232064
0:   -8.985306  -8.781403  -8.944622  -8.969271  -9.02467   -9.286386
0:   -9.39123   -9.469093  -9.42678   -9.142893  -8.848847  -8.619022
0:  -10.989932 -11.071738]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.2105584  7.662196   8.250515   8.76482    9.072031   9.129852
0:   9.168508   9.194606   9.16672    9.121699   8.9618845  8.637736
0:   8.396005   8.357881   8.648633   9.281228  10.062109  10.803223
0:   8.991137   9.451155 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.722214 35.503716 35.173695 34.811996 34.55542  34.09796  34.45966
0:  34.667854 34.966934 34.962505 34.231422 33.62717  33.06586  33.07278
0:  33.58162  34.28588  34.84432  34.9324   33.833504 33.80402 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5657327  3.8373063  4.324251   4.9396563  5.365108   5.5200963
0:  5.5584617  5.4383473  5.0828867  4.7178345  4.188653   3.4731107
0:  2.8563757  2.3892522  2.2454448  2.4362555  2.7577572  3.0413594
0:  0.94551563 1.1983237 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.82235   -4.809364  -4.505901  -3.8647552 -3.1890311 -2.652801
0:  -2.4737773 -2.391891  -2.7675796 -3.1021543 -3.491588  -4.1019464
0:  -4.586123  -5.005468  -5.1406217 -4.8325925 -4.1854043 -3.416267
0:  -4.223905  -4.2263923]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.57058  21.114994 21.812218 22.418232 22.723427 22.759453 22.957699
0:  23.248638 23.52604  23.807362 23.790466 23.478504 23.238735 23.242619
0:  23.784046 24.789127 26.03125  27.11229  25.167282 25.671581]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.0381036 5.088932  5.351417  5.7036304 5.928686  5.917626  5.792188
0:  5.598743  5.196896  4.8028307 4.294758  3.5809438 2.963355  2.5250335
0:  2.4079003 2.7427602 3.3113189 3.9512374 1.6712914 1.6091356]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.126047 11.734446 11.543274 11.583149 11.724221 11.709387 12.4564
0:  12.853191 13.38496  13.589538 13.14563  12.736759 12.441013 12.860485
0:  13.907183 15.405796 16.912302 18.039047 19.49549  19.628569]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.20610571  0.16592884  0.47261906  1.0331745   1.4981837   1.7277126
0:   1.8069444   1.7419691   1.3937254   1.0811391   0.6634693   0.0236125
0:  -0.4671259  -0.8251066  -0.89925003 -0.5903816  -0.14269352  0.25847816
0:  -1.2646537  -1.3371501 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.420351 13.380646 13.55093  13.746983 13.881094 13.858295 14.235012
0:  14.545464 14.936607 15.181671 15.022034 14.789568 14.59749  14.742512
0:  15.318304 16.214672 17.12449  17.807953 16.836483 16.883266]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.307829 23.310043 23.33602  23.22085  22.896788 22.343851 21.989334
0:  21.545128 21.03389  20.413996 19.43835  18.29614  17.258772 16.63634
0:  16.58166  17.012184 17.683016 18.274202 16.773071 16.61106 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5819631  -1.5283694  -1.15313    -0.5058837   0.13798094  0.6578417
0:   1.2948184   1.8658929   2.219428    2.3823338   2.2327776   1.743845
0:   1.3583708   1.2186565   1.4926052   2.2796726   3.2156785   4.051544
0:   2.5637631   2.7570586 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.818523 14.915676 15.216932 15.690401 16.127491 16.371723 16.785429
0:  17.076345 17.257618 17.371632 17.241375 17.005695 16.951834 17.206057
0:  17.87597  18.822624 19.829264 20.573639 19.383343 19.577333]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.398334 12.71192  13.187899 13.62295  13.932577 14.084833 14.480034
0:  14.837487 15.185369 15.480956 15.492674 15.372352 15.353954 15.589501
0:  16.20149  17.107246 18.024937 18.705471 17.118118 17.619003]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.27309  24.159576 23.766026 23.148806 22.49939  21.709707 21.923555
0:  22.108356 22.557941 22.730562 22.110348 21.615944 21.001793 21.093086
0:  21.78523  22.74038  23.543049 23.62043  24.025003 24.220566]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.085884    3.693185    3.5636656   3.6981986   3.78134     3.6502633
0:   3.2571392   2.8294296   2.1523876   1.6463809   1.1949172   0.5577712
0:   0.0826745  -0.31204653 -0.39752817  0.0173173   0.7912278   1.6458402
0:   0.29632902  0.18271208]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6011643  -3.6215043  -3.2365413  -2.6044374  -1.9752965  -1.5337124
0:  -1.0763736  -0.7891879  -0.7459707  -0.7070341  -0.87608767 -1.1957574
0:  -1.392983   -1.2985649  -0.9352312  -0.31564522  0.18712902  0.48117495
0:  -1.1259875  -1.1663465 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.208506  11.119011  11.023312  10.885229  10.650579  10.229603
0:  10.136635   9.944574   9.739813   9.396484   8.698284   7.9615974
0:   7.2839713  7.1024156  7.356551   7.953994   8.604136   8.9918785
0:   7.0291514  6.9930058]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4470968  -1.5750046  -1.3382177  -0.7057643  -0.02980518  0.47341394
0:   0.62893057  0.6833825   0.31243658 -0.01262903 -0.3908224  -1.0648537
0:  -1.653882   -2.262023   -2.588542   -2.4541416  -1.9319353  -1.2422256
0:  -2.0491266  -2.3661184 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.067154  11.269983  11.729473  12.2813    12.6066885 12.673165
0:  12.633261  12.580927  12.321915  12.139807  11.827892  11.2859335
0:  10.865101  10.652761  10.865848  11.504098  12.3183365 13.020298
0:  11.212057  11.438734 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.053531  3.8925593 3.7422514 3.6690278 3.5361447 3.3081756 3.7988703
0:  4.232503  4.7282686 4.9262114 4.4421573 3.8682973 3.2674851 3.545961
0:  4.5607243 6.0061445 7.3690543 8.074252  9.637507  9.788193 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.19213  20.936647 20.482265 19.814772 19.123266 18.398754 18.586262
0:  18.822363 19.308777 19.59424  19.272272 19.083538 18.852892 19.320644
0:  20.298468 21.493803 22.454563 22.74854  23.597174 23.596268]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-22.026875 -22.06495  -21.887947 -21.288662 -20.743319 -20.437454
0:  -20.329363 -20.44023  -20.95517  -21.423416 -22.211906 -23.16703
0:  -23.832123 -24.133501 -23.819645 -23.137531 -22.586746 -22.285852
0:  -21.245886 -21.501041]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2622442 1.3439903 1.7308216 2.284937  2.7277617 2.948907  3.16705
0:  3.2694936 3.2206135 3.1387897 2.8723984 2.4001698 2.0692306 1.9718051
0:  2.1928716 2.7244997 3.2431421 3.5815477 2.1313357 2.251906 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.933371 11.078588 11.416933 11.823664 12.248425 12.580272 13.284082
0:  13.958427 14.653357 15.221306 15.397474 15.383995 15.385408 15.695295
0:  16.372183 17.364315 18.420351 19.3142   17.087519 17.32531 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.47349  22.332335 22.239264 22.18138  22.053291 21.647108 21.725132
0:  21.716646 21.835114 21.798697 21.294573 20.743422 20.174383 19.906942
0:  20.081877 20.581549 21.194904 21.605028 20.150434 20.025316]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5277314 6.927904  7.44635   8.102747  8.516728  8.568346  8.243597
0:  7.7437463 6.871227  6.057939  5.201211  4.133977  3.1874795 2.3260593
0:  1.8580351 1.9597082 2.5164747 3.2844274 1.9044609 1.4429312]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.243565  8.349036  8.616824  8.903048  9.06505   9.03784   9.078715
0:  9.033891  8.875105  8.71594   8.372292  7.9166026 7.5783615 7.482687
0:  7.71883   8.247197  8.79748   9.178701  7.8775206 8.148734 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.59068155 -0.646646   -0.414299    0.09400845  0.62718534  0.99299383
0:   1.1596751   1.0972457   0.629796    0.11368513 -0.50857115 -1.1949649
0:  -1.626432   -1.8974013  -1.8575168  -1.6230254  -1.4538941  -1.4303126
0:  -0.592484   -0.62078524]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.392116  -4.2813506 -3.8943791 -3.2363534 -2.577373  -2.0937524
0:  -1.8099895 -1.6281562 -1.8107028 -2.0063758 -2.2958226 -2.7904859
0:  -3.1064067 -3.306767  -3.2645469 -2.8721528 -2.3939672 -1.9571176
0:  -3.1429849 -3.0862775]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.730297  11.424128  11.277573  11.205713  11.150288  10.877161
0:  11.037455  11.062616  11.206533  11.284093  10.9954815 10.611486
0:  10.260177  10.345316  10.83033   11.746807  12.717964  13.445612
0:  12.767199  12.5531845]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.91656  24.93269  24.720896 24.303102 23.91016  23.446245 23.894997
0:  24.372969 25.079338 25.585892 25.491991 25.50485  25.503408 26.058533
0:  27.041626 28.201036 29.17802  29.553028 29.01086  29.096596]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -5.05432    -5.425298   -5.5049443  -5.3221736  -5.1735797  -5.179545
0:   -5.1786647  -5.339192   -5.8147073  -6.4068084  -7.268741   -8.393322
0:   -9.292097   -9.892893  -10.064766   -9.806726   -9.462437   -9.117546
0:  -11.068235  -11.214645 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.801496   9.782721   9.937644  10.151482  10.193871  10.0211935
0:   9.804267   9.501541   9.060386   8.645675   8.105962   7.3762617
0:   6.761713   6.333413   6.2344413  6.4615     6.798545   7.0276613
0:   5.286338   5.3523884]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.18254  17.808214 17.490355 17.241835 17.022581 16.671736 16.903635
0:  17.027126 17.18315  17.146147 16.575926 15.936533 15.231422 14.961035
0:  15.1859   15.803429 16.581951 17.16136  16.417906 16.252455]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.047748   9.982435  10.076715  10.187648  10.213132  10.048843
0:  10.31839   10.50596   10.804943  11.0565815 11.025032  11.037796
0:  11.183979  11.802235  12.817893  14.018949  15.0903635 15.853863
0:  16.844866  16.869873 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.2184095 -7.009225  -6.320181  -5.256242  -4.233507  -3.4202852
0:  -2.8138332 -2.5001106 -2.7141104 -2.9282808 -3.3643084 -4.103793
0:  -4.684556  -5.177557  -5.272791  -4.988864  -4.572011  -4.069538
0:  -4.2467995 -3.9651756]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.497193  6.5621266 6.802711  7.2434545 7.6338406 7.8505807 7.871766
0:  7.840611  7.4709997 7.1365724 6.7198358 6.0483212 5.4678802 4.9291697
0:  4.6884966 4.8561907 5.306716  5.793517  4.56221   4.391306 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.419379 15.571189 15.82847  16.052845 16.14252  16.169094 16.460426
0:  16.839245 17.198301 17.550917 17.66515  17.653101 17.783401 18.16181
0:  18.93069  19.90118  20.85817  21.642685 18.220655 18.494087]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.703734  -5.9560456 -5.8716607 -5.300245  -4.52494   -3.7664628
0:  -3.2720675 -2.8360648 -2.8684373 -2.862865  -2.9207196 -3.2559962
0:  -3.4606462 -3.6949086 -3.7065692 -3.2881713 -2.5274615 -1.6221395
0:  -1.9646516 -2.173962 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.0814657   5.770817    6.5576396   7.0735626   6.861561    6.0093737
0:   4.8656163   3.6942585   2.3515997   1.1898642  -0.020082   -1.384778
0:  -2.5249014  -3.1791916  -3.0952902  -2.3126822  -1.1370049  -0.05519104
0:  -0.2985134   0.04948616]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.98188  17.824825 17.725761 17.558243 17.332655 16.84354  16.830004
0:  16.64656  16.524256 16.248909 15.514128 14.768362 14.05875  13.83696
0:  14.07606  14.802561 15.580654 16.213095 15.646889 15.678221]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3975234 4.3763714 4.6138763 5.1305566 5.6480827 5.972936  6.1447325
0:  6.22451   5.913953  5.640598  5.2068887 4.628892  4.1836553 3.8292868
0:  3.810339  4.112563  4.5249977 4.8372917 4.286887  4.245056 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.64075  37.834747 37.92603  37.93575  37.855625 37.495144 37.776093
0:  37.893055 38.11107  38.119343 37.582176 36.95473  36.36101  36.17862
0:  36.410103 36.897915 37.41801  37.55341  35.870777 35.877613]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.139099  8.388132  8.883053  9.420658  9.797748 10.017627 10.556708
0:  11.173891 11.983173 12.846916 13.453482 13.845423 14.229741 14.843677
0:  15.784738 17.028542 18.310087 19.282024 19.816729 20.305065]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.058559   -1.1042628  -0.7669673  -0.16373253  0.39950562  0.6999974
0:   0.75426435  0.5212941  -0.12429523 -0.78251886 -1.5554442  -2.4928336
0:  -3.1778235  -3.6854424  -3.8494658  -3.7290711  -3.554565   -3.4697976
0:  -4.397325   -4.497146  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.419599  -6.2196765 -5.7282977 -5.020414  -4.4205604 -4.090318
0:  -4.0406117 -4.197574  -4.752992  -5.231295  -5.785415  -6.469126
0:  -6.9466414 -7.257438  -7.255009  -6.914106  -6.4316792 -5.942529
0:  -6.862347  -6.7769237]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.901598 17.851667 17.697971 17.526903 17.400553 17.125135 17.619307
0:  17.998798 18.60063  19.000668 18.82967  18.75083  18.686611 19.206467
0:  20.176409 21.403263 22.521069 23.056578 22.573534 22.659262]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -6.032986   -6.010052   -5.5886064  -4.8582253  -4.17384    -3.7514844
0:   -3.6322036  -3.7571445  -4.326816   -4.965138   -5.7663026  -6.8254113
0:   -7.670402   -8.389994   -8.734259   -8.611834   -8.284843   -7.8916955
0:  -10.077156  -10.137249 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.9877906  -2.95787    -2.5138974  -1.7438083  -1.0048404  -0.45398426
0:   0.07718182  0.57964516  0.91216564  1.2652035   1.4581757   1.3217621
0:   1.1853352   1.1304984   1.2789941   1.818562    2.4286723   2.9207876
0:   3.586558    3.7208276 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.5863132 -6.510706  -6.037689  -5.160265  -4.1803226 -3.3482208
0:  -2.8361416 -2.425374  -2.491952  -2.5275793 -2.6386814 -3.0657244
0:  -3.3487005 -3.672903  -3.7886114 -3.483429  -2.925621  -2.2514143
0:  -3.1850204 -3.2465248]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.683317  5.5099096 5.679311  6.024411  6.3633814 6.497433  6.6338024
0:  6.637255  6.4381394 6.1396046 5.6067343 4.8626184 4.267667  3.94318
0:  4.082655  4.733508  5.599587  6.5260596 4.452633  4.3167048]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.609566 22.693502 22.926306 23.221788 23.42685  23.376915 23.54457
0:  23.558939 23.5414   23.450554 22.999382 22.423244 21.849876 21.535126
0:  21.56262  21.88983  22.243694 22.436098 19.109    18.827969]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.874994    7.275029    5.949807    4.839921    3.8677595   2.8880582
0:   2.4464912   2.0677762   1.8691144   1.6124616   1.0536184   0.42460537
0:  -0.03793907  0.08927965  0.8740063   2.198207    3.6949477   4.9967976
0:   5.5958543   5.688841  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8354278  -3.3100848  -2.456141   -1.3323956  -0.26885653  0.5774436
0:   1.2117095   1.6213255   1.573997    1.3978028   0.9882884   0.3206911
0:  -0.23386908 -0.60360813 -0.63544464 -0.29227018  0.26333952  0.7984462
0:  -0.33091927  0.01166058]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.981468 18.87698  18.567411 17.993263 17.369772 16.775364 17.092834
0:  17.602188 18.301159 18.8536   18.752083 18.70633  18.62799  19.230856
0:  20.412518 21.833511 23.02602  23.626968 26.336456 26.758957]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.861423 12.319354 13.748785 15.036495 16.110897 16.863228 17.628178
0:  18.136738 18.40468  18.570162 18.474857 18.260418 18.214855 18.478935
0:  19.102196 20.044775 21.251333 22.465225 26.988874 29.23032 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.014545  1.28024   1.844121  2.722379  3.6533496 4.5049253 5.1903763
0:  5.765041  5.943177  6.0704036 6.0240364 5.7195616 5.4463    5.150152
0:  5.0331593 5.2045007 5.551033  5.9323487 4.271952  4.343138 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.937501  11.745314  11.79344   11.971754  12.006104  11.762501
0:  11.508068  11.180488  10.7377205 10.376845   9.919406   9.270321
0:   8.722215   8.264965   8.115835   8.372259   8.864448   9.3188095
0:   7.367177   7.1117773]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.113645    3.9350774   3.9694111   4.141277    4.218224    4.110229
0:   4.1798544   4.1166334   3.8975065   3.4877682   2.6500921   1.5115623
0:   0.34376192 -0.46069527 -0.90879726 -0.9821992  -0.95991373 -1.0801282
0:  -4.830936   -5.157953  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.301949 24.083054 23.717064 23.16862  22.585863 21.830359 21.906084
0:  21.950838 22.119707 22.064299 21.277826 20.608047 19.843918 19.765104
0:  20.212263 20.95208  21.549454 21.655365 21.485224 21.459719]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.28064  32.563576 32.805107 32.930405 32.942394 32.80364  33.039856
0:  33.23262  33.48351  33.576347 33.220528 32.805298 32.55783  32.66916
0:  33.31363  34.17258  35.04524  35.725002 32.56139  32.64763 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.251452 15.362036 15.584635 15.769783 15.791886 15.59579  15.762798
0:  15.898114 16.057964 16.200813 15.949481 15.590353 15.226108 15.276983
0:  15.753662 16.554747 17.358255 17.893867 16.79464  16.874792]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.056632   1.096756   1.4113445  1.8042197  2.053916   2.059393
0:  2.2858725  2.419815   2.5026126  2.5417314  2.2044053  1.62644
0:  1.0294809  0.7822499  0.90679073 1.5006437  2.1731286  2.6922975
0:  1.0710325  1.2043872 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.13562298 -0.05669737  0.01823521  0.2991953   0.4627304   0.36244535
0:   0.27580786  0.01553297 -0.5125909  -1.138618   -2.0948873  -3.3016944
0:  -4.4488006  -5.2204146  -5.5655932  -5.3906074  -4.9960647  -4.6386104
0:  -4.7285485  -4.5510793 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.667417   -1.5373597  -1.0968432  -0.4756565   0.03472757  0.34864378
0:   0.59584236  0.79423     0.75681925  0.7363281   0.5587983   0.10822582
0:  -0.2470169  -0.4918766  -0.39961052  0.080235    0.71590567  1.3080745
0:   0.11437607  0.41696405]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.729709  4.720323  4.856472  4.996914  5.1105847 5.0851774 5.4168253
0:  5.681473  5.94266   6.099358  5.865919  5.5381727 5.2340126 5.334215
0:  5.798528  6.6416144 7.5142136 8.209405  6.815587  6.9065585]
0: validation loss for strategy=forecast at epoch 22 : 0.2404455542564392
0: validation loss for velocity_u : 0.1434422880411148
0: validation loss for velocity_v : 0.25753068923950195
0: validation loss for specific_humidity : 0.1460460126399994
0: validation loss for velocity_z : 0.5012345910072327
0: validation loss for temperature : 0.1136869266629219
0: validation loss for total_precip : 0.28073254227638245
0: 23 : 19:29:51 :: batch_size = 96, lr = 1.1907725714318652e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 23, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4631, -0.4829, -0.5003, -0.5210, -0.5474, -0.5643, -0.5800, -0.6109, -0.6239, -0.6037, -0.5695, -0.5020,
0:         -0.4163, -0.3762, -0.3698, -0.3493, -0.3119, -0.2679, -0.4746, -0.4873, -0.4971, -0.5093, -0.5321, -0.5524,
0:         -0.5665], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4899,  0.4724,  0.4512,  0.4235,  0.3879,  0.3543,  0.3224,  0.2574,  0.1588,  0.0898,  0.0877,  0.1378,
0:          0.1811,  0.1438,  0.0468, -0.0362, -0.0910, -0.1290,  0.5114,  0.4985,  0.4816,  0.4580,  0.4212,  0.3785,
0:          0.3290], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4795, -0.4861, -0.4986, -0.5198, -0.5460, -0.5683, -0.5871,  0.0546,  0.9193,  1.1893,  0.9461,  0.2625,
0:         -0.3205, -0.6170, -0.6560, -0.6356, -0.6087, -0.5856, -0.3977, -0.4055, -0.4194, -0.4415, -0.4761, -0.4999,
0:         -0.5241], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1471, -0.0271, -0.1752, -0.2079, -0.2297, -0.3211, -0.3190, -0.1469, -0.0533, -0.0620,  0.0796,  0.2364,
0:          0.2690,  0.2821,  0.1885,  0.0752,  0.0817,  0.0382,  0.0992, -0.0206, -0.0816, -0.0511, -0.1251, -0.3059,
0:         -0.3429], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.4101, -1.3744, -1.3334, -1.2739, -1.2303, -1.1724, -1.1470, -1.2993, -1.5129, -1.6204, -1.6033, -1.3308,
0:         -0.8423, -0.4825, -0.3814, -0.4185, -0.4869, -0.5345, -0.5669, -0.5888, -0.6010, -0.6200, -0.6395, -0.6632,
0:         -0.6930], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.0693,  0.1430,  0.0961, -0.0894, -0.1184, -0.1274, -0.1073,  0.0022,  0.2033, -0.1006, -0.0045,  0.0827,
0:         -0.0112, -0.0782, -0.0112, -0.0849, -0.1229,  0.1340, -0.0916, -0.0112,  0.0760,  0.0514,  0.0447,  0.1184,
0:         -0.0022], device='cuda:0')
0: [DEBUG] Epoch 23, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2346,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2357,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1653,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0771,     nan,     nan,     nan,
0:          0.1352,     nan,     nan, -0.0559, -0.0793,     nan, -0.0581,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2263,  0.1659,     nan,     nan,     nan, -0.0991, -0.2080,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1347,     nan,     nan,     nan,
0:             nan,     nan, -0.0183,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1638,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0302,     nan,     nan,     nan, -0.2209,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1401,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1853,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2198,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 23, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5265, -0.5408, -0.5329, -0.5149, -0.5005, -0.4936, -0.4802, -0.4578, -0.4421, -0.4205, -0.4106, -0.4241,
0:         -0.4321, -0.4246, -0.3938, -0.3253, -0.2483, -0.1886, -0.3885, -0.4270, -0.4436, -0.4568, -0.4739, -0.4789,
0:         -0.4703], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2854, 0.2994, 0.2933, 0.2742, 0.2363, 0.1977, 0.1543, 0.1307, 0.1243, 0.1282, 0.1411, 0.1587, 0.1596, 0.1367,
0:         0.1062, 0.0960, 0.1180, 0.1722, 0.2530, 0.2926, 0.3051, 0.2904, 0.2509, 0.1998, 0.1561], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9494, 0.9779, 0.9882, 0.9896, 0.9738, 0.9385, 0.8892, 0.8272, 0.7443, 0.6617, 0.5670, 0.4655, 0.3723, 0.3016,
0:         0.2144, 0.1667, 0.1177, 0.1089, 1.0011, 1.0327, 1.0525, 1.0497, 1.0235, 0.9707, 0.9196], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1434, 0.3434, 0.4164, 0.3439, 0.2461, 0.2823, 0.3300, 0.3404, 0.4040, 0.3683, 0.3289, 0.4266, 0.5037, 0.5512,
0:         0.5181, 0.5875, 0.7892, 0.6691, 0.2955, 0.2990, 0.2156, 0.1703, 0.1605, 0.2307, 0.2988], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.6795, -1.7710, -1.8037, -1.7910, -1.7609, -1.7542, -1.7706, -1.7935, -1.7981, -1.7771, -1.7370, -1.6748,
0:         -1.6018, -1.5048, -1.4096, -1.3255, -1.2771, -1.2487, -1.2193, -1.1791, -1.1356, -1.1053, -1.0960, -1.1074,
0:         -1.1169], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.3324, 0.2756, 0.2271, 0.1764, 0.1535, 0.1083, 0.0928, 0.0690, 0.0421, 0.3926, 0.3261, 0.2622, 0.2058, 0.1385,
0:         0.0939, 0.0475, 0.0421, 0.0262, 0.5234, 0.4387, 0.3730, 0.2794, 0.2075, 0.1332, 0.0622], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.26071465015411377; velocity_v: 0.35435447096824646; specific_humidity: 0.17855899035930634; velocity_z: 0.7874156832695007; temperature: 0.8295624852180481; total_precip: 0.7133790254592896; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20830398797988892; velocity_v: 0.2878113389015198; specific_humidity: 0.22027426958084106; velocity_z: 0.5987986922264099; temperature: 0.20351676642894745; total_precip: 0.7822098731994629; 
0: epoch: 23 [1/5 (20%)]	Loss: 0.74779 : 0.41666 :: 0.20449 (2.50 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19609521329402924; velocity_v: 0.28537717461586; specific_humidity: 0.17644695937633514; velocity_z: 0.579847514629364; temperature: 0.17283445596694946; total_precip: 0.804991602897644; 
0: epoch: 23 [2/5 (40%)]	Loss: 0.80499 : 0.33566 :: 0.20609 (15.87 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18660731613636017; velocity_v: 0.2630321979522705; specific_humidity: 0.2032371461391449; velocity_z: 0.573218047618866; temperature: 0.15493714809417725; total_precip: 0.655993640422821; 
0: epoch: 23 [3/5 (60%)]	Loss: 0.65599 : 0.30481 :: 0.20454 (15.63 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17443940043449402; velocity_v: 0.26021525263786316; specific_humidity: 0.17302905023097992; velocity_z: 0.4215908646583557; temperature: 0.1336531937122345; total_precip: 0.33315902948379517; 
0: epoch: 23 [4/5 (80%)]	Loss: 0.33316 : 0.21677 :: 0.20290 (15.72 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [2.70175934e-03 9.08851624e-04 1.29699707e-04 1.30653381e-04
0:  4.07218933e-04 4.05311584e-04 9.44137573e-05 9.82284546e-05
0:  9.25064087e-05 5.43594360e-05 3.91006470e-05 4.86373901e-05
0:  4.95910645e-05 7.43865967e-05 6.29425049e-05 4.48226892e-05
0:  1.42097473e-04 1.75476074e-04 1.89781189e-04 2.99453735e-04
0:  4.13894653e-04 3.31878662e-04 1.45912170e-04 9.15527344e-05
0:  3.33786011e-05 9.63211060e-05 1.13487244e-04 5.91278076e-05
0:  2.00271606e-05 1.90734863e-05 1.71661377e-05 4.10079956e-05
0:  1.43051147e-05 1.90734863e-05 2.09808350e-05 5.34057617e-05
0:  9.44137573e-05 9.15527344e-05 5.53131104e-05 4.76837158e-05
0:  7.34329224e-05 8.48770142e-05 9.91821289e-05 1.99317932e-04
0:  1.57356262e-04 7.72476196e-05 3.14712524e-05 1.62124634e-05
0:  1.62124634e-05 1.90734863e-05 2.76565552e-05 3.81469727e-06
0:  9.53674316e-07 4.38690149e-05 1.48773193e-04 3.10897827e-04
0:  4.15802002e-04 4.47273254e-04 7.49588013e-04 7.61985779e-04
0:  5.39779663e-04 6.59942627e-04 8.93592834e-04 1.43337250e-03
0:  2.17914581e-03 2.59494781e-03 2.07614899e-03 9.84191895e-04
0:  6.79016113e-04 3.03268433e-04 1.46865845e-04 8.48770142e-05
0:  4.65393066e-04 9.98497009e-04 1.12152100e-03 6.51359558e-04
0:  2.61306763e-04 2.68936157e-04 1.57356262e-04 1.48773193e-04
0:  1.31607056e-04 2.04086304e-04 3.59535217e-04 3.74794006e-04
0:  1.76429749e-04 2.38418579e-04 6.00814819e-05 1.07765198e-04
0:  4.55856323e-04 2.41279602e-04 4.48226892e-05 2.20298767e-04
0:  3.32832366e-04 1.52587891e-05 9.53674316e-07 6.67572021e-06
0:  2.06947327e-04 6.07490540e-04 2.93731689e-04 1.41143799e-04
0:  3.58581543e-04 9.73701535e-04 1.71756744e-03 1.43146515e-03
0:  8.96453857e-05 2.06947327e-04 8.48770142e-05 2.28881836e-05
0:  8.94546509e-04 2.88009644e-04 6.67572021e-05 3.43322754e-05
0:  5.53131104e-05 6.10351562e-05 9.82284546e-05 9.72747803e-05
0:  1.01089478e-04 6.29425049e-05 3.91006470e-05 4.19616736e-05
0:  4.19616736e-05 6.67572021e-05 1.43051147e-05 7.15255737e-05
0:  1.51634216e-04 1.75476074e-04 2.00271606e-04 2.55584717e-04
0:  3.10897827e-04 2.72750854e-04 9.63211060e-05 4.95910645e-05
0:  2.67028809e-05 8.96453857e-05 8.48770142e-05 4.57763635e-05
0:  2.28881836e-05 1.62124634e-05 5.72204590e-06 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 1.90734863e-06 1.90734863e-05
0:  2.47955322e-05 3.24249268e-05 4.10079956e-05 3.71932983e-05
0:  4.10079956e-05 8.29696655e-05 7.82012939e-05 8.96453857e-05
0:  9.82284546e-05 1.32560730e-04 1.14440918e-04 4.19616736e-05
0:  2.67028809e-05 2.67028809e-05 2.00271606e-05 1.52587891e-05
0:  9.53674316e-06 3.81469727e-05 1.37329102e-04 2.58445740e-04
0:  3.01361084e-04 2.42233276e-04 3.18527251e-04 3.35693359e-04
0:  2.40325928e-04 3.24249268e-04 3.97682190e-04 4.70161438e-04
0:  5.49316464e-04 9.73701535e-04 1.60121918e-03 1.24740601e-03
0:  1.23977661e-03 8.01086426e-04 7.76290894e-04 4.56809998e-04
0:  2.47001648e-04 3.69071960e-04 6.78062439e-04 8.16345215e-04
0:  7.92503357e-04 3.74794006e-04 2.05039978e-04 1.64031982e-04
0:  1.31607056e-04 9.34600830e-05 1.18255615e-04 1.61170959e-04
0:  1.20162964e-04 1.02996826e-04 4.76837158e-06 8.20159912e-05
0:  2.92778015e-04 2.13623047e-04 6.67572021e-05 1.19209290e-04]
0: Target values (first 200):
0: [4.67300379e-05 2.67028809e-05 1.29699707e-04 4.30107117e-04
0:  6.25610352e-04 3.37600708e-04 1.85489655e-04 1.24931335e-04
0:  7.62939453e-05 3.86238098e-05 1.47819519e-05 4.29153442e-06
0:  8.10623169e-06 2.67028809e-05 3.48091125e-05 1.38282776e-05
0:  5.24520874e-06 7.62939453e-06 7.15255737e-06 5.24520874e-06
0:  4.29153442e-06 6.19888306e-06 3.33786011e-06 4.29153442e-06
0:  1.47819519e-05 1.66893005e-05 1.66893005e-05 5.91278076e-05
0:  6.24656677e-05 2.43186951e-05 3.05175781e-05 8.01086426e-05
0:  1.27315521e-04 2.43663788e-04 3.02791595e-04 3.04222107e-04
0:  3.95774841e-04 5.34534454e-04 7.27176666e-04 9.89913940e-04
0:  1.19543076e-03 1.09338760e-03 8.03470612e-04 3.49521637e-04
0:  1.09672546e-04 2.81333923e-05 1.00135803e-05 1.07288361e-04
0:  4.15325165e-04 8.16822052e-04 8.99314880e-04 1.28841400e-03
0:  1.51157379e-03 1.30558014e-03 1.12056732e-03 1.18064880e-03
0:  1.01995468e-03 3.80992889e-04 1.58309937e-04 1.33037567e-04
0:  1.12056732e-04 1.19686127e-04 2.70843506e-04 2.85148621e-04
0:  3.25679808e-04 4.09603119e-04 4.96864319e-04 7.67230988e-04
0:  9.89913940e-04 5.96046448e-04 1.04999542e-03 1.65176392e-03
0:  1.23262405e-03 3.95298004e-04 1.33514404e-05 2.38418579e-06
0:  9.53674316e-07 1.43051147e-06 4.76837158e-06 2.87055969e-04
0:  1.37329102e-04 1.57356262e-05 1.16348267e-04 1.86443329e-04
0:  5.15460968e-04 1.86014175e-03 2.52723694e-05 2.52723694e-05
0:  1.53541565e-04 7.07626343e-04 8.00132751e-04 9.33647156e-04
0:  1.12819672e-03 7.18593597e-04 8.15391541e-05 7.39097595e-05
0:  3.43322783e-04 7.61508942e-04 7.21454620e-04 3.94821167e-04
0:  2.45094299e-04 2.97546387e-04 2.03132629e-04 7.72476196e-05
0:  6.53266907e-05 1.32083893e-04 1.38759613e-04 1.45912170e-04
0:  1.36375427e-04 9.01222229e-05 9.63211060e-05 3.06129456e-04
0:  7.30991364e-04 4.21524048e-04 2.88009644e-04 1.38759613e-04
0:  5.14984131e-05 1.95503235e-05 1.23977661e-05 2.33650208e-05
0:  5.48362732e-05 4.95910645e-05 3.57627869e-05 2.28881836e-05
0:  8.58306885e-06 6.67572021e-06 1.47819519e-05 1.90734863e-05
0:  1.85966492e-05 3.52859497e-05 7.24792480e-05 6.48498535e-05
0:  7.86781311e-05 8.15391541e-05 7.05718994e-05 6.29425049e-05
0:  5.62667847e-05 5.00679016e-05 3.62396240e-05 2.71797180e-05
0:  5.81741333e-05 7.86781311e-05 1.42097473e-04 9.10758972e-05
0:  7.96318054e-05 1.01089478e-04 2.24590302e-04 4.75883484e-04
0:  7.28607178e-04 8.74042511e-04 6.33239746e-04 3.19004059e-04
0:  1.60694122e-04 7.15255737e-05 2.47955322e-05 3.52859497e-05
0:  1.85012817e-04 7.20500946e-04 9.37938690e-04 8.10623169e-04
0:  7.96318054e-04 3.94344330e-04 1.62601471e-04 1.42097473e-04
0:  1.26361847e-04 1.52111053e-04 1.47819519e-04 1.09672546e-04
0:  9.87052917e-05 7.39097595e-05 5.53131104e-05 6.67572021e-05
0:  7.39097595e-05 2.49385834e-04 1.08289719e-03 2.43091583e-03
0:  2.54726410e-03 2.49767303e-03 1.48773193e-03 1.31893158e-03
0:  9.31262970e-04 3.77178192e-04 1.04904175e-04 8.10623169e-06
0:  2.86102295e-06 1.90734863e-06 8.10623169e-06 2.25067139e-04
0:  2.02178955e-04 6.96182251e-05 1.53541565e-04 2.45571136e-04
0:  5.21659851e-04 1.94263458e-03 1.90734863e-06 9.91821289e-05
0:  2.21252441e-04 2.05993652e-04 2.32219696e-04 7.37190247e-04]
0: Prediction values (first 20):
0: [5.9835396 6.2013974 6.629773  7.165657  7.569696  7.7829223 7.97209
0:  8.08897   8.010728  7.900968  7.6046576 7.063722  6.619834  6.3442745
0:  6.447294  6.9343543 7.5951085 8.167269  6.8958516 7.0117855]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.486, max = 1.332, mean = -0.126
0:          sample (first 20): tensor([-0.0751, -0.0564, -0.0196,  0.0264,  0.0611,  0.0794,  0.0956,  0.1057,  0.0990,  0.0895,  0.0641,  0.0176,
0:         -0.0205, -0.0442, -0.0353,  0.0065,  0.0633,  0.1124, -0.0232, -0.0276])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [42.535595 42.486755 42.10803  41.550182 40.996006 40.32442  40.64119
0:  41.06111  41.70383  42.030518 41.499714 41.06848  40.58177  40.738873
0:  41.55367  42.51504  43.33919  43.48469  43.811764 44.069813]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.010452  16.762955  16.636047  16.497744  16.16336   15.559931
0:  15.188017  14.778151  14.394083  13.900492  13.115938  12.176207
0:  11.317767  10.776727  10.792931  11.27932   12.065498  12.808334
0:  11.8027315 11.886356 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3256197 6.1896505 6.2445793 6.398679  6.4860244 6.3814325 6.422262
0:  6.405319  6.3302393 6.244849  5.9614315 5.5648317 5.2599916 5.2012854
0:  5.446601  6.0106077 6.6104293 7.0458946 5.053116  5.023701 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.539038  9.704203  9.930906 10.041508 10.022159  9.838065 10.105336
0:  10.402487 10.741699 11.033425 10.843115 10.503876 10.101433 10.192425
0:  10.66852  11.486424 12.193411 12.55587  10.912589 11.196676]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.320674   -2.3890953  -2.2324495  -1.6921706  -1.0664468  -0.5592241
0:  -0.35293245 -0.3807211  -0.88515854 -1.408217   -1.9605899  -2.6314106
0:  -3.1252718  -3.4439182  -3.4441328  -3.0066519  -2.2517004  -1.3515024
0:  -1.4187737  -1.1549973 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.162149 13.276409 13.533312 13.854824 14.007    13.915639 13.936577
0:  13.767567 13.400087 12.855039 11.894739 10.678494  9.510571  8.717869
0:   8.439695  8.709325  9.299671  9.819954  8.765617  8.873072]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.3171377   1.2361398   1.2868977   1.5421343   1.7525051   1.8113768
0:   1.8351276   1.7931659   1.5093551   1.2442088   0.86366224  0.33136415
0:  -0.06645775 -0.31579018 -0.28711033 -0.01173973  0.30138302  0.5293226
0:  -1.0525775  -0.9366722 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.480303 11.807264 12.401028 13.117371 13.668982 13.911369 14.033777
0:  13.804546 13.264187 12.561125 11.630124 10.629717  9.907358  9.654307
0:   9.973293 10.774939 11.713753 12.421599 10.657596 10.872125]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.074497  4.0061927 4.38236   5.098181  5.916088  6.5724463 7.138637
0:  7.555243  7.5554714 7.571781  7.475093  7.185323  7.132474  7.1425858
0:  7.3388987 7.765858  8.276868  8.757059  5.447317  5.3220086]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.4805088  -0.5845814  -0.35538483  0.08516312  0.41463518  0.52388906
0:   0.47939634  0.41008854  0.13965034 -0.03568792 -0.24612999 -0.64253473
0:  -0.92858267 -1.0727954  -0.9356208  -0.4046936   0.3393221   1.0519047
0:  -0.6039357  -0.51855135]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.62129354 -0.46549416  0.01291466  0.814425    1.6374187   2.2373009
0:   2.5834894   2.778162    2.5223284   2.3051553   2.0005603   1.4590206
0:   1.0622778   0.68975496  0.58100796  0.8501992   1.2946315   1.7862949
0:   0.19139862  0.13182592]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1720195 3.8029346 4.6999674 5.5866346 6.075556  6.1687064 5.9835725
0:  5.901901  5.663018  5.6419983 5.6165047 5.2904873 5.011399  4.785644
0:  4.940147  5.727747  7.002855  8.3746395 7.7918386 8.5147   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1274567 6.206929  6.414016  6.652389  6.7818775 6.718015  7.028315
0:  7.27855   7.510752  7.6217175 7.326643  6.8733716 6.404508  6.427303
0:  6.854002  7.6347585 8.399499  8.857697  8.138162  8.323302 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.140675  -5.3334002 -5.2337794 -4.816995  -4.4107566 -4.18088
0:  -4.109341  -4.165297  -4.578692  -4.9762883 -5.4742713 -6.1601596
0:  -6.6466994 -7.007832  -7.087624  -6.793447  -6.378881  -5.967323
0:  -7.5496345 -7.696509 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.766216  17.041193  17.415066  17.751898  17.829088  17.644264
0:  17.46593   17.143526  16.687693  16.197878  15.526718  14.684074
0:  13.944286  13.413367  13.247259  13.4200535 13.734522  14.0380945
0:  11.922247  11.979176 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.6288524  5.625208   5.868466   6.2266254  6.4923105  6.53372
0:   6.7406588  6.8665943  6.947515   7.044803   7.0379534  6.949935
0:   7.0887804  7.5624886  8.36595    9.465726  10.4904785 11.22641
0:   8.868316   8.90244  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.340963   -3.012259   -2.2967677  -1.25031    -0.24923277  0.54399204
0:   1.1996541   1.6832042   1.8781328   2.080398    2.1298676   1.9058719
0:   1.6473379   1.3255839   1.0955977   1.0642371   1.1199989   1.2162108
0:  -0.3178091  -0.3657298 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.32963657 -0.23423862  0.05465364  0.36514568  0.5289402   0.51594496
0:   0.76944447  0.9349942   0.9219942   0.69608355 -0.06695557 -1.1795092
0:  -2.3949594  -3.1621428  -3.4658203  -3.2786646  -3.032689   -2.902275
0:  -3.534606   -3.2544336 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.464258  -6.5680237 -6.3283005 -5.760723  -5.1725683 -4.7308197
0:  -4.3952045 -4.1418695 -4.2121544 -4.2907    -4.51501   -4.9715242
0:  -5.2612123 -5.3805857 -5.216528  -4.717862  -4.190297  -3.7526274
0:  -4.581214  -4.7719517]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.706245   8.360767   9.245329  10.000376  10.32902   10.15336
0:   9.802647   9.260104   8.623509   8.173491   7.704994   7.260355
0:   6.984142   7.033693   7.5521007  8.467817   9.524161  10.39864
0:   8.625305   9.14393  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.27736330e+00 -3.14485884e+00 -2.69369841e+00 -1.97759676e+00
0:  -1.23762369e+00 -7.35608578e-01 -2.85399914e-01 -2.88915634e-03
0:  -1.07982635e-01 -2.20528126e-01 -5.74483395e-01 -1.16297865e+00
0:  -1.53021860e+00 -1.78622770e+00 -1.68790483e+00 -1.33061838e+00
0:  -1.02734661e+00 -8.18709850e-01 -3.06793070e+00 -2.95515108e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.821114  13.535517  13.418605  13.405663  13.29782   12.973471
0:  12.876116  12.682676  12.426426  12.12896   11.54602   10.841215
0:  10.189804   9.901065  10.033392  10.553684  11.28282   11.8746605
0:  10.163389  10.145372 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.792553 36.647877 36.478027 36.314552 36.242878 36.064335 36.536602
0:  36.836945 37.140312 37.182457 36.630234 36.13381  35.76882  35.882034
0:  36.499454 37.338577 38.119526 38.51652  36.52873  36.393387]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.701536 26.659578 26.615366 26.461761 26.227404 25.860435 25.938559
0:  25.950716 26.090656 26.108103 25.707106 25.282711 24.955126 24.999521
0:  25.517921 26.24151  26.923525 27.373432 25.165031 25.317047]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.348566    6.435947    5.6744833   5.3010287   5.019578    4.6705723
0:   4.148472    3.6771016   2.8837094   2.2514052   1.600214    0.6273823
0:  -0.39915133 -1.6363182  -2.8262262  -3.6244087  -4.025959   -4.0295124
0:  -6.1016817  -7.1428394 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.19440985 0.24584007 0.61351156 1.2513876  1.8270321  2.1644473
0:  2.3774757  2.4637623  2.262117   2.0929704  1.791245   1.2400742
0:  0.77961874 0.46518707 0.4403987  0.8662987  1.5099878  2.1675982
0:  1.3553653  1.3809915 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.015032   9.03759    9.147793   9.249041   9.176973   8.931055
0:   8.908181   8.953993   8.986605   9.012391   8.8619175  8.5906105
0:   8.411631   8.527766   9.042536   9.881846  10.757584  11.474826
0:   9.354022   9.5689945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.9992404 -7.9039865 -7.4953427 -6.830582  -6.184875  -5.7627854
0:  -5.452742  -5.2498975 -5.4185586 -5.5822043 -5.954451  -6.566752
0:  -7.083014  -7.4674506 -7.579374  -7.3463597 -7.0234866 -6.743709
0:  -8.174414  -8.223032 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.855507 26.21035  26.571167 26.831535 27.018303 27.055313 27.59887
0:  28.083054 28.637718 29.096867 29.149921 29.1566   29.369713 29.930042
0:  30.934242 32.174885 33.39353  34.307644 30.378078 30.615986]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.217983  12.082028  12.136921  12.293686  12.433136  12.474639
0:  12.724253  12.920728  13.014269  13.052794  12.871709  12.586311
0:  12.4429455 12.630475  13.176358  13.987322  14.813072  15.463392
0:  11.984605  11.848535 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.787944  -8.678391  -8.185662  -7.3196363 -6.457795  -5.851448
0:  -5.53319   -5.4733925 -5.955514  -6.425741  -7.0182495 -7.7933903
0:  -8.298608  -8.684359  -8.767023  -8.51184   -8.142765  -7.7255306
0:  -8.132241  -8.109952 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-20.810259 -21.333244 -21.249147 -20.659193 -20.022074 -19.631134
0:  -19.204435 -19.051981 -19.135973 -19.134487 -19.422329 -19.881557
0:  -20.167093 -20.197636 -19.758707 -19.142231 -18.57213  -18.3195
0:  -19.30972  -19.605717]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7031422  -1.5790753  -1.1302614  -0.4145789   0.20467758  0.56557226
0:   0.7272768   0.6412654   0.23955774 -0.1706171  -0.6128359  -1.1184716
0:  -1.3658104  -1.363944   -1.0311718  -0.37782145  0.33574104  0.9194288
0:  -1.2725492  -1.1308484 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.134536  -5.046603  -4.5968704 -3.965703  -3.4868193 -3.30789
0:  -3.1818914 -3.2085466 -3.4439974 -3.657854  -4.031907  -4.657242
0:  -5.157052  -5.4931    -5.528762  -5.1170473 -4.581581  -4.0561047
0:  -5.0847034 -5.0028944]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.9402122 5.0841837 5.4282513 5.9412274 6.3080044 6.4188156 6.2558875
0:  5.9471045 5.264299  4.631969  3.9440815 3.126603  2.4757442 1.9623098
0:  1.804976  2.0340047 2.490932  2.9925432 1.6137981 1.5905738]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.2462797 4.6245823 5.179445  5.701265  5.946449  5.8825555 5.7299504
0:  5.5782623 5.3601775 5.2378736 5.044361  4.6825395 4.349599  4.133135
0:  4.252606  4.7740955 5.5442367 6.28255   4.961316  5.261844 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.09579229 -0.23108578 -0.12159491  0.2882924   0.7528033   1.0714655
0:   1.1802554   1.274396    0.9625201   0.69052505  0.3116684  -0.36650944
0:  -0.9564986  -1.490416   -1.8393188  -1.8392053  -1.6874194  -1.4439192
0:  -2.179583   -2.6213212 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.334871  13.301477  13.366783  13.396424  13.443274  13.358743
0:  13.889444  14.334648  14.848719  15.191501  14.98937   14.700222
0:  14.378551  14.5713215 15.238353  16.307205  17.381899  18.163477
0:  16.919262  16.766716 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.6327817 3.8173246 4.275078  4.78632   5.1987987 5.4506173 5.7497325
0:  5.9527526 5.9967318 5.9925237 5.8085427 5.413914  5.1841407 5.2160897
0:  5.5781174 6.2836266 7.0376787 7.654665  5.365417  5.5515246]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9788013  -3.7257886  -3.1607633  -2.3645902  -1.6439834  -1.1292896
0:  -0.6378331  -0.25964546 -0.18425083 -0.12068892 -0.31399155 -0.79306984
0:  -1.1877894  -1.4250107  -1.3695264  -0.9552593  -0.4936695  -0.11147928
0:  -1.8779955  -1.8194442 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.057842  8.383991  9.021314  9.733625 10.315211 10.700789 11.076195
0:  11.369848 11.474266 11.535596 11.418701 11.117008 10.977699 11.02474
0:  11.496491 12.30012  13.209452 13.966614 12.04793  12.543959]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.777378 20.86176  21.07545  21.269905 21.314919 21.137976 21.28057
0:  21.310675 21.329287 21.199066 20.695354 20.111633 19.742386 19.738308
0:  20.293259 21.256557 22.32262  23.273571 21.080406 21.502363]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.27272558 -0.20068121  0.19457722  0.8907137   1.5086122   1.909235
0:   2.0037847   1.9986105   1.6319385   1.3423147   1.005702    0.42391443
0:  -0.04028273 -0.46608973 -0.57929325 -0.20758295  0.47505188  1.2245932
0:  -0.35345697 -0.41622448]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.712296 32.57846  32.44382  32.19362  31.907413 31.343405 31.391533
0:  31.279943 31.293116 31.152899 30.472172 29.8025   29.203691 29.047443
0:  29.407944 30.107855 30.898758 31.42207  29.980772 30.011845]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.271736  -12.341587  -11.972008  -11.084019  -10.017899   -9.037033
0:   -8.287624   -7.67063    -7.531526   -7.3887076  -7.3067393  -7.4904447
0:   -7.474355   -7.48002    -7.246512   -6.741403   -6.0973983  -5.4102693
0:   -5.4133925  -5.488419 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.3408113 7.7000823 8.114723  8.413938  8.51218   8.376679  8.236512
0:  7.945756  7.4941897 7.004891  6.3586407 5.651968  5.1740146 5.0571737
0:  5.358546  6.000249  6.7103534 7.342674  7.306742  7.776863 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.356967  11.698937  12.004679  12.334854  12.556746  12.539202
0:  12.425768  12.2068615 11.651459  11.169142  10.607475   9.958139
0:   9.547875   9.230693   9.198717   9.42639    9.82369   10.237312
0:   8.804163   9.094251 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2507057 1.2587743 1.309228  1.2574244 1.0837989 0.7974458 1.0554547
0:  1.4163742 1.9074655 2.3612669 2.3561866 2.2969582 2.0866985 2.3883522
0:  2.9973    3.7673764 4.2467256 4.2455225 4.4097037 4.5610576]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.061942  9.383764  9.830996 10.362077 10.803777 11.071712 11.230804
0:  11.348806 11.205305 11.001497 10.636047 10.095968  9.68873   9.382958
0:   9.482133  9.93659  10.645218 11.301352 10.266581 11.059393]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.037273  15.6839485 15.526565  15.488719  15.390671  15.149018
0:  15.209937  15.219125  15.191683  15.050536  14.548123  13.853027
0:  13.232971  12.936003  13.112045  13.685141  14.386883  14.932898
0:  12.599335  12.122213 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.563929  10.723824  11.063836  11.472341  11.7960205 11.959195
0:  12.351019  12.648155  12.829245  12.796232  12.400946  11.917061
0:  11.511433  11.4976845 11.921094  12.622264  13.304407  13.8255625
0:  11.525789  11.359631 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [50.05121  49.527275 48.689095 47.74516  46.816463 45.6942   45.580826
0:  45.28451  45.28297  44.7927   43.610195 42.544983 41.468178 41.138985
0:  41.50671  42.01194  42.29708  41.761044 42.417698 42.19113 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.40298  37.40436  37.0852   36.559452 35.970818 35.1938   35.175495
0:  34.988552 34.780178 34.17725  32.82013  31.605633 30.482046 30.07976
0:  30.342556 30.839378 31.241941 31.062872 30.99297  30.857615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.0259886e+00 6.8377151e+00 6.8131490e+00 6.8607693e+00 6.7883525e+00
0:  6.5396109e+00 6.1705689e+00 5.7727199e+00 5.1812825e+00 4.6625400e+00
0:  4.0953131e+00 3.3361146e+00 2.6815760e+00 2.1160574e+00 1.7891259e+00
0:  1.7434821e+00 1.8393617e+00 1.9189782e+00 1.5692711e-03 3.5707951e-02]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.938675  12.10828   12.459716  12.8345375 13.041496  13.013966
0:  12.991039  12.907086  12.639538  12.364979  11.869545  11.149037
0:  10.5171    10.022419   9.910255  10.176665  10.641942  11.035048
0:   9.111425   9.265337 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8501439 2.010025  2.4811425 3.1467586 3.7046092 4.055118  4.4558296
0:  4.7277308 4.7661705 4.761121  4.4796085 3.966584  3.5657427 3.4275434
0:  3.625471  4.194948  4.8332157 5.3357196 4.291172  4.3709745]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.351231  7.414276  7.769948  8.309844  8.863533  9.237957  9.589018
0:  9.845221  9.793672  9.750484  9.564709  9.111448  8.806583  8.49639
0:  8.375473  8.47044   8.603609  8.622421  4.635993  4.4497147]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.969833  -10.346441  -10.343048   -9.969465   -9.572481   -9.259417
0:   -8.780556   -8.480663   -8.431227   -8.659151   -9.337324  -10.29734
0:  -11.179636  -11.605307  -11.547243  -11.089102  -10.66407   -10.365623
0:   -8.514952   -8.6009655]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.203533  4.297667  4.6015825 5.08738   5.4387026 5.6117077 5.4892673
0:  5.426544  5.119867  4.963355  4.821329  4.431486  4.0830717 3.7371087
0:  3.716643  4.24809   5.297142  6.495211  5.219801  5.218296 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.47314596 -0.15679789 -0.7250767  -1.2661996  -1.9547725  -2.8076277
0:  -3.221138   -3.72373    -4.2525434  -4.969327   -6.1752043  -7.6054554
0:  -8.950322   -9.576202   -9.605      -9.143681   -8.669197   -8.575248
0:  -8.689991   -8.798147  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.137697   -8.222008   -7.9866853  -7.4953523  -7.1449857  -7.187512
0:   -7.5207095  -8.175364   -9.192728  -10.112772  -11.0463505 -11.953402
0:  -12.482422  -12.5984    -12.296545  -11.708004  -11.269039  -11.070402
0:  -11.080265  -10.959087 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.76754  14.659739 14.758947 14.850597 14.854214 14.656429 14.950443
0:  15.209761 15.456619 15.548454 14.972721 14.115612 13.043539 12.351827
0:  12.058924 12.125347 12.196421 12.145183 11.01886  10.775703]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.206615 21.001741 20.99911  21.033766 20.903597 20.505926 20.404
0:  20.170406 20.001518 19.799623 19.301796 18.59142  17.997046 17.760933
0:  18.019    18.753515 19.667093 20.47932  17.845446 17.8499  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.107914 26.164787 25.934803 25.424587 24.765957 23.941116 24.064224
0:  24.216095 24.488182 24.470814 23.609856 22.799335 21.835123 21.557808
0:  21.776913 22.24374  22.420898 22.044735 21.124191 21.129269]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.090102   9.199228   9.460533   9.8687315 10.254704  10.561885
0:  10.85058   11.133089  11.217539  11.311792  11.325794  11.138648
0:  11.042791  11.009807  11.232028  11.7214575 12.419304  13.070246
0:  11.910103  12.252219 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.060885   2.2854443  2.7715273  3.4573495  4.068717   4.3848095
0:  4.536545   4.4728956  4.0156813  3.539715   2.8797088  2.075552
0:  1.4557343  1.0410872  0.9950142  1.3117185  1.6548066  1.8849115
0:  0.4378519  0.41065502]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7501183 2.75411   2.9987264 3.3460236 3.4513876 3.2573566 3.0401695
0:  2.8648803 2.6156    2.4930637 2.2938426 1.8303118 1.4229531 1.1641927
0:  1.2177982 1.7561731 2.5417006 3.2944407 1.1290841 1.1537895]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4756413 4.696227  5.1288595 5.7157054 6.1008058 6.2527266 6.1466684
0:  6.0335026 5.6093907 5.2732806 4.8646364 4.238555  3.6951652 3.2064958
0:  3.0536215 3.3179977 3.8450797 4.438528  3.0232534 3.1472912]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4392214  4.2325544  4.3593674  4.664674   4.88282    4.8737435
0:  4.8219256  4.6696663  4.30648    3.9602873  3.448421   2.7220426
0:  2.117887   1.6956806  1.571229   1.8082404  2.1755986  2.5060773
0:  0.37229443 0.2721076 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.092968 23.161942 23.292967 23.311049 23.186293 22.83354  22.839865
0:  22.779135 22.766455 22.743778 22.336714 21.844427 21.400686 21.268538
0:  21.60119  22.26169  23.016768 23.618128 21.403648 21.445095]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.100866  -16.182543  -15.735914  -14.700134  -13.4294815 -12.32869
0:  -11.403353  -10.876417  -10.882456  -10.916754  -11.098421  -11.475718
0:  -11.520506  -11.425249  -11.004253  -10.281431   -9.484919   -8.778036
0:  -11.178764  -11.208477 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.614155 25.585804 25.677704 25.786304 26.026878 26.15712  27.075085
0:  27.837704 28.614468 29.037575 28.642773 28.170368 27.52251  27.382244
0:  27.561909 27.8633   27.988651 27.668621 24.389877 24.374088]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.474659  11.521996  11.752753  12.020306  12.198914  12.166072
0:  12.275848  12.31649   12.309222  12.282589  12.052902  11.675058
0:  11.403433  11.302729  11.544151  12.11631   12.873381  13.581448
0:  11.9580555 12.079439 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.4522066 -4.3266716 -3.902341  -3.1480732 -2.4105282 -1.8557205
0:  -1.6734061 -1.6575923 -2.1390972 -2.6651087 -3.296277  -4.1547976
0:  -4.8947325 -5.5396757 -5.8543596 -5.7225695 -5.2552786 -4.6175165
0:  -4.220769  -4.213601 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5434663  3.4111695  3.454255   3.7253208  3.9805782  4.127567
0:  4.113941   4.037019   3.637096   3.2747822  2.8427238  2.2711911
0:  1.8327088  1.4713197  1.3971162  1.6095977  1.9621396  2.353641
0:  0.9588127  0.90317965]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.64557  17.562656 17.602766 17.700077 17.64954  17.350018 17.285824
0:  17.157543 17.001822 16.791561 16.227654 15.529177 14.817968 14.389645
0:  14.356825 14.623016 14.981764 15.25893  13.645658 13.444787]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.93898535 1.3901181  2.1276197  2.9238448  3.4888577  3.742199
0:  3.8943288  3.9566147  3.8234825  3.6976376  3.4010754  2.859661
0:  2.4188633  2.1203547  2.106399   2.4367027  2.850171   3.1813927
0:  0.9564786  1.052288  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.01654   17.756481  17.633198  17.535881  17.280704  16.732693
0:  16.414192  15.900475  15.2604885 14.492575  13.349451  12.132109
0:  11.109074  10.664277  10.894482  11.723669  12.817351  13.779829
0:  14.234597  14.237082 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.166337  8.0380125 8.17189   8.408506  8.502257  8.334724  8.407251
0:  8.390229  8.3027    8.200196  7.7255683 7.0718317 6.370263  6.0568957
0:  6.1340213 6.589169  7.075886  7.3334045 4.37215   4.2037287]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6325607  -2.5119648  -2.0117283  -1.1901569  -0.39537144  0.20625162
0:   0.58156395  0.8604336   0.780777    0.69383574  0.46197462 -0.08991814
0:  -0.5884476  -1.0892868  -1.3431311  -1.1958141  -0.8328495  -0.39561224
0:  -2.6482487  -2.7093172 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.163615  12.244165  12.48439   12.637476  12.63199   12.313601
0:  12.417274  12.4282    12.56333   12.6430855 12.272794  11.832726
0:  11.403944  11.445448  11.9315605 12.862388  13.783735  14.436108
0:  12.2832985 12.466637 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.16222  24.44253  24.649025 24.71032  24.662983 24.393589 24.706865
0:  24.96422  25.365885 25.671751 25.50403  25.229988 24.951021 25.081057
0:  25.646708 26.50274  27.361385 27.89312  26.323826 26.588583]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.7487426 4.6834373 4.8269916 5.1051073 5.3641605 5.5975676 6.02866
0:  6.5108805 6.8877263 7.186744  7.160045  6.779685  6.34421   6.0321016
0:  5.980541  6.3045034 6.783918  7.1961646 5.6870317 5.677369 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.120115 14.444061 14.896113 15.290678 15.563572 15.641058 16.10397
0:  16.471079 16.87504  17.163364 17.023014 16.734522 16.426167 16.475788
0:  16.877985 17.601345 18.377312 18.939442 16.527515 16.575432]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.760944 22.14088  22.474663 22.571918 22.528904 22.376228 22.64118
0:  22.881369 23.222536 23.404371 23.212572 22.907518 22.745518 22.900337
0:  23.524101 24.401585 25.37024  26.193514 24.610998 25.052969]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.62594  40.277634 39.632107 38.95745  38.397087 37.67572  37.783897
0:  37.844276 38.071575 37.977467 37.144997 36.44978  35.664616 35.36447
0:  35.50341  35.735355 35.83994  35.43552  34.344963 34.16799 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.2183075 14.134634  14.216965  14.373041  14.48818   14.462313
0:  14.810961  15.03298   15.248104  15.342293  15.028555  14.531664
0:  14.061395  13.960151  14.273005  14.974797  15.757875  16.324871
0:  14.271399  14.159807 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.34570312  0.38486862  0.7768383   1.4518561   2.1319418   2.6082234
0:   3.0263543   3.2211747   3.0507793   2.8670373   2.4768233   1.9141421
0:   1.5375834   1.298132    1.3106894   1.6015863   1.8612266   2.0596337
0:  -0.17193127 -0.13352013]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.530544 13.226158 12.853245 12.359957 11.911517 11.379175 11.685735
0:  11.985666 12.457216 12.697943 12.245674 11.848636 11.319723 11.429127
0:  12.060531 12.978645 13.765144 14.051662 15.554653 15.365894]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.216955   8.510548   8.734457   8.682106   8.441088   8.086993
0:   8.288378   8.5489645  8.919259   9.167334   8.936995   8.614746
0:   8.228471   8.44445    9.113162  10.158912  11.125906  11.730842
0:  11.807177  12.293852 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7917833 3.1785867 3.8416946 4.5441246 4.9706407 5.115021  5.1107264
0:  5.18346   5.0898294 5.098798  5.0371532 4.6784844 4.3991685 4.1875935
0:  4.303028  4.8801465 5.7243605 6.5748596 4.9963474 5.3093534]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4361     -1.4577227  -1.227315   -0.72502327 -0.25090456  0.06660032
0:   0.00461292 -0.08828545 -0.5386715  -0.91649437 -1.3055553  -1.9685421
0:  -2.5457125  -3.1013403  -3.3307285  -3.1189318  -2.5242872  -1.7303848
0:  -2.6316032  -2.7792406 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.891232 26.541416 26.120525 25.663418 25.250946 24.581585 24.5997
0:  24.443665 24.395271 24.093338 23.208603 22.415157 21.606556 21.337734
0:  21.59293  22.173485 22.755627 22.988625 23.354969 22.956825]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.5086584 -4.4927034 -4.129551  -3.4836664 -2.8850555 -2.511529
0:  -2.3716974 -2.3421154 -2.6359262 -2.879859  -3.1637268 -3.68437
0:  -4.039342  -4.3212194 -4.313389  -3.857933  -3.1637592 -2.4290552
0:  -3.4393206 -3.3147955]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.358122  12.51188   12.797186  13.071812  13.146105  13.028442
0:  12.9274645 12.815512  12.591635  12.36775   11.968645  11.375132
0:  10.839848  10.440188  10.400688  10.675395  11.11578   11.458057
0:   9.619263   9.848139 ]
0: validation loss for strategy=forecast at epoch 23 : 0.2678859829902649
0: validation loss for velocity_u : 0.1533699482679367
0: validation loss for velocity_v : 0.27195730805397034
0: validation loss for specific_humidity : 0.14633984863758087
0: validation loss for velocity_z : 0.462721586227417
0: validation loss for temperature : 0.10907597094774246
0: validation loss for total_precip : 0.46385106444358826
0: 24 : 19:34:00 :: batch_size = 96, lr = 1.1617293379823076e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 24, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7489, 0.7316, 0.7092, 0.6806, 0.6445, 0.6029, 0.5581, 0.5126, 0.4703, 0.4326, 0.3996, 0.3701, 0.3424, 0.3176,
0:         0.2984, 0.2864, 0.2830, 0.2853, 0.7090, 0.6885, 0.6622, 0.6303, 0.5932, 0.5536, 0.5146], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2200, 0.2184, 0.2117, 0.2052, 0.2029, 0.2092, 0.2253, 0.2488, 0.2763, 0.3022, 0.3238, 0.3418, 0.3550, 0.3630,
0:         0.3669, 0.3685, 0.3712, 0.3799, 0.1603, 0.1513, 0.1430, 0.1421, 0.1519, 0.1736, 0.2031], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1181, -0.0543, -0.2327, -0.3621, -0.4288, -0.4700, -0.4866, -0.4945, -0.5075, -0.5225, -0.5344, -0.5399,
0:         -0.5383, -0.5281, -0.5080, -0.4814, -0.4592, -0.4414, -0.0695, -0.2307, -0.3563, -0.4176, -0.4451, -0.4595,
0:         -0.4730], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0618,  0.0024,  0.1087,  0.2626,  0.4464,  0.5849,  0.6613,  0.6336,  0.5018,  0.3678,  0.2460,  0.1386,
0:          0.0113, -0.1460, -0.2612, -0.3686, -0.4527, -0.4483,  0.2272,  0.3257,  0.4343,  0.5361,  0.6458,  0.6635,
0:          0.5926], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0820, -0.0146,  0.0492,  0.1032,  0.1452,  0.1781,  0.2093,  0.2462,  0.2951,  0.3589,  0.4360,  0.5180,
0:          0.5903,  0.6403,  0.6627,  0.6617,  0.6446,  0.6262,  0.6166,  0.6188,  0.6294,  0.6385,  0.6362,  0.6164,
0:          0.5710], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2424,
0:         -0.1867, -0.2223, -0.2412, -0.2401, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.2436, -0.1594, -0.1819,
0:         -0.2389], device='cuda:0')
0: [DEBUG] Epoch 24, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2436,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  1.1136,     nan,     nan,     nan,     nan,     nan,  0.7316,
0:             nan,  0.7138,     nan,     nan,  0.1740,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2009,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1819,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.6036,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  1.2192,  1.7471,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.8455,     nan,     nan, -0.0906,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1179,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  2.1030,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2365, -0.2341])
0: [DEBUG] Epoch 24, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.3677, 1.3809, 1.4031, 1.4263, 1.4354, 1.4328, 1.4419, 1.4615, 1.4739, 1.4919, 1.4909, 1.4697, 1.4579, 1.4553,
0:         1.4864, 1.5399, 1.6119, 1.6688, 1.4878, 1.4988, 1.5042, 1.4902, 1.4421, 1.3966, 1.3551], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0725,  0.0016,  0.0435,  0.0617,  0.0543,  0.0483,  0.0405,  0.0590,  0.0963,  0.1437,  0.2044,  0.2752,
0:          0.3153,  0.3297,  0.3251,  0.3317,  0.3712,  0.4422, -0.1392, -0.0527,  0.0037,  0.0187,  0.0121, -0.0064,
0:         -0.0026], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7384, 0.7635, 0.7887, 0.8117, 0.8311, 0.8310, 0.8204, 0.7836, 0.7440, 0.6930, 0.6472, 0.6242, 0.6167, 0.6471,
0:         0.6813, 0.7429, 0.8053, 0.8720, 0.6887, 0.7151, 0.7370, 0.7645, 0.7862, 0.7893, 0.7858], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2667, -0.1291,  0.0437, -0.0391, -0.2110, -0.1002, -0.0580, -0.1198, -0.1231, -0.1950, -0.3080, -0.3687,
0:         -0.1809,  0.0017, -0.1404, -0.1525,  0.0455,  0.1276, -0.1706, -0.1524, -0.0682, -0.1548, -0.2812, -0.1191,
0:         -0.0638], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0043,  0.0089,  0.0320,  0.0634,  0.0901,  0.1013,  0.1032,  0.1057,  0.1243,  0.1545,  0.1778,  0.1821,
0:          0.1557,  0.1022,  0.0297, -0.0548, -0.1439, -0.2248, -0.2821, -0.3140, -0.3274, -0.3336, -0.3371, -0.3370,
0:         -0.3244], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0184,  0.0387,  0.0747,  0.0595,  0.0579,  0.0755,  0.1036,  0.1345,  0.1517, -0.0030,  0.0073,  0.0192,
0:          0.0450,  0.0593,  0.0868,  0.1245,  0.1580,  0.2247, -0.0149,  0.0037,  0.0226,  0.0495,  0.0806,  0.1274,
0:          0.1684], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.176458939909935; velocity_v: 0.26852256059646606; specific_humidity: 0.18508204817771912; velocity_z: 0.5779106616973877; temperature: 0.15202540159225464; total_precip: 0.5419993996620178; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21935896575450897; velocity_v: 0.313975989818573; specific_humidity: 0.1861046850681305; velocity_z: 0.5745470523834229; temperature: 0.1822911947965622; total_precip: 0.6536855697631836; 
0: epoch: 24 [1/5 (20%)]	Loss: 0.59784 : 0.30132 :: 0.21460 (2.77 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20543934404850006; velocity_v: 0.3178078234195709; specific_humidity: 0.17403171956539154; velocity_z: 0.5776745676994324; temperature: 0.15211394429206848; total_precip: 0.6452186107635498; 
0: epoch: 24 [2/5 (40%)]	Loss: 0.64522 : 0.31054 :: 0.21154 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19619062542915344; velocity_v: 0.3002394437789917; specific_humidity: 0.15467554330825806; velocity_z: 0.67784583568573; temperature: 0.15837082266807556; total_precip: 0.511327862739563; 
0: epoch: 24 [3/5 (60%)]	Loss: 0.51133 : 0.29949 :: 0.20547 (16.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1944146305322647; velocity_v: 0.25077545642852783; specific_humidity: 0.224977508187294; velocity_z: 0.6334758996963501; temperature: 0.16701525449752808; total_precip: 0.8783507943153381; 
0: epoch: 24 [4/5 (80%)]	Loss: 0.87835 : 0.35643 :: 0.20535 (16.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  1.1444092e-05 7.2956085e-05 5.7220459e-05 8.1062317e-05 6.3896179e-05
0:  3.0040741e-05 2.8610229e-06 3.4809113e-05 5.2928925e-05 2.1457672e-05
0:  1.4305115e-06 3.3378601e-06 3.8146973e-06 2.3841858e-06 1.9073486e-06
0:  1.4305115e-06 1.4305115e-06 8.1062317e-06 2.2888184e-05 7.7247620e-05
0:  3.3378601e-05 6.0558319e-05 3.8146973e-05 1.9073486e-06 4.7683716e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 1.9073486e-06 1.3208389e-04 5.0544739e-05
0:  6.1988831e-06 0.0000000e+00 9.5367432e-07 5.2452087e-06 9.5367432e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 9.5367432e-07 4.2915344e-06
0:  2.3841858e-06 2.3841858e-06 2.8610229e-06 3.8146973e-06 3.3378601e-06
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 9.5367432e-07 3.8146973e-06 1.4781952e-05 3.2901764e-05
0:  4.6253204e-05 4.1961670e-05 3.0040741e-05 3.1948090e-05 3.6239624e-05
0:  2.3365021e-05 2.3841858e-06 1.9073486e-06 9.5367432e-07 4.7683716e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9550323e-05 1.2207031e-04
0:  1.5592575e-04 7.2479248e-05 2.1934509e-05 2.4795532e-05 3.6716461e-05
0:  4.7683716e-07 9.5367432e-07 4.7683716e-06 2.3365021e-05 3.7670135e-05
0:  2.6226044e-05 1.9073486e-06 8.1062317e-06 4.7683716e-06 8.5830688e-06
0:  1.0967255e-05 3.3378601e-06 8.1062317e-06 3.1948090e-05 4.3869019e-05
0:  2.4795532e-05 2.3841858e-06 1.4305115e-06 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07 1.4305115e-06
0:  0.0000000e+00 1.4305115e-06 9.5367432e-07 0.0000000e+00 0.0000000e+00
0:  9.5367432e-07 5.2452087e-06 9.5367432e-07 0.0000000e+00 0.0000000e+00
0:  4.7683716e-07 1.4305115e-06 4.7683716e-06 3.3378601e-06 7.6293945e-06
0:  7.6293945e-06 5.2452087e-06 3.8146973e-06 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.1062317e-06
0:  2.9563904e-05 4.7206879e-05 4.5776367e-05 3.3378601e-05 4.6730042e-05]
0: Target values (first 200):
0: [6.19888306e-06 4.76837158e-07 9.53674316e-07 4.76837158e-07
0:  9.53674316e-07 3.81469727e-06 1.90734863e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.33786011e-06
0:  7.72476196e-05 8.86917114e-05 6.19888306e-05 5.96046448e-05
0:  7.48634338e-05 2.62260437e-05 4.76837158e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.86102295e-06
0:  1.90734863e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 9.53674316e-07
0:  8.58306885e-06 3.24249268e-05 3.33786011e-05 6.19888306e-06
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 1.81198120e-05 1.28746033e-05
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 1.90734863e-05
0:  6.19888306e-06 3.91006470e-05 9.20295715e-05 1.03473663e-04
0:  1.31130219e-04 4.76837158e-05 1.52587891e-05 8.58306885e-06
0:  1.09672546e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 4.76837158e-06 4.76837158e-06 2.38418579e-06
0:  2.38418579e-06 2.38418579e-06 2.38418579e-06 2.05039978e-05
0:  3.67164612e-05 4.48226929e-05 5.62667847e-05 6.34193420e-05
0:  5.43594360e-05 7.34329224e-05 7.53402710e-05 5.10215759e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.38418579e-06
0:  2.90870667e-05 3.71932983e-05 5.57899475e-05 1.03950500e-04
0:  9.10758972e-05 6.67572021e-05 2.00271606e-05 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.14440918e-05
0:  4.76837158e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  9.53674316e-07 2.86102295e-06 0.00000000e+00 0.00000000e+00
0:  1.38282776e-05 1.04904175e-05 1.90734863e-06 1.90734863e-06
0:  2.38418579e-06 9.53674316e-07 9.53674316e-07 4.76837158e-07
0:  4.76837158e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [-0.9478588  -0.85859585 -0.42786503  0.3816595   1.2617226   1.9710698
0:   2.3752465   2.65207     2.4313703   2.2161946   1.8788605   1.3150935
0:   0.87762547  0.45222473  0.31206512  0.46380186  0.80837774  1.1751585
0:   0.33721685  0.37186098]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.682, max = 0.933, mean = -0.439
0:          sample (first 20): tensor([-0.6589, -0.6516, -0.6167, -0.5509, -0.4795, -0.4219, -0.3890, -0.3666, -0.3845, -0.4020, -0.4293, -0.4751,
0:         -0.5106, -0.5452, -0.5566, -0.5443, -0.5163, -0.4865, -0.6734, -0.7208])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.89137125 0.9263692  1.119935   1.3880587  1.5857282  1.6364489
0:  2.010382   2.3476455  2.6054761  2.7846467  2.5601768  2.2081099
0:  1.8198009  1.8848405  2.3310733  3.012786   3.5613725  3.7177935
0:  2.4309309  2.5311925 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.7454934  -6.766908   -6.6366687  -6.3530316  -6.055799   -5.8974175
0:  -5.117433   -4.453937   -3.6965137  -3.1496205  -3.1888309  -3.3493361
0:  -3.657143   -3.3488965  -2.5621285  -1.3888307  -0.21549511  0.575109
0:  -0.24433231  0.05792522]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.4176574  6.210697   6.0784073  5.9925365  5.9678164  5.922008
0:   6.420411   6.8786845  7.387682   7.6810617  7.4501114  7.1418204
0:   6.8309565  7.1025553  7.980957   9.386683  10.988992  12.496772
0:  15.047222  15.794666 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2599268  -1.0797954  -0.74331    -0.3517456   0.0514431   0.37233973
0:   1.295898    2.2002933   3.1163743   3.8495252   4.0547934   4.056305
0:   4.075024    4.563228    5.5231886   6.8193703   8.08838     8.989421
0:   9.168453    9.408349  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.0342293  7.065626   7.2473392  7.4799395  7.586241   7.5016184
0:   7.7549276  7.9402294  8.166478   8.331549   8.165281   7.8798714
0:   7.6333513  7.8373857  8.453628   9.46096   10.500336  11.299579
0:  11.08607   11.161803 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0653276  -1.050148   -0.63253784  0.0449543   0.59925413  0.8561888
0:   0.9577303   0.91266537  0.58949566  0.29140806 -0.11891508 -0.73229694
0:  -1.1322203  -1.2800407  -1.0903397  -0.46133804  0.24735832  0.8132472
0:  -1.8365011  -2.0706844 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.095638   -9.04775    -8.740644   -8.212397   -7.8483567  -7.763901
0:   -7.8457007  -8.116747   -8.75716    -9.335747  -10.087397  -10.94389
0:  -11.595827  -11.884466  -11.735968  -11.20282   -10.677574  -10.181065
0:  -10.300632  -10.156916 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.5101342   0.3514123   0.47596884  0.73943233  0.929029    0.83997107
0:   0.7855067   0.573411    0.09595251 -0.5215478  -1.4968572  -2.6181226
0:  -3.6225934  -4.1319547  -4.1602006  -3.7311635  -3.2487102  -2.9865232
0:  -6.7383122  -6.6518264 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5923567 3.873871  4.4025583 5.105252  5.6631794 5.911005  5.951666
0:  5.7259474 5.114584  4.564682  3.9202263 3.2343392 2.7222104 2.4257836
0:  2.4775705 2.8838205 3.4302824 3.9515455 3.4661927 3.7728343]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.534887 24.479353 24.48188  24.49343  24.444736 24.158468 24.353682
0:  24.388224 24.481785 24.400509 23.825762 23.126987 22.441439 22.071434
0:  22.141603 22.592834 23.263645 23.798634 22.741673 22.708475]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.254792  4.559556  5.1256943 5.7761097 6.307954  6.5507193 6.7337503
0:  6.7662735 6.5070477 6.175984  5.6290274 4.825197  4.1508446 3.597597
0:  3.3795507 3.4826486 3.778946  4.093167  1.8696136 2.0551996]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9439979  -1.8002124  -1.3439498  -0.6231189   0.05596828  0.54004955
0:   0.8545003   1.0738945   0.9342489   0.84562254  0.6533127   0.23762989
0:  -0.02210999 -0.22872353 -0.14302874  0.31164885  0.952909    1.5992851
0:   0.9294796   1.0732021 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.4494734 -7.1617937 -6.5099735 -5.656378  -4.9227247 -4.425723
0:  -4.0944824 -3.958806  -4.124798  -4.3268223 -4.6784897 -5.247612
0:  -5.682213  -5.8899145 -5.818595  -5.38694   -4.857545  -4.3318195
0:  -5.9164934 -5.5316324]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.392063  19.753849  18.939943  17.961405  16.94205   15.813934
0:  15.539608  15.244832  14.997901  14.489608  13.263071  12.09998
0:  10.846538  10.232317  10.017006   9.987429   9.7231045  9.062742
0:   7.1212087  6.2350006]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8192134  -1.5343208  -0.9308219  -0.06801748  0.7713938   1.3700528
0:   1.7063694   1.7666388   1.348638    0.8521514   0.21099138 -0.62023926
0:  -1.2423825  -1.6851773  -1.761085   -1.4388604  -0.9284706  -0.3951397
0:  -1.0769229  -1.017045  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.0196567 0.8772874 1.000565  1.4013319 1.7905774 2.010769  2.0935938
0:  2.1486878 1.960674  1.8512137 1.6682708 1.3071194 1.0360041 0.8680854
0:  0.9718704 1.4557719 2.1312504 2.7761838 2.6570044 2.585639 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.289187   -1.098639   -0.60690355  0.07556772  0.6955414   1.1442218
0:   1.559989    1.847065    1.8138032   1.7697325   1.5371552   1.0829978
0:   0.84363747  0.751575    0.9804964   1.5697737   2.2287717   2.8872972
0:   2.052896    2.3834686 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.42661858  0.32101202  0.51222706  1.0569296   1.6415443   2.1163678
0:   2.280205    2.4177692   2.1242998   1.8927307   1.5893269   0.9704404
0:   0.39074278 -0.23946524 -0.6285243  -0.56958675 -0.14673567  0.43986702
0:  -1.035203   -1.3951073 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.176365 28.157917 27.95163  27.450447 26.879776 26.184315 26.57763
0:  27.046032 27.758118 28.272652 27.956753 27.681423 27.267803 27.595684
0:  28.472618 29.583912 30.340881 30.293468 27.86795  27.94746 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.042244  8.118924  8.461962  8.920297  9.211454  9.255162  9.229099
0:  9.127798  8.9017    8.756743  8.526808  8.070371  7.681021  7.3584776
0:  7.28052   7.5368547 7.9626875 8.38216   6.639527  6.4980683]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.194375  3.339054  3.6326265 4.0417624 4.2322693 4.190483  3.9283752
0:  3.7760189 3.42045   3.3031914 3.2519743 2.998067  2.846609  2.754405
0:  2.9535103 3.6443565 4.734858  5.9196434 5.2853665 5.2105894]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.9336224 4.987775  5.351517  5.8331804 6.1802254 6.321121  6.5473676
0:  6.649393  6.64095   6.640814  6.4746366 6.1918755 6.0638394 6.2144885
0:  6.7212434 7.5258245 8.348742  8.995284  7.453698  7.5209565]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.50177  12.361676 12.286444 12.169173 11.922531 11.482916 11.433707
0:  11.348379 11.298523 11.201866 10.709532 10.111628  9.463184  9.259893
0:   9.466707 10.070983 10.773983 11.289576 10.01069  10.00108 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.324785  -8.668575  -8.64433   -8.212139  -7.6889105 -7.2556524
0:  -7.1350937 -7.057678  -7.3769608 -7.6374454 -7.9180546 -8.470313
0:  -8.877276  -9.2618065 -9.348616  -8.946955  -8.176252  -7.241991
0:  -6.8444996 -7.186224 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.912577  9.007265  9.177907  9.323452  9.313111  9.187478  9.15834
0:   9.227461  9.242789  9.302508  9.228875  8.955993  8.739993  8.653692
0:   8.860403  9.342838  9.951279 10.437045  9.312574  9.669443]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.434161  -5.5350895 -5.2198696 -4.542761  -3.8219862 -3.2428665
0:  -2.720521  -2.3517637 -2.3146677 -2.3256369 -2.5145626 -2.9121532
0:  -3.0993357 -3.1110497 -2.8040981 -2.2193213 -1.6202097 -1.1523266
0:  -3.1473713 -3.1609678]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.473766 38.338852 39.38061  40.43641  41.393982 42.16039  43.24923
0:  43.99009  44.385902 44.166206 43.140682 41.80784  40.64902  39.828293
0:  39.57645  39.59188  39.608875 39.55673  37.95935  38.37071 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5392909  -0.10472059  0.70434284  1.8165846   2.8334198   3.6323059
0:   4.226473    4.6076784   4.5793343   4.4600153   4.1316767   3.5376024
0:   3.012431    2.5676484   2.3364863   2.3843064   2.5428457   2.6513944
0:   0.11023617 -0.20394945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.405392  11.360243  11.523653  11.773481  11.91033   11.8994465
0:  11.940831  11.929462  11.733101  11.521943  11.046635  10.36256
0:   9.703835   9.282817   9.30287    9.744446  10.390745  10.934167
0:   8.399789   8.183118 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.292773 10.655575 11.156277 11.586775 11.887711 12.01495  12.272266
0:  12.479786 12.584891 12.674344 12.566233 12.355104 12.280703 12.401489
0:  12.833066 13.413335 13.94585  14.244067 11.969168 12.396414]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2446675 2.1947184 2.4484632 2.832617  3.144479  3.2754579 3.481412
0:  3.6044846 3.6232173 3.6369524 3.4813828 3.1328857 2.9407597 2.9490457
0:  3.2662308 3.9267118 4.6192875 5.1121426 3.995111  3.9658134]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [48.04306  49.331055 50.220325 50.749584 50.690193 50.18797  50.516716
0:  50.31538  50.22567  49.60925  48.506355 47.14984  45.867783 45.4708
0:  45.725586 46.344006 46.765293 46.37143  43.47432  43.771954]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.425426  11.335991  11.357355  11.447514  11.547237  11.47188
0:  11.836277  12.061306  12.381328  12.5544815 12.361208  12.13623
0:  11.94567   12.167098  12.771286  13.698696  14.668894  15.407331
0:  13.872416  13.76441  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.990055    3.9619455   4.2275553   4.7605877   5.134181    5.1702843
0:   5.409071    5.2484922   4.7981744   3.8388085   2.1152654  -0.02946997
0:  -2.2607608  -3.9225821  -4.941728   -5.2457547  -5.1761136  -4.929395
0:  -7.0961127  -7.8269324 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.3129425 11.600175  12.220518  13.10553   14.018206  14.878053
0:  15.7255745 16.503462  17.002811  17.414923  17.54056   17.273548
0:  16.955103  16.565123  16.327217  16.22425   16.086004  15.77681
0:  11.998058  11.634002 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.048971  -9.187735  -8.994265  -8.391493  -7.612655  -6.89204
0:  -6.168657  -5.517388  -5.2726912 -4.9742455 -4.805468  -4.8436007
0:  -4.5985837 -4.2941175 -3.7393956 -2.9949002 -2.288031  -1.7157216
0:  -3.8021684 -3.785274 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4159284 -2.484078  -2.3156219 -1.9018788 -1.5251255 -1.332571
0:  -1.3528967 -1.492013  -2.0302815 -2.56877   -3.238421  -4.1054215
0:  -4.798633  -5.338982  -5.5370193 -5.328737  -4.956142  -4.5511975
0:  -5.616492  -5.730189 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.106907 10.363343 10.715398 11.170862 11.630226 12.045038 12.538733
0:  13.07732  13.344124 13.624479 13.74308  13.788679 13.935333 14.234631
0:  14.831849 15.529848 16.272615 16.850739 15.11982  15.321934]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.540866  10.647704  10.961548  11.318764  11.574052  11.666006
0:  11.84662   11.968004  11.980821  11.950396  11.671173  11.137077
0:  10.639933  10.264998  10.21853   10.555275  11.094114  11.59097
0:   9.672958   9.8421755]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.70837593 0.7845211  1.2480459  2.170486   3.1804576  4.109413
0:  4.6915846  5.227652   5.313992   5.431693   5.435649   5.0172877
0:  4.5337296  3.927159   3.58818    3.815163   4.6125064  5.694828
0:  3.9094136  3.7002654 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.0966733  3.4138823  3.7072415  3.958189   4.168109   4.370468
0:   4.970536   5.7702417  6.5421686  7.3478107  7.8804626  8.260156
0:   8.677026   9.332374  10.224634  11.2461    12.239517  12.873053
0:  10.975296  11.537167 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5863414 -5.74081   -5.514459  -4.925221  -4.358849  -4.0337358
0:  -4.013924  -4.2116942 -4.8221965 -5.418431  -6.0697637 -6.9173207
0:  -7.5853343 -8.145681  -8.401188  -8.274801  -7.943011  -7.4831176
0:  -9.352716  -9.558941 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.994062 23.711857 23.445946 23.175745 22.858553 22.263817 22.239527
0:  22.029526 21.880907 21.513668 20.552635 19.534332 18.395876 17.7651
0:  17.69519  18.036798 18.577532 18.890366 17.631937 17.539593]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.0134296 1.0634027 1.2968388 1.567131  1.7009826 1.6165948 1.8357787
0:  1.9529667 2.1770597 2.3271184 2.1601791 1.8726234 1.60497   1.6938324
0:  2.123539  2.901803  3.6305609 4.1570897 1.8889642 1.9856687]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.785643  12.041207  12.53843   13.021265  13.235114  13.2073145
0:  13.132982  13.031837  12.838302  12.666998  12.356268  11.875326
0:  11.541011  11.435096  11.833832  12.603794  13.529507  14.27423
0:  11.181187  11.471998 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.9474211   1.9352283   2.2284484   2.794557    3.2561936   3.4779193
0:   3.3130052   3.0992293   2.5246067   2.0309215   1.5024586   0.62298584
0:  -0.2654624  -1.2050762  -1.8885036  -1.9807658  -1.5215087  -0.7667918
0:  -3.218924   -3.7856412 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5586324  -0.7047429  -0.6395254  -0.24211597  0.14605188  0.39602423
0:   0.31845236  0.21454763 -0.2549472  -0.6146288  -0.9763627  -1.5958848
0:  -2.1580882  -2.7286315  -3.035459   -2.8061452  -2.116138   -1.1428027
0:  -1.9146924  -2.1938605 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.33538  23.406387 23.41232  23.24002  23.061115 22.69971  23.107643
0:  23.422318 23.91212  24.212978 23.960724 23.717735 23.50296  23.86834
0:  24.752743 25.964775 27.158031 27.857037 27.353508 27.616482]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.3961585   2.4002204   2.6683993   3.098068    3.3792012   3.4038167
0:   3.2507164   3.0281272   2.5242176   2.0768602   1.5503159   0.82326126
0:   0.20769978 -0.29452467 -0.5170808  -0.35779572  0.00344753  0.38892794
0:  -1.201499   -1.3324442 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.49731   7.500879  7.7239814 8.029821  8.172479  8.061964  8.008472
0:  7.919401  7.766385  7.6389627 7.406626  6.983101  6.680081  6.5857077
0:  6.800556  7.3829265 8.050356  8.621814  6.661625  6.6611857]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.087914 24.363426 24.68854  24.940798 25.15855  25.16134  25.65544
0:  26.079962 26.462067 26.707298 26.452883 26.10804  25.870962 26.015095
0:  26.61107  27.48538  28.325945 28.87507  27.982185 28.030287]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.633947  9.769148 10.114178 10.560024 10.861319 10.922348 10.907236
0:  10.822551 10.482924 10.178565  9.744005  9.096689  8.565574  8.16624
0:   8.139911  8.57112   9.266196  9.966028  8.040928  8.005618]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.07561   -9.209867  -9.191922  -9.108912  -9.044803  -9.035779
0:  -8.468452  -7.865431  -7.3066683 -6.96006   -7.243067  -7.8741703
0:  -8.559423  -8.660262  -8.227346  -7.492919  -7.024068  -7.0445967
0:  -9.460589  -9.485156 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.356374  9.560755  9.996132 10.520121 10.840756 10.893275 10.867458
0:  10.776877 10.453609 10.204147  9.77778   9.141743  8.584166  8.169572
0:   8.159856  8.552895  9.097408  9.601173  7.982711  8.095753]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.15927    7.0259266  7.035901   7.127421   7.16904    7.143727
0:   7.5136757  7.852599   8.2390995  8.555356   8.560891   8.444014
0:   8.34162    8.607683   9.242545  10.14309   11.0287285 11.655633
0:   9.863187   9.990614 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.474808 11.533716 11.715645 12.064707 12.271828 12.28713  12.044352
0:  11.814316 11.310433 10.9216   10.509127  9.842897  9.175199  8.508796
0:   8.138118  8.269259  8.832612  9.56135   8.075293  7.962498]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.876457  11.816787  12.035483  12.438521  12.7953615 12.926762
0:  13.304577  13.551712  13.7267475 13.841614  13.630696  13.248589
0:  12.87072   12.85272   13.25791   14.050276  14.965515  15.723612
0:  13.039402  13.283001 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.4686055  -5.4081492  -5.1338477  -4.6686425  -4.237784   -3.9584613
0:  -3.491528   -3.100782   -2.8627267  -2.6842914  -2.765861   -3.0739932
0:  -3.1784978  -3.0016427  -2.3824353  -1.49192    -0.5885987   0.04742575
0:  -0.6497531  -0.41046476]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0284026 3.129093  3.5207767 4.270403  4.9925737 5.5490856 5.857126
0:  6.0925307 5.8942003 5.656251  5.2492676 4.5203915 3.8368227 3.2022634
0:  2.8763669 3.019825  3.5086203 4.08123   2.406667  2.0576649]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.289108 19.345413 19.429966 19.453632 19.36874  19.025494 18.935925
0:  18.67237  18.337904 17.888111 17.072247 16.255276 15.522408 15.184297
0:  15.317305 15.792232 16.453253 16.976585 15.74434  15.736437]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.9457464  -2.9123983  -2.5503693  -1.8343201  -1.0681276  -0.46142626
0:  -0.23865747 -0.15739346 -0.5590472  -0.91986656 -1.3261662  -1.9705772
0:  -2.496478   -3.0154123  -3.301033   -3.1385984  -2.6147223  -1.9004564
0:  -3.4437819  -3.7488031 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.818837  -4.585532  -4.047742  -3.3110857 -2.684041  -2.2848425
0:  -1.9526215 -1.7057848 -1.7352667 -1.7865686 -2.030179  -2.561119
0:  -2.990532  -3.300912  -3.2933578 -2.8628654 -2.2465658 -1.5850782
0:  -2.679081  -2.4315672]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.316043  -11.8579645 -10.985694   -9.8579     -8.827066   -8.102389
0:   -7.5476685  -7.22884    -7.148263   -7.0301943  -6.9115705  -6.8891563
0:   -6.6432905  -6.15368    -5.409793   -4.3604794  -3.3416471  -2.500742
0:   -2.8224497  -2.2272243]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3911228  -1.6062412  -1.6428227  -1.5773687  -1.6426668  -1.840311
0:  -1.8763742  -1.8759379  -1.9485397  -2.093052   -2.531002   -3.1810036
0:  -3.805811   -4.0684967  -3.8794594  -3.2553425  -2.46352    -1.8007836
0:  -1.100625   -0.86670065]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.792197 16.28497  17.042088 17.903065 18.651299 19.169704 19.642353
0:  20.001978 20.1529   20.33629  20.367535 20.23362  20.210926 20.300701
0:  20.659924 21.20681  21.720728 21.981632 19.736423 19.933529]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6591234 -2.5397396 -2.2566023 -1.8654394 -1.657898  -1.6959524
0:  -1.9524508 -2.1792154 -2.6483784 -2.9607115 -3.2548246 -3.7180853
0:  -3.9750738 -4.0859375 -3.8529158 -3.1332836 -2.166893  -1.2129116
0:  -1.9061546 -2.051365 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.733784  12.646727  12.917599  13.421947  13.871876  14.151089
0:  14.538588  14.87781   15.109913  15.413454  15.5391445 15.444954
0:  15.425674  15.508217  15.909409  16.647762  17.499157  18.173733
0:  15.837944  15.538321 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3674655 -2.3084612 -2.0211868 -1.6231875 -1.3602719 -1.2905612
0:  -1.251843  -1.2065811 -1.3779554 -1.5653648 -1.9263129 -2.548101
0:  -3.0479026 -3.373365  -3.3229613 -2.8224845 -2.0905037 -1.3981819
0:  -3.1483998 -3.0936933]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.840488 16.127209 16.631456 17.099833 17.361416 17.469343 17.820328
0:  18.192709 18.596708 18.959253 19.049702 18.998472 19.032846 19.257517
0:  19.92089  20.843018 21.817526 22.605669 19.601778 19.88608 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.745764 10.683535 10.787384 10.93488  10.911844 10.673752 10.670275
0:  10.712568 10.705854 10.679919 10.404284  9.960415  9.600906  9.576131
0:   9.909528 10.544617 11.232639 11.688379  8.732416  8.556065]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.471018   -8.6509285  -8.40728    -7.703322   -6.9319777  -6.309504
0:   -6.2202992  -6.3415627  -7.027555   -7.6532435  -8.281378   -9.136482
0:   -9.772871  -10.299856  -10.431435   -9.952175   -9.004553   -7.8575606
0:   -8.285572   -8.591911 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.968395 20.201424 20.603436 21.035587 21.366909 21.465683 21.860012
0:  22.184462 22.53824  22.838047 22.817238 22.661774 22.520573 22.546497
0:  22.877438 23.442204 24.10167  24.579018 22.212683 22.363577]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.12863302 -0.18975878 -0.02523613  0.37410307  0.82764435  1.2134852
0:   1.4589424   1.6859517   1.5377316   1.3529158   1.0612683   0.5470815
0:   0.14063454 -0.20491838 -0.26851702 -0.07238436  0.33686352  0.7637067
0:  -0.791111   -0.989213  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.145382 10.252962 10.566938 11.10942  11.652075 12.095919 12.396065
0:  12.628222 12.440662 12.32132  12.088021 11.618937 11.229131 10.816835
0:  10.772543 11.072569 11.66058  12.257469 10.652613 10.768509]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.48209476 -0.20205069 -0.62432194 -0.6366472  -0.41132975 -0.08152103
0:   0.19318724  0.5272784   0.58741665  0.71166515  0.7999191   0.6329994
0:   0.55003595  0.51161766  0.56094074  0.8453274   1.2154083   1.4501157
0:  -1.9874783  -2.63902   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.45835   3.6940238 4.156038  4.8123674 5.357582  5.7076306 5.875297
0:  5.9968457 5.792234  5.610362  5.3590946 4.908864  4.62226   4.448633
0:  4.646953  5.23977   6.0750685 6.885057  5.9978423 6.214068 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2613072 5.4369397 5.8753214 6.588727  7.246891  7.683222  7.8530016
0:  7.9767704 7.7638693 7.674889  7.586573  7.2633543 6.990241  6.656558
0:  6.5268364 6.8377094 7.4690833 8.183977  7.1245923 7.2274084]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.504536  4.539076  4.8621135 5.415906  5.929985  6.3175454 6.4863987
0:  6.593334  6.3961887 6.2887726 6.145623  5.8169713 5.623989  5.4589763
0:  5.6039696 6.104813  6.8604455 7.651685  6.112992  6.17493  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6280475  -1.7087498  -1.4871125  -0.9775572  -0.45275736 -0.13428736
0:   0.03626251 -0.03433847 -0.49643707 -0.990571   -1.6247721  -2.4175959
0:  -2.976715   -3.3947954  -3.5192022  -3.3866978  -3.307478   -3.3349056
0:  -4.3074646  -4.5238256 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.9510293 5.9407806 6.207807  6.569997  6.785328  6.7499986 6.778734
0:  6.7014246 6.5478706 6.3968716 6.030173  5.4557767 4.956947  4.6971803
0:  4.7908726 5.301909  5.918689  6.437971  4.9404173 4.9673176]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.314406  -6.0986037 -5.583382  -4.8234115 -4.117933  -3.6105094
0:  -3.3800063 -3.267116  -3.5296474 -3.8103566 -4.200853  -4.822434
0:  -5.2859597 -5.626141  -5.671911  -5.33452   -4.8300824 -4.31107
0:  -5.302975  -5.2363353]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.947918 22.901733 22.862343 22.72702  22.474514 21.930344 21.888828
0:  21.68011  21.492817 21.163622 20.326948 19.465492 18.647018 18.372917
0:  18.69648  19.474403 20.38538  20.989498 20.367466 20.28439 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.968561  -9.210602  -9.102646  -8.557598  -7.895329  -7.3114066
0:  -7.159162  -7.0650353 -7.462377  -7.7722287 -8.059658  -8.6225605
0:  -9.066652  -9.549976  -9.787403  -9.533417  -8.825712  -7.8409743
0:  -8.03813   -8.330536 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0400758 -2.1065173 -1.9039779 -1.5585322 -1.4197917 -1.6061187
0:  -1.9566693 -2.3889022 -3.0028462 -3.505177  -3.9940734 -4.6323285
0:  -5.022516  -5.177509  -4.9894557 -4.343248  -3.5408196 -2.808889
0:  -3.2132468 -2.9820256]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.386898 18.413588 18.387405 18.202656 17.897476 17.406864 17.72799
0:  18.022984 18.465744 18.676552 18.157688 17.669115 17.089188 17.289858
0:  18.095442 19.29781  20.410265 20.965878 21.432198 21.72977 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.948008  4.1322584 4.6051373 5.217424  5.8157825 6.214782  6.7127023
0:  7.1004124 7.2636285 7.434689  7.356407  7.105263  6.9867105 7.0108013
0:  7.305577  7.819493  8.313551  8.64453   6.7477345 6.971362 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.886964   6.4483438  7.3506165  8.327191   9.119626   9.641884
0:  10.194698  10.612629  10.905962  11.13199   11.081036  10.813918
0:  10.647963  10.786093  11.341581  12.283359  13.333611  14.209831
0:  13.463836  13.516042 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.2365713  7.3255596  7.540563   7.718998   7.7956486  7.6845117
0:   7.985846   8.243364   8.527403   8.716676   8.401523   7.9773693
0:   7.488644   7.516367   8.002802   8.88023    9.775951  10.363546
0:   8.91065    9.143189 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.683498 17.544867 17.47279  17.260153 16.950161 16.384201 16.504904
0:  16.497587 16.607883 16.593113 15.974153 15.407472 14.853308 15.019365
0:  15.752448 16.917187 18.014248 18.652603 19.198874 19.332005]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.082596 40.36794  40.41635  40.2965   40.132725 39.75674  40.222507
0:  40.56226  41.0192   41.197186 40.598938 40.12229  39.629955 39.70366
0:  40.27176  41.007896 41.623604 41.6752   38.674484 38.69891 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.812935 13.131892 13.486442 13.644198 13.634461 13.474427 13.647806
0:  13.829667 13.974834 14.066821 13.825508 13.467074 13.207055 13.390388
0:  13.997392 14.900906 15.7631   16.346132 15.87662  16.32834 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-19.356756 -19.659962 -19.487453 -18.868578 -18.182425 -17.72026
0:  -17.364916 -17.272194 -17.568354 -17.865677 -18.438385 -19.152061
0:  -19.635511 -19.84871  -19.643667 -19.13906  -18.710464 -18.418018
0:  -19.436178 -19.84802 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.3329363 0.5365734 1.10108   1.8872571 2.5076544 2.862394  3.0980587
0:  3.184448  3.0163612 2.841558  2.501802  1.9445605 1.4926815 1.1784697
0:  1.1640306 1.514905  1.9868283 2.402092  1.3666573 1.4769139]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.183608  2.4035935 2.9167225 3.624654  4.176243  4.4813724 4.5767636
0:  4.6894226 4.5727177 4.5394487 4.4315414 4.016613  3.6410792 3.2515402
0:  3.1243136 3.455741  4.1255074 4.912321  3.4015887 3.5495331]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.267295  -2.4528418 -2.4845934 -2.39859   -2.541628  -3.0198069
0:  -3.7409391 -4.5871964 -5.7246265 -6.5943356 -7.3494806 -8.089519
0:  -8.501388  -8.654388  -8.482281  -7.944908  -7.3163624 -6.7100697
0:  -7.495288  -7.165353 ]
0: validation loss for strategy=forecast at epoch 24 : 0.3646364212036133
0: validation loss for velocity_u : 0.15137331187725067
0: validation loss for velocity_v : 0.2499341517686844
0: validation loss for specific_humidity : 0.1718316674232483
0: validation loss for velocity_z : 0.5677586197853088
0: validation loss for temperature : 0.13533562421798706
0: validation loss for total_precip : 0.9115849137306213
0: 25 : 19:37:57 :: batch_size = 96, lr = 1.1333944760803001e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 25, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4450, 0.4292, 0.4141, 0.3999, 0.3866, 0.3746, 0.3645, 0.3564, 0.3502, 0.3464, 0.3449, 0.3458, 0.3491, 0.3544,
0:         0.3618, 0.3711, 0.3819, 0.3937, 0.6305, 0.6101, 0.5887, 0.5666, 0.5442, 0.5218, 0.4997], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4818,  0.4515,  0.4200,  0.3874,  0.3536,  0.3187,  0.2827,  0.2457,  0.2083,  0.1706,  0.1327,  0.0951,
0:          0.0581,  0.0218, -0.0135, -0.0475, -0.0804, -0.1114,  0.4593,  0.4345,  0.4088,  0.3826,  0.3552,  0.3270,
0:          0.2976], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8572, 0.8189, 0.7529, 0.6778, 0.6024, 0.5266, 0.4505, 0.3855, 0.3201, 0.2543, 0.1883, 0.1190, 0.0928, 0.0809,
0:         0.0691, 0.0571, 0.0450, 0.0705, 1.0843, 1.0657, 1.0181, 0.9702, 0.9220, 0.8737, 0.8180], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5963, -0.5893, -0.5755, -0.5570, -0.5339, -0.5108, -0.4900, -0.4715, -0.4577, -0.4507, -0.4484, -0.4484,
0:         -0.4531, -0.4623, -0.4715, -0.4831, -0.4969, -0.5131, -0.7233, -0.7210, -0.7187, -0.7141, -0.7025, -0.6864,
0:         -0.6610], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.8169, 1.7748, 1.7342, 1.6956, 1.6585, 1.6236, 1.5910, 1.5604, 1.5317, 1.5050, 1.4801, 1.4571, 1.4355, 1.4157,
0:         1.3972, 1.3800, 1.3640, 1.3490, 1.3348, 1.3208, 1.3067, 1.2930, 1.2790, 1.2646, 1.2500], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2606, -0.2606, -0.2594, -0.2582, -0.2570, -0.2558, -0.2534, -0.2534, -0.2523, -0.2606, -0.2606, -0.2606,
0:         -0.2606, -0.2606, -0.2606, -0.2606, -0.2594, -0.2582, -0.2606, -0.2606, -0.2606, -0.2606, -0.2606, -0.2606,
0:         -0.2606], device='cuda:0')
0: [DEBUG] Epoch 25, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2166,
0:             nan, -0.2047,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1774,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1702,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1940,     nan,     nan,     nan,     nan, -0.1952,     nan, -0.1976,     nan,
0:         -0.1845,     nan,     nan,     nan,     nan, -0.1702,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1809,     nan,     nan,     nan,     nan,
0:             nan, -0.1667,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2558,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2475, -0.2475,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1690,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1215,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1346,     nan, -0.1334,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 25, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1943, 1.1881, 1.1922, 1.1907, 1.1930, 1.1898, 1.2265, 1.2601, 1.3009, 1.3283, 1.3217, 1.2997, 1.2803, 1.2806,
0:         1.3233, 1.4041, 1.5156, 1.6271, 1.2954, 1.3173, 1.3394, 1.3287, 1.2925, 1.2659, 1.2572], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2238, 0.2200, 0.2013, 0.1802, 0.1558, 0.1322, 0.1018, 0.0786, 0.0633, 0.0541, 0.0515, 0.0541, 0.0492, 0.0289,
0:         0.0103, 0.0061, 0.0223, 0.0596, 0.2471, 0.2481, 0.2325, 0.2090, 0.1802, 0.1475, 0.1227], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2459, -0.2373, -0.2396, -0.2431, -0.2509, -0.2599, -0.2792, -0.2940, -0.3063, -0.3116, -0.3127, -0.3015,
0:         -0.2885, -0.2790, -0.2942, -0.3044, -0.3222, -0.3141, -0.3124, -0.3039, -0.3023, -0.3031, -0.3126, -0.3220,
0:         -0.3325], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0976, 1.2559, 1.3607, 1.3022, 1.2206, 1.2124, 1.1472, 1.0902, 1.0974, 1.0855, 1.0985, 1.1835, 1.2904, 1.2658,
0:         1.1674, 1.2109, 1.2432, 1.0901, 1.3836, 1.3579, 1.3557, 1.3113, 1.2894, 1.3430, 1.2500], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.6778, 1.7704, 1.8064, 1.7819, 1.7488, 1.7448, 1.7823, 1.8272, 1.8552, 1.8529, 1.8265, 1.7930, 1.7707, 1.7740,
0:         1.7678, 1.7538, 1.7328, 1.7251, 1.7413, 1.7789, 1.8269, 1.8723, 1.9062, 1.9124, 1.8879], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1476, -0.1472, -0.1474, -0.1546, -0.1543, -0.1512, -0.1474, -0.1443, -0.1363, -0.1451, -0.1396, -0.1508,
0:         -0.1472, -0.1430, -0.1464, -0.1430, -0.1452, -0.1397, -0.1331, -0.1348, -0.1324, -0.1303, -0.1335, -0.1369,
0:         -0.1459], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2171253114938736; velocity_v: 0.31330713629722595; specific_humidity: 0.2171575129032135; velocity_z: 0.48610246181488037; temperature: 0.17453990876674652; total_precip: 0.5262298583984375; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20030811429023743; velocity_v: 0.2805883288383484; specific_humidity: 0.22877763211727142; velocity_z: 0.5612145662307739; temperature: 0.16719265282154083; total_precip: 0.7900365591049194; 
0: epoch: 25 [1/5 (20%)]	Loss: 0.65813 : 0.31196 :: 0.20677 (2.84 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21087265014648438; velocity_v: 0.3058585822582245; specific_humidity: 0.1852903515100479; velocity_z: 0.7606721520423889; temperature: 0.1564161479473114; total_precip: 0.6901921629905701; 
0: epoch: 25 [2/5 (40%)]	Loss: 0.69019 : 0.34977 :: 0.21150 (15.82 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2122953236103058; velocity_v: 0.35290706157684326; specific_humidity: 0.1640259474515915; velocity_z: 0.6455591917037964; temperature: 0.15442007780075073; total_precip: 0.8297576308250427; 
0: epoch: 25 [3/5 (60%)]	Loss: 0.82976 : 0.35849 :: 0.21413 (15.72 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2100219577550888; velocity_v: 0.32888033986091614; specific_humidity: 0.1879889965057373; velocity_z: 0.6594164967536926; temperature: 0.19719652831554413; total_precip: 0.9119067192077637; 
0: epoch: 25 [4/5 (80%)]	Loss: 0.91191 : 0.38048 :: 0.20716 (15.91 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [1.62124634e-05 1.62124634e-05 2.57492065e-05 2.28881836e-05
0:  5.72204590e-06 5.72204590e-06 3.81469727e-06 1.90734863e-06
0:  9.53681592e-07 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 4.76837158e-06 4.76837158e-06 3.81469727e-06
0:  8.58306885e-06 1.43051147e-05 1.14440918e-05 1.90734863e-05
0:  2.67028809e-05 4.38690186e-05 5.34057617e-05 6.77108765e-05
0:  5.72204590e-05 4.19616699e-05 4.00543213e-05 5.62667847e-05
0:  7.62939453e-05 5.91278076e-05 4.67300415e-05 4.29153479e-05
0:  6.77108765e-05 7.34329224e-05 1.07765198e-04 1.02996826e-04
0:  8.96453857e-05 1.02996826e-04 1.02996826e-04 6.96182251e-05
0:  6.77108765e-05 5.24520874e-05 7.05718994e-05 6.77108765e-05
0:  6.29425049e-05 4.38690186e-05 2.76565552e-05 2.47955322e-05
0:  4.00543213e-05 4.38690186e-05 4.00543213e-05 5.53131104e-05
0:  1.08718872e-04 1.79290771e-04 2.05993652e-04 1.85012817e-04
0:  1.71661377e-04 1.44958496e-04 1.29699707e-04 1.23023987e-04
0:  1.21116638e-04 1.26838684e-04 1.37329102e-04 1.64031982e-04
0:  1.94549561e-04 2.36511230e-04 2.73704529e-04 2.69889832e-04
0:  1.84059143e-04 1.41143799e-04 1.24931335e-04 2.08854675e-04
0:  1.64985657e-04 1.42097473e-04 1.66893005e-04 1.42097473e-04
0:  8.20159912e-05 7.82012939e-05 6.67572021e-05 1.07765198e-04
0:  8.10623169e-05 9.15527344e-05 1.17301941e-04 8.67843628e-05
0:  1.14440918e-04 1.13487244e-04 1.23977661e-04 1.56402588e-04
0:  2.67982483e-04 3.76701355e-04 5.46455383e-04 6.81877136e-04
0:  6.67572021e-06 9.53674316e-06 1.62124634e-05 1.33514404e-05
0:  7.62939453e-06 7.62939453e-06 4.76837158e-06 1.90734863e-06
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.62939453e-06 1.14440918e-05 1.23977661e-05
0:  1.62124634e-05 2.38418579e-05 2.47955322e-05 3.33786011e-05
0:  4.29153479e-05 4.00543213e-05 4.67300415e-05 6.67572021e-05
0:  5.53131104e-05 4.00543213e-05 3.33786011e-05 4.57763635e-05
0:  5.81741333e-05 4.76837158e-05 3.52859497e-05 1.23977661e-05
0:  5.91278076e-05 1.02043152e-04 8.96453857e-05 8.58306885e-05
0:  7.91549683e-05 8.20159912e-05 8.01086426e-05 7.91549683e-05
0:  4.76837158e-05 1.62124634e-05 2.38418579e-05 3.24249268e-05
0:  4.19616699e-05 3.24249268e-05 2.38418579e-05 9.53674316e-06
0:  3.33786011e-05 5.72204590e-05 8.01086426e-05 8.96453857e-05
0:  1.02043152e-04 1.96456909e-04 2.26974487e-04 1.48773193e-04
0:  9.05990601e-05 5.14984094e-05 5.81741333e-05 5.05447388e-05
0:  4.10079956e-05 5.62667847e-05 6.96182251e-05 8.86917114e-05
0:  1.13487244e-04 1.46865845e-04 1.66893005e-04 1.73568726e-04
0:  1.77383423e-04 1.46865845e-04 1.32560730e-04 1.32560730e-04
0:  1.25885010e-04 1.24931335e-04 1.63078308e-04 1.55448914e-04]
0: Target values (first 200):
0: [9.6845633e-04 5.3548813e-04 4.0578842e-04 2.9325485e-04 1.4972687e-04
0:  5.1021580e-05 3.1471252e-05 2.9563904e-05 3.7193298e-05 3.8146973e-05
0:  3.7670135e-05 3.3855438e-05 3.0994415e-05 2.6702881e-05 2.8610229e-05
0:  3.8146973e-05 3.0040741e-05 2.0980835e-05 9.5367432e-06 7.1525574e-06
0:  1.0013580e-05 3.3378601e-06 1.8119812e-05 7.4863434e-05 8.7738037e-05
0:  1.1014938e-04 1.4448166e-04 1.2207031e-04 6.2942505e-05 6.1035156e-05
0:  8.1539154e-05 1.7309189e-04 2.7418137e-04 3.8433075e-04 3.3903122e-04
0:  3.6811829e-04 4.0388107e-04 4.0626526e-04 4.1198730e-04 4.5347214e-04
0:  4.9018860e-04 5.1450729e-04 5.3262711e-04 5.6791306e-04 6.3085556e-04
0:  6.9379807e-04 7.0333481e-04 6.2131882e-04 4.5299530e-04 2.8276443e-04
0:  1.5258789e-04 6.7710876e-05 1.8596649e-05 7.6293945e-06 4.7683716e-07
0:  7.2759576e-12 7.2759576e-12 7.2759576e-12 7.2759576e-12 7.2759576e-12
0:  7.2759576e-12 7.2759576e-12 7.2759576e-12 7.2759576e-12 9.5368159e-07
0:  2.3841858e-06 1.9073486e-06 3.3378601e-06 5.2452087e-06 4.2915344e-06
0:  1.2397766e-05 2.1934509e-05 2.4795532e-05 2.2411346e-05 3.0517578e-05
0:  3.3378601e-05 3.7670135e-05 3.0040741e-05 2.3841858e-05 2.0980835e-05
0:  1.6212463e-05 1.2397766e-05 1.5735626e-05 1.8119812e-05 2.9087067e-05
0:  3.3855438e-05 3.7193298e-05 2.5272369e-05 2.6226044e-05 2.4318695e-05
0:  1.2874603e-05 4.7683716e-06 3.8146973e-06 6.6757202e-06 8.5830688e-06
0:  6.6757202e-06 7.1525574e-06 9.0599060e-06 7.1525574e-06 8.1062317e-06
0:  8.1062317e-06 6.1988831e-06 7.6293945e-06 1.5735626e-05 1.7642975e-05
0:  1.8596649e-05 2.2411346e-05 2.4318695e-05 3.9625168e-04 4.1770935e-04
0:  2.0217896e-04 1.0681152e-04 6.7234039e-05 5.0067902e-05 4.1007996e-05
0:  3.9100647e-05 3.9100647e-05 3.7193298e-05 2.8610229e-05 3.0517578e-05
0:  3.3855438e-05 2.9563904e-05 2.6226044e-05 3.3855438e-05 2.3841858e-05
0:  1.2397766e-05 7.1525574e-06 3.8146973e-06 1.9073486e-06 8.5830688e-06
0:  2.3841858e-05 6.4849854e-05 9.9658966e-05 1.3685226e-04 1.3732910e-04
0:  1.3923645e-04 9.0599060e-05 9.7751617e-05 1.6736984e-04 3.2043457e-04
0:  3.8671494e-04 4.1675568e-04 4.6157837e-04 4.8732758e-04 4.7636032e-04
0:  4.9829483e-04 5.0449371e-04 5.0926208e-04 5.3358078e-04 5.5646896e-04
0:  5.4645538e-04 5.9890747e-04 6.8616867e-04 7.4386597e-04 7.2908401e-04
0:  6.3657761e-04 4.6539307e-04 2.8753281e-04 1.5258789e-04 6.8187714e-05
0:  1.8596649e-05 7.6293945e-06 9.5368159e-07 7.2759576e-12 7.2759576e-12
0:  7.2759576e-12 7.2759576e-12 7.2759576e-12 7.2759576e-12 7.2759576e-12
0:  9.5368159e-07 1.4305115e-06 1.4305115e-06 1.4305115e-06 3.8146973e-06
0:  4.7683716e-06 6.1988831e-06 7.6293945e-06 1.5735626e-05 3.4332275e-05
0:  3.6716461e-05 3.2424927e-05 2.3365021e-05 2.8133392e-05 4.1961670e-05
0:  3.4809113e-05 2.7179718e-05 2.4795532e-05 2.0027161e-05 1.8596649e-05
0:  2.4795532e-05 2.5272369e-05 2.8610229e-05 3.2901764e-05 3.6239624e-05
0:  3.2424927e-05 2.5272369e-05 2.2888184e-05 2.3841858e-06 5.2452087e-06]
0: Prediction values (first 20):
0: [7.100623  7.3521113 7.7685738 8.081324  8.168296  7.985505  8.084667
0:  8.110284  8.075977  8.006121  7.5534177 6.9959545 6.457727  6.43016
0:  6.8193855 7.539885  8.170644  8.498036  8.051609  8.132788 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.234, max = 2.691, mean = 0.629
0:          sample (first 20): tensor([0.0971, 0.1184, 0.1537, 0.1802, 0.1875, 0.1720, 0.1804, 0.1826, 0.1797, 0.1738, 0.1354, 0.0882, 0.0426, 0.0403,
0:         0.0733, 0.1343, 0.1877, 0.2155, 0.0962, 0.1293])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.220251 25.958456 25.494587 24.927946 24.470024 23.79704  24.17488
0:  24.444235 24.99366  25.261557 24.773815 24.421583 23.957043 24.151615
0:  24.900583 25.949934 26.8889   27.189156 27.959068 27.85534 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.074829  9.37672   9.752018 10.068496 10.152198  9.884932  9.946809
0:   9.87412   9.852989  9.773193  9.315837  8.863355  8.404456  8.438908
0:   8.953854  9.856707 10.889388 11.634903 10.227715 10.301712]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.0513067   1.9407811   2.1860747   2.66216     3.0462322   3.1991243
0:   3.1367548   2.894679    2.2857027   1.6600208   0.87727404 -0.10904026
0:  -0.9605279  -1.5637441  -1.7948518  -1.5803838  -1.2001777  -0.7924628
0:  -1.8089147  -1.9963098 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.712282 25.340899 24.83892  24.311913 23.977743 23.505667 23.928946
0:  24.159887 24.489109 24.452372 23.692255 23.0694   22.432795 22.458157
0:  23.077234 24.045134 25.009897 25.485468 26.135809 25.957062]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.985685  10.174388  10.674368  11.295635  11.860067  12.248616
0:  12.594022  12.854897  12.922821  12.985512  12.881126  12.6606045
0:  12.5747795 12.713019  13.190752  13.918669  14.651014  15.200375
0:  12.836212  12.731138 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1271377  -0.99811363 -0.528749    0.14662409  0.7160106   1.0350924
0:   1.1845627   1.1662841   0.8377223   0.48594284 -0.01350164 -0.7421551
0:  -1.2568536  -1.5344014  -1.4180789  -0.86973286 -0.1812787   0.42188597
0:  -0.7779298  -0.41387892]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.945904 17.99414  18.287716 18.607624 18.734953 18.548717 18.462854
0:  18.310547 18.115475 17.955036 17.606712 17.094955 16.650747 16.361794
0:  16.432257 16.87935  17.486208 17.944069 15.413242 15.437727]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.944311 20.941244 21.021292 21.097372 21.139633 20.930689 21.358038
0:  21.577871 21.78665  21.773079 21.177408 20.602219 20.04237  20.135143
0:  20.729498 21.710266 22.639471 23.11361  21.953012 21.981853]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4536767  -2.3329597  -1.8754363  -1.1580205  -0.43011713  0.13157701
0:   0.63000154  1.0040493   1.1057825   1.1646671   1.074595    0.78143215
0:   0.6049051   0.5245514   0.6876297   1.0813017   1.4870152   1.8070307
0:   0.4202237   0.5873413 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.0421114   4.1675572   4.5244756   5.0714316   5.475245    5.559306
0:   5.3104277   4.7867017   3.8622506   2.973628    2.0820704   1.1354074
0:   0.44723845 -0.03812361 -0.17545462  0.03545904  0.41048193  0.82481194
0:  -0.12520838  0.01148891]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.164228  1.4825325 2.046691  2.6327715 2.9032433 2.8544168 2.661569
0:  2.5439515 2.3709688 2.4047365 2.4635558 2.2962365 2.2562814 2.3300023
0:  2.7297142 3.5648398 4.6101217 5.562523  4.183894  4.634507 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.5965209  0.84314203 1.4464083  2.3367572  3.286153   4.123358
0:  5.144608   6.033135   6.705536   7.1872005  7.280532   7.051445
0:  6.8078594  6.723644   6.913895   7.359766   7.82477    8.099526
0:  5.464663   5.4265394 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.310305  -8.199194  -7.697662  -6.7877564 -5.8261437 -5.0121036
0:  -4.6478715 -4.412935  -4.7147202 -5.0339265 -5.434685  -6.181105
0:  -6.8665586 -7.534673  -7.8725877 -7.625871  -6.876777  -5.8513846
0:  -6.124764  -6.290323 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.124706 14.115574 14.235388 14.415855 14.56562  14.485628 14.754772
0:  14.893131 15.094971 15.253754 15.100718 14.794516 14.534567 14.537676
0:  14.89172  15.619947 16.5207   17.260485 15.849602 15.933521]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.0000916 -6.959347  -6.6519403 -6.065729  -5.5063295 -5.103977
0:  -4.9212227 -4.912282  -5.313098  -5.7292776 -6.21883   -6.78761
0:  -7.046633  -7.0459404 -6.660531  -5.930995  -5.1657476 -4.460845
0:  -4.260891  -4.1368694]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.579768  6.447701  6.5094132 6.564947  6.4802256 6.155305  6.3043475
0:  6.3627696 6.4492493 6.4623733 5.9707355 5.3836775 4.7429376 4.7508006
0:  5.2218056 6.039113  6.661179  6.8144293 5.7848167 5.9691844]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.623188 24.508043 24.373857 24.152056 23.701427 22.93069  22.448946
0:  21.781115 21.019402 20.09239  18.735556 17.235027 15.77817  14.807432
0:  14.413885 14.553256 14.909948 15.191696 14.028063 13.824667]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.8086617   2.6520514   2.8872263   3.3765113   3.7542405   3.845427
0:   3.703552    3.2680404   2.3651214   1.4804926   0.40250587 -0.9634838
0:  -2.1755724  -3.255396   -4.0076475  -4.2744455  -4.3034163  -4.17554
0:  -7.318388   -7.3266983 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4777880e+00 -2.5270691e+00 -2.3024302e+00 -1.6980066e+00
0:  -1.0019488e+00 -3.9806414e-01 -1.4422560e-01  6.2059879e-02
0:  -1.8295193e-01 -3.5521936e-01 -5.2695942e-01 -9.2647743e-01
0:  -1.2351074e+00 -1.5601425e+00 -1.6835709e+00 -1.3851995e+00
0:  -7.1873856e-01  1.3492250e-01 -1.1820793e-03 -9.0733051e-02]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.19089413 -0.0923357   0.29967976  0.972415    1.5708127   1.9656751
0:   2.030231    2.0149593   1.6086059   1.2544651   0.8594856   0.20111895
0:  -0.34668446 -0.8201914  -0.9565382  -0.55713415  0.24712515  1.1728458
0:   0.44328547  0.55763626]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.607218 21.101225 20.791084 20.610758 20.52771  20.30094  20.738451
0:  21.066587 21.557274 21.865643 21.64173  21.399462 21.216753 21.516163
0:  22.287245 23.350204 24.378933 25.056124 24.013115 23.66724 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.465379  -5.531384  -5.3316903 -4.776797  -4.187778  -3.8355985
0:  -3.809568  -4.0372024 -4.750088  -5.448426  -6.1833177 -6.9610486
0:  -7.4492984 -7.7702184 -7.767507  -7.482756  -7.1354346 -6.873508
0:  -7.6873612 -7.616819 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.181068 16.4827   16.894697 17.22418  17.437788 17.415306 17.587227
0:  17.668627 17.700584 17.707966 17.466251 17.106092 16.83778  16.806774
0:  17.140928 17.791103 18.6013   19.269703 17.051054 17.259453]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.983425 38.81882  38.183567 37.318474 36.33256  35.180584 35.058186
0:  34.956024 35.104027 34.980892 34.05379  33.371296 32.670525 32.745743
0:  33.54378  34.4784   35.25683  35.24561  36.37431  36.493404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1367612 -3.279099  -3.1312852 -2.6887012 -2.2763352 -2.0573926
0:  -2.1038918 -2.2492576 -2.8071313 -3.3075404 -3.8750148 -4.6807685
0:  -5.313885  -5.8655334 -6.1096187 -5.8787007 -5.370374  -4.721686
0:  -5.82295   -5.9060826]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.847889  12.2757435 12.803423  13.25596   13.371633  13.2392235
0:  13.046576  12.939178  12.759169  12.660327  12.467951  12.041946
0:  11.693022  11.438739  11.578602  12.132924  12.954515  13.739917
0:  11.147725  11.386301 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.901351   7.4454823  7.149413   7.006793   6.976169   6.830907
0:   7.4339147  7.7938986  8.239159   8.388416   7.9778576  7.5938053
0:   7.3796234  7.95639    9.165125  10.868703  12.659231  14.036537
0:  15.095087  15.019041 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-17.447353   -17.548464   -17.232632   -16.698677   -16.04335
0:  -15.302937   -13.835646   -12.175547   -10.42643     -8.796949
0:   -7.6815667   -6.9475837   -6.2485757   -5.1368203   -3.6328998
0:   -1.9528975   -0.54000854   0.2591467    1.0686436    1.4791775 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.42959  23.655867 23.675137 23.464636 23.238258 22.996979 23.724567
0:  24.464928 25.437273 26.213884 26.386553 26.60265  26.850328 27.776403
0:  29.24699  30.998257 32.614716 33.527725 34.808258 35.351246]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6191282  -1.4739285  -0.99790525 -0.33626223  0.17195463  0.39957714
0:   0.58470535  0.58998823  0.42601395  0.24680853 -0.08701372 -0.60211086
0:  -0.92058563 -0.9082842  -0.47083473  0.37698412  1.2662711   2.0331125
0:   2.0615458   2.4228358 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.565491  8.618313  8.831868  9.2222    9.434465  9.408646  9.053028
0:  8.723285  8.074808  7.5192966 6.900529  5.9238844 4.912974  3.8996031
0:  3.261869  3.272849  3.9198482 4.8049316 2.996385  2.6465418]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.956051  10.8756485 10.868399  10.842506  10.805699  10.610005
0:  11.0287285 11.377252  11.821112  12.139433  11.950445  11.677512
0:  11.310858  11.453772  12.0436325 13.001989  13.976963  14.636784
0:  14.690491  14.761555 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5774784  -5.328869   -4.678604   -3.7864094  -2.898759   -2.209362
0:  -1.5092001  -0.9992986  -0.7354336  -0.4604869  -0.36072397 -0.42077923
0:  -0.30035973  0.0749321   0.7600651   1.7065983   2.5723689   3.2036974
0:   3.0097878   3.52473   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.628904 14.27244  13.971268 13.699135 13.479257 13.016182 13.237191
0:  13.283426 13.506535 13.56604  13.043177 12.542858 11.952862 11.903806
0:  12.316113 13.12097  13.918383 14.313046 13.697578 13.323816]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5157716 3.4923618 3.5724814 3.758413  3.7408447 3.583901  3.2832818
0:  3.0942636 2.732446  2.493277  2.2191343 1.7622786 1.3995914 1.1138797
0:  1.1392283 1.5115857 2.0738316 2.5899534 1.6520519 1.5767136]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.606613  8.750877  9.11853   9.593681  9.957468 10.108102 10.42802
0:  10.666946 10.853006 11.017771 10.942989 10.728668 10.583691 10.705694
0:  11.212446 12.068043 12.996447 13.777266 12.245117 12.436529]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.586702  15.170963  14.9669895 14.938925  14.857346  14.498644
0:  14.501818  14.310509  14.14776   13.828363  13.060102  12.193632
0:  11.352161  10.96445   11.109269  11.723836  12.484686  13.013819
0:  10.69343   10.296457 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.316727  15.275549  15.383553  15.494154  15.5497265 15.439537
0:  15.634552  15.725481  15.808933  15.824518  15.459476  14.968304
0:  14.472584  14.300463  14.521029  15.094828  15.768929  16.30942
0:  14.355076  14.338995 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.229642  -7.1822634 -6.7965603 -6.1424627 -5.470667  -4.9676113
0:  -4.5795364 -4.409974  -4.5721617 -4.7386246 -5.0481014 -5.469763
0:  -5.6832476 -5.653488  -5.2900486 -4.653248  -4.0288477 -3.4692488
0:  -4.6537952 -4.5732956]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.59346   10.6797285 10.890785  11.101812  11.207676  11.073648
0:  11.305973  11.447564  11.625376  11.718237  11.428526  11.006106
0:  10.587803  10.575579  10.94935   11.749117  12.665157  13.351021
0:  12.581011  12.704325 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.623235 13.880713 14.270394 14.643597 14.79711  14.721937 14.841337
0:  14.889166 14.969587 15.020388 14.838707 14.582758 14.407949 14.460308
0:  14.874304 15.588207 16.377018 17.078365 15.364414 15.665865]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.441671 15.552906 15.846443 16.19657  16.51345  16.641083 17.39417
0:  17.984085 18.706993 19.271841 19.30121  19.248905 19.170326 19.585651
0:  20.4238   21.605938 22.850533 23.7408   22.0992   22.101576]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.865813  7.4464498 6.955098  6.3264265 5.640139  4.875922  4.759037
0:  4.7131066 4.794247  4.740387  4.141293  3.5697284 2.9004102 2.8061728
0:  3.070128  3.561408  3.83989   3.729478  3.1606941 2.7000988]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.489265 13.014326 12.855232 12.706966 12.478818 12.099251 12.008722
0:  11.936359 11.961861 11.924176 11.587505 11.030527 10.586091 10.462858
0:  10.882614 11.733799 12.766838 13.801797 12.189731 12.105963]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.296955 15.455252 15.802561 16.150572 16.488317 16.733711 17.543116
0:  18.34368  19.15878  19.78343  19.933311 19.932785 19.946678 20.456572
0:  21.373602 22.518618 23.612555 24.250742 21.940538 21.964462]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.16268   8.906948  9.99382  11.126911 11.936481 12.346151 12.397989
0:  12.152969 11.471585 10.724666  9.82287   8.878727  8.242206  7.979375
0:   8.236525  8.864843  9.521744  9.927074  6.089722  6.080502]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3730779 2.2883568 2.4813128 2.8529212 3.1752539 3.3400471 3.5830529
0:  3.7347631 3.698363  3.6254008 3.3064158 2.7892125 2.3675694 2.2128415
0:  2.3872328 2.8667793 3.3259184 3.6126373 2.049059  2.1524978]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.456879 21.267965 20.960783 20.58238  20.232151 19.662342 19.873312
0:  19.846512 19.933619 19.678276 18.718506 17.737797 16.729864 16.35523
0:  16.589888 17.359291 18.266512 18.848547 19.615543 19.680912]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.987745  8.857577  8.938029  9.234088  9.511246  9.677025  9.875902
0:  10.003532  9.895504  9.799246  9.528697  9.052551  8.640412  8.306907
0:   8.241029  8.425566  8.711934  8.89487   6.594671  6.575527]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.9069786  -0.8807292  -0.6860671  -0.3913908  -0.23387432 -0.3186226
0:  -0.67997265 -1.0744476  -1.7618461  -2.2762094  -2.6932502  -3.2520595
0:  -3.5787263  -3.8408294  -3.862905   -3.5058103  -2.9377255  -2.246717
0:  -2.8650756  -2.9139943 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.183863  12.303762  12.573769  12.918547  13.21888   13.424952
0:  14.047532  14.629225  15.199131  15.66021   15.689434  15.549877
0:  15.441345  15.7263775 16.440042  17.504549  18.575726  19.314877
0:  17.820938  18.11826  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0851574 -4.065522  -3.7295027 -3.076374  -2.4408283 -1.9904871
0:  -1.7879744 -1.7233076 -2.0355444 -2.278923  -2.546157  -2.9307365
0:  -3.1123228 -3.1820211 -2.983448  -2.5226479 -1.9766369 -1.46558
0:  -1.8511753 -1.8183804]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.017984 24.8774   24.632101 24.19492  23.503166 22.554108 21.768566
0:  20.930002 20.074387 19.187252 18.061718 16.912266 15.86113  15.199877
0:  14.967365 15.084036 15.28278  15.346697 12.67423  12.369521]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.423345  11.682453  12.090475  12.409754  12.499947  12.325311
0:  12.243256  12.179804  12.1024885 12.05248   11.880133  11.568098
0:  11.419831  11.538071  12.060144  12.900951  13.833073  14.627651
0:  12.5973215 12.80909  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.902237 28.76336  28.482944 28.130219 27.966202 27.768505 28.423794
0:  29.04396  29.803093 30.317616 30.218788 30.18531  30.109558 30.465137
0:  31.249207 32.194443 33.08703  33.56604  32.36011  32.235302]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.2983036  -0.22357035  0.07667828  0.6285744   1.1548357   1.524066
0:   1.7205138   1.8585196   1.6559544   1.4902434   1.2269163   0.7528496
0:   0.4049368   0.11648941  0.10652447  0.43536997  0.91446733  1.3856115
0:   0.51191854  0.62762594]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.20788383 -0.39755917 -0.24973726  0.12719011  0.42070007  0.5139532
0:   0.42322493  0.3866105   0.19680977  0.17476797  0.19935417 -0.05417204
0:  -0.21417809 -0.38034964 -0.34817266  0.14144564  0.9386687   1.8233666
0:   0.29853106  0.26257515]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7135625 -5.7793384 -5.499577  -4.8557467 -4.159927  -3.6269822
0:  -3.3986325 -3.3196912 -3.6991048 -4.0590196 -4.465732  -5.091432
0:  -5.481954  -5.815904  -5.843332  -5.5149684 -4.9914613 -4.391064
0:  -5.1386228 -5.1267495]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.5122457 -6.5225434 -6.071422  -5.171396  -4.18615   -3.3824382
0:  -2.8831477 -2.6247754 -2.8165898 -3.0088277 -3.239122  -3.6784205
0:  -3.9177508 -4.063872  -3.9793153 -3.5004535 -2.8381305 -2.0909648
0:  -3.1179852 -3.7019181]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.61132336 -0.40268278  0.2092328   1.2075777   2.152116    2.920691
0:   3.389314    3.7666702   3.7219489   3.6431751   3.3189433   2.5401683
0:   1.6886053   0.8147173   0.2700343   0.30340433  0.78581953  1.4472852
0:  -1.3523932  -1.5982556 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.12052  10.942987 10.97131  11.141774 11.206941 11.04081  11.018272
0:  10.855583 10.612113 10.32513   9.80453   9.160767  8.605421  8.408908
0:   8.653573  9.302309 10.125392 10.848318  9.980297 10.414495]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1854687 6.1120415 6.183529  6.334943  6.41489   6.333818  6.4034767
0:  6.4082613 6.3993063 6.316247  6.0498443 5.6677303 5.380786  5.386907
0:  5.7343664 6.3959775 7.1430855 7.7707815 6.733421  7.0599656]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3859067 2.8758447 3.562275  4.173036  4.5694246 4.7509246 4.9998426
0:  5.1960163 5.371007  5.5084486 5.458972  5.201682  5.0836835 5.2329445
0:  5.7478037 6.616489  7.527772  8.287356  5.8181458 6.1775885]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.742256 26.526054 26.132862 25.553638 25.01213  24.373962 24.625366
0:  24.875938 25.271568 25.453999 24.949734 24.53864  24.09964  24.301018
0:  25.08746  26.177254 27.182297 27.672699 28.837318 28.863392]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.676451 10.94438  11.503899 12.226391 12.845278 13.199585 13.657296
0:  14.024167 14.311144 14.589876 14.746483 14.671881 14.783266 15.088537
0:  15.707095 16.58088  17.476685 18.034964 14.987164 15.120236]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4620109  -2.4217887  -2.0146184  -1.2779078  -0.6431494  -0.33514404
0:  -0.4278183  -0.78471375 -1.649363   -2.537417   -3.5502763  -4.8372426
0:  -5.962438   -7.010446   -7.720382   -7.8814664  -7.706716   -7.309607
0:  -8.917686   -9.277069  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.4472165  7.47851    7.640963   7.8000135  7.943862   7.9592686
0:   8.428413   8.864634   9.366861   9.750812   9.721653   9.588826
0:   9.42003    9.687891  10.332272  11.301413  12.246276  12.905961
0:  11.422623  11.3807125]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2531533 5.0516973 5.149682  5.4239707 5.554434  5.3996496 5.1901765
0:  4.906062  4.476138  4.1649036 3.7859733 3.3042834 2.9214642 2.7191687
0:  2.8007708 3.1405275 3.5283024 3.7550027 1.9158783 1.7707453]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.3415997 3.8416357 4.785749  6.0528426 7.3136916 8.268496  9.003937
0:  9.185556  8.674409  7.8421836 6.527438  4.882862  3.3922558 2.1258082
0:  1.395761  1.1411681 1.0669932 1.1410532 2.0179043 1.923327 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5810838  -0.4282112  -0.01573992  0.5024667   0.8619456   0.9827318
0:   0.96868706  0.93750143  0.7218857   0.5735464   0.3740759  -0.05610561
0:  -0.41316652 -0.66276836 -0.64250994 -0.13204002  0.66444683  1.5472426
0:   0.32847023  0.57112455]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.452539  -3.4745355 -3.0968394 -2.3586879 -1.6549993 -1.1493268
0:  -0.9110465 -0.8449416 -1.2515388 -1.7300935 -2.4247375 -3.4729352
0:  -4.390593  -5.2021384 -5.57865   -5.325189  -4.667809  -3.806046
0:  -4.4177337 -4.2482905]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.445002 -10.65041  -10.560745 -10.146015  -9.739597  -9.523973
0:   -9.708065 -10.010662 -10.775978 -11.46376  -12.112108 -12.880909
0:  -13.295004 -13.498892 -13.356894 -12.787731 -12.023945 -11.223942
0:  -11.052481 -11.220423]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.110895 22.205826 22.307472 22.339159 22.408205 22.329344 23.104753
0:  23.764729 24.577463 25.156113 25.087984 24.95873  24.806929 25.184336
0:  26.057287 27.242626 28.296274 28.838686 27.533978 27.651955]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.891993 25.102093 25.067732 24.779505 24.564632 24.238653 24.984257
0:  25.606003 26.446083 26.964066 26.713652 26.584927 26.494574 27.190434
0:  28.54399  30.284115 31.978468 33.11838  35.460957 35.98503 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.17306    -3.1128287  -2.7137504  -1.9228616  -1.0443845  -0.24189043
0:   0.23304129  0.6531639   0.59282875  0.49818707  0.28814602 -0.27973557
0:  -0.8334689  -1.3717656  -1.5838132  -1.2830358  -0.5331397   0.4009881
0:  -0.32523394 -0.4579425 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.1810789 1.4325018 1.9459543 2.6306334 3.1532793 3.428578  3.538591
0:  3.5498407 3.3264704 3.1411228 2.9112535 2.4929514 2.277279  2.2017698
0:  2.4649801 3.0922842 3.9180667 4.6733313 3.8643966 4.1889353]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.17145729 -0.3363576  -0.40017176 -0.19503069  0.14794111  0.44593334
0:   0.95219326  1.2932587   1.3403926   1.4316478   1.2737813   1.0446239
0:   0.9595008   1.0299644   1.3943744   1.8975887   2.453196    2.925089
0:   2.2698603   2.3433738 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.93889  24.067396 24.120338 23.881851 23.544455 23.052128 23.249641
0:  23.392181 23.652748 23.662813 23.017355 22.367731 21.73997  21.77152
0:  22.370232 23.373707 24.345673 24.899946 24.805088 25.157928]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.603966 29.700954 29.864883 29.970634 29.85772  29.432388 29.08065
0:  28.550964 27.944721 27.227684 26.248497 25.199795 24.331024 23.795963
0:  23.78564  24.123558 24.595772 24.904278 23.404026 23.403107]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.505016  2.3699703 2.449837  2.6268477 2.7508595 2.7522814 3.0643897
0:  3.3872442 3.674944  3.9376006 3.866815  3.6217418 3.3378613 3.427545
0:  3.8899522 4.654874  5.374787  5.8276024 4.700264  4.8951883]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.29559  38.313004 37.963528 37.48825  37.054634 36.517452 36.98851
0:  37.387367 37.95035  38.124863 37.40962  36.83246  36.26081  36.37975
0:  37.17931  38.185497 39.093353 39.346825 39.867855 40.151768]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.922693 23.74876  23.37318  22.886097 22.43299  21.826366 22.216713
0:  22.550909 23.114216 23.415047 22.935541 22.582188 22.090899 22.254377
0:  22.960316 23.9324   24.763515 24.982426 26.926805 26.899677]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.461262 -14.72698  -14.516821 -13.858488 -13.090858 -12.516821
0:  -12.170347 -12.095396 -12.475931 -12.850111 -13.348101 -13.998068
0:  -14.389198 -14.562965 -14.370634 -13.862045 -13.351097 -12.922388
0:  -14.235291 -14.453892]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.784698 30.715778 30.523182 30.234787 29.91885  29.38623  29.360489
0:  29.17788  28.966312 28.510872 27.465237 26.413761 25.417988 24.954388
0:  25.068985 25.534912 26.097443 26.379276 25.11644  24.93637 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.32593107  0.2692566   0.47168255  0.9695635   1.4471097   1.7937231
0:   1.8327222   1.8669467   1.535317    1.2803001   1.0221763   0.4800043
0:  -0.00195312 -0.49092245 -0.7152047  -0.43845844  0.25855255  1.1572165
0:   0.63273335  0.56694174]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.89437  20.851904 21.055346 21.625122 22.598858 23.61729  25.349064
0:  26.809765 28.24226  29.28968  29.625175 29.84055  29.918873 30.3205
0:  30.953693 31.545101 31.966476 31.952099 29.932716 30.016724]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.883267   -2.9574838  -2.8213086  -2.4570012  -2.1024566  -1.8804011
0:  -1.6951346  -1.5636773  -1.674192   -1.777906   -1.9968886  -2.3977418
0:  -2.622044   -2.638928   -2.3482413  -1.7462935  -1.1523676  -0.65538216
0:  -1.2133412  -1.09589   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.34254  32.292915 32.14071  31.872879 31.589354 31.082186 31.321918
0:  31.494944 31.760517 31.800087 31.122267 30.440865 29.638596 29.396864
0:  29.670103 30.255041 30.860792 31.06147  30.327131 30.29357 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.118124  -13.500547  -13.431726  -12.836002  -12.1030655 -11.507078
0:  -11.2979965 -11.2103615 -11.663235  -12.144987  -12.7427025 -13.688889
0:  -14.431623  -15.07662   -15.287296  -15.02026   -14.451221  -13.744011
0:  -14.072403  -14.565193 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9003646 4.015445  4.3185387 4.6808343 4.9019575 4.9178076 4.9843426
0:  4.9998813 4.870475  4.7197084 4.34828   3.754405  3.249952  2.948444
0:  3.020347  3.4697287 4.0000076 4.4899297 3.2525492 3.4591918]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9875717  -1.8695679  -1.4432917  -0.7483859  -0.11542082  0.3149457
0:   0.49071407  0.5032878   0.06885195 -0.33400154 -0.8716364  -1.6563959
0:  -2.3277597  -2.9041576  -3.1364145  -2.8473864  -2.2460966  -1.4776516
0:  -2.4250422  -2.4275846 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.712337 17.909042 18.272076 18.64355  18.918335 19.052637 19.60916
0:  20.173792 20.783577 21.261993 21.358143 21.348942 21.447094 21.908836
0:  22.866814 24.144257 25.462772 26.556404 23.95248  23.90701 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.634869   -7.622354   -7.246474   -6.595301   -5.792213   -5.061277
0:  -4.1005607  -3.2959561  -2.7049918  -2.3156152  -2.3333545  -2.540052
0:  -2.6590295  -2.3908725  -1.7188258  -0.7607899   0.04245472  0.506783
0:  -2.4181466  -2.7242637 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.481264 13.604122 13.786364 14.050219 14.092761 13.901451 13.462511
0:  13.109932 12.598036 12.35523  12.227166 11.900896 11.628643 11.34197
0:  11.37608  11.938312 13.027876 14.299437 13.340929 13.444892]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.0366707 2.2994266 2.8496232 3.5196285 4.0576954 4.291333  4.5531797
0:  4.6175146 4.5633345 4.489195  4.2121444 3.8254    3.5998938 3.6958776
0:  4.100898  4.825657  5.4573407 5.924124  3.4056466 3.4321706]
0: validation loss for strategy=forecast at epoch 25 : 0.3041897118091583
0: validation loss for velocity_u : 0.17917804419994354
0: validation loss for velocity_v : 0.27738484740257263
0: validation loss for specific_humidity : 0.15789131820201874
0: validation loss for velocity_z : 0.5569657683372498
0: validation loss for temperature : 0.11029783636331558
0: validation loss for total_precip : 0.5434204936027527
0: 26 : 19:41:53 :: batch_size = 96, lr = 1.1057507083710246e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 26, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2618, -0.2618, -0.2618, -0.2618, -0.2620, -0.2622, -0.2623, -0.2625, -0.2626, -0.2630, -0.2631, -0.2635,
0:         -0.2638, -0.2641, -0.2644, -0.2647, -0.2652, -0.2655, -0.1967, -0.1968, -0.1972, -0.1975, -0.1976, -0.1981,
0:         -0.1984], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3432, 0.3406, 0.3381, 0.3355, 0.3332, 0.3306, 0.3283, 0.3259, 0.3236, 0.3212, 0.3189, 0.3166, 0.3142, 0.3119,
0:         0.3098, 0.3076, 0.3053, 0.3032, 0.2936, 0.2902, 0.2870, 0.2838, 0.2806, 0.2774, 0.2742], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6152, -0.6154, -0.6156, -0.6158, -0.6160, -0.6163, -0.6164, -0.6165, -0.6166, -0.6167, -0.6168, -0.6169,
0:         -0.6170, -0.6171, -0.6172, -0.6173, -0.6174, -0.6176, -0.6082, -0.6084, -0.6086, -0.6089, -0.6092, -0.6094,
0:         -0.6096], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1088, -0.1110, -0.1132, -0.1165, -0.1187, -0.1210, -0.1232, -0.1254, -0.1265, -0.1287, -0.1309, -0.1332,
0:         -0.1354, -0.1365, -0.1387, -0.1409, -0.1420, -0.1443, -0.1099, -0.1121, -0.1143, -0.1165, -0.1199, -0.1221,
0:         -0.1243], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.6408, -0.6408, -0.6411, -0.6411, -0.6413, -0.6414, -0.6416, -0.6416, -0.6419, -0.6419, -0.6421, -0.6424,
0:         -0.6424, -0.6426, -0.6429, -0.6431, -0.6431, -0.6434, -0.6436, -0.6439, -0.6441, -0.6444, -0.6444, -0.6447,
0:         -0.6449], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2046, -0.2034, -0.2023, -0.2011, -0.1999, -0.1976, -0.1953, -0.1918, -0.1895, -0.1976, -0.1965, -0.1953,
0:         -0.1942, -0.1918, -0.1907, -0.1895, -0.1872, -0.1860, -0.1918, -0.1907, -0.1884, -0.1860, -0.1837, -0.1814,
0:         -0.1791], device='cuda:0')
0: [DEBUG] Epoch 26, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.1327,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1408,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1675,     nan, -0.1733,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2081,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2023,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1362,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1594,     nan,     nan,     nan, -0.1745,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2046,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2127,     nan,     nan, -0.2057,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2081,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1953,     nan,     nan,     nan,     nan,     nan, -0.1837,     nan,     nan,
0:             nan,     nan,     nan, -0.1582,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1826,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2081,     nan,
0:         -0.2104,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2162,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2092,     nan,     nan, -0.2092,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2011,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 26, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5564, -0.5551, -0.5424, -0.5125, -0.4944, -0.4871, -0.4962, -0.5034, -0.5392, -0.5671, -0.6047, -0.6626,
0:         -0.7133, -0.7575, -0.7723, -0.7474, -0.6893, -0.6172, -0.4961, -0.5263, -0.5426, -0.5435, -0.5475, -0.5446,
0:         -0.5390], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2699,  0.2722,  0.2519,  0.2227,  0.1782,  0.1250,  0.0609,  0.0252,  0.0193,  0.0355,  0.0507,  0.0570,
0:          0.0205, -0.0505, -0.1066, -0.1028, -0.0160,  0.1086,  0.2420,  0.2593,  0.2592,  0.2326,  0.1834,  0.1146,
0:          0.0546], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6693, -0.6647, -0.6628, -0.6570, -0.6483, -0.6445, -0.6502, -0.6530, -0.6585, -0.6688, -0.6770, -0.6831,
0:         -0.6839, -0.6845, -0.6873, -0.6887, -0.6943, -0.6902, -0.6657, -0.6580, -0.6531, -0.6421, -0.6360, -0.6385,
0:         -0.6383], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0186,  0.1008,  0.1640,  0.0551, -0.0417,  0.0143,  0.0066, -0.0409,  0.0165, -0.0490, -0.1698, -0.0778,
0:          0.0051, -0.0702, -0.1135, -0.0157,  0.1173,  0.0803,  0.0860,  0.0226, -0.0153, -0.0983, -0.1499, -0.0740,
0:         -0.0522], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.0042, -1.0175, -0.9972, -0.9621, -0.9345, -0.9430, -0.9738, -1.0083, -1.0222, -1.0108, -0.9860, -0.9572,
0:         -0.9357, -0.9133, -0.8954, -0.8814, -0.8838, -0.8939, -0.9026, -0.9118, -0.9206, -0.9346, -0.9402, -0.9357,
0:         -0.9100], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1930, -0.1957, -0.2063, -0.2117, -0.2060, -0.2090, -0.2075, -0.2037, -0.1964, -0.1965, -0.1930, -0.2007,
0:         -0.2065, -0.2062, -0.2018, -0.2018, -0.2112, -0.2142, -0.1955, -0.1968, -0.1993, -0.2026, -0.2003, -0.2094,
0:         -0.2069], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.18921460211277008; velocity_v: 0.2611560821533203; specific_humidity: 0.16221342980861664; velocity_z: 0.4331126809120178; temperature: 0.14034639298915863; total_precip: 0.6335394382476807; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19428931176662445; velocity_v: 0.3102566599845886; specific_humidity: 0.19825971126556396; velocity_z: 0.5249847769737244; temperature: 0.15794241428375244; total_precip: 0.6936246156692505; 
0: epoch: 26 [1/5 (20%)]	Loss: 0.66358 : 0.29166 :: 0.20659 (2.57 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20004625618457794; velocity_v: 0.29953625798225403; specific_humidity: 0.19863653182983398; velocity_z: 0.5991550087928772; temperature: 0.15884095430374146; total_precip: 0.7279720306396484; 
0: epoch: 26 [2/5 (40%)]	Loss: 0.72797 : 0.32987 :: 0.20780 (15.88 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21775411069393158; velocity_v: 0.32354897260665894; specific_humidity: 0.20687268674373627; velocity_z: 0.6252899169921875; temperature: 0.16988897323608398; total_precip: 1.004648208618164; 
0: epoch: 26 [3/5 (60%)]	Loss: 1.00465 : 0.38941 :: 0.21092 (15.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18483476340770721; velocity_v: 0.27452561259269714; specific_humidity: 0.2099476158618927; velocity_z: 0.5813540816307068; temperature: 0.16633431613445282; total_precip: 0.7463228106498718; 
0: epoch: 26 [4/5 (80%)]	Loss: 0.74632 : 0.32594 :: 0.20700 (15.73 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [1.43051147e-06 9.53674316e-07 9.53674316e-07 1.43051147e-06
0:  9.53674316e-07 4.29153442e-06 4.29153442e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.86102295e-06
0:  2.86102295e-06 4.76837158e-06 4.76837158e-06 4.76837158e-06
0:  1.23977661e-05 3.71932983e-05 5.29289246e-05 1.38282776e-05
0:  3.62396240e-05 5.76972961e-05 4.72068787e-05 2.28881836e-05
0:  1.00135803e-05 1.33514404e-05 5.24520874e-06 1.47819519e-05
0:  3.05175781e-05 2.52723694e-05 2.19345093e-05 1.81198120e-05
0:  1.81198120e-05 1.47819519e-05 4.86373901e-05 1.85966492e-05
0:  1.81198120e-05 3.05175781e-05 4.95910645e-05 3.95774841e-05
0:  1.33514404e-05 6.19888306e-06 2.38418579e-06 3.81469727e-06
0:  4.76837158e-06 1.23977661e-05 4.29153442e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.76429749e-05 2.90870667e-05 1.85966492e-05
0:  9.53674316e-07 9.53674316e-07 3.33786011e-06 2.86102295e-06
0:  5.24520874e-06 6.67572021e-06 5.72204590e-06 1.43051147e-06
0:  1.04904175e-05 1.43051147e-06 9.53674316e-06 9.53674316e-06
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.43051147e-06 4.76837158e-07 0.00000000e+00 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.95503235e-05 3.00407410e-05 4.14848328e-05
0:  3.33786011e-05 2.47955322e-05 1.52587891e-05 4.76837158e-06
0:  0.00000000e+00 1.95503235e-05 1.90734863e-05 2.43186951e-05
0:  1.71661377e-05 1.66893005e-05 1.90734863e-05 1.09672546e-05
0:  1.23977661e-05 2.57492065e-05 2.71797180e-05 2.14576721e-05
0:  3.05175781e-05 4.14848328e-05 3.76701355e-05 2.52723694e-05
0:  1.62124634e-05 5.72204590e-06 1.43051147e-06 4.76837158e-06
0:  7.15255737e-06 6.19888306e-06 3.33786011e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  9.53674316e-07 0.00000000e+00 4.29153442e-06 4.76837158e-07
0:  8.58306885e-06 1.09672546e-05 1.33514404e-05 9.53674316e-07
0:  2.86102295e-06 4.29153442e-06 3.81469727e-06 5.24520874e-06
0:  6.67572021e-06 1.90734863e-06 2.43186951e-05 2.47955322e-05
0:  3.33786011e-06 2.86102295e-06 3.33786011e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Target values (first 200):
0: [4.76837158e-06 5.72204590e-06 2.86102295e-06 9.53674316e-07
0:  2.38418579e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.33786011e-06
0:  1.04904175e-05 3.09944153e-05 5.05447388e-05 5.72204590e-05
0:  3.43322754e-05 1.90734863e-06 0.00000000e+00 1.90734863e-06
0:  2.86102295e-06 2.38418579e-06 6.67572021e-06 6.48498535e-05
0:  2.43186951e-05 2.71797180e-05 3.00407410e-05 3.05175781e-05
0:  8.39233398e-05 7.20024109e-05 4.29153479e-05 2.86102295e-05
0:  6.29425049e-05 2.14576721e-04 5.05447388e-05 1.19209290e-05
0:  4.76837158e-07 1.90734863e-06 3.33786011e-06 4.76837158e-06
0:  1.47819519e-05 3.62396240e-05 1.33514404e-04 1.97887421e-04
0:  3.49521637e-04 2.02178955e-04 9.91821289e-05 6.53266907e-05
0:  6.81877136e-05 5.57899475e-05 3.86238098e-05 1.52587891e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.57356262e-05 8.10623169e-06 1.90734863e-06 4.76837158e-07
0:  4.29153442e-06 2.86102295e-06 1.90734863e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 3.33786011e-06
0:  4.76837158e-06 1.38282776e-05 2.28881836e-05 2.52723694e-05
0:  7.15255737e-06 1.90734863e-06 4.76837158e-07 1.90734863e-06
0:  3.81469727e-06 2.86102295e-06 4.29153442e-06 3.76701355e-05
0:  1.38282776e-05 1.43051147e-05 2.24113464e-05 5.57899475e-05
0:  6.29425049e-05 6.58035278e-05 1.07288361e-04 9.29832458e-05
0:  4.57763635e-05 1.37805939e-04 3.09944153e-05 9.53674316e-06
0:  4.76837158e-07 9.53674316e-07 1.43051147e-06 2.38418579e-06
0:  1.09672546e-05 3.09944153e-05 9.29832458e-05 2.35080719e-04
0:  2.50816345e-04 1.33514404e-04 3.67164612e-05 2.24113464e-05
0:  2.28881836e-05 1.81198120e-05 1.81198120e-05 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [13.729382  13.503503  13.5461445 13.806433  13.975517  13.902353
0:  14.028275  14.026308  13.861179  13.636379  13.028212  12.229269
0:  11.467957  11.059106  11.155741  11.68      12.340274  12.814667
0:  10.726118  10.513052 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.184, max = 3.187, mean = 0.462
0:          sample (first 20): tensor([0.5901, 0.5707, 0.5743, 0.5967, 0.6112, 0.6049, 0.6157, 0.6156, 0.6014, 0.5821, 0.5299, 0.4612, 0.3959, 0.3608,
0:         0.3691, 0.4141, 0.4708, 0.5115, 0.6747, 0.6494])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.582596 29.15186  29.275719 28.39656  26.593472 24.243832 22.295189
0:  20.62069  19.426521 18.545256 17.489697 16.378754 15.413369 15.065613
0:  15.313442 15.983092 16.882393 17.57486  16.673183 16.994614]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.656764  -6.5038776 -6.026435  -5.2651434 -4.4988155 -3.9482775
0:  -3.585227  -3.361269  -3.5759115 -3.7603207 -4.099352  -4.6900787
0:  -5.0659285 -5.3822727 -5.411213  -5.119624  -4.773877  -4.4066725
0:  -5.7967596 -5.670965 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3336105 -3.5260935 -3.3475819 -2.8567657 -2.4155731 -2.1765041
0:  -2.225194  -2.313263  -2.707304  -3.034606  -3.4386683 -4.1482754
0:  -4.802622  -5.4019737 -5.6971607 -5.4479647 -4.8406887 -4.1069326
0:  -4.927536  -5.114856 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-25.407047 -25.18137  -24.596989 -23.483974 -22.284096 -21.095848
0:  -20.18734  -19.443264 -19.144104 -18.981205 -18.977406 -19.310215
0:  -19.385933 -19.289318 -18.75735  -17.830544 -16.781555 -15.730588
0:  -16.801659 -16.521492]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0055475 3.153531  3.474976  4.0723457 4.7941704 5.4543467 6.051314
0:  6.53426   6.5471983 6.5334115 6.402983  6.132909  6.0631757 5.994822
0:  6.11326   6.3179855 6.5551414 6.7307253 5.17366   5.3605266]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.293894   8.324988   8.706342   9.358343   9.931778  10.257754
0:  10.77072   11.073079  11.234301  11.232903  10.823381  10.146275
0:   9.4922695  9.142127   9.233229   9.702659  10.26712   10.601572
0:   7.760746   7.499382 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.946871  -8.015429  -7.721755  -7.121484  -6.468993  -5.9859753
0:  -5.774248  -5.707382  -6.0766926 -6.4823895 -6.994973  -7.720768
0:  -8.249115  -8.68096   -8.783762  -8.543852  -8.129547  -7.641881
0:  -8.089369  -8.1399   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.040011  3.6024613 3.5961487 3.8038065 3.967317  3.9050252 4.1007843
0:  4.140753  4.2039165 4.2037644 3.8729162 3.380342  2.9715927 2.9403949
0:  3.3635495 4.274759  5.331656  6.23635   4.542201  4.2768717]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.659146   5.9544377  6.43814    6.919644   7.362875   7.7036357
0:   8.368783   9.152054   9.926497  10.663315  11.104031  11.40285
0:  11.731319  12.267944  13.128807  14.215025  15.326405  16.226915
0:  14.375317  14.813889 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.855484 19.956154 20.22829  20.37881  20.146332 19.491997 18.869492
0:  18.420082 18.081629 18.015568 17.975883 17.807512 17.796305 17.85968
0:  18.24168  19.00016  19.887629 20.658665 17.818882 18.184984]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.587098  -4.7374277 -4.5584373 -3.9588366 -3.3210497 -2.8256807
0:  -2.7336826 -2.7529936 -3.2453556 -3.6998138 -4.2009897 -4.968253
0:  -5.6316414 -6.2878046 -6.669695  -6.5240107 -5.944809  -5.119198
0:  -6.1830816 -6.572274 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.100683    2.7876327   2.7282214   2.9637966   3.1168084   3.0500433
0:   2.7298028   2.311583    1.5835881   0.91106796  0.23623466 -0.55181646
0:  -1.157352   -1.5345507  -1.510376   -1.0067596  -0.24372005  0.46451187
0:  -2.4107394  -2.9683852 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.189081  10.250965  10.538737  10.891568  11.077383  10.981562
0:  11.113556  11.125732  11.124447  11.102381  10.766794  10.3344965
0:   9.95601    9.913761  10.27599   11.000402  11.808552  12.4165535
0:  10.817649  10.97707  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.5095305   2.3614488   2.4865317   2.8251803   3.1093056   3.2180564
0:   3.4101934   3.5022957   3.4398904   3.3123698   2.931411    2.3152466
0:   1.7338924   1.3538785   1.2686539   1.5265851   1.8811979   2.104416
0:   0.05746889 -0.20724726]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.66325   1.797535  2.110911  2.4915981 2.8642917 3.0744956 3.5927908
0:  4.007004  4.359865  4.602937  4.482036  4.3515453 4.234769  4.6041493
0:  5.368023  6.446091  7.4974575 8.262876  8.383709  8.706455 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2635498  -3.1550694  -2.73353    -2.0318537  -1.391284   -0.9189353
0:  -0.69799376 -0.61888885 -0.86552334 -1.0345016  -1.1851711  -1.5496111
0:  -1.762435   -1.9857388  -2.0047398  -1.7260494  -1.2471428  -0.635159
0:  -2.680355   -2.873825  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.6897788  -0.808691   -0.6462574  -0.22152805  0.15542555  0.3171544
0:   0.26297474  0.10468817 -0.34593678 -0.70595694 -1.0354133  -1.5093312
0:  -1.7355433  -1.8089237  -1.5744848  -0.92192984 -0.0959897   0.73094845
0:  -0.00191355  0.1043601 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.685135 16.714808 16.899284 17.210142 17.536997 17.713005 18.111546
0:  18.362463 18.491009 18.5122   18.236383 17.799582 17.481773 17.487694
0:  17.921865 18.712152 19.589146 20.302294 18.199    18.026852]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.748289  4.796171  5.039996  5.5845356 6.1228065 6.5248914 6.5917444
0:  6.6289163 6.2487173 5.9686437 5.6863775 5.141986  4.625738  4.0525045
0:  3.7152965 3.866829  4.4864435 5.3527007 4.3640814 4.197888 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.79277277 -0.6738596  -0.30759048  0.26903582  0.70613146  0.8747587
0:   0.6540308   0.36767292 -0.28077745 -0.8363223  -1.397861   -2.1827388
0:  -2.8765311  -3.541615   -3.953545   -3.8886428  -3.488236   -2.917357
0:  -4.528751   -4.664456  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.07717991 -0.10473442  0.11460495  0.68437576  1.336874    1.8260732
0:   2.1470108   2.3754153   2.1647234   1.8753114   1.4299955   0.70920277
0:   0.07001066 -0.43066645 -0.65134573 -0.43047667 -0.02152872  0.42504978
0:  -1.4787989  -2.1753588 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.385973    2.0601907   2.0755677   2.2990255   2.4010527   2.2508726
0:   2.1229215   1.8825378   1.4959459   1.098764    0.50132513 -0.29198694
0:  -0.9897928  -1.3521256  -1.2783995  -0.7597399  -0.12440634  0.44795847
0:  -1.9163771  -2.0015059 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.936272   9.463285   9.102772   8.772387   8.381037   7.8772473
0:   7.5590053  7.222525   6.749453   6.254405   5.465594   4.5704064
0:   3.7386208  3.2105126  2.9581583  2.9754274  2.9482028  2.7746878
0:  -2.6238818 -3.6913962]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8265696  -4.4524784  -4.618471   -4.487376   -4.328535   -4.2973166
0:  -3.8828764  -3.6041508  -3.2274265  -3.0112028  -3.139699   -3.441389
0:  -3.633441   -3.2536106  -2.3002758  -0.82433414  0.7709341   2.1839857
0:   2.1245723   2.2000766 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-19.265848 -19.360044 -19.004292 -18.174112 -17.263412 -16.561525
0:  -16.057331 -15.890743 -16.231382 -16.611238 -17.199905 -17.94772
0:  -18.366993 -18.54003  -18.322386 -17.75674  -17.286554 -16.922943
0:  -17.931004 -18.013535]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.522988  -4.7068133 -4.515762  -3.9427552 -3.3273816 -2.8631167
0:  -2.6555352 -2.5627894 -2.9214864 -3.2738194 -3.7384162 -4.445825
0:  -5.024031  -5.572745  -5.8999705 -5.9454865 -5.8600554 -5.7267747
0:  -6.201179  -6.485646 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.95032644 0.91202927 1.1365838  1.5584307  1.8560171  1.949244
0:  2.0244727  1.9573708  1.7519436  1.5483127  1.1762338  0.6231046
0:  0.21319628 0.1126256  0.48076153 1.3545475  2.4998095  3.596295
0:  5.261286   6.082599  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.723278 35.309944 34.87983  34.69135  34.598946 34.33862  34.705124
0:  34.798378 34.940254 34.733875 33.89914  33.174046 32.511414 32.34505
0:  32.716587 33.312016 33.939846 34.197285 31.607338 31.195206]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.8642952 4.273588  5.045733  5.9359245 6.4648514 6.5203753 6.336274
0:  6.076396  5.5885468 5.1313515 4.46517   3.480298  2.5165834 1.7979574
0:  1.6509576 2.2013888 3.1987038 4.236805  3.329314  3.6586838]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -5.7609444  -6.0041914  -6.002386   -5.7437506  -5.5582695  -5.58469
0:   -5.8660173  -6.279667   -7.0036726  -7.6053247  -8.204254   -8.9975
0:   -9.625167  -10.085324  -10.23142    -9.834266   -9.088378   -8.131348
0:   -9.417215   -9.673127 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.293826  -3.1109934 -2.610653  -1.8303728 -1.1501122 -0.7238631
0:  -0.6026139 -0.6354079 -1.0368562 -1.4116778 -1.8432908 -2.4098005
0:  -2.8292246 -3.0743265 -2.9921942 -2.5346403 -1.9180794 -1.2775578
0:  -2.4392538 -2.441311 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.03408957 -0.12846184  0.04590607  0.45593452  0.7335119   0.76749945
0:   0.57322216  0.3190503  -0.24443483 -0.71204567 -1.2587442  -2.0609932
0:  -2.8116274  -3.4618673  -3.7961159  -3.5812674  -2.997243   -2.2904625
0:  -3.9472666  -4.236315  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.698792  4.586093  4.722075  5.0671425 5.3580146 5.4323735 5.642351
0:  5.789589  5.9198546 6.0253158 5.922668  5.7327104 5.497218  5.5498433
0:  5.895807  6.4997225 7.1795154 7.6285744 4.9588547 4.754396 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.827848 19.112516 19.573668 20.068941 20.428328 20.550499 20.882433
0:  21.074795 21.184542 21.191801 20.913055 20.601147 20.474718 20.781178
0:  21.599855 22.764795 23.998716 24.94132  23.848251 24.060377]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.712778  9.010231  9.415636  9.7308655 9.795252  9.586884  9.54
0:  9.481303  9.410767  9.308199  8.950855  8.473021  8.077905  7.9725847
0:  8.228945  8.7926655 9.440231  9.990362  7.956043  8.25913  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.6377254 1.730689  2.0919209 2.6628218 3.0598435 3.20461   3.1404932
0:  3.043781  2.6763663 2.433272  2.124195  1.6201367 1.2013831 0.8646002
0:  0.8606415 1.2606764 1.895155  2.5093584 1.4792132 1.4777083]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.750076  4.8226724 5.1180477 5.4699664 5.703765  5.727183  5.896175
0:  5.9693723 5.8771424 5.7567306 5.3048244 4.659913  4.0514307 3.7526352
0:  3.7821217 4.143587  4.4979973 4.6482353 2.5777285 2.598264 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.024216  11.019989  11.011793  10.7908325 10.520866  10.199904
0:  10.60915   11.118067  11.798572  12.288225  12.177761  12.021259
0:  11.729026  12.018927  12.730764  13.702192  14.469223  14.748079
0:  14.77869   14.793596 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6915765  -1.6252322  -1.2385554  -0.5869708   0.05391121  0.47832727
0:   0.69345236  0.72879934  0.36908054 -0.00723362 -0.4814453  -1.1139832
0:  -1.5442195  -1.8363814  -1.8411298  -1.5164161  -1.1344166  -0.81524324
0:  -1.849937   -1.8601727 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.57077  16.834589 17.26823  17.714756 17.985065 18.072027 18.29542
0:  18.49737  18.645535 18.827583 18.798016 18.560638 18.407196 18.450235
0:  18.83839  19.506922 20.251701 20.76787  18.52383  18.638084]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.430174 18.444891 18.75655  19.082718 19.236246 19.071054 19.038513
0:  18.960213 18.888172 18.776527 18.42477  17.860054 17.385668 17.08189
0:  17.244524 17.811691 18.620743 19.35542  16.74379  16.729351]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.66951656 -0.7885542  -0.6747031  -0.2870078   0.08428955  0.28998184
0:   0.15364885 -0.01397276 -0.56325626 -0.9632635  -1.2936406  -1.7821937
0:  -2.0911756  -2.4059863  -2.500095   -2.2079492  -1.6335635  -0.93753576
0:  -1.7286611  -1.8820777 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.6336775 10.699743  11.100456  11.585838  11.909501  11.936506
0:  11.948168  11.724701  11.30113   10.824848  10.2338705  9.474155
0:   8.986762   8.746777   8.818201   9.250767   9.732235  10.0999365
0:   6.6520596  6.4271355]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.847173  -7.9683375 -7.641848  -7.023705  -6.4893746 -6.17444
0:  -6.0667176 -6.18072   -6.674129  -7.0827746 -7.5718446 -8.1625
0:  -8.488325  -8.622391  -8.457325  -7.996104  -7.5208545 -7.121945
0:  -7.336986  -7.2536407]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.57129   11.873259  12.293682  12.821506  13.170557  13.335077
0:  13.36565   13.476631  13.390137  13.462822  13.526945  13.287346
0:  13.105312  12.8847885 12.930941  13.425959  14.287914  15.216616
0:  13.057493  13.181503 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.932893  7.0494747 7.4666214 7.9476647 8.090626  7.8201313 7.358079
0:  6.898713  6.359537  6.0073514 5.662451  5.0810304 4.581214  4.1783485
0:  4.1093225 4.5346212 5.241885  5.95095   3.5846999 3.5268397]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.49318    -4.6844916  -4.7146716  -4.665631   -4.592253   -4.5369697
0:  -4.202518   -3.8979707  -3.6436095  -3.4577384  -3.5173335  -3.606607
0:  -3.537005   -2.9900517  -2.052824   -0.9222207   0.06331348  0.7149162
0:   1.0442214   1.2708578 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.626711   6.4143233  6.461122   6.6402216  6.7872996  6.8501534
0:   7.0096374  7.2319374  7.4161963  7.647519   7.7438254  7.635953
0:   7.5895414  7.7336774  8.258279   9.175581  10.299808  11.32148
0:  10.70551   11.2758045]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.232824  -6.2041745 -5.8340144 -5.1150393 -4.3596253 -3.7967925
0:  -3.5183206 -3.3888097 -3.6909451 -3.9637723 -4.295339  -4.840732
0:  -5.2404513 -5.6062636 -5.7468686 -5.538296  -5.161837  -4.7234654
0:  -6.1916947 -6.1462398]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6159663 -5.846581  -5.6649575 -5.0674357 -4.4132953 -3.951552
0:  -3.7934651 -3.8502622 -4.356705  -4.8402257 -5.380482  -6.1195073
0:  -6.6433444 -7.0462613 -7.1510987 -6.824245  -6.3269963 -5.763951
0:  -6.6100497 -6.532315 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.81824  37.774166 37.6771   37.74983  37.79082  37.679955 38.18149
0:  38.53128  39.097767 39.42683  39.242622 39.062405 39.09924  39.43641
0:  40.315483 41.336243 42.32536  42.91603  41.817657 41.951347]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.55019    1.53691    1.8437214  2.3984313  2.8618007  3.0960128
0:  3.2092433  3.2169645  2.9399781  2.6825004  2.2714329  1.6115408
0:  1.0470815  0.61742115 0.4823537  0.7790408  1.2764616  1.7598605
0:  0.15230894 0.08517075]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.803531  8.68925   8.735407  8.7777195 8.648859  8.315244  8.216492
0:  8.090822  7.987959  7.877626  7.4636455 6.869956  6.273686  6.042266
0:  6.1936088 6.713366  7.344244  7.782173  5.947583  6.1146894]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.149968   7.5626926  8.329524   9.236234   9.941242  10.362036
0:  10.68317   10.971157  11.087641  11.218905  11.234194  10.998821
0:  10.8862705 10.917466  11.32893   12.083965  12.968494  13.678148
0:  11.886374  12.391713 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.291147  -9.274749  -8.917117  -8.024928  -6.95656   -5.9488783
0:  -5.215781  -4.5525846 -4.4489546 -4.363158  -4.4324875 -4.8352833
0:  -5.099513  -5.4093323 -5.366023  -5.0136976 -4.4743223 -3.915801
0:  -5.0233326 -4.945201 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.672882  11.918701  12.461269  13.062305  13.551648  13.858035
0:  14.289727  14.7494335 15.154574  15.547606  15.794424  15.864068
0:  16.017464  16.342773  17.01918   17.898659  18.770138  19.267666
0:  16.489141  16.799982 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.712715 33.364197 32.621918 31.86744  31.198093 30.401272 30.616753
0:  30.874783 31.381714 31.575287 30.943417 30.5127   29.959534 30.105553
0:  30.850582 31.768143 32.547768 32.581917 34.275246 34.243732]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.69838  27.661667 27.635612 27.507238 27.263798 26.85545  26.834276
0:  26.837788 26.92457  26.974005 26.62853  26.093842 25.602505 25.382132
0:  25.662298 26.317028 27.208693 27.938147 25.144503 25.287968]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.568016 24.460194 24.464008 24.459263 24.229874 23.752396 23.587978
0:  23.339169 23.044281 22.449032 21.328228 19.817444 18.301838 17.138512
0:  16.517834 16.3141   16.293598 16.080688 12.307881 11.758835]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.531271 18.859406 19.400232 19.954216 20.355196 20.505621 20.730743
0:  20.830425 20.823717 20.730492 20.340206 19.7052   19.040182 18.479906
0:  18.186138 18.119064 18.1357   18.030176 14.807186 14.804304]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9195848 -3.905599  -3.5429444 -2.8194861 -2.073339  -1.5421395
0:  -1.3802133 -1.4931016 -2.1486259 -2.760944  -3.4118552 -4.1544127
0:  -4.6331286 -4.93475   -4.8749394 -4.386432  -3.6441712 -2.8152747
0:  -2.1816459 -2.1650481]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5124454 1.4112091 1.6093779 1.9444456 2.2331185 2.3407927 2.7564943
0:  3.1475852 3.5468676 3.882527  3.901382  3.7089067 3.5278037 3.708757
0:  4.2097993 5.025617  5.8229365 6.309196  4.278948  4.2806253]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4328084  -2.4003167  -2.4850965  -2.6681828  -2.9082885  -3.259107
0:  -2.787127   -2.2836766  -1.6870947  -1.3499889  -1.6835546  -2.1248488
0:  -2.6932387  -2.4344716  -1.6138487  -0.4534421   0.46662712  0.7034521
0:   2.1206589   2.101893  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.890038  -8.958929  -8.615335  -7.833155  -6.972138  -6.281286
0:  -5.9781766 -5.865212  -6.2704453 -6.697724  -7.162788  -7.840469
0:  -8.28186   -8.620939  -8.660353  -8.28124   -7.670978  -6.9481587
0:  -7.789786  -7.9422593]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.297256   -7.2835317  -6.887939   -6.243322   -5.763922   -5.6375175
0:   -5.847839   -6.4253287  -7.467034   -8.540291   -9.730343  -10.976818
0:  -11.841822  -12.369793  -12.472242  -12.203184  -11.880316  -11.591429
0:  -14.733625  -15.000824 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5949383  -0.69510365 -0.48730755 -0.04949903  0.49456882  0.90298176
0:   1.674685    2.4057078   3.1819482   3.943925    4.416547    4.7779784
0:   5.0821514   5.686317    6.5155067   7.588809    8.612541    9.389379
0:   6.0239053   5.5904646 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.687702 23.421738 23.181562 22.93692  22.636831 22.090538 21.984385
0:  21.710514 21.482327 21.140575 20.407778 19.696213 19.051815 18.921833
0:  19.308949 20.147436 21.177958 22.01148  21.524765 21.429783]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.774641 22.816654 23.021782 23.181114 23.123684 22.756737 22.505184
0:  22.176712 21.823086 21.462133 20.884888 20.20279  19.653866 19.323565
0:  19.404467 19.765839 20.247955 20.553154 17.99165  18.03042 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.750535 11.764038 12.022656 12.387875 12.738609 12.89768  13.057546
0:  13.025307 12.750769 12.494729 12.127752 11.689314 11.415679 11.331491
0:  11.61663  12.235476 12.977945 13.667715 12.909927 12.9659  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.083178 23.671259 24.410711 25.02229  25.456291 25.74759  26.09874
0:  26.516584 26.767752 26.948599 26.852938 26.587551 26.397913 26.366495
0:  26.769714 27.348398 27.878515 28.14568  25.870441 25.906464]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.490501  10.522418  10.817644  11.145433  11.255186  11.094031
0:  10.9065    10.65275   10.306959  10.0096855  9.606659   9.047474
0:   8.660267   8.470294   8.725222   9.404764  10.2977    11.120292
0:   9.40329    9.571809 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.873716 16.285328 16.812723 17.400898 17.734528 17.832392 17.652475
0:  17.510664 17.16953  16.97927  16.81937  16.5014   16.259281 16.095705
0:  16.30568  16.932838 17.799215 18.602764 16.832495 16.842031]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.303112 16.983826 16.90355  16.912275 16.804234 16.409943 16.249483
0:  15.948826 15.663525 15.400957 14.88043  14.296217 13.769423 13.567406
0:  13.795013 14.3988   15.120878 15.756189 14.030479 13.848545]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.023161 14.602342 14.378673 14.288558 14.139428 13.771776 13.880861
0:  13.935454 14.039673 14.029016 13.527336 12.870916 12.12447  11.809729
0:  12.024448 12.674763 13.442768 13.987383 11.874506 11.675112]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.640129  7.2557154 6.9679646 6.7645283 6.4981384 6.173761  5.924615
0:  5.755461  5.472073  5.295064  5.059642  4.696129  4.48001   4.395664
0:  4.566944  4.9815054 5.44223   5.7995315 4.560036  4.6964035]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9520485 3.78152   3.6746657 3.5098684 3.2159386 2.822802  2.8628008
0:  2.9827087 3.1405284 3.2307577 2.883902  2.3027549 1.5961761 1.3959403
0:  1.6602974 2.2621548 2.867354  3.1275172 2.394967  2.7040808]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.049367  6.4968314 7.131471  7.701766  8.096269  8.329393  8.668341
0:  8.959227  9.0979805 9.147227  8.898689  8.471134  8.162781  8.074865
0:  8.306783  8.671447  8.97265   9.070196  5.323346  5.502988 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6552286  -2.7471676  -2.569765   -2.1101065  -1.5888195  -1.1893053
0:  -0.86664486 -0.74065876 -0.9898844  -1.2714043  -1.6953726  -2.1986828
0:  -2.4969273  -2.5643358  -2.3184958  -1.8392344  -1.4200106  -1.1902437
0:  -4.2508287  -4.377054  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.824362 12.14775  12.707632 13.260483 13.550552 13.567453 13.545357
0:  13.602064 13.619326 13.743164 13.771675 13.539471 13.329823 13.17371
0:  13.36096  13.923233 14.735834 15.510138 13.554602 13.948849]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.077392  13.5588875 14.119175  14.662172  14.969446  14.970766
0:  15.1071205 15.29211   15.538893  15.797873  15.91662   15.852749
0:  15.864838  16.117443  16.74584   17.69734   18.759335  19.561203
0:  17.385477  17.637173 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.758183   -8.739874   -8.313557   -7.5122247  -6.719506   -6.1385455
0:   -5.920812   -5.857884   -6.1985908  -6.534118   -6.938825   -7.5912204
0:   -8.119497   -8.605366   -8.863369   -8.73111    -8.363991   -7.900766
0:   -9.828337  -10.18754  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.082563   8.514473   9.172376   9.834646  10.36854   10.709371
0:  11.029238  11.24076   11.2689    11.267509  11.118647  10.852035
0:  10.765831  10.853489  11.25016   11.830904  12.383706  12.7467985
0:  10.51143   10.97172  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.55246544 -0.4897623  -0.0756073   0.58216906  1.1680331   1.601232
0:   2.004435    2.2943974   2.2939215   2.3079438   2.1135702   1.6939626
0:   1.4325867   1.3288074   1.6089306   2.2197309   2.9075508   3.4872224
0:   2.4389853   2.6497273 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.181718 14.057098 14.115081 14.26735  14.358732 14.241598 14.426285
0:  14.519183 14.601204 14.63739  14.378771 14.050083 13.757183 13.799854
0:  14.28953  15.139718 16.117563 16.926714 16.038052 16.058735]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.038038 19.045498 19.231497 19.500168 19.702806 19.746902 20.007938
0:  20.26295  20.498596 20.724026 20.700468 20.534866 20.485338 20.782722
0:  21.520355 22.612175 23.790096 24.668968 21.306862 21.121672]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.997004 15.343355 15.840757 16.352892 16.597391 16.606047 16.504787
0:  16.511944 16.291105 16.140928 15.825491 15.167597 14.548195 13.935544
0:  13.685951 13.878991 14.385883 14.873735 12.136152 12.214306]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.703508  8.412332  8.238084  8.089611  7.897246  7.637108  7.7312384
0:  7.8638654 7.9626083 7.990646  7.638972  7.1183386 6.5732145 6.4313993
0:  6.6645074 7.1495376 7.5646915 7.71832   5.0897627 4.976731 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.201931  -4.6288013 -4.7503347 -4.4916945 -4.1902103 -4.0222425
0:  -4.277424  -4.5790462 -5.335251  -6.003623  -6.62665   -7.485813
0:  -8.167214  -8.773333  -9.048333  -8.773529  -8.078122  -7.1903324
0:  -7.6696196 -8.004095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.10006666 -0.14387274  0.13349819  0.73286724  1.3251672   1.7942576
0:   1.9051895   1.9751787   1.6183915   1.296413    0.91387796  0.23063183
0:  -0.40803528 -1.0235491  -1.3257465  -1.079278   -0.37166882  0.53819036
0:  -1.1342659  -1.1784339 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.5092463  -2.236874   -1.641511   -0.78623295 -0.0228548   0.490108
0:   0.7432494   0.8250575   0.5081825   0.16768885 -0.30387783 -1.0248518
0:  -1.5988011  -2.062254   -2.2047548  -1.9152207  -1.4531031  -0.9929848
0:  -2.4562306  -2.4129872 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.746696  12.647594  12.780782  13.014544  13.1174965 12.965609
0:  13.020374  12.994368  12.924876  12.831867  12.411518  11.811054
0:  11.182242  10.791316  10.75804   11.137388  11.670889  12.078518
0:  10.093538  10.046958 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.037647  5.0570793 5.270665  5.6511064 5.935811  6.0361576 5.9898615
0:  5.86491   5.4227867 5.079445  4.630702  4.0226183 3.5137212 3.1216033
0:  3.089582  3.5171273 4.2393804 5.0369525 3.8116856 3.8952377]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.2541685 13.314227  13.53653   13.783676  13.8866205 13.73921
0:  13.692551  13.545844  13.389273  13.267726  12.969363  12.514963
0:  12.178991  12.05895   12.319769  12.9699745 13.780389  14.528938
0:  12.568295  12.591442 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1369333  -3.2144032  -3.04663    -2.6634607  -2.322855   -2.172965
0:  -1.8750591  -1.6589184  -1.5455618  -1.5235481  -1.7332697  -2.1199045
0:  -2.3292437  -2.2253027  -1.7747192  -0.9901457  -0.23766422  0.35540915
0:  -1.1527443  -1.1793919 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.838152 30.150976 30.433924 30.698496 30.931133 30.947237 31.570677
0:  31.98972  32.49101  32.798878 32.46825  32.26775  32.15825  32.534237
0:  33.38224  34.3609   35.210003 35.683075 34.226707 34.40524 ]
0: validation loss for strategy=forecast at epoch 26 : 0.300464928150177
0: validation loss for velocity_u : 0.15846166014671326
0: validation loss for velocity_v : 0.24533827602863312
0: validation loss for specific_humidity : 0.16867126524448395
0: validation loss for velocity_z : 0.5120459198951721
0: validation loss for temperature : 0.10867181420326233
0: validation loss for total_precip : 0.6096010804176331
0: 27 : 19:45:49 :: batch_size = 96, lr = 1.0787811788985607e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 27, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0290, -1.0510, -1.0851, -1.1758, -1.2811, -1.3355, -1.3386, -1.3314, -1.3408, -1.3589, -1.3704, -1.3745,
0:         -1.3626, -1.3416, -1.3106, -1.2710, -1.2692, -1.2993, -1.0418, -1.0525, -1.0969, -1.1578, -1.2084, -1.2143,
0:         -1.1799], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3713, 0.3383, 0.3603, 0.4399, 0.5122, 0.5435, 0.5629, 0.5909, 0.6252, 0.6669, 0.6757, 0.6278, 0.5486, 0.5098,
0:         0.5148, 0.5055, 0.4925, 0.4574, 0.3273, 0.2673, 0.2755, 0.3659, 0.4725, 0.5312, 0.5447], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.9317, 3.0646, 3.2283, 3.2556, 3.1446, 3.1656, 3.2732, 3.3116, 3.3207, 3.2311, 3.0052, 3.0282, 3.0923, 2.7753,
0:         1.9856, 1.8424, 1.8656, 1.7730, 3.0439, 3.1788, 3.2941, 3.3057, 3.2798, 3.4144, 3.5941], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5736,  0.5892,  0.7115,  0.5135,  0.5714,  0.5914,  0.7761,  0.6058,  0.1886,  0.0941,  0.5035,  0.2743,
0:         -0.7002,  0.0640,  0.4312, -0.3420,  0.2854, -0.2730,  0.2509,  0.4423,  0.3833,  0.1831,  0.1174, -0.2352,
0:         -0.4310], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.9053, -0.4659, -0.3105, -0.3777, -0.2465,  0.0748,  0.3754,  0.4807,  0.4452,  0.3811,  0.3040,  0.0260,
0:         -0.1026,  0.1347,  0.3790,  0.4749,  0.5019,  0.9397,  1.0333,  0.2786, -0.0058,  0.1512,  0.1835,  0.1867,
0:          0.1738], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1477, -0.1442, -0.0546,  0.1694,  0.4724,  0.0916,  0.1116,  1.1244,  1.1998, -0.0475, -0.0522, -0.0369,
0:          0.1258,  0.2531,  0.0916,  0.2248,  1.3896,  1.5158,  0.1553,  0.3616,  0.6528,  0.0586,  0.1906,  0.3156,
0:          0.7565], device='cuda:0')
0: [DEBUG] Epoch 27, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  2.3729,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.9051,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0499,     nan,  0.2873,     nan,
0:             nan,  1.1739,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.5311,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0181,     nan,     nan,
0:             nan,     nan,  0.0444,     nan,     nan, -0.0322,     nan,     nan,     nan,     nan,     nan,  0.1140,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0303,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1100,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2314,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0640,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0197,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1949,     nan,     nan, -0.0817,  0.3274,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0845,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0845,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.2154,     nan,     nan, -0.2185,     nan, -0.0581,     nan,  0.0185,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 27, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2040, -1.2154, -1.1913, -1.1264, -1.0655, -1.0319, -1.0487, -1.0771, -1.1420, -1.1985, -1.2619, -1.3368,
0:         -1.4073, -1.4762, -1.5206, -1.5193, -1.4802, -1.4249, -1.0927, -1.2096, -1.2640, -1.2423, -1.1895, -1.1328,
0:         -1.0992], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3332, 0.3392, 0.3306, 0.3167, 0.2909, 0.2651, 0.2333, 0.2220, 0.2329, 0.2555, 0.2745, 0.2892, 0.2787, 0.2494,
0:         0.2173, 0.2133, 0.2424, 0.2838, 0.3313, 0.3493, 0.3507, 0.3326, 0.2984, 0.2494, 0.2130], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 2.9159,  2.8882,  2.8362,  2.7578,  2.6567,  2.5351,  2.4319,  2.3151,  2.2064,  2.0698,  1.8688,  1.5484,
0:          1.1502,  0.6915,  0.2276, -0.0784, -0.2207, -0.1817,  2.7934,  2.7708,  2.7499,  2.7011,  2.6506,  2.5864,
0:          2.5096], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4380, -0.4290, -0.0031,  0.0585,  0.1503,  0.3154, -0.3202, -0.6301, -0.0746,  0.0916,  0.1892,  0.0448,
0:         -0.5410,  0.0134,  0.1271, -0.5271,  0.1923,  0.3789, -0.0761, -0.4151, -0.1529, -0.0413,  0.2543,  0.4825,
0:          0.0667], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.8621, -1.7857, -1.6812, -1.6034, -1.5685, -1.5897, -1.6595, -1.7677, -1.8894, -2.0005, -2.0616, -2.0238,
0:         -1.8520, -1.5719, -1.2380, -0.9350, -0.7173, -0.5972, -0.5602, -0.5885, -0.6636, -0.7610, -0.8406, -0.8512,
0:         -0.7785], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([1.0735, 1.0604, 1.0170, 0.9513, 0.8903, 0.8274, 0.7775, 0.7684, 0.7805, 0.9708, 0.9512, 0.9034, 0.8686, 0.8053,
0:         0.7551, 0.7496, 0.7566, 0.7777, 0.8047, 0.7542, 0.7168, 0.7055, 0.6805, 0.7101, 0.7136], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.17827749252319336; velocity_v: 0.2795594334602356; specific_humidity: 0.19077302515506744; velocity_z: 0.5115177035331726; temperature: 0.14661702513694763; total_precip: 0.5145494341850281; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19913077354431152; velocity_v: 0.3544187545776367; specific_humidity: 0.1794188916683197; velocity_z: 0.6914628148078918; temperature: 0.1603759527206421; total_precip: 0.7044046521186829; 
0: epoch: 27 [1/5 (20%)]	Loss: 0.60948 : 0.30816 :: 0.20721 (2.61 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21515192091464996; velocity_v: 0.33296412229537964; specific_humidity: 0.16858907043933868; velocity_z: 0.5625938177108765; temperature: 0.14598701894283295; total_precip: 0.7490204572677612; 
0: epoch: 27 [2/5 (40%)]	Loss: 0.74902 : 0.32825 :: 0.21186 (15.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21060778200626373; velocity_v: 0.29184967279434204; specific_humidity: 0.20392045378684998; velocity_z: 0.616032063961029; temperature: 0.1557741016149521; total_precip: 0.5041928291320801; 
0: epoch: 27 [3/5 (60%)]	Loss: 0.50419 : 0.29614 :: 0.20878 (16.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23527970910072327; velocity_v: 0.26652270555496216; specific_humidity: 0.20262104272842407; velocity_z: 0.4889642596244812; temperature: 0.17915716767311096; total_precip: 0.4815872311592102; 
0: epoch: 27 [4/5 (80%)]	Loss: 0.48159 : 0.27401 :: 0.20836 (15.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [7.62939453e-06 6.67572021e-06 5.72204590e-06 5.24520874e-06
0:  6.19888306e-06 9.53674316e-06 1.62124634e-05 3.05175781e-05
0:  3.24249268e-05 2.14576721e-05 2.76565552e-05 3.19480896e-05
0:  2.90870667e-05 1.85966492e-05 1.47819519e-05 1.23977661e-05
0:  1.23977661e-05 2.19345093e-05 3.57627869e-05 4.81605530e-05
0:  5.53131104e-05 3.00407410e-05 1.28746033e-05 8.10623169e-06
0:  2.86102295e-06 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  9.53674316e-07 1.43051147e-06 1.90734863e-06 2.86102295e-06
0:  4.29153442e-06 6.19888306e-06 1.57356262e-05 3.29017639e-05
0:  9.44137573e-05 1.01566315e-04 8.82148743e-05 1.52587891e-04
0:  2.07424164e-04 1.76429749e-04 1.64508820e-04 1.11103058e-04
0:  1.32560730e-04 2.05039978e-04 1.74045563e-04 1.05857849e-04
0:  8.48770142e-05 5.81741333e-05 1.00612640e-04 1.41143799e-04
0:  1.38282776e-04 1.87397003e-04 1.93595886e-04 1.82628632e-04
0:  2.14099884e-04 2.52723694e-04 2.19821930e-04 1.42097473e-04
0:  1.26361847e-04 1.31130219e-04 1.23977661e-04 1.40190125e-04
0:  9.05990601e-06 8.10623169e-06 8.10623169e-06 7.15255737e-06
0:  8.58306885e-06 1.38282776e-05 2.24113464e-05 3.76701391e-05
0:  3.86238098e-05 3.09944153e-05 4.00543213e-05 4.81605530e-05
0:  3.57627869e-05 2.14576721e-05 1.71661377e-05 1.57356262e-05
0:  1.57356262e-05 2.28881836e-05 3.43322754e-05 3.95774805e-05
0:  4.72068787e-05 2.67028809e-05 1.09672546e-05 5.24520874e-06
0:  1.43051147e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  9.53674316e-07 1.90734863e-06 2.38418579e-06 3.81469727e-06
0:  5.72204590e-06 8.58306885e-06 1.28746033e-05 1.90734863e-05
0:  2.67028809e-05 4.91142273e-05 7.15255737e-05 1.02996826e-04
0:  1.24931335e-04 1.24931335e-04 1.33991241e-04 1.02043152e-04
0:  1.23023987e-04 1.39713287e-04 1.51157379e-04 1.79290771e-04]
0: Target values (first 200):
0: [1.62124634e-05 8.58306885e-06 4.29153442e-06 4.76837158e-06
0:  6.67572021e-06 1.28746033e-05 1.33514404e-05 1.52587891e-05
0:  1.71661377e-05 2.00271606e-05 2.14576721e-05 2.24113464e-05
0:  2.33650208e-05 2.47955322e-05 2.52723694e-05 2.67028809e-05
0:  2.81333923e-05 2.90870667e-05 2.86102295e-05 2.71797180e-05
0:  2.47955322e-05 2.09808350e-05 1.71661377e-05 1.28746033e-05
0:  1.00135803e-05 8.58306885e-06 8.10623169e-06 7.15255737e-06
0:  7.62939453e-06 9.05990601e-06 1.14440918e-05 1.47819519e-05
0:  2.86102295e-05 1.04427338e-04 1.37805939e-04 1.00612640e-04
0:  7.91549683e-05 6.43730164e-05 6.34193420e-05 5.34057617e-05
0:  5.14984131e-05 5.38825989e-05 5.29289246e-05 5.29289246e-05
0:  5.24520874e-05 5.34057617e-05 5.57899475e-05 6.15119934e-05
0:  6.43730164e-05 7.67707825e-05 9.34600830e-05 1.11579895e-04
0:  1.40190125e-04 1.56402588e-04 1.55448914e-04 1.42574310e-04
0:  1.10149384e-04 1.10626221e-04 1.01089478e-04 7.48634338e-05
0:  6.00814819e-05 4.43458594e-05 4.05311584e-05 3.57627869e-05
0:  3.48091125e-05 3.52859497e-05 3.91006470e-05 7.29560852e-05
0:  1.04904175e-04 1.27315521e-04 1.58309937e-04 1.47819519e-04
0:  1.50680542e-04 1.48296356e-04 2.15053558e-04 2.98976898e-04
0:  4.21524048e-04 5.56945801e-04 7.23838806e-04 7.91549683e-04
0:  7.24315643e-04 6.93321228e-04 6.61849976e-04 5.99384308e-04
0:  4.69207764e-04 3.88622284e-04 3.17096710e-04 3.04222136e-04
0:  2.93254852e-04 2.54631042e-04 1.58786774e-04 1.05857849e-04
0:  7.91549683e-05 8.05854797e-05 1.14917755e-04 1.04427338e-04
0:  6.10351562e-05 5.91278076e-05 6.15119934e-05 5.86509705e-05
0:  6.15119934e-05 6.96182251e-05 1.10626221e-04 1.84059143e-04
0:  2.92301178e-04 7.69615173e-04 1.06048584e-03 1.26171112e-03
0:  8.10623169e-06 6.19888306e-06 3.81469727e-06 5.72204590e-06
0:  1.19209290e-05 1.71661377e-05 2.14576721e-05 1.71661377e-05
0:  1.71661377e-05 1.81198120e-05 1.95503235e-05 2.09808350e-05
0:  2.33650208e-05 2.52723694e-05 2.71797180e-05 2.90870667e-05
0:  3.05175781e-05 3.05175781e-05 2.95639038e-05 2.62260437e-05
0:  2.33650208e-05 1.90734863e-05 1.52587891e-05 1.19209290e-05
0:  9.53674316e-06 8.10623169e-06 7.62939453e-06 7.15255737e-06
0:  8.10623169e-06 9.53674316e-06 1.23977661e-05 3.00407410e-05
0:  1.03950500e-04 1.35421753e-04 9.01222229e-05 6.48498535e-05
0:  5.10215759e-05 5.00679016e-05 5.86509705e-05 5.19752502e-05
0:  5.05447388e-05 5.14984131e-05 5.24520874e-05 5.48362732e-05
0:  5.76972961e-05 6.00814819e-05 6.72340393e-05 7.67707825e-05
0:  8.53538513e-05 9.53674316e-05 1.13964081e-04 1.27792358e-04
0:  1.66416168e-04 1.78337097e-04 1.74522400e-04 1.56402588e-04
0:  1.02996826e-04 1.23500824e-04 1.19209290e-04 1.02043152e-04
0:  8.39233398e-05 6.62803650e-05 5.43594360e-05 4.72068787e-05
0:  4.57763672e-05 4.91142273e-05 5.29289246e-05 6.29425049e-05
0:  7.72476196e-05 8.58306885e-05 1.28746033e-04 1.52587891e-04
0:  1.68800354e-04 1.96456909e-04 2.31742859e-04 2.92778015e-04
0:  3.60965729e-04 4.82082367e-04 5.85079193e-04 6.48975372e-04
0:  7.03811646e-04 6.86645508e-04 7.15255737e-04 7.55786896e-04
0:  6.27517700e-04 5.04016876e-04 4.26292419e-04 3.70979309e-04
0:  3.71932983e-04 2.94685364e-04 1.67369843e-04 1.05857849e-04]
0: Prediction values (first 20):
0: [41.288105 41.183083 40.9024   40.597202 40.347694 39.91923  40.200684
0:  40.382397 40.7252   40.841206 40.30976  39.89762  39.514404 39.646236
0:  40.257404 40.981956 41.618885 41.711193 40.35284  40.413063]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.362, max = 3.670, mean = 1.075
0:          sample (first 20): tensor([2.9724, 2.9632, 2.9384, 2.9116, 2.8896, 2.8519, 2.8766, 2.8926, 2.9228, 2.9331, 2.8862, 2.8499, 2.8162, 2.8278,
0:         2.8816, 2.9455, 3.0016, 3.0097, 2.9650, 3.0206])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.187746   9.009192   8.561614   8.045479   7.4738927  6.8704658
0:   7.14003    7.328459   7.556412   7.469948   6.678627   5.8075237
0:   4.9059973  5.033655   5.910463   7.3186865  8.662869   9.328215
0:   9.807976  10.03799  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.342224 37.111637 36.47599  35.666664 34.893448 33.95845  34.15625
0:  34.374813 34.81629  34.911316 34.04421  33.366825 32.54054  32.509357
0:  33.125782 33.995266 34.698242 34.60797  35.986477 36.04847 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.4807277  7.9025173  8.5567     9.26351    9.739449  10.006137
0:  10.133992  10.424783  10.58094   10.9047165 11.189238  11.121714
0:  11.051822  10.949411  11.119916  11.797962  12.8677635 13.954425
0:  11.836292  12.085781 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1519017  -3.294973   -3.0023732  -2.4799647  -1.9785838  -1.610868
0:  -1.1102238  -0.8183112  -0.7810612  -0.81116104 -1.1341801  -1.6327634
0:  -1.8922877  -1.7757249  -1.3199573  -0.6015067  -0.07404852  0.1489892
0:  -2.588163   -2.6436276 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.533769  7.41558   7.5310116 7.729355  7.6917157 7.342067  6.9467807
0:  6.502667  5.9314213 5.426401  4.8215933 4.023546  3.3335824 2.849718
0:  2.7504315 3.1413753 3.8038507 4.4476247 2.4562163 2.5270686]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.0040026 0.9845128 1.2995343 1.8713531 2.414274  2.8621237 3.2706418
0:  3.597526  3.603872  3.4781196 3.0638368 2.3531592 1.772429  1.4209218
0:  1.566441  2.2016459 3.028259  3.7345972 2.3631353 2.4790668]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.8661795 5.9598465 6.2916737 6.671978  6.850105  6.718983  6.724621
0:  6.7076077 6.7469277 6.8675294 6.836251  6.6797376 6.555145  6.6637444
0:  7.059964  7.7703047 8.559665  9.190992  7.115858  7.3126793]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1082935 6.00236   6.1228967 6.3230386 6.434231  6.3484936 6.53857
0:  6.693837  6.8673115 6.978919  6.8371716 6.5076685 6.277179  6.37633
0:  6.8236785 7.6295886 8.4265175 9.017888  6.35739   6.240418 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.987329    4.6986628   4.6743636   4.8015833   4.79173     4.5718284
0:   4.3093977   4.043089    3.601628    3.1958075   2.6700394   1.953217
0:   1.3398061   0.896348    0.77641296  1.0036988   1.3228602   1.5604949
0:  -1.8703842  -2.2438822 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.170197  11.270828  11.460498  11.640765  11.709707  11.5684595
0:  11.670494  11.660885  11.62187   11.490051  10.999212  10.377846
0:   9.785894   9.602429   9.771554  10.280724  10.780715  10.989372
0:   8.3659315  8.394569 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2481174  3.03138    3.0837898  3.3347564  3.5692816  3.6616037
0:  3.6389294  3.450057   2.9332767  2.3770108  1.7023158  0.93970585
0:  0.395442   0.18878984 0.36747265 0.86835957 1.3836598  1.7446017
0:  1.950469   1.9863849 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.667831  -4.848994  -4.6176295 -3.9923043 -3.308044  -2.8054128
0:  -2.550815  -2.5712008 -3.1021981 -3.7274852 -4.51405   -5.481238
0:  -6.195291  -6.7502513 -6.9496484 -6.7408595 -6.316549  -5.8672843
0:  -7.952128  -8.170013 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.677763 23.546911 23.325577 23.021313 22.817919 22.586168 23.323107
0:  23.986546 24.81891  25.401255 25.249022 25.182724 25.045767 25.595531
0:  26.646965 27.971828 29.092476 29.564512 28.8125   29.1405  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.51279  26.27174  27.265732 28.282888 28.912848 28.853233 28.572998
0:  27.885529 26.92313  25.912708 24.716425 23.567654 22.804136 22.459833
0:  22.748459 23.485035 24.391127 25.171982 24.67821  25.375282]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6860738  -1.6123672  -1.1906705  -0.5792413  -0.14091682 -0.04619408
0:  -0.22803402 -0.5609579  -1.1623402  -1.715467   -2.301948   -3.1049724
0:  -3.738669   -4.180673   -4.2828784  -3.8404412  -3.1277251  -2.3311105
0:  -3.332151   -3.14107   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.052338 15.608986 16.378244 17.083895 17.577703 17.89588  18.343718
0:  18.873253 19.418495 19.925106 20.237934 20.330618 20.581284 21.013119
0:  21.840237 22.937962 24.125101 25.130417 21.251596 21.62176 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.882204 12.007326 12.316229 12.716914 12.895126 12.734665 12.51775
0:  12.166828 11.689506 11.20213  10.544792  9.731778  9.004595  8.510522
0:   8.477568  8.910613  9.630922 10.323679  8.509076  8.565342]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.860996  8.691641  8.709602  8.92438   9.033648  8.945278  8.533934
0:  8.0889015 7.2643785 6.535244  5.800505  4.888916  4.053875  3.2596064
0:  2.7671094 2.7124386 3.0230715 3.47061   1.6383018 0.9786954]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8361983  -2.6099162  -2.3293242  -1.9540372  -1.621594   -1.4620147
0:  -0.95288324 -0.47770786 -0.14288902  0.12628984  0.04352236 -0.1542635
0:  -0.21567345  0.22328424  1.1629024   2.4055676   3.626322    4.4591503
0:   4.2873025   4.668207  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.02013206  0.08069706  0.51146746  1.1527705   1.6706901   1.9660864
0:   2.138588    2.221558    2.0177798   1.8072252   1.4317966   0.81956387
0:   0.3460846   0.0780735   0.13793182  0.6094599   1.188128    1.6587563
0:  -0.8197155  -0.9195075 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.178409  -6.1285176 -5.719764  -5.0024657 -4.269481  -3.711698
0:  -3.4140716 -3.241572  -3.524118  -3.8031783 -4.180122  -4.7734647
0:  -5.139153  -5.437142  -5.4018817 -5.0158677 -4.4993434 -3.9520907
0:  -5.165793  -5.189633 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.149908 14.384542 14.849481 15.317826 15.528595 15.395401 15.368838
0:  15.159694 14.921148 14.610971 14.01683  13.315098 12.720795 12.465901
0:  12.69445  13.377657 14.27743  15.090924 13.369932 13.484997]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1866088   1.1215258   1.3196087   1.6538706   1.8274388   1.7575183
0:   1.710866    1.5521755   1.230691    0.9097862   0.38811398 -0.35541487
0:  -0.9645395  -1.2370033  -1.1039872  -0.5565634   0.09317017  0.64806557
0:  -0.84000015 -0.79671097]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.144564  8.477383  8.839719  9.1914215 9.417904  9.408249  9.430189
0:  9.3081    8.916774  8.509546  7.8964233 7.235281  6.771836  6.6766014
0:  6.9704323 7.5539813 8.193553  8.713808  7.643552  7.90238  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2258964  -1.2065487  -0.8265004  -0.17292929  0.44688225  0.8796754
0:   1.148829    1.2804251   1.0909686   0.91256285  0.5982895   0.02491283
0:  -0.4711342  -0.8698349  -1.0165462  -0.7383671  -0.3009243   0.19974041
0:  -1.7516942  -1.7378726 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.719892  13.062302  13.609194  14.180052  14.628091  14.865232
0:  15.079079  15.230758  15.180771  15.160008  14.996941  14.699397
0:  14.558284  14.53175   14.807504  15.291622  15.8032055 16.155445
0:  13.650567  14.01226  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.765617 33.717472 33.41528  32.91745  32.483032 31.915756 32.19693
0:  32.390285 32.676617 32.70585  32.02781  31.403177 30.801838 30.786419
0:  31.318037 32.17904  33.047615 33.534386 33.72032  33.76806 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.252501 10.200112 10.237686 10.20101   9.98296   9.564324  9.570136
0:   9.596886  9.736423  9.909728  9.701759  9.33222   8.895906  8.886457
0:   9.280346 10.073185 10.931828 11.515539 10.375839 10.603949]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.720913 -18.916252 -18.60063  -17.857311 -17.088327 -16.46125
0:  -15.825888 -15.375143 -15.187218 -15.142786 -15.442476 -16.072739
0:  -16.514017 -16.712671 -16.570202 -16.09298  -15.689886 -15.390659
0:  -17.014685 -17.072086]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.4385104 7.49353   7.84202   8.3743725 8.715763  8.68355   8.661874
0:  8.474899  8.10298   7.7112503 7.1127653 6.359909  5.6953063 5.3558207
0:  5.450982  5.951147  6.572532  6.9736886 4.1307483 3.9767284]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.654808 35.531876 35.279087 34.88965  34.483536 33.90738  34.057472
0:  34.219814 34.58347  34.733746 34.255527 33.869354 33.515156 33.74603
0:  34.54732  35.550896 36.45172  36.91945  36.623486 36.899952]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.6858249e+00 -6.1624694e+00 -5.2115140e+00 -4.0846295e+00
0:  -3.0320888e+00 -2.1717591e+00 -1.3186984e+00 -5.6956625e-01
0:  -5.0511360e-03  4.3587542e-01  6.8030977e-01  7.1954679e-01
0:   8.1433582e-01  1.1924472e+00  1.8705935e+00  2.8510735e+00
0:   3.8130283e+00  4.6701946e+00  2.9933658e+00  3.3709474e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.0856347  7.3494215  7.842609   8.395359   8.759277   8.852232
0:   8.997673   9.025993   8.942805   8.862485   8.586927   8.21516
0:   7.985667   8.106589   8.5816345  9.358377  10.134264  10.658924
0:   8.334076   8.431392 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6196036 2.3008785 2.2455068 2.5526233 2.929082  3.271041  3.3865213
0:  3.5026107 3.3110251 3.1961622 3.102345  2.7833009 2.5420823 2.358068
0:  2.4474874 2.9351115 3.7090821 4.536372  4.239258  4.153258 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8395185 6.317193  5.9850397 5.7287025 5.4257016 4.9412255 4.8398027
0:  4.7063055 4.673684  4.576777  4.1069956 3.5750685 2.9240608 2.7014709
0:  2.8800406 3.3485942 3.783243  3.9307017 3.2562957 3.0343928]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.9747305 7.881986  8.087999  8.389473  8.547762  8.457358  8.414392
0:  8.327636  8.165521  8.029805  7.7144904 7.2177114 6.8004217 6.573009
0:  6.760952  7.3610473 8.18379   8.941465  7.5614314 7.635752 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.608619  10.457426  10.4225235 10.412061  10.337701  10.219849
0:  10.481298  10.76663   11.137747  11.504621  11.565032  11.657892
0:  11.89022   12.61397   13.836332  15.385439  16.932055  18.24035
0:  18.915876  19.575512 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.379007  -19.76419   -20.706581  -21.038054  -20.999456  -20.674042
0:  -19.733381  -18.455334  -16.987648  -15.236069  -13.562799  -12.045599
0:  -10.302504   -8.490068   -6.4860682  -4.45064    -2.807406   -1.724935
0:   -2.6604314  -2.0893188]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.5533085  9.00742    9.671936  10.4172535 10.945755  11.221903
0:  11.392216  11.555714  11.517736  11.487252  11.355217  11.049276
0:  10.869271  10.878719  11.3216505 12.213966  13.335314  14.33135
0:  12.922074  13.1497135]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.505035  8.111082  7.9058595 7.8997173 7.855578  7.742437  7.541717
0:  7.525888  7.291066  7.2382293 7.156793  6.7475905 6.3016686 5.675252
0:  5.173369  5.052766  5.31711   5.8141713 4.2747617 4.802902 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.3956437 7.1027617 7.071779  7.21267   7.263341  7.1207027 7.221302
0:  7.1955886 7.117605  6.9717803 6.515638  5.856882  5.2804403 5.0741014
0:  5.3018847 6.00161   6.8542333 7.5640965 5.650463  5.5707083]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.3641405 12.7673435 12.36279   12.19125   12.087942  11.754024
0:  11.963951  11.973251  12.046522  11.93191   11.249294  10.4786
0:   9.65439    9.325114   9.5429    10.209179  11.109745  11.789055
0:  10.478073   9.977087 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.206422  -4.224169  -3.812584  -2.9854822 -2.1391983 -1.477354
0:  -1.0604291 -0.8029232 -0.9383044 -1.1068158 -1.3993759 -1.9465733
0:  -2.357728  -2.682458  -2.737072  -2.4266176 -1.9898033 -1.550909
0:  -2.558611  -2.5006952]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.03971   -9.281648  -9.11779   -8.485942  -7.7041836 -7.028676
0:  -6.775404  -6.6596956 -7.088254  -7.489402  -7.906188  -8.569612
0:  -9.016868  -9.453378  -9.609378  -9.279689  -8.561955  -7.617379
0:  -8.55896   -8.910335 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.893784 32.081715 32.18557  32.099136 32.06401  32.067287 33.034344
0:  34.08729  35.290077 36.252926 36.514095 36.74314  36.969425 37.67313
0:  38.878796 40.243305 41.429653 41.998966 40.485916 40.840374]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.741005 10.135443 10.76456  11.477858 11.963957 12.161157 12.20018
0:  12.232201 12.051817 11.965757 11.835131 11.446924 11.161631 10.909126
0:  10.92837  11.309342 11.892442 12.448584  9.665781  9.705509]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.623501   -4.3792953  -3.7717218  -2.964983   -2.122715   -1.4315038
0:  -0.54022884  0.19907427  0.8786645   1.4318786   1.762527    1.9247603
0:   2.1764388   2.6994479   3.526652    4.796381    6.209723    7.585395
0:   6.8412642   7.318273  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.320888   7.125151   7.1735835  7.389431   7.579437   7.5996594
0:   7.815709   7.9255276  7.983902   7.974595   7.749721   7.4342394
0:   7.221275   7.3814297  7.929055   8.804355   9.802199  10.606712
0:   9.98348   10.25134  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.8824747 2.932438  3.2921698 3.8710287 4.4115787 4.86411   5.1412497
0:  5.422422  5.409499  5.4031715 5.3281612 4.976643  4.7261467 4.5773945
0:  4.741068  5.290734  6.094543  6.910839  5.9021688 6.3790574]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.637732 -16.680874 -16.328056 -15.579765 -14.910181 -14.474749
0:  -14.330479 -14.399183 -14.910471 -15.426025 -16.112497 -16.99692
0:  -17.650301 -17.969217 -17.80395  -17.154007 -16.447477 -15.701532
0:  -15.477995 -15.403723]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.4391613 7.2193966 7.3547773 7.775703  8.268366  8.58243   8.854324
0:  8.991021  8.8123045 8.688795  8.425562  8.046167  7.8137593 7.6818266
0:  7.823584  8.087602  8.3013    8.222445  8.864997  8.844095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.234379  4.5244365 5.047374  5.642044  6.147525  6.4482937 6.725935
0:  6.9212637 6.9001184 6.8876696 6.7421494 6.49408   6.416117  6.5346975
0:  6.9828267 7.645639  8.301239  8.763494  7.332821  7.7780623]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-19.21632  -19.281546 -18.973236 -18.214634 -17.47868  -16.944994
0:  -17.009802 -17.352589 -18.359875 -19.358265 -20.443718 -21.695541
0:  -22.638622 -23.324137 -23.387892 -22.859093 -21.958004 -20.791782
0:  -19.171284 -19.192488]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.358784   -8.492796   -8.242308   -7.658131   -7.1714807  -7.0044217
0:   -6.9104524  -7.090692   -7.5829597  -8.145271   -8.950121  -10.039717
0:  -10.859566  -11.4448185 -11.514469  -11.278744  -11.112598  -11.086771
0:  -11.193037  -11.333731 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.676403 38.532486 38.487297 38.498093 38.388496 38.107307 38.450237
0:  38.75837  39.11848  39.287422 38.78256  38.08416  37.417404 37.02593
0:  37.098232 37.373894 37.62285  37.553757 32.826527 32.4967  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4918118 3.5418727 3.7216077 4.088834  4.451077  4.6325207 4.7019405
0:  4.6859193 4.3946877 4.2528415 4.1858516 4.0621586 4.1492558 4.4106326
0:  4.906929  5.6475472 6.4119873 7.0568495 5.1777925 4.7902403]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.92758  35.98175  36.033096 35.994385 35.83285  35.602577 35.839836
0:  36.143066 36.500618 36.734875 36.50797  36.29431  36.259193 36.62816
0:  37.567787 38.657993 39.662937 40.352173 37.584667 37.871513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.354869   -3.3426561  -2.9532127  -2.1841946  -1.4457827  -0.96831703
0:  -0.9102955  -1.0133476  -1.5578718  -2.0402517  -2.5117226  -3.2160187
0:  -3.7368155  -4.2075996  -4.4014854  -4.096299   -3.4591866  -2.6608057
0:  -3.7862349  -3.9588156 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.679203  5.684119  5.934997  6.31188   6.6003532 6.6648498 6.9229994
0:  7.116061  7.203854  7.2693095 7.061138  6.662954  6.3174973 6.284213
0:  6.5990705 7.264051  7.981209  8.523837  6.633212  6.7229533]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.9717536   1.024538    1.4003892   2.096343    2.8047647   3.2498057
0:   3.3150542   3.2354543   2.7143173   2.2910671   1.8789601   1.2936974
0:   0.7982106   0.30775547  0.05779314  0.12148905  0.43428993  0.7903452
0:  -0.5753846  -0.658618  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.914433 14.581873 14.258072 13.807432 13.331318 12.766628 13.008852
0:  13.378689 13.950467 14.472151 14.411301 14.341344 14.110523 14.487202
0:  15.339481 16.500793 17.462276 17.920933 18.39837  18.819967]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2593594 5.241087  5.3881254 5.62206   5.8177986 5.8399754 6.039387
0:  6.1350255 6.0916014 5.9626074 5.545827  4.9772625 4.403757  4.1999173
0:  4.327215  4.8159604 5.439612  5.893644  5.556873  5.778999 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.48093   -7.3178287 -6.8845115 -6.2515645 -5.6744742 -5.33497
0:  -5.0737157 -4.9149575 -5.11188   -5.3036838 -5.724894  -6.337356
0:  -6.8312345 -7.157786  -7.15039   -6.8604865 -6.5829053 -6.4272647
0:  -7.28863   -7.109671 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.019672 29.424698 29.98056  30.448215 30.6366   30.495079 30.563835
0:  30.607002 30.710476 30.816303 30.67323  30.360168 30.202675 30.196163
0:  30.588566 31.26337  32.055595 32.679882 28.222736 28.509972]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9983919 4.0997353 4.4485188 4.9068594 5.2502513 5.439068  5.640562
0:  5.8025355 5.773888  5.722816  5.440564  4.982669  4.633068  4.4563293
0:  4.615507  5.080765  5.591004  5.959826  4.348908  4.4823966]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.6815038 1.9489746 2.5342116 3.3431058 4.03833   4.4864845 4.7289486
0:  4.9645367 4.93767   5.008601  5.0305233 4.7910743 4.611944  4.4308977
0:  4.4840384 4.925991  5.564947  6.2041774 4.8612566 5.065569 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.768577   -2.4305606  -1.7273259  -0.7859926   0.07445717  0.73176146
0:   1.1736722   1.5563183   1.6263933   1.693944    1.6505637   1.333457
0:   1.0684929   0.8522091   0.8996644   1.3297768   1.9674301   2.5914829
0:   0.6303992   0.6755018 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.4631524 5.3761606 5.4732804 5.636037  5.6815386 5.582795  5.7440586
0:  5.8365808 5.815254  5.6915913 5.182997  4.4879427 3.774121  3.5028682
0:  3.6246033 4.002486  4.260213  4.205179  3.1960685 3.1828587]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.2998815 -5.4358277 -5.3338504 -4.9153543 -4.472407  -4.13506
0:  -4.078089  -4.123958  -4.640059  -5.1700244 -5.782065  -6.563371
0:  -7.174372  -7.613692  -7.7146163 -7.4252286 -6.978646  -6.4111495
0:  -6.7905183 -7.0053353]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.511483  -12.586736  -12.1818495 -11.348969  -10.427358   -9.643046
0:   -8.923627   -8.439546   -8.355397   -8.378274   -8.648531   -9.0960245
0:   -9.293732   -9.258159   -8.821267   -8.148438   -7.5820684  -7.2172422
0:   -9.750323   -9.780143 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.149963  3.4863808 4.1153655 4.8710775 5.469533  5.8376274 6.0840025
0:  6.1237545 5.882912  5.6471715 5.2747235 4.7698774 4.4572563 4.333764
0:  4.5572295 5.095662  5.7235665 6.259997  5.021289  5.4283648]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.46044  17.736177 17.991941 18.103456 17.917208 17.495571 17.237616
0:  17.079184 17.074291 17.242777 17.339565 17.273054 17.274162 17.379805
0:  17.829327 18.61259  19.647694 20.671728 19.513454 19.714212]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.354204 16.659782 16.920607 16.94499  16.827942 16.585117 16.92638
0:  17.218246 17.60873  17.868757 17.609987 17.219744 16.892319 17.102015
0:  17.831343 18.963623 20.185741 21.133684 19.792976 20.120348]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.150309  2.3374414 2.844266  3.4959733 4.0011673 4.215928  4.3242836
0:  4.3273516 4.170579  4.069622  3.868976  3.49124   3.20007   3.0854259
0:  3.2113926 3.665568  4.174331  4.5376554 3.1827567 3.3227785]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.023432  -12.189539  -11.906786  -11.20397   -10.475981  -10.012449
0:   -9.888721  -10.057671  -10.723465  -11.389917  -12.167656  -13.098049
0:  -13.743     -14.126518  -14.080471  -13.688049  -13.243551  -12.8184395
0:  -13.649147  -14.000166 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.610508  -8.528114  -8.047797  -7.275859  -6.596738  -6.208357
0:  -6.0635633 -6.1433983 -6.6401234 -7.10972   -7.695962  -8.484625
0:  -8.99448   -9.293284  -9.217298  -8.689344  -8.072951  -7.5054245
0:  -8.922024  -8.878115 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.397957  10.736083  11.244316  11.771378  12.061258  12.099484
0:  12.042295  12.0031185 11.8455715 11.743102  11.572358  11.186971
0:  10.841593  10.637599  10.817469  11.415889  12.307645  13.197812
0:  11.23891   11.563397 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6797466  -1.9700947  -2.1280785  -2.0850039  -2.0382786  -2.1931114
0:  -1.9749584  -1.747591   -1.5226994  -1.2576933  -1.2048731  -1.2909479
0:  -1.1811428  -0.50570536  0.6283345   2.0259018   3.2210793   3.8684473
0:   4.0891476   4.2446117 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.688128 20.31057  20.02844  19.766726 19.41567  18.84706  18.67335
0:  18.404343 18.248287 18.059477 17.544476 16.983938 16.510782 16.48772
0:  16.91918  17.76157  18.65232  19.266129 17.973347 17.86687 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.4748287   7.243684    7.0400767   6.8515806   6.507167    6.002941
0:   5.346765    4.665178    3.6741765   2.8069823   1.9437709   1.0040312
0:   0.19243526 -0.52253056 -0.9835124  -1.1507306  -1.0633073  -0.86308384
0:  -2.0505714  -1.9438338 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.2859063   0.02054691 -0.01950932  0.18407154  0.3909216   0.45976734
0:   0.3375907   0.08884192 -0.454988   -1.0055652  -1.5593166  -2.1897535
0:  -2.5838423  -2.7108698  -2.4784155  -1.8141727  -0.9699807  -0.11693573
0:  -0.49072933 -0.33416414]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [  0.14020586  -1.2182336   -2.407535    -3.42067     -4.778589
0:   -6.2244964   -7.6501985   -8.731405    -9.70327    -10.576866
0:  -11.55282    -12.760637   -13.768845   -14.424072   -14.380453
0:  -13.853888   -13.009956   -12.185947   -12.495037   -13.359293  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1612525 2.0813556 2.3968205 2.9212193 3.3825655 3.635675  3.9402375
0:  4.088255  4.0342593 3.9221857 3.573943  3.050159  2.6505208 2.5486112
0:  2.781618  3.3151417 3.8129456 4.092163  1.7663231 1.9748106]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.070726  13.186308  13.491433  13.828847  14.002037  13.917492
0:  13.942226  13.843226  13.650091  13.441708  13.002735  12.361143
0:  11.812641  11.501981  11.535282  11.905682  12.36162   12.66047
0:  10.5960045 10.385103 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.602489  10.235694   9.949896   9.608158   9.057776   8.22115
0:   7.472245   6.7571797  6.05311    5.5211887  4.8911905  4.2457333
0:   3.7013886  3.4259825  3.4383898  3.6961854  3.9589028  4.003268
0:   3.0051012  2.314735 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.5628676  5.9842677  6.6816025  7.461573   8.081769   8.435464
0:   8.693078   8.879258   8.843168   8.832785   8.710294   8.408038
0:   8.212439   8.201899   8.512753   9.123459   9.768659  10.215796
0:   7.837836   8.27795  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1687884 6.3238063 6.7901564 7.357031  7.679543  7.6986113 7.584916
0:  7.539061  7.342712  7.3062716 7.2118206 6.8265123 6.490189  6.146313
0:  6.055091  6.4015408 7.0466914 7.7439995 6.049432  6.2640815]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.494724  8.840751  9.394222 10.03927  10.563962 10.903953 11.253964
0:  11.515713 11.566456 11.573345 11.395437 10.989954 10.733378 10.67006
0:  10.966257 11.570946 12.285111 12.828814 10.561955 10.762785]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.155282  -4.3492775 -4.2101355 -3.8430786 -3.5333004 -3.4019632
0:  -3.307211  -3.326436  -3.5616436 -3.7997694 -4.1676903 -4.6845503
0:  -5.0053644 -5.087262  -4.792033  -4.162814  -3.457241  -2.8977556
0:  -3.8628387 -3.774043 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.7605495 -7.8168793 -7.486682  -6.730404  -5.9068937 -5.271035
0:  -5.0030994 -4.888274  -5.231124  -5.5289984 -5.798295  -6.214636
0:  -6.441599  -6.553363  -6.4269886 -5.9233093 -5.2056475 -4.428237
0:  -4.3496733 -4.296651 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.83867  34.893383 34.857777 34.74437  34.54768  34.03131  34.20398
0:  34.166992 34.13108  33.795662 32.82727  31.854828 31.003517 30.718203
0:  31.065136 31.781618 32.519836 32.91249  31.584661 31.683321]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.99775696  1.0195222   1.400743    2.0081487   2.5812936   2.9461436
0:   3.231895    3.3577054   3.1792827   2.91686     2.4505115   1.7473769
0:   1.1287937   0.70935965  0.60029554  0.7987704   1.0803771   1.2690382
0:   0.08657217 -0.06587076]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.314766  2.3242679 2.6117978 3.1686907 3.654194  3.9582388 4.086884
0:  4.167486  3.9321947 3.7216556 3.4265025 2.8906918 2.443325  2.0775523
0:  2.0408812 2.4832482 3.2575443 4.127638  3.5355947 3.8412545]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.982698 17.133333 17.385132 17.523724 17.501045 17.347042 17.20245
0:  17.125607 16.8543   16.4876   15.855168 15.049102 14.323139 13.878303
0:  13.906643 14.293592 14.843473 15.292075 12.325554 12.332515]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.778189  4.7487893 4.9701986 5.3454056 5.615081  5.6922674 5.8621664
0:  5.939249  5.865101  5.740488  5.3599544 4.800542  4.3172107 4.150106
0:  4.3291616 4.8270054 5.348499  5.703118  3.8215265 3.69283  ]
0: validation loss for strategy=forecast at epoch 27 : 0.2993551790714264
0: validation loss for velocity_u : 0.16695834696292877
0: validation loss for velocity_v : 0.28357553482055664
0: validation loss for specific_humidity : 0.154388889670372
0: validation loss for velocity_z : 0.5621323585510254
0: validation loss for temperature : 0.09831881523132324
0: validation loss for total_precip : 0.5307573080062866
0: 28 : 19:49:45 :: batch_size = 96, lr = 1.0524694428278642e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 28, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6265, -0.6477, -0.6678, -0.6875, -0.7087, -0.7322, -0.7576, -0.7836, -0.8083, -0.8296, -0.8441, -0.8495,
0:         -0.8466, -0.8380, -0.8274, -0.8178, -0.8114, -0.8090, -0.5380, -0.5542, -0.5706, -0.5869, -0.6053, -0.6277,
0:         -0.6552], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0036, -0.7543, -0.4521, -0.1250,  0.2003,  0.5065,  0.7838,  1.0267,  1.2300,  1.3887,  1.5037,  1.5802,
0:          1.6278,  1.6554,  1.6692,  1.6745,  1.6737,  1.6672, -1.0432, -0.8109, -0.5200, -0.1979,  0.1268,  0.4352,
0:          0.7185], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5628, -0.5760, -0.5631, -0.5471, -0.5250, -0.5018, -0.4910, -0.4834, -0.4747, -0.4731, -0.3858, -0.2889,
0:         -0.1458,  0.0091,  0.0077,  0.0044, -0.0531, -0.1123, -0.5609, -0.5748, -0.5622, -0.5530, -0.5308, -0.5072,
0:         -0.4959], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3201,  0.2146,  0.1070,  0.0037, -0.0760, -0.0932, -0.0221,  0.1027,  0.1866,  0.1694,  0.0769, -0.0329,
0:         -0.1255, -0.1986, -0.2460, -0.2374, -0.1707, -0.0953,  0.3954,  0.2577,  0.0962, -0.0394, -0.1233, -0.1405,
0:         -0.0803], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.6477, -0.7480, -0.8408, -0.9166, -0.9684, -0.9917, -0.9851, -0.9502, -0.8933, -0.8225, -0.7465, -0.6722,
0:         -0.6030, -0.5369, -0.4700, -0.4017, -0.3355, -0.2774, -0.2291, -0.1873, -0.1480, -0.1078, -0.0640, -0.0133,
0:          0.0444], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1816, -0.1805, -0.2074, -0.2319, -0.2272, -0.2261, -0.2331, -0.2460, -0.2483, -0.2062, -0.2039, -0.2319,
0:         -0.2389, -0.2413, -0.2401, -0.2460, -0.2483, -0.2483, -0.2366, -0.2448, -0.2436, -0.2413, -0.2460, -0.2483,
0:         -0.2483], device='cuda:0')
0: [DEBUG] Epoch 28, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.2366,     nan,     nan, -0.2460,     nan, -0.2471,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2343,     nan, -0.2401,     nan,     nan,     nan, -0.2413,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2343,     nan, -0.2401,     nan,     nan,
0:         -0.2202,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2424,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2460,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2191,     nan,     nan,     nan,     nan,     nan, -0.2331,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,
0:             nan,     nan,     nan, -0.2413,     nan,     nan,     nan,     nan, -0.2471,     nan,     nan,     nan,
0:             nan, -0.2471,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2003, -0.2249,     nan, -0.2471,     nan, -0.2401,     nan,
0:             nan,     nan,     nan, -0.2319, -0.2471,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 28, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1828, -0.1737, -0.1380, -0.0851, -0.0467, -0.0384, -0.0570, -0.0968, -0.1623, -0.2198, -0.2897, -0.3772,
0:         -0.4616, -0.5405, -0.5777, -0.5662, -0.5196, -0.4577, -0.2365, -0.2704, -0.2764, -0.2440, -0.2186, -0.2052,
0:         -0.2213], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2075, 0.2416, 0.2502, 0.2549, 0.2521, 0.2478, 0.2350, 0.2278, 0.2350, 0.2566, 0.2888, 0.3230, 0.3323, 0.3036,
0:         0.2631, 0.2525, 0.2984, 0.3822, 0.1764, 0.2176, 0.2290, 0.2190, 0.2059, 0.1836, 0.1703], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4367, -0.4391, -0.4476, -0.4502, -0.4552, -0.4639, -0.4813, -0.4987, -0.5138, -0.5243, -0.5296, -0.5361,
0:         -0.5345, -0.5379, -0.5572, -0.5746, -0.6089, -0.6353, -0.4303, -0.4326, -0.4347, -0.4341, -0.4394, -0.4515,
0:         -0.4653], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3866, 0.3973, 0.3767, 0.2848, 0.2380, 0.2322, 0.1833, 0.2327, 0.3089, 0.3434, 0.4081, 0.4464, 0.4672, 0.4261,
0:         0.3693, 0.4953, 0.6056, 0.5067, 0.4082, 0.2373, 0.1736, 0.1339, 0.1519, 0.2155, 0.2016], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.7343, 0.7492, 0.7601, 0.7655, 0.7776, 0.8032, 0.8398, 0.8763, 0.9063, 0.9267, 0.9386, 0.9482, 0.9634, 0.9836,
0:         1.0052, 1.0199, 1.0293, 1.0422, 1.0640, 1.0980, 1.1433, 1.1915, 1.2362, 1.2708, 1.2909], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2370, -0.2402, -0.2486, -0.2481, -0.2429, -0.2360, -0.2317, -0.2304, -0.2213, -0.2355, -0.2304, -0.2328,
0:         -0.2366, -0.2335, -0.2289, -0.2228, -0.2321, -0.2325, -0.2228, -0.2196, -0.2269, -0.2266, -0.2207, -0.2264,
0:         -0.2166], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.21166004240512848; velocity_v: 0.27201417088508606; specific_humidity: 0.19847923517227173; velocity_z: 0.613383412361145; temperature: 0.15253673493862152; total_precip: 0.7265225648880005; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20880812406539917; velocity_v: 0.3054773807525635; specific_humidity: 0.18869033455848694; velocity_z: 0.6095810532569885; temperature: 0.14025017619132996; total_precip: 0.6848638653755188; 
0: epoch: 28 [1/5 (20%)]	Loss: 0.70569 : 0.32488 :: 0.21030 (2.61 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1619146168231964; velocity_v: 0.28106215596199036; specific_humidity: 0.1940465271472931; velocity_z: 0.574931263923645; temperature: 0.15929372608661652; total_precip: 0.5710667371749878; 
0: epoch: 28 [2/5 (40%)]	Loss: 0.57107 : 0.28962 :: 0.19943 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2107275277376175; velocity_v: 0.3195315897464752; specific_humidity: 0.17755766212940216; velocity_z: 0.5833733081817627; temperature: 0.13838383555412292; total_precip: 0.48503127694129944; 
0: epoch: 28 [3/5 (60%)]	Loss: 0.48503 : 0.28503 :: 0.21212 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2280830591917038; velocity_v: 0.32434049248695374; specific_humidity: 0.16935105621814728; velocity_z: 0.6466110944747925; temperature: 0.14230504631996155; total_precip: 0.4970340430736542; 
0: epoch: 28 [4/5 (80%)]	Loss: 0.49703 : 0.29981 :: 0.21728 (15.78 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [6.5326691e-05 1.2540817e-04 1.1396408e-04 7.5340271e-05 5.1975247e-05
0:  2.2888184e-05 1.4305115e-05 6.6757202e-06 1.9073486e-06 1.9073486e-06
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  4.4345856e-05 6.9618225e-05 6.4849854e-05 5.2452087e-05 5.2928925e-05
0:  6.7710876e-05 1.5258789e-05 3.8146973e-06 4.7684443e-07 4.7684443e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 1.4305042e-06 8.5830688e-06 2.1457672e-05 3.0040741e-05
0:  5.6266785e-05 8.7261200e-05 1.3113022e-04 2.0456314e-04 2.4414062e-04
0:  2.6512146e-04 2.0217896e-04 2.0503998e-04 2.5558472e-04 3.0946732e-04
0:  3.8242340e-04 5.1450729e-04 6.7806244e-04 8.3208084e-04 9.3364716e-04
0:  8.7356567e-04 7.3194504e-04 6.5183640e-04 5.3405762e-04 4.5871735e-04
0:  3.8433075e-04 3.2663345e-04 8.0060959e-04 7.9774857e-04 6.6471100e-04
0:  1.3685226e-04 7.1239471e-04 1.3332367e-03 7.7199936e-04 3.4523010e-04
0:  1.3828278e-05 1.1920929e-05 1.7547607e-04 1.7309189e-04 1.4305115e-04
0:  2.1219254e-04 2.1219254e-04 9.0599060e-06 9.5367432e-06 9.5367432e-06
0:  4.2438507e-05 1.0967255e-04 7.6293945e-05 5.2928925e-05 9.3460083e-05
0:  1.1997223e-03 1.2240410e-03 3.4189224e-04 2.2029877e-04 4.5776364e-05
0:  9.5367432e-07 1.4305042e-06 2.3841858e-06 2.8610229e-06 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 7.8678131e-05 3.6239624e-05
0:  9.1075897e-05 1.4209747e-04 1.8215179e-04 1.0061264e-04 6.2465668e-05
0:  5.0067898e-05 2.1457672e-05 8.1062317e-06 2.3841858e-06 1.9073486e-06
0:  4.7684443e-07 4.7684443e-07 9.5367432e-07 9.5367432e-07 1.9550323e-05
0:  5.7697296e-05 5.7697296e-05 4.7206879e-05 3.3855438e-05 4.1484833e-05
0:  2.4318695e-05 6.6757202e-06 5.2452087e-06 4.7683716e-06 4.7684443e-07
0:  4.7684443e-07 4.7684443e-07 4.7684443e-07 4.7684443e-07 4.7684443e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4305042e-06
0:  8.5830688e-06 2.2411346e-05 6.1035156e-05 8.7261200e-05 1.2588501e-04
0:  2.3365021e-04 3.3950806e-04 3.8862228e-04 3.2186508e-04 1.9693375e-04
0:  1.4019012e-04 1.8596649e-04 2.7322769e-04 4.3725967e-04 6.4706802e-04
0:  7.9536438e-04 9.0265274e-04 9.7370148e-04 9.8705292e-04 1.0209084e-03
0:  1.0209084e-03 1.0004045e-03 9.7560883e-04 9.0265274e-04 8.0871582e-04
0:  6.9236755e-04 5.7792664e-04 4.5537949e-04 3.4523010e-04 6.2894821e-04
0:  7.8201294e-04 2.2077560e-04 1.0585785e-04 7.6770782e-05 6.5803528e-05
0:  7.0190430e-04 8.9883804e-04 7.0905685e-04 1.1286736e-03 8.0394745e-04
0:  1.4305115e-04 1.4209747e-04 1.9598007e-04 3.0469894e-04 2.1791458e-04
0:  2.5939941e-04 2.3365021e-04 1.2874603e-04 3.8814545e-04 6.2608719e-04]
0: Target values (first 200):
0: [6.38961792e-05 4.72068787e-05 1.15871429e-04 1.54972076e-04
0:  7.91549683e-05 6.62803650e-05 4.91142273e-05 4.57763635e-05
0:  3.43322754e-05 7.29560852e-05 1.04427338e-04 6.81877136e-05
0:  4.57763635e-05 4.43458557e-05 6.38961792e-05 5.91278076e-05
0:  6.38961792e-05 7.00950623e-05 4.67300415e-05 4.86373901e-05
0:  5.29289246e-05 2.05039978e-05 2.14576721e-05 4.72068787e-05
0:  8.48770142e-05 9.34600830e-05 1.16825104e-04 1.71184540e-04
0:  1.51157379e-04 1.18732452e-04 9.63211060e-05 1.05857849e-04
0:  9.05990601e-05 6.34193420e-05 6.86645508e-05 5.72204590e-05
0:  4.86373901e-05 1.71661377e-05 1.19209290e-05 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76844434e-07 1.43050420e-06
0:  1.71661377e-05 1.15871429e-04 1.72138214e-04 1.46389008e-04
0:  1.01089478e-04 7.86781311e-05 6.24656677e-05 2.90870667e-05
0:  1.85966492e-05 1.43051147e-05 3.43322754e-05 5.05447388e-05
0:  6.10351562e-05 7.67707825e-05 6.00814819e-05 3.62396240e-05
0:  1.14440918e-05 3.81469727e-06 2.38418579e-06 4.76844434e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76844434e-07 4.76844434e-07 1.43050420e-06
0:  1.43050420e-06 0.00000000e+00 0.00000000e+00 1.90734863e-06
0:  6.67572021e-06 1.14440918e-05 1.14440918e-05 9.05990601e-06
0:  1.23977661e-05 1.66893005e-05 2.09808350e-05 2.43186951e-05
0:  2.52723694e-05 3.81469727e-05 5.72204590e-05 2.88486481e-04
0:  6.82353973e-04 1.03235245e-03 1.38282776e-03 1.54113770e-03
0:  8.34465027e-05 1.36852264e-04 1.02043152e-04 6.43730164e-05
0:  2.28881836e-05 1.24454498e-04 1.48773193e-04 1.07765198e-04
0:  5.00678980e-05 2.76565552e-05 2.19345093e-05 1.09672546e-05
0:  8.58306885e-06 1.47819519e-05 3.48091125e-05 6.24656677e-05
0:  7.77244568e-05 7.62939453e-05 1.37329102e-04 1.37805939e-04
0:  9.05990601e-05 6.58035278e-05 5.86509705e-05 5.57899475e-05
0:  4.43458557e-05 7.34329224e-05 1.04427338e-04 1.30176544e-04
0:  1.38282776e-04 1.41620636e-04 1.37805939e-04 1.07288361e-04
0:  9.34600830e-05 8.48770142e-05 7.00950623e-05 5.43594360e-05
0:  4.91142273e-05 6.67572021e-05 3.24249268e-05 3.33786011e-06
0:  1.43050420e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76844434e-07 4.76844434e-07
0:  0.00000000e+00 4.76844434e-07 1.43050420e-06 1.90734863e-06
0:  2.38418579e-06 1.04904175e-05 3.09944153e-05 6.96182251e-05
0:  6.96182251e-05 5.81741333e-05 4.10079956e-05 4.19616699e-05
0:  3.57627869e-05 2.71797180e-05 2.38418579e-05 5.86509705e-05
0:  1.00135803e-04 1.12056732e-04 9.87052917e-05 7.67707825e-05
0:  4.43458557e-05 2.33650208e-05 6.19888306e-06 1.90734863e-06
0:  1.43050420e-06 4.76844434e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76844434e-07 1.43050420e-06 9.53674316e-07
0:  4.76844434e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [38.81739  38.98427  38.88122  38.53292  38.183643 37.76387  38.40232
0:  39.056404 39.83028  40.32133  39.991123 39.753727 39.503708 39.989582
0:  41.11891  42.47594  43.630775 44.00981  43.734577 44.166794]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = 0.145, max = 4.210, mean = 1.703
0:          sample (first 20): tensor([2.7529, 2.7669, 2.7582, 2.7289, 2.6995, 2.6642, 2.7179, 2.7730, 2.8381, 2.8795, 2.8517, 2.8317, 2.8107, 2.8516,
0:         2.9466, 3.0609, 3.1581, 3.1900, 2.5942, 2.6937])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.072525   9.849343   9.84359    9.793692   9.458314   8.81261
0:   8.251369   7.6095166  6.9731426  6.401395   5.660272   4.8305264
0:   4.213298   4.0545816  4.44681    5.2865267  6.222497   6.9838185
0:   5.5461655  5.91183  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.005707 25.29344  25.71743  26.079245 26.202055 25.960314 25.986765
0:  25.953247 25.997313 26.032482 25.746595 25.381395 25.09769  25.08131
0:  25.46115  26.11414  26.83523  27.362978 23.6047   23.715391]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.714943 20.166264 19.778368 19.426645 19.161484 18.597216 18.561893
0:  18.246792 17.957169 17.421177 16.303648 15.168478 14.111635 13.609427
0:  13.775661 14.502712 15.439234 16.18248  15.654276 15.29701 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.178288 22.82236  22.442785 22.04221  21.629173 20.936407 20.837414
0:  20.539362 20.444412 20.305202 19.804459 19.451702 19.23519  19.55798
0:  20.385078 21.540876 22.747551 23.63287  23.535522 23.616066]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.888323  13.62235   13.447058  13.304178  13.136118  12.682465
0:  12.699308  12.570175  12.584844  12.476183  11.920416  11.394962
0:  10.836976  10.728331  11.077621  11.810678  12.560503  13.0447035
0:  13.190606  13.012043 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.624672 22.52438  22.576637 22.62231  22.596912 22.2448   22.407875
0:  22.266727 22.29895  22.268059 21.771118 21.205988 20.844135 20.90707
0:  21.53574  22.621597 23.791573 24.767357 22.53244  22.383213]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.060567 15.133123 15.335842 15.428864 15.434211 15.201746 15.299822
0:  15.324228 15.254872 15.12772  14.641438 14.056047 13.532757 13.470866
0:  13.750552 14.316565 14.961551 15.349633 12.256266 12.348684]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4163487 3.394961  3.615814  3.93661   4.145148  4.135907  4.2737293
0:  4.31026   4.245595  4.148034  3.7446642 3.2031262 2.6830406 2.466209
0:  2.5570912 2.9619775 3.3375688 3.5380912 1.9173274 1.9582658]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.05101013  0.0088954   0.3780632   1.0796804   1.789948    2.3587499
0:   2.611163    2.8118458   2.6406438   2.5381129   2.460169    2.1775932
0:   1.9831786   1.7774978   1.819663    2.245988    2.9836767   3.8285983
0:   3.0178666   3.0346556 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.002869  20.446724  19.97639   19.606533  19.23397   18.644478
0:  18.641605  18.513176  18.492414  18.313221  17.586502  16.784088
0:  15.884327  15.496082  15.6959915 16.357004  17.192324  17.769287
0:  16.379982  16.16188  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1590595 2.140172  2.336217  2.690812  2.929113  3.0208783 3.1591108
0:  3.2868161 3.2520175 3.2198822 3.0152416 2.5724678 2.2037873 2.025177
0:  2.1837559 2.7579837 3.4998012 4.161845  4.1062813 4.600912 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.4956    5.426551  5.566068  5.817354  5.9626956 5.902759  6.0268264
0:  6.0789194 6.0931664 6.100513  5.8082047 5.350115  4.86089   4.711666
0:  4.8558245 5.3329535 5.7709293 5.9235387 4.3077297 4.27695  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.685051  -6.334201  -5.591646  -4.4726086 -3.3495026 -2.5015535
0:  -2.094387  -1.9392924 -2.3355637 -2.769024  -3.2701187 -3.983983
0:  -4.5107346 -5.0233774 -5.298296  -5.235532  -5.021634  -4.7323065
0:  -7.5167923 -7.8746033]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.9353828 -3.1398115 -3.0765123 -2.7213192 -2.3500137 -2.1126947
0:  -1.9889803 -1.9428091 -2.1602907 -2.318532  -2.5212908 -2.8875551
0:  -3.132144  -3.2585945 -3.159707  -2.721592  -2.1202106 -1.5371208
0:  -1.904953  -1.7500858]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.502603  -11.347941  -10.7212     -9.652307   -8.508492   -7.5586557
0:   -6.997901   -6.6094427  -6.6715326  -6.6735125  -6.672858   -6.8681917
0:   -6.923745   -6.907379   -6.5685787  -5.882721   -4.9914846  -4.119131
0:   -4.451705   -4.119231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.653465 11.803127 12.133569 12.44909  12.4991   12.296223 12.024792
0:  11.783873 11.458061 11.245935 10.974184 10.521866 10.155109  9.935951
0:  10.08041  10.626219 11.427677 12.204013  9.989239 10.162249]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.8572497  -8.214849   -8.305307   -8.057373   -7.987293   -8.193897
0:   -8.894434   -9.677603  -10.796944  -11.683855  -12.376938  -13.064953
0:  -13.364496  -13.439936  -13.12904   -12.35726   -11.390003  -10.442699
0:   -9.8666115  -9.742994 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.80757    -2.7663236  -2.48459    -1.9205966  -1.3534675  -0.8917508
0:  -0.6787391  -0.51681614 -0.8187151  -1.1549573  -1.6266084  -2.3847537
0:  -3.024569   -3.6015162  -3.898665   -3.7643685  -3.359301   -2.809505
0:  -3.661384   -3.7483916 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.590298 17.34958  17.178137 17.00348  16.826632 16.47187  16.653648
0:  16.725403 16.864656 16.869858 16.39547  15.903901 15.410439 15.423229
0:  15.901108 16.752703 17.66537  18.361359 18.686974 18.688505]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6467624 -3.9417987 -3.9144855 -3.415423  -2.6645885 -1.9554572
0:  -1.4616246 -1.1472449 -1.4132152 -1.7965941 -2.3243537 -3.1072326
0:  -3.6950307 -4.210632  -4.5089393 -4.365293  -3.9660716 -3.4049497
0:  -4.4791512 -4.8799233]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.736542  9.396175  9.404979  9.532977  9.613522  9.416302  9.44287
0:   9.354801  9.320919  9.265884  8.989371  8.611809  8.316887  8.284248
0:   8.65213   9.429258 10.353526 11.15234   9.210411  8.815815]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.8734884 -5.879792  -5.570215  -4.9082575 -4.21599   -3.6724644
0:  -3.5090437 -3.403864  -3.7474623 -4.02705   -4.2995358 -4.746149
0:  -5.0715733 -5.3465548 -5.386999  -5.0529747 -4.43958   -3.736269
0:  -3.6856852 -3.7461386]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.854023   9.259334   9.77239   10.190821  10.477595  10.601013
0:  11.069541  11.523355  11.907211  12.19927   12.074977  11.790777
0:  11.410874  11.3929825 11.659615  12.181887  12.609579  12.758827
0:  11.09226   11.35589  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.6097589 3.446563  3.4578652 3.468196  3.3675523 3.0376017 3.2441587
0:  3.4607205 3.841571  4.1908526 4.074785  3.854098  3.4503884 3.5257847
0:  4.0012064 4.8181095 5.5942755 6.030953  5.7200055 5.7716475]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.207582 15.309765 15.594908 15.917555 16.102388 16.08082  16.086788
0:  16.090448 15.924286 15.843236 15.579143 15.14412  14.760945 14.496719
0:  14.639402 15.090206 15.582098 15.873802 12.910725 13.063186]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.935539  -8.854579  -8.297115  -7.294421  -6.2202435 -5.3471518
0:  -4.7734184 -4.481243  -4.656786  -4.909686  -5.2522964 -5.733143
0:  -5.990676  -6.093878  -5.903063  -5.4153214 -4.826377  -4.2816563
0:  -4.4497175 -4.3417177]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.174431  8.001162  8.076992  8.213475  8.256971  8.06852   8.18095
0:  8.229921  8.284576  8.343032  8.08815   7.6198573 7.164426  7.0693
0:  7.3464537 8.074692  8.938002  9.594235  7.610952  7.5663447]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7233706 3.1216674 2.810612  2.768656  2.6957545 2.4158378 2.433416
0:  2.3561146 2.3591206 2.3474977 2.0952785 1.791029  1.5000267 1.630928
0:  2.1708968 3.0338736 3.9652174 4.6046715 2.792435  2.6897738]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.671666  -15.778649  -15.538434  -15.082868  -14.617934  -14.288619
0:  -13.7163515 -13.15643   -12.665784  -12.274333  -12.296582  -12.51678
0:  -12.851693  -12.781835  -12.383983  -11.69542   -10.993033  -10.53489
0:  -12.257365  -12.355142 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.782768  -8.831593  -8.479044  -7.725639  -6.8571763 -6.1757865
0:  -5.596885  -5.1970143 -5.2576156 -5.3037686 -5.5561404 -6.019893
0:  -6.2730527 -6.362675  -6.0749164 -5.467674  -4.82009   -4.2656083
0:  -6.267222  -6.3266454]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6009212  -3.0040727  -3.1146045  -3.0518775  -3.0095973  -3.0159783
0:  -2.8813891  -2.6654897  -2.4560742  -2.182572   -1.9922223  -1.7947526
0:  -1.3889332  -0.63908386  0.5356817   1.9745033   3.43752     4.7683144
0:   4.922778    5.6921473 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.82703   12.5467205 13.419848  14.243631  14.866964  15.157927
0:  15.511356  15.573282  15.446753  15.0763645 14.353998  13.460867
0:  12.710541  12.32976   12.394747  12.873214  13.502512  14.081747
0:  11.679375  11.803217 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.353656  -9.506573  -9.3181715 -8.994989  -8.740125  -8.755482
0:  -8.36593   -8.077562  -7.6383457 -7.2765026 -7.369624  -7.6551247
0:  -8.082624  -8.026178  -7.6216555 -6.797344  -6.065727  -5.58027
0:  -7.8372335 -7.903779 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.8821297 7.974384  8.330079  8.781028  9.104912  9.234682  9.392323
0:  9.465108  9.409928  9.331228  9.064056  8.59201   8.199175  7.9933357
0:  8.123854  8.60889   9.210562  9.715031  8.231557  8.555317 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.420528  16.036139  15.955614  16.00502   15.9038925 15.440617
0:  15.223217  14.832354  14.471399  14.035673  13.302201  12.523385
0:  11.882997  11.681932  11.967826  12.670721  13.482644  14.039
0:  12.666637  12.566481 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.2808275 -13.531014  -13.386935  -12.760621  -12.076186  -11.58949
0:  -11.619912  -11.848184  -12.658027  -13.38742   -14.143094  -15.047671
0:  -15.762723  -16.375725  -16.702827  -16.521008  -15.9558935 -15.185599
0:  -15.856975  -15.99726  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.378874 11.68288  12.253683 13.036249 13.65147  13.997885 14.130674
0:  14.162441 13.964533 13.869907 13.716316 13.316782 12.857353 12.285093
0:  11.882125 11.830799 12.176731 12.733911 10.1092   10.2413  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.792939  -5.9607267 -5.733881  -5.1340475 -4.5188417 -4.1551294
0:  -4.1035156 -4.2986174 -4.9913645 -5.7053833 -6.486455  -7.4540243
0:  -8.112558  -8.565529  -8.63307   -8.233429  -7.676238  -7.0626574
0:  -8.121567  -8.272713 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.665503   -2.7409987  -2.387196   -1.657846   -1.0117574  -0.6526437
0:  -0.76657534 -1.0492435  -1.7673306  -2.3886533  -3.0120063  -3.8425975
0:  -4.5295615  -5.1028094  -5.333079   -4.9365067  -4.1034217  -3.119145
0:  -4.8839774  -5.2859836 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8586755 -7.1658587 -7.1002994 -6.566155  -5.9485    -5.4313865
0:  -5.428057  -5.4923396 -6.0814395 -6.5933156 -7.068271  -7.8245273
0:  -8.495146  -9.167799  -9.525501  -9.269665  -8.437571  -7.280337
0:  -6.600159  -6.80491  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5428076 6.7514    7.195891  7.7580996 8.130211  8.254318  8.317993
0:  8.331543  8.152614  7.996758  7.6780977 7.136943  6.6640015 6.294183
0:  6.272585  6.6342764 7.175471  7.668454  6.3273067 6.475403 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.374437 30.312243 30.22365  30.068953 29.947338 29.695402 30.042707
0:  30.30969  30.688042 30.927298 30.624937 30.32515  30.14891  30.371912
0:  31.103514 32.12111  33.21569  34.058613 33.149048 33.3172  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.2405152   2.2213058   2.4465618   2.8186555   3.0749907   3.117937
0:   3.1999624   3.1645284   2.9098144   2.625895    2.0773926   1.3342996
0:   0.65971327  0.28678513  0.2988944   0.67858076  1.1006632   1.450336
0:  -0.38998747 -0.40437508]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.8356285 12.758734  12.889066  13.132966  13.178175  12.92745
0:  12.492389  11.90328   11.058209  10.203089   9.230442   8.115374
0:   7.161764   6.488312   6.274336   6.548574   7.0908904  7.607139
0:   6.737073   6.743367 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.370857 22.356827 22.586308 22.933006 23.147514 22.986801 22.936329
0:  22.53587  21.897064 20.967274 19.584196 18.08111  16.76093  16.023968
0:  16.002182 16.560886 17.429724 18.20091  16.283867 16.106853]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.345497  13.192816  13.061305  12.843715  12.538437  11.9765
0:  12.062897  12.093758  12.296259  12.401258  11.969903  11.528525
0:  10.991215  11.0188465 11.4987335 12.354963  13.167462  13.620974
0:  15.491131  15.433807 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.328357 14.658379 14.928684 15.028597 14.84926  14.354355 14.284536
0:  14.139906 13.837294 13.491686 12.694418 11.775362 10.764719 10.312005
0:  10.237402 10.400482 10.607431 10.385645 10.451228 10.345963]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0747085 -4.067208  -3.7227178 -3.0337634 -2.333775  -1.8128619
0:  -1.5752234 -1.4410653 -1.6941495 -1.9039435 -2.1390147 -2.6097322
0:  -2.9286466 -3.1908298 -3.1970096 -2.7616496 -2.0587516 -1.2563515
0:  -1.4205985 -1.2561059]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.9165928  1.9849224  2.366406   2.940749   3.403369   3.6279325
0:  3.7109766  3.6802583  3.384019   3.091752   2.668883   2.0124924
0:  1.4583077  1.0511532  0.95709324 1.2922311  1.8199608  2.357098
0:  0.61509895 0.6473193 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2930412  -0.9896498  -0.33235598  0.59104586  1.3611765   1.8193932
0:   1.977478    1.8426661   1.2925382   0.7184062   0.05530691 -0.7634959
0:  -1.3575935  -1.775567   -1.871355   -1.6049385  -1.1823835  -0.70259285
0:  -1.4902916  -1.1002245 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.965838   4.1509943  4.7842817  5.5389395  6.0495596  6.3385954
0:   6.757176   7.250394   7.7637644  8.191066   8.413003   8.381671
0:   8.474251   8.938974   9.975571  11.542334  13.300621  14.929502
0:  11.944315  11.919754 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.646547 28.814041 29.140448 29.645807 30.244135 30.623438 31.507452
0:  32.044735 32.411053 32.367504 31.630846 30.770483 29.900593 29.507355
0:  29.647835 30.215176 30.940563 31.380844 30.836018 30.942978]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.01401   -6.1138463 -5.8053355 -5.081778  -4.2801194 -3.6608195
0:  -3.4741921 -3.462792  -3.9811606 -4.481378  -5.020884  -5.7826257
0:  -6.354305  -6.9065375 -7.21446   -7.100474  -6.69105   -6.1035275
0:  -7.3904357 -7.6622834]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.077549 21.787943 21.669737 21.598352 21.432508 21.00213  20.963905
0:  20.767645 20.65601  20.465267 19.86182  19.211138 18.701248 18.52521
0:  18.916887 19.732658 20.765095 21.707277 19.30279  19.180008]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.201731  3.2961876 3.6904528 4.216507  4.556078  4.6189356 4.536423
0:  4.438819  4.193057  4.0548487 3.8357081 3.3717027 2.9366531 2.5999398
0:  2.5721126 3.0327728 3.7618752 4.508454  3.2560842 3.3945909]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.216898 20.037031 20.024755 20.126427 20.22226  20.137821 20.534285
0:  20.778687 21.060383 21.200792 20.865582 20.498755 20.188143 20.344397
0:  21.042242 22.130426 23.327114 24.244251 23.114008 23.364168]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.621567   -9.628344   -9.2212715  -8.448051   -7.7122626  -7.266233
0:   -7.1348844  -7.3013816  -7.8795295  -8.435913   -9.032059   -9.752832
0:  -10.187212  -10.400076  -10.28504    -9.789867   -9.242243   -8.76058
0:   -9.989527  -10.06451  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.929111 17.735    17.726074 17.811369 17.771847 17.418083 17.437157
0:  17.33397  17.262314 17.106766 16.52887  15.776642 14.969307 14.479605
0:  14.423045 14.785198 15.28002  15.609478 12.94599  12.487631]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.2235594  4.6997685  5.510382   6.566296   7.4609437  8.128792
0:   8.450174   8.838945   8.957353   9.219149   9.4221735  9.225805
0:   8.906275   8.450277   8.206925   8.491487   9.2746935 10.200718
0:   8.000305   7.992802 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.538767 13.560862 13.710436 13.85778  13.831575 13.55677  13.377537
0:  13.185823 12.909779 12.612596 12.163035 11.576689 11.187051 11.06306
0:  11.451027 12.25229  13.245272 14.151049 13.824535 13.93727 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7219553  -1.5024872  -1.038199   -0.3368454   0.36102772  0.8348818
0:   1.1373668   1.228116    0.93324757  0.6835666   0.30002546 -0.21521425
0:  -0.56777763 -0.78362894 -0.6288614  -0.17146873  0.43942165  0.9899564
0:   0.6301875   0.9040842 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.05070114 -0.04913235  0.11432123  0.5924077   1.0532494   1.3853922
0:   1.3390212   1.2720866   0.8242607   0.51459074  0.26759005 -0.19129896
0:  -0.5613656  -0.9399276  -1.0531077  -0.6723552   0.14763069  1.1701727
0:   1.0627017   1.1180887 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.8544     -9.983472   -9.673764   -8.920587   -8.1182     -7.5202312
0:   -7.2317038  -7.198447   -7.7119217  -8.256029   -8.916096   -9.781409
0:  -10.407485  -10.890492  -11.046338  -10.784202  -10.351111   -9.8227415
0:  -10.443849  -10.2954645]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2191315 -2.4396114 -2.2927246 -1.9450798 -1.6524458 -1.5567656
0:  -1.428855  -1.3813581 -1.5271196 -1.6845045 -2.0450373 -2.618556
0:  -3.0644722 -3.2612705 -3.1388254 -2.6422224 -2.0997753 -1.6721106
0:  -4.5903215 -4.977763 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.059479 19.619287 19.299297 19.051985 18.801764 18.28196  18.256254
0:  18.037415 17.901962 17.590343 16.808092 16.010973 15.239467 14.901945
0:  15.045437 15.558861 16.195019 16.630648 15.83918  15.570608]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.064398 13.218391 13.521053 13.850816 14.021212 13.96794  14.0701
0:  14.118456 14.139923 14.127399 13.897322 13.493911 13.098049 12.936142
0:  13.117466 13.652872 14.405598 15.05554  13.838383 14.113787]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.606974  11.769556  11.950732  11.918647  11.748595  11.434711
0:  11.747591  12.129927  12.6095085 12.953261  12.7104645 12.341028
0:  11.870363  11.986752  12.582812  13.610038  14.607372  15.272074
0:  14.008341  14.1669235]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.592604  9.93853  10.453019 10.935253 11.263954 11.431873 11.839586
0:  12.20323  12.55021  12.834093 12.736176 12.510099 12.286791 12.395988
0:  12.911878 13.737162 14.60018  15.308891 13.75869  14.049608]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.2324753  4.714091   5.340599   6.121332   6.697284   6.924018
0:   7.455726   7.9067507  8.261925   8.613215   8.641626   8.523675
0:   8.426133   8.521141   8.904417   9.507697  10.043374  10.3226795
0:   9.280394   9.436394 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2248797  1.7704124  2.7339888  4.0728526  5.3530416  6.419267
0:   7.2120905  7.8813167  8.21607    8.596616   8.891703   8.921415
0:   8.954777   8.97917    9.221175   9.794096  10.526274  11.171138
0:   8.984392   8.981249 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.0325556 -5.4231224 -5.416191  -5.188308  -5.1314135 -5.281643
0:  -5.624037  -5.9770327 -6.472419  -6.6400385 -6.683707  -6.879586
0:  -7.0744686 -7.2606916 -7.368886  -7.2348814 -6.9999695 -6.712101
0:  -7.1685114 -7.4758615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.556386 10.640544 11.009165 11.50542  11.896667 12.060682 12.371187
0:  12.574836 12.674995 12.70688  12.434099 11.943329 11.518627 11.346008
0:  11.599921 12.255661 13.077155 13.721159 11.768151 11.787498]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.377768  15.573654  13.842858  12.357513  11.196511  10.365619
0:  10.347689  10.6154785 11.015331  11.360554  11.230125  10.825802
0:  10.3050165 10.074311  10.122236  10.415495  10.735083  10.885373
0:   9.19106    8.764892 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8934922  -1.9703641  -1.7926192  -1.4116654  -1.0965409  -0.98842955
0:  -0.8685832  -0.8229599  -1.0209413  -1.2195225  -1.6439509  -2.284594
0:  -2.8589234  -3.1902127  -3.2051454  -2.824081   -2.3121934  -1.8370996
0:  -3.2183776  -3.2397904 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.743818 29.443443 29.113422 28.805887 28.530529 27.97479  28.028255
0:  27.80441  27.599323 27.183968 26.249926 25.46081  24.789312 24.702822
0:  25.154215 25.973307 26.792368 27.280771 27.073963 27.032373]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6008434 -3.681036  -3.4018784 -2.891224  -2.4564362 -2.2059574
0:  -2.0291562 -1.8868523 -1.9929943 -2.094942  -2.369547  -2.9200506
0:  -3.3679857 -3.6795206 -3.669959  -3.265462  -2.7445498 -2.2949877
0:  -3.8713465 -3.8640985]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.2695794  -6.352571   -4.9310536  -3.174811   -1.5746293  -0.31250715
0:   0.72518635  1.4284711   1.6096506   1.5441351   1.0878592   0.29783964
0:  -0.41307354 -0.9672766  -1.122766   -0.8718686  -0.5433097  -0.2049141
0:  -2.1524558  -2.1271558 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.095354  -10.185085   -9.850252   -9.040861   -8.114384   -7.362087
0:   -7.0069785  -6.850737   -7.2102847  -7.55537    -7.9108424  -8.411194
0:   -8.709236   -8.970554   -9.076191   -8.936361   -8.68766    -8.354382
0:   -9.921652  -10.4523945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.297719  -15.29635   -14.775306  -13.704074  -12.539973  -11.618918
0:  -11.02926   -10.748355  -10.958339  -11.216178  -11.57191   -12.0992
0:  -12.376643  -12.433072  -12.157159  -11.468844  -10.669335   -9.9789
0:  -12.220956  -12.4117365]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.89023   8.559653  8.481659  8.514525  8.41797   8.043087  7.6579084
0:  7.242634  6.7377253 6.3780527 5.987519  5.5958853 5.342224  5.3559856
0:  5.618767  6.0958414 6.581689  6.8400936 3.7273693 3.6616395]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.633985  11.386684  11.450911  11.622572  11.732969  11.608307
0:  11.713117  11.67523   11.609766  11.4911785 11.023197  10.422922
0:   9.848993   9.634869   9.930366  10.674904  11.622002  12.486769
0:  10.966189  10.814703 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.2434716 -5.3847136 -5.2077174 -4.7145524 -4.1112027 -3.592484
0:  -3.2393584 -3.0007539 -3.1733174 -3.3825545 -3.6777396 -4.14817
0:  -4.4564614 -4.64343   -4.5512376 -4.0654387 -3.378467  -2.6637197
0:  -4.6546206 -4.8889413]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6510339  -1.6312318  -1.3233266  -0.7364278  -0.18424034  0.17255497
0:   0.20214081  0.16474819 -0.2809105  -0.7106662  -1.1929021  -1.8921208
0:  -2.4525619  -2.9046683  -3.0373225  -2.7283149  -2.197558   -1.5840602
0:  -2.9716344  -3.0283732 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.080148  -10.928742  -10.696595  -10.330298  -10.13453   -10.241866
0:   -9.866996   -9.581688   -9.324821   -9.216752   -9.651968  -10.420426
0:  -11.341918  -11.82771   -11.969947  -11.85584   -11.859963  -12.261664
0:  -12.7511835 -12.440009 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.505023  -5.5875835 -5.3223543 -4.712022  -4.1224575 -3.7178822
0:  -3.6352606 -3.6678314 -4.1240587 -4.473893  -4.8229427 -5.3268003
0:  -5.655964  -5.901204  -5.8551598 -5.3694706 -4.640212  -3.8207474
0:  -4.6384935 -4.572109 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.609446 25.53477  25.430876 25.236084 25.014551 24.598162 24.708769
0:  24.731977 24.79378  24.67837  24.027456 23.362404 22.713606 22.45829
0:  22.646126 23.107494 23.575472 23.837439 23.224318 23.23173 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5186615 6.535433  6.7145667 7.0248294 7.360922  7.5940332 7.925185
0:  8.213392  8.2635765 8.290549  8.12718   7.864215  7.746411  7.821787
0:  8.159326  8.626455  9.037328  9.301445  7.132281  7.2453613]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.575164    3.413461    3.6029058   4.048683    4.415575    4.512301
0:   4.44759     4.1270027   3.4139626   2.7191658   1.8786063   0.93015337
0:   0.20433474 -0.29733467 -0.47849655 -0.45697737 -0.5981846  -0.95453596
0:  -2.295279   -2.5665965 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.7593007 2.1727777 2.8915925 3.7803118 4.409763  4.7709618 4.900681
0:  5.0749035 5.062175  5.1380024 5.129057  4.7914524 4.4572268 4.157669
0:  4.2377944 4.749462  5.58753   6.4496474 5.8192115 6.014181 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.556391  -5.6278915 -5.3533936 -4.6830406 -3.9565191 -3.3626223
0:  -3.2523909 -3.204526  -3.6728182 -4.038701  -4.4091825 -4.9914017
0:  -5.436863  -5.84692   -5.9289293 -5.503639  -4.6574793 -3.703804
0:  -4.6304636 -4.857997 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7826047  -1.4805284  -1.008328   -0.38826084  0.12584925  0.4484601
0:   0.70021915  0.8896632   0.86149454  0.8134413   0.63320494  0.23189974
0:  -0.11098003 -0.27020788 -0.13082981  0.42306852  1.2148924   2.0093617
0:   2.2147956   2.6128178 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.907959  -4.0443144 -3.7861738 -3.3382053 -3.0068989 -2.9054604
0:  -2.7928    -2.752635  -2.864275  -2.9841805 -3.3379097 -3.9579077
0:  -4.4989576 -4.8065166 -4.8213882 -4.530715  -4.225548  -4.0972767
0:  -6.953994  -7.012971 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.4878087 -4.3529162 -3.8309608 -3.1070647 -2.5320067 -2.2565646
0:  -2.218001  -2.2855287 -2.6076293 -2.8661857 -3.20262   -3.745399
0:  -4.182573  -4.43006   -4.3653417 -3.8615642 -3.1735988 -2.546618
0:  -4.3477254 -4.125179 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.822666  6.559448  6.481102  6.482661  6.3812265 6.1861067 6.16362
0:  6.2698984 6.373192  6.565891  6.68174   6.6582017 6.7653823 7.068674
0:  7.6339855 8.463663  9.269085  9.857334  7.237418  6.9231253]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.482019 15.477882 15.498665 15.438919 15.317281 15.005937 15.419712
0:  15.777992 16.23925  16.536705 16.16944  15.767599 15.230095 15.377864
0:  16.058586 17.122837 18.096502 18.509094 17.985981 18.082493]
0: validation loss for strategy=forecast at epoch 28 : 0.29373395442962646
0: validation loss for velocity_u : 0.17323727905750275
0: validation loss for velocity_v : 0.2864726185798645
0: validation loss for specific_humidity : 0.17259156703948975
0: validation loss for velocity_z : 0.47923222184181213
0: validation loss for temperature : 0.1118253543972969
0: validation loss for total_precip : 0.5390444397926331
0: 29 : 19:53:39 :: batch_size = 96, lr = 1.0267994564174285e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 29, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7498, -0.7841, -0.8116, -0.8279, -0.8204, -0.7886, -0.7568, -0.7463, -0.7556, -0.7751, -0.7961, -0.8143,
0:         -0.8252, -0.8233, -0.8115, -0.7940, -0.7701, -0.7346, -0.8107, -0.8395, -0.8521, -0.8465, -0.8241, -0.7934,
0:         -0.7705], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.4026, 1.3536, 1.2819, 1.1927, 1.0828, 0.9665, 0.8865, 0.8553, 0.8445, 0.8362, 0.8301, 0.8293, 0.8380, 0.8551,
0:         0.8773, 0.8981, 0.9149, 0.9345, 1.3908, 1.3305, 1.2493, 1.1528, 1.0512, 0.9644, 0.9128], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.9537,  0.8675,  0.7818,  0.7059,  0.6402,  0.5822,  0.5250,  0.4549,  0.3566,  0.2144,  0.0224, -0.1755,
0:         -0.3115, -0.3882, -0.4085, -0.4000, -0.3828, -0.3581,  1.0068,  0.8972,  0.7906,  0.6965,  0.6316,  0.5826,
0:          0.5456], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0857, -0.6410, -0.1941,  0.2463,  0.3705,  0.0108, -0.4186, -0.5756, -0.5473, -0.4470, -0.3314, -0.2551,
0:         -0.2639, -0.3293, -0.3772, -0.3598, -0.0415,  0.6649, -0.7805, -0.3729,  0.0065,  0.2594,  0.2114, -0.1134,
0:         -0.4121], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0207,  0.0179,  0.0574,  0.1057,  0.1611,  0.2241,  0.2936,  0.3629,  0.4256,  0.4857,  0.5402,  0.5798,
0:          0.5969,  0.5976,  0.5949,  0.5917,  0.5863,  0.5751,  0.5513,  0.5323,  0.5345,  0.5487,  0.5707,  0.5995,
0:          0.6335], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2285, -0.1617, -0.2203, -0.2449, -0.2449, -0.2449, -0.2449, -0.2390, -0.2296, -0.2449, -0.2320, -0.2449,
0:         -0.2449, -0.2449, -0.2449, -0.2449, -0.2437, -0.2413, -0.2449, -0.2449, -0.2449, -0.2449, -0.2449, -0.2449,
0:         -0.2449], device='cuda:0')
0: [DEBUG] Epoch 29, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2449,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan, -0.2449,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan, -0.2449,
0:             nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan, -0.2449,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449, -0.2449,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2449, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2449,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2449,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,
0:             nan,     nan,     nan, -0.2449,     nan,     nan, -0.2449,     nan, -0.2449,     nan,     nan,     nan,
0:             nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 29, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1473, -0.1509, -0.1412, -0.1105, -0.0888, -0.0703, -0.0566, -0.0425, -0.0605, -0.0918, -0.1473, -0.2349,
0:         -0.3235, -0.3907, -0.4143, -0.3801, -0.3131, -0.2485, -0.1199, -0.1725, -0.2088, -0.2174, -0.2147, -0.1865,
0:         -0.1430], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6382, 0.6390, 0.6170, 0.5901, 0.5649, 0.5453, 0.5188, 0.5083, 0.5055, 0.5067, 0.5022, 0.4861, 0.4454, 0.3814,
0:         0.3263, 0.3096, 0.3413, 0.4022, 0.6040, 0.6149, 0.6001, 0.5717, 0.5418, 0.5140, 0.5025], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3420, 0.5547, 0.7868, 0.9906, 1.1361, 1.2178, 1.2720, 1.3067, 1.3590, 1.4124, 1.4613, 1.4731, 1.4340, 1.3112,
0:         1.0545, 0.7373, 0.3756, 0.0512, 0.2599, 0.4985, 0.7519, 0.9631, 1.1250, 1.2155, 1.2789], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0265,  0.2857,  0.5156,  0.4330,  0.1335,  0.1441,  0.2314,  0.1390,  0.1798,  0.3030,  0.4543,  0.6425,
0:          0.7921,  0.7620,  0.6187,  0.6753,  0.7190,  0.5729,  0.2159,  0.2701,  0.2328,  0.1953,  0.0254,  0.0302,
0:          0.1098], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.2466, 1.1484, 1.0357, 0.9308, 0.8626, 0.8389, 0.8459, 0.8513, 0.8360, 0.7893, 0.7145, 0.6274, 0.5510, 0.5101,
0:         0.5027, 0.5282, 0.5761, 0.6370, 0.7037, 0.7705, 0.8364, 0.8890, 0.9252, 0.9400, 0.9250], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2498, -0.2534, -0.2620, -0.2558, -0.2484, -0.2407, -0.2391, -0.2368, -0.2333, -0.2553, -0.2496, -0.2487,
0:         -0.2487, -0.2443, -0.2383, -0.2329, -0.2448, -0.2499, -0.2468, -0.2421, -0.2459, -0.2407, -0.2354, -0.2431,
0:         -0.2353], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.22832363843917847; velocity_v: 0.29393061995506287; specific_humidity: 0.18243537843227386; velocity_z: 0.5473541617393494; temperature: 0.19491340219974518; total_precip: 0.620945155620575; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17927855253219604; velocity_v: 0.2834251821041107; specific_humidity: 0.18305328488349915; velocity_z: 0.5192660093307495; temperature: 0.13970662653446198; total_precip: 0.5659228563308716; 
0: epoch: 29 [1/5 (20%)]	Loss: 0.59343 : 0.29422 :: 0.20386 (2.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2464890033006668; velocity_v: 0.35966888070106506; specific_humidity: 0.19199194014072418; velocity_z: 0.6492301225662231; temperature: 0.1727931797504425; total_precip: 0.5475012063980103; 
0: epoch: 29 [2/5 (40%)]	Loss: 0.54750 : 0.32657 :: 0.20729 (15.92 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21218840777873993; velocity_v: 0.35748088359832764; specific_humidity: 0.18086031079292297; velocity_z: 0.586056649684906; temperature: 0.15037347376346588; total_precip: 0.7538201212882996; 
0: epoch: 29 [3/5 (60%)]	Loss: 0.75382 : 0.33865 :: 0.21323 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17386853694915771; velocity_v: 0.328875869512558; specific_humidity: 0.19192148745059967; velocity_z: 0.4919191598892212; temperature: 0.13444112241268158; total_precip: 0.48684659600257874; 
0: epoch: 29 [4/5 (80%)]	Loss: 0.48685 : 0.26780 :: 0.20041 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [1.05381012e-04 7.34329224e-05 4.24385071e-05 5.38825989e-05
0:  8.53538513e-05 1.15394592e-04 1.08242035e-04 8.63075256e-05
0:  6.58035278e-05 5.00679016e-05 3.48091125e-05 3.05175781e-05
0:  3.71932983e-05 4.33921814e-05 3.00407410e-05 2.05039978e-05
0:  1.09672546e-05 1.04904175e-05 1.23977661e-05 1.81198120e-05
0:  2.00271606e-05 1.95503235e-05 1.43051147e-05 1.52587891e-05
0:  1.95503235e-05 2.52723694e-05 2.90870667e-05 3.00407410e-05
0:  3.19480896e-05 3.24249268e-05 3.24249268e-05 3.48091125e-05
0:  3.71932983e-05 3.95774841e-05 4.48226965e-05 4.19616663e-05
0:  3.76701355e-05 2.76565552e-05 2.81333923e-05 2.62260437e-05
0:  2.14576721e-05 1.62124634e-05 1.47819519e-05 1.62124634e-05
0:  1.71661377e-05 2.14576721e-05 3.05175781e-05 4.00543213e-05
0:  4.24385071e-05 4.62532043e-05 4.95910645e-05 4.76837158e-05
0:  4.48226965e-05 4.38690186e-05 4.52995264e-05 4.67300415e-05
0:  5.29289246e-05 7.58171082e-05 9.82284546e-05 1.01089478e-04
0:  9.72747803e-05 9.39369202e-05 7.96318054e-05 5.00679016e-05
0:  3.24249268e-05 3.38554382e-05 3.00407410e-05 2.52723694e-05
0:  2.81333923e-05 3.14712524e-05 3.24249268e-05 3.52859497e-05
0:  3.86238062e-05 4.00543213e-05 4.00543213e-05 4.05311584e-05
0:  4.24385071e-05 3.86238062e-05 3.48091125e-05 4.10079956e-05
0:  5.29289246e-05 6.38961792e-05 5.38825989e-05 4.52995264e-05
0:  3.57627869e-05 3.86238062e-05 4.86373901e-05 5.24520874e-05
0:  6.00814819e-05 6.86645508e-05 7.72476196e-05 8.63075256e-05
0:  9.53674316e-05 8.58306885e-05 8.20159912e-05 7.86781311e-05
0:  7.39097595e-05 6.48498535e-05 6.29425049e-05 5.76972961e-05
0:  4.76837158e-05 4.52995264e-05 3.43322754e-05 2.28881836e-05
0:  1.33514404e-05 1.09672546e-05 8.10623169e-06 4.76837158e-06
0:  1.26838684e-04 1.00612640e-04 7.15255737e-05 8.34465027e-05
0:  7.24792480e-05 5.10215759e-05 5.34057617e-05 5.96046448e-05
0:  4.00543213e-05 2.67028809e-05 2.43186951e-05 2.19345093e-05
0:  1.76429749e-05 1.71661377e-05 1.62124634e-05 1.09672546e-05
0:  9.05990601e-06 7.62939453e-06 6.67572021e-06 7.62939453e-06
0:  7.62939453e-06 1.04904175e-05 1.38282776e-05 2.05039978e-05
0:  2.67028809e-05 3.00407410e-05 2.86102295e-05 2.14576721e-05
0:  1.62124634e-05 1.38282776e-05 1.23977661e-05 1.38282776e-05
0:  1.81198120e-05 2.33650208e-05 2.24113464e-05 1.95503235e-05
0:  1.66893005e-05 1.66893005e-05 1.81198120e-05 1.90734863e-05
0:  1.90734863e-05 1.85966492e-05 1.52587891e-05 1.81198120e-05
0:  2.24113464e-05 2.76565552e-05 3.38554382e-05 4.19616663e-05
0:  4.76837158e-05 5.19752502e-05 5.67436218e-05 7.00950623e-05
0:  8.44001770e-05 8.29696655e-05 8.01086426e-05 5.91278076e-05
0:  4.05311584e-05 2.90870667e-05 2.86102295e-05 3.95774841e-05
0:  3.86238062e-05 4.05311584e-05 4.38690186e-05 4.81605530e-05
0:  2.90870667e-05 2.05039978e-05 2.00271606e-05 1.95503235e-05
0:  1.62124634e-05 1.38282776e-05 1.66893005e-05 1.81198120e-05
0:  1.47819519e-05 1.23977661e-05 1.23977661e-05 1.23977661e-05
0:  1.04904175e-05 9.05990601e-06 9.05990601e-06 9.53674316e-06
0:  1.00135803e-05 1.95503235e-05 1.28746033e-05 1.38282776e-05
0:  1.47819519e-05 1.81198120e-05 2.28881836e-05 3.24249268e-05
0:  3.86238062e-05 4.19616663e-05 4.29153442e-05 4.10079956e-05]
0: Target values (first 200):
0: [2.38418579e-05 2.47955322e-05 2.57492065e-05 2.38418579e-05
0:  2.09808350e-05 2.00271606e-05 3.33786011e-05 5.14984131e-05
0:  6.00814819e-05 4.95910645e-05 3.81469763e-05 3.62396240e-05
0:  3.43322754e-05 3.24249268e-05 3.14712524e-05 2.47955322e-05
0:  1.81198120e-05 2.57492065e-05 4.95910645e-05 7.53402710e-05
0:  6.48498535e-05 4.48226965e-05 2.28881836e-05 3.43322754e-05
0:  3.81469763e-05 4.10079956e-05 4.76837158e-05 5.91278076e-05
0:  6.77108765e-05 6.48498535e-05 6.00814819e-05 4.10079956e-05
0:  2.38418579e-05 6.67572021e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 3.81469727e-06 7.62939453e-06 1.14440918e-05
0:  1.90734863e-05 2.86102295e-05 3.52859497e-05 3.05175781e-05
0:  2.57492065e-05 2.47955322e-05 2.57492065e-05 2.76565552e-05
0:  2.09808350e-05 1.23977661e-05 5.72204590e-06 8.58306885e-06
0:  1.23977661e-05 1.62124634e-05 2.19345093e-05 2.76565552e-05
0:  3.24249268e-05 4.10079956e-05 4.95910645e-05 5.05447388e-05
0:  4.95910645e-05 4.86373901e-05 5.81741333e-05 7.91549683e-05
0:  9.82284546e-05 8.10623169e-05 5.14984131e-05 2.95639038e-05
0:  1.81198120e-05 6.67572021e-06 1.90734863e-06 1.90734863e-06
0:  9.53674316e-07 9.53674316e-07 4.76837158e-06 9.53674316e-06
0:  1.14440918e-05 1.52587891e-05 1.90734863e-05 2.19345093e-05
0:  2.86102295e-05 3.52859497e-05 4.29153442e-05 5.53131104e-05
0:  6.86645508e-05 6.38961792e-05 5.14984131e-05 5.43594360e-05
0:  7.05718994e-05 8.58306885e-05 8.29696655e-05 7.62939453e-05
0:  6.96182251e-05 6.96182251e-05 7.72476196e-05 8.39233398e-05
0:  7.72476196e-05 7.05718994e-05 6.48498535e-05 6.10351562e-05
0:  5.72204590e-05 5.53131104e-05 5.05447388e-05 4.57763672e-05
0:  4.57763672e-05 3.24249268e-05 1.90734863e-05 2.00271606e-05
0:  2.19345093e-05 2.38418579e-05 2.57492065e-05 2.38418579e-05
0:  2.28881836e-05 2.28881836e-05 2.28881836e-05 2.28881836e-05
0:  3.71932983e-05 2.86102295e-05 2.95639038e-05 3.33786011e-05
0:  4.86373901e-05 5.81741333e-05 5.43594360e-05 3.91006433e-05
0:  1.81198120e-05 1.04904175e-05 1.14440918e-05 2.86102295e-05
0:  1.62124634e-05 3.05175781e-05 4.86373901e-05 5.62667847e-05
0:  6.38961792e-05 6.86645508e-05 7.62939453e-05 7.34329224e-05
0:  6.38961792e-05 4.38690186e-05 3.14712524e-05 2.00271606e-05
0:  1.04904175e-05 5.72204590e-06 4.76837158e-06 4.76837158e-06
0:  4.76837158e-06 9.53674316e-06 3.14712524e-05 5.24520874e-05
0:  6.19888306e-05 7.24792480e-05 7.72476196e-05 7.34329224e-05
0:  7.15255737e-05 7.05718994e-05 6.86645508e-05 6.86645508e-05
0:  6.29425049e-05 4.67300415e-05 2.76565552e-05 3.62396240e-05
0:  4.86373901e-05 5.34057617e-05 6.10351562e-05 7.53402710e-05
0:  8.10623169e-05 5.72204590e-05 4.38690186e-05 4.86373901e-05
0:  8.01086426e-05 1.39236450e-04 1.21116638e-04 5.62667847e-05
0:  5.14984131e-05 3.24249268e-05 2.19345093e-05 1.33514404e-05
0:  1.81198120e-05 2.19345093e-05 1.33514404e-05 1.14440918e-05
0:  6.67572021e-06 4.76837158e-06 7.62939453e-06 1.14440918e-05
0:  1.52587891e-05 1.52587891e-05 1.23977661e-05 1.33514404e-05
0:  1.52587891e-05 2.19345093e-05 3.33786011e-05 3.81469763e-05
0:  4.29153442e-05 5.05447388e-05 5.34057617e-05 4.95910645e-05
0:  6.00814819e-05 6.29425049e-05 6.38961792e-05 6.38961792e-05]
0: Prediction values (first 20):
0: [-5.868558  -6.0179434 -5.8267097 -5.1877155 -4.4675264 -3.8596144
0:  -3.6628332 -3.555048  -3.9834642 -4.3313017 -4.6740627 -5.19374
0:  -5.5405464 -5.9018707 -5.9346056 -5.5416946 -4.8050222 -3.9212341
0:  -4.602544  -4.7181034]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.939, max = 0.694, mean = -0.661
0:          sample (first 20): tensor([-0.9431, -0.9546, -0.9399, -0.8908, -0.8354, -0.7887, -0.7735, -0.7652, -0.7982, -0.8249, -0.8513, -0.8912,
0:         -0.9179, -0.9457, -0.9482, -0.9180, -0.8614, -0.7934, -0.8964, -0.9574])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.917912 18.707123 18.579086 18.438416 18.17715  17.676657 17.359755
0:  16.950735 16.505804 15.962049 15.094805 14.190893 13.29948  12.697678
0:  12.459628 12.509976 12.661297 12.720825 10.332426  9.899761]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.614088  13.511155  13.596454  13.7180195 13.732215  13.676115
0:  13.64629   13.643402  13.444902  13.156717  12.5909815 11.6554575
0:  10.666877   9.785011   9.290954   9.251468   9.584098  10.03485
0:   7.826189   8.401867 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2475486  -3.1957974  -2.7465425  -2.0855727  -1.52354    -1.2265034
0:  -0.9895015  -0.95683384 -1.1571326  -1.3239822  -1.7000937  -2.2918153
0:  -2.765616   -3.0141158  -2.9258533  -2.5294242  -2.0908313  -1.7578692
0:  -2.938129   -2.772325  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.9499826 7.28149   7.8363123 8.525042  9.108097  9.460222  9.602917
0:  9.674384  9.396735  9.179956  8.839613  8.309966  7.840141  7.4490595
0:  7.3917785 7.6940546 8.215482  8.742968  8.531899  8.9251   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.63617086 -0.68466663 -0.3511839   0.1798234   0.5994592   0.76059484
0:   0.8454013   0.7502661   0.34953117 -0.09189701 -0.7773557  -1.7481546
0:  -2.6193142  -3.2161617  -3.4501276  -3.1971512  -2.7663174  -2.3720675
0:  -3.682231   -3.8016357 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8620281 1.8717828 2.2207668 2.8489418 3.458414  3.9141755 4.3488846
0:  4.6541905 4.773195  4.888073  4.880025  4.691549  4.6704063 4.729656
0:  5.070669  5.7222795 6.485037  7.196086  5.89779   5.9432364]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.897865  10.577509  10.394385  10.16987    9.878338   9.364111
0:   9.4580965  9.535474   9.761878   9.874034   9.439464   8.889905
0:   8.1784935  8.029997   8.370363   9.112954   9.816771  10.159082
0:   9.482575   9.293422 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7445416 -4.5971246 -4.091995  -3.3663764 -2.7778888 -2.5026793
0:  -2.5073476 -2.8298001 -3.6354213 -4.3502994 -5.1491284 -6.0392375
0:  -6.656347  -7.0067496 -6.9997897 -6.5864415 -6.08853   -5.526008
0:  -5.2925334 -4.8763328]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2243543 -3.1720738 -2.8632007 -2.3055024 -1.7883267 -1.5026035
0:  -1.4360728 -1.4773755 -1.842197  -2.1406283 -2.4608579 -2.910952
0:  -3.2058663 -3.3203197 -3.1155677 -2.4927154 -1.6725373 -0.8734822
0:  -1.6428866 -1.6797175]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.127346 23.934296 23.630962 23.096952 22.471397 21.681545 21.705526
0:  21.735332 21.92195  21.957531 21.314056 20.711414 20.021614 20.05476
0:  20.648571 21.5629   22.37371  22.64241  22.155628 22.13255 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.70955086  0.36611605  0.4040494   0.7160826   0.99967575  1.1250234
0:   1.2481313   1.2803273   1.0542588   0.8776655   0.5643511   0.03070164
0:  -0.40589333 -0.6748624  -0.74327946 -0.62421036 -0.5619068  -0.6274128
0:  -4.6951575  -4.786855  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.6595964   1.8234453   2.327511    3.042794    3.6561995   4.025318
0:   4.1680613   4.198821    3.8830645   3.5285935   3.0166535   2.211499
0:   1.4883084   0.8069277   0.4550023   0.5685344   0.95703983  1.4290037
0:  -0.36078072 -0.3261118 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.31017   5.250619  5.4642963 5.8525004 6.1698084 6.2843885 6.563035
0:  6.7503433 6.8953004 6.9813466 6.793467  6.4516306 6.1285076 6.1331673
0:  6.510295  7.2479677 8.064013  8.703117  7.641227  7.7037277]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.5496917   5.184366    5.0030947   4.728195    4.249771    3.4969628
0:   3.0155575   2.451361    1.8462505   1.2014136   0.195117   -0.81849766
0:  -1.8070936  -2.0944376  -2.0081658  -1.6506147  -1.6390076  -2.029008
0:  -4.85037    -5.568624  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.268879  -7.233042  -6.84408   -6.066102  -5.1611986 -4.3611336
0:  -3.614417  -3.022071  -2.8511944 -2.78092   -2.911046  -3.2600956
0:  -3.4337988 -3.4314785 -3.1543322 -2.6198292 -2.073184  -1.6785002
0:  -2.3615398 -2.254037 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.793241  -4.7112556 -4.3332744 -3.6673694 -3.0782247 -2.6788354
0:  -2.498796  -2.4772172 -2.8765006 -3.2499766 -3.741136  -4.3891377
0:  -4.8304744 -5.07846   -4.9820447 -4.5302806 -4.0247083 -3.5573764
0:  -4.2713428 -4.18101  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.224518 10.222534 10.416708 10.811939 11.124971 11.310319 11.466377
0:  11.609594 11.564463 11.553689 11.432054 11.107723 10.860077 10.67836
0:  10.867756 11.411255 12.108515 12.707563 11.705616 11.751528]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.6615143 5.012542  5.628954  6.371852  6.973873  7.334694  7.6028767
0:  7.7270226 7.6127987 7.500308  7.247865  6.8198214 6.5050335 6.3287654
0:  6.4704967 6.959538  7.589553  8.181275  6.9475718 7.209034 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7555375  -2.7166562  -2.3338718  -1.6500525  -1.033771   -0.65614414
0:  -0.58624506 -0.5978823  -0.96758366 -1.2910571  -1.6385803  -2.2617989
0:  -2.7827764  -3.2497602  -3.4303699  -3.0761876  -2.391603   -1.5577722
0:  -2.506949   -2.6456752 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9899621 -2.3154387 -2.3507657 -2.0227551 -1.6468768 -1.3931127
0:  -1.385838  -1.3804498 -1.7345433 -2.002729  -2.3270297 -2.9384036
0:  -3.5027976 -4.0519643 -4.32361   -4.035785  -3.3102937 -2.3948255
0:  -2.4505992 -2.73459  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.072714 18.028694 18.196869 18.48608  18.703388 18.662106 18.926891
0:  18.953403 18.89087  18.638088 17.91837  17.10438  16.375538 16.095472
0:  16.309656 16.893555 17.583887 18.045332 15.754599 15.495287]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.559486   5.823291   6.338258   6.924037   7.3273225  7.4897814
0:   7.54973    7.619509   7.5325747  7.5390897  7.5082083  7.266782
0:   7.194055   7.2157903  7.572156   8.297985   9.205812  10.032238
0:   8.550297   8.899395 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.430777  12.309548  12.41218   12.574188  12.6192465 12.45435
0:  12.486031  12.410914  12.264257  12.043594  11.498108  10.837334
0:  10.220058   9.945091  10.080398  10.607036  11.249886  11.801937
0:  10.572157  10.674121 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.557774  -8.625338  -8.299936  -7.566405  -6.7867656 -6.2008862
0:  -6.015346  -6.0032525 -6.5075707 -6.975536  -7.485416  -8.163479
0:  -8.647049  -9.032841  -9.134502  -8.832745  -8.288223  -7.6139274
0:  -8.775965  -9.027842 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.043638  -5.124646  -4.859359  -4.2741127 -3.7473621 -3.440793
0:  -3.3167076 -3.3604145 -3.7907228 -4.2491045 -4.8832226 -5.7427597
0:  -6.416046  -6.8472614 -6.864992  -6.4590816 -5.959751  -5.482832
0:  -6.4843874 -6.6070776]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3046646 6.400424  6.71369   7.0280066 7.2023783 7.2297177 7.4856873
0:  7.731549  7.8994093 8.050369  7.902565  7.583576  7.2928267 7.3419585
0:  7.668969  8.19625   8.6039    8.698046  6.772841  6.9418974]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.592428   9.597576   9.8931265 10.276424  10.568661  10.638615
0:  10.979794  11.223785  11.480694  11.710033  11.630896  11.4728775
0:  11.395052  11.659622  12.338585  13.357747  14.442729  15.298219
0:  13.390757  13.625857 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.365363  10.026931  10.015316  10.206629  10.478874  10.66629
0:  11.436783  12.185228  13.080584  13.8834915 14.237859  14.430358
0:  14.531134  15.036225  15.924338  17.151257  18.319897  19.034292
0:  17.104105  17.133297 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4708142 3.7622662 4.333326  4.9818487 5.40516   5.531158  5.6009445
0:  5.5184584 5.322386  5.1788473 4.913184  4.502557  4.168543  4.088023
0:  4.3028584 4.83182   5.395151  5.8011293 4.5263247 4.8710337]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.956654   9.229802   9.671739  10.16864   10.477575  10.574357
0:  10.522346  10.472344  10.162933   9.962668   9.644606   9.095099
0:   8.532748   8.043028   7.8118973  7.9938555  8.407171   8.8653
0:   8.015314   8.048833 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9025543 4.0301857 4.423373  4.9402485 5.30303   5.431425  5.435983
0:  5.3848696 5.109736  4.8685203 4.4863358 3.893943  3.3783283 3.0146875
0:  2.9682202 3.3162289 3.8338797 4.3165393 2.6367023 2.7681236]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.615572 20.766565 21.022089 21.215822 21.181807 20.86443  20.775858
0:  20.601023 20.391191 20.144682 19.551142 18.84519  18.165401 17.800545
0:  17.87711  18.361185 18.998753 19.547604 17.620344 17.702358]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.210631   -3.2497697  -2.949326   -2.2864003  -1.5446153  -0.95202065
0:  -0.6000819  -0.3399639  -0.5218582  -0.68495417 -0.9459667  -1.4363093
0:  -1.8122334  -2.1383004  -2.215425   -1.9394698  -1.5247808  -1.1109848
0:  -2.690516   -2.8497329 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.892796 13.877003 13.903271 13.814913 13.609961 13.202026 13.364356
0:  13.540829 13.784996 13.923791 13.465083 12.919868 12.219115 12.042206
0:  12.35239  13.05926  13.800973 14.270249 15.314846 15.26256 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.804482 22.81047  22.888096 22.906384 22.72694  22.341343 22.166512
0:  21.880798 21.55215  21.118723 20.328999 19.38852  18.539051 18.015005
0:  18.002277 18.3699   18.943943 19.408691 17.53991  17.281715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.238113 29.960468 29.649063 29.3297   29.069714 28.619034 28.859535
0:  28.951244 29.204178 29.26744  28.723534 28.284767 27.887218 27.979725
0:  28.586906 29.45449  30.307655 30.795311 30.102158 30.062225]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.307591   8.801828   9.5582905 10.194374  10.366421  10.101229
0:   9.749252   9.532649   9.267226   9.244357   9.2009325  8.951055
0:   8.842882   8.949514   9.4876995 10.56118   11.908604  13.124575
0:  10.909978  11.576377 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.11838  21.065508 20.247286 19.547058 18.681366 17.582987 16.521158
0:  15.504292 14.460701 13.579039 12.571169 11.363393 10.256107  9.40236
0:   9.03392   9.236776  9.853155 10.628384  9.199497  8.553158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.490067 16.622452 16.914042 17.2134   17.32852  17.198982 17.05752
0:  16.931377 16.697304 16.509077 16.188295 15.656435 15.154297 14.765717
0:  14.760151 15.155338 15.839436 16.533916 14.28738  14.368726]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.070576  -10.128557   -9.8122635  -9.105553   -8.364866   -7.7602983
0:   -7.477982   -7.2846665  -7.5029626  -7.713245   -7.9616437  -8.426495
0:   -8.724386   -8.9435425  -8.892035   -8.452814   -7.8262677  -7.1583304
0:   -8.279745   -8.339104 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.6804214 -6.6672044 -6.2798085 -5.5488343 -4.8121095 -4.3185945
0:  -4.119821  -4.1988254 -4.7480454 -5.295361  -5.945535  -6.7136035
0:  -7.1907964 -7.4681325 -7.3772817 -6.9729314 -6.5491834 -6.161143
0:  -7.5425153 -7.658705 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.103624 19.755825 19.503693 19.222778 18.870508 18.330814 17.977404
0:  17.561646 17.063097 16.641335 16.001318 15.339558 14.780257 14.493392
0:  14.549337 14.851646 15.044798 15.081745 13.399902 13.143229]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.4770308 5.5408726 5.842413  6.375633  6.9166393 7.358571  7.6955843
0:  8.006824  8.02795   8.039453  7.928025  7.6112313 7.396606  7.2435713
0:  7.3461123 7.660396  8.028046  8.293581  6.7174773 6.757037 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.9354463 1.8352189 2.0776172 2.5162911 2.9294345 3.1337752 3.5254061
0:  3.8193436 4.1200304 4.3545365 4.3333645 4.1816854 4.014657  4.121266
0:  4.557022  5.3571763 6.2573037 7.00629   6.256831  6.347782 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.150575 -12.891543 -13.497781 -13.702394 -13.856671 -14.072606
0:  -13.750842 -13.538344 -13.485538 -13.628184 -14.224343 -15.294204
0:  -16.18451  -16.671185 -16.635029 -16.242851 -15.716466 -15.473586
0:  -15.409961 -15.727609]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.875725  -10.664978   -9.97275    -8.888166   -7.788714   -6.954745
0:   -6.506391   -6.3536572  -6.705443   -7.053162   -7.449225   -7.950291
0:   -8.178289   -8.257492   -8.057505   -7.569793   -7.0723023  -6.636948
0:   -7.580643   -7.3384757]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.855383  -15.00518   -14.8091755 -14.188467  -13.502798  -12.975626
0:  -12.672874  -12.5678425 -12.923752  -13.362621  -13.995684  -14.812237
0:  -15.4554405 -15.838442  -15.7488785 -15.337212  -14.866901  -14.438068
0:  -13.951315  -14.297249 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.047417  10.48221   11.180684  11.949854  12.5794935 12.937372
0:  13.364141  13.652855  13.767372  13.848901  13.7631    13.466587
0:  13.369389  13.51253   13.986684  14.712597  15.322552  15.626443
0:  12.86071   12.711561 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.535454  11.605319  11.672119  11.670537  11.613179  11.455014
0:  11.805999  12.127633  12.533843  12.828804  12.704908  12.553476
0:  12.403093  12.743967  13.567828  14.7428665 15.96579   16.963287
0:  17.908886  18.17213  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.352941  -9.267223  -8.706844  -7.747759  -6.7653484 -6.0541553
0:  -5.6365805 -5.498007  -5.8180237 -6.167638  -6.619672  -7.281045
0:  -7.700197  -7.9744387 -7.9030933 -7.420887  -6.793747  -6.1591864
0:  -8.117834  -7.978634 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.665915  -7.551187  -7.0472903 -6.285585  -5.5981755 -5.2213006
0:  -5.1720214 -5.323856  -5.853865  -6.330852  -6.8048663 -7.410826
0:  -7.820169  -8.087307  -8.112513  -7.8838544 -7.60072   -7.3420835
0:  -8.295099  -7.914788 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.539097 21.506222 21.378235 21.231237 21.141949 20.873142 21.36443
0:  21.65827  22.056503 22.217903 21.679718 21.162731 20.642788 20.703041
0:  21.259056 22.159813 23.071865 23.577244 23.169964 23.48766 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.163816  18.791372  18.507067  18.287592  18.099424  17.62481
0:  17.802843  17.754416  17.82438   17.719927  17.018646  16.32877
0:  15.581329  15.358263  15.6231575 16.33587   17.15532   17.701769
0:  16.98762   16.822674 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.742875 10.028986 10.454077 10.839875 11.007902 10.970136 11.222723
0:  11.448504 11.630709 11.67197  11.290199 10.689726 10.115234  9.937931
0:  10.203209 10.906946 11.683675 12.260199 10.522156 10.621557]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.662288  10.341575  10.198379  10.063748   9.79702    9.386153
0:   9.112318   8.864941   8.6303215  8.474559   8.160559   7.7102437
0:   7.387677   7.355953   7.691816   8.328995   9.026003   9.511047
0:   7.357921   7.394522 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.11628342 0.34587383 0.9776368  1.9515672  2.8796692  3.570713
0:  3.9129605  4.0906215  3.8605998  3.6311247  3.3739343  3.0074158
0:  2.8426418  2.8605561  3.211235   3.9011915  4.705043   5.4177413
0:  5.3718176  5.645698  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.656682  6.701245  7.029666  7.433896  7.634232  7.518592  7.6258674
0:  7.589378  7.444707  7.1985245 6.489925  5.581963  4.605405  4.0959463
0:  4.1462593 4.6866775 5.3662786 5.8973923 5.211112  5.354037 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.229042 39.063747 38.42036  37.58265  36.77497  35.943737 36.257214
0:  36.60034  37.097565 37.222137 36.42942  35.80542  35.132835 35.249996
0:  36.047478 37.00511  37.777107 37.7798   38.272484 38.221684]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.9939523 5.475214  5.2618713 5.2942934 5.2242823 4.984029  4.7924147
0:  4.638244  4.373891  4.2029667 3.9640067 3.5968826 3.374876  3.4120133
0:  3.8977754 4.7880588 5.8444333 6.6736298 4.0484915 3.9602149]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7295136 3.9758801 4.4560604 4.8489866 5.0375533 5.016756  5.213771
0:  5.389854  5.5488276 5.6609783 5.4260473 4.980873  4.5166426 4.4372396
0:  4.686034  5.28796   5.8085546 6.1176434 5.0716057 5.289678 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.874964 20.267418 19.6391   19.048576 18.517859 17.851887 17.972813
0:  17.954039 17.974506 17.736357 16.783125 15.914194 15.019562 14.919954
0:  15.419588 16.291813 17.10743  17.421282 16.518492 16.256554]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.493693  8.578705  8.74567   8.796722  8.803868  8.819071  9.346283
0:  10.051807 10.754726 11.431723 11.776464 11.976492 12.244235 13.003233
0:  14.070822 15.331312 16.405767 17.009495 16.25016  16.780418]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.367268 18.537066 18.78785  18.980844 19.01344  18.773342 18.922647
0:  18.874979 18.874203 18.780579 18.272224 17.660006 17.093378 16.988096
0:  17.328602 18.09803  18.977884 19.597385 18.513863 18.754467]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.805141  10.464443  10.357296  10.280838  10.0255995  9.526354
0:   9.019728   8.501724   7.9328732  7.3794613  6.7557983  6.0972114
0:   5.5926266  5.332796   5.522905   6.0648437  6.762386   7.4033127
0:   4.7254777  4.704399 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.50025   9.39705   9.486641  9.526367  9.394263  9.017756  8.918266
0:  8.749927  8.577137  8.438635  8.023417  7.517526  7.0420494 7.0026736
0:  7.315036  7.956894  8.545275  8.924257  7.166787  7.1681886]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.995073 18.759302 18.44388  17.880116 17.238928 16.399096 16.420992
0:  16.456121 16.753605 16.865812 16.298443 15.727688 15.063393 15.068989
0:  15.620371 16.557467 17.390886 17.689592 18.361101 18.269407]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7865586 -5.029038  -4.890166  -4.3461623 -3.7282634 -3.30022
0:  -3.1143231 -3.053145  -3.4284472 -3.8397803 -4.3996835 -5.25857
0:  -5.971399  -6.537832  -6.8044286 -6.6056685 -6.2536645 -5.754809
0:  -6.1864295 -6.4989285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.4388957 -4.19196   -3.5953813 -2.7764592 -2.0414028 -1.5373001
0:  -1.3049493 -1.2076583 -1.5169187 -1.836525  -2.2385244 -2.816626
0:  -3.1914372 -3.4728675 -3.4800735 -3.1496224 -2.734344  -2.3147078
0:  -4.299561  -4.23443  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.847553  11.240419  11.892573  12.519039  12.877062  12.997452
0:  13.1515665 13.309625  13.381594  13.547118  13.560688  13.3888
0:  13.303642  13.386055  13.8666725 14.675113  15.573758  16.342478
0:  14.488629  15.023185 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2682557 -3.090529  -2.570291  -1.8896132 -1.373827  -1.1496048
0:  -1.0790391 -1.1857891 -1.5436249 -1.9292378 -2.4714131 -3.174109
0:  -3.756126  -4.1107364 -4.114351  -3.7711549 -3.3475947 -2.9795632
0:  -5.1505523 -5.2020926]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.144558 21.136343 21.234959 21.326818 21.31211  21.035877 21.085217
0:  20.895113 20.720406 20.376984 19.634647 18.89366  18.317123 18.24038
0:  18.753906 19.713022 20.829716 21.78432  21.005196 20.923088]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.665833 -16.976622 -16.752768 -15.979832 -15.081755 -14.334785
0:  -13.92263  -13.706659 -14.03068  -14.365576 -14.871708 -15.66555
0:  -16.266094 -16.789555 -16.991125 -16.769012 -16.402506 -15.942781
0:  -17.606174 -17.878881]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.136436 28.334911 28.246845 27.703087 26.967415 25.98011  25.853123
0:  25.677921 25.646275 25.332008 24.31921  23.39719  22.580587 22.60768
0:  23.367512 24.587242 25.838432 26.593414 28.412544 28.66589 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8904929 2.2281902 2.8472757 3.5869067 4.138467  4.448716  4.579467
0:  4.671423  4.558235  4.47633   4.304292  3.9113119 3.5860045 3.3634024
0:  3.494981  3.996035  4.6797695 5.3127418 4.0213137 4.2613306]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.236827  7.196125  7.4473667 7.914832  8.326942  8.535751  8.782527
0:  8.909331  8.880801  8.805379  8.516913  8.051696  7.639469  7.4553885
0:  7.595859  8.013523  8.430244  8.652932  6.6608963 6.552493 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.619833 23.989359 23.291636 22.618118 22.031525 21.218054 21.109724
0:  20.771004 20.612656 20.268135 19.41339  18.73191  18.176704 18.212751
0:  18.833303 19.768896 20.766727 21.398148 21.146805 20.901684]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [41.10553  40.924084 40.492046 40.019867 39.583458 38.958054 39.28138
0:  39.49581  39.89259  39.965637 39.28227  38.74368  38.20825  38.398335
0:  39.280434 40.379433 41.366665 41.668304 42.76288  42.88722 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.1740465  4.49596    5.0587006  5.8187666  6.6046224  7.2557583
0:   7.9632897  8.565261   8.935961   9.351849   9.67165    9.864515
0:  10.211143  10.689032  11.383662  12.219372  13.050926  13.6941395
0:  12.343109  12.954805 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7736931  -1.5700331  -0.9876127  -0.2719679   0.29934263  0.614326
0:   0.9053221   1.025075    0.90799665  0.7148371   0.24330473 -0.4706092
0:  -1.0757756  -1.4402275  -1.4321175  -1.0568819  -0.68054056 -0.4231186
0:  -2.7506247  -3.0165586 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.605656  13.668556  14.051769  14.530488  14.808914  14.768944
0:  14.741027  14.682117  14.571118  14.522734  14.328613  13.92281
0:  13.589246  13.331598  13.44696   13.914566  14.557819  15.0982895
0:  12.616518  12.584999 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.33991814 -0.46855974 -0.30015326  0.0472908   0.26338053  0.24037075
0:   0.27537775  0.2225976   0.02443123 -0.19142294 -0.644289   -1.2839589
0:  -1.8583016  -2.1420536  -2.0407515  -1.5204687  -0.9002662  -0.38601017
0:  -1.8170791  -1.840992  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.949218  4.9810724 5.295615  5.8049674 6.1437864 6.243001  6.215058
0:  6.2006564 5.963801  5.810114  5.533139  5.025506  4.57086   4.199799
0:  4.1773434 4.5960255 5.257721  5.922088  4.6926327 4.708323 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1234035 -1.5748076 -2.182365  -2.7125278 -3.3528247 -4.092796
0:  -4.3996377 -4.6530538 -4.8804994 -5.086766  -5.4886956 -5.9966702
0:  -6.4921336 -6.490606  -6.271026  -5.690701  -4.9824452 -4.598045
0:  -5.3424935 -5.79748  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8248649 1.8898878 2.2043905 2.7921348 3.496556  4.165723  4.7765484
0:  5.2848954 5.3640037 5.3860407 5.272855  4.979989  4.9171257 4.9039235
0:  5.176513  5.6231465 6.086084  6.4872785 4.6384053 4.8072176]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.4745297 -5.878465  -5.8994927 -5.443149  -4.8404737 -4.321467
0:  -4.1886926 -4.189384  -4.7337613 -5.296457  -5.9377646 -6.869594
0:  -7.640618  -8.391628  -8.802164  -8.712191  -8.228624  -7.5164313
0:  -8.911715  -9.2517185]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.928623  4.9818077 5.326618  5.897022  6.3451185 6.550785  6.610042
0:  6.5662827 6.2561064 5.9918566 5.597849  4.949785  4.3814254 3.927955
0:  3.763987  4.034857  4.5616736 5.08099   3.0518413 3.2825289]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2148614   1.0838947   1.2370634   1.678154    2.1266203   2.368433
0:   2.268814    2.004972    1.2585464   0.4975109  -0.33126497 -1.4098558
0:  -2.3094063  -3.118082   -3.6025276  -3.543951   -3.1049967  -2.4549265
0:  -3.468504   -3.7055464 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.084995  10.065262  10.1231575 10.2256    10.225338   9.9568615
0:  10.047277   9.997677  10.027504  10.018239   9.692223   9.387239
0:   9.095313   9.311295   9.885496  10.765232  11.685195  12.296981
0:  11.893188  11.971357 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.471605 17.447598 17.761936 18.177614 18.46925  18.516548 18.70643
0:  18.835648 18.96817  19.201881 19.24781  19.12967  19.052965 19.132425
0:  19.569866 20.30928  21.102207 21.59798  19.175482 19.246668]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.6734147 -6.207317  -5.408131  -4.396481  -3.5499005 -3.0400481
0:  -2.884297  -2.9643846 -3.4596572 -3.9105268 -4.3845544 -4.980232
0:  -5.348189  -5.5812707 -5.4914165 -5.0044446 -4.327721  -3.6193643
0:  -4.27325   -3.9234233]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.269892  -11.130327  -10.637247   -9.881163   -9.243422   -8.881355
0:   -8.727507   -8.837332   -9.353655   -9.958357  -10.762697  -11.790279
0:  -12.553755  -13.021587  -12.987438  -12.474306  -11.801384  -11.1098385
0:  -10.542946  -10.156717 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.571322   8.597693   8.913321   9.315073   9.621441   9.685737
0:   9.8658695  9.889548   9.887407   9.8548155  9.608751   9.214162
0:   8.899647   8.841755   9.14942    9.888775  10.833811  11.710281
0:  10.078339  10.177768 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2195792   1.2002778   1.4886718   1.982821    2.3114285   2.3798661
0:   2.2533045   2.0574093   1.6269336   1.2783103   0.908618    0.31905985
0:  -0.12299585 -0.44493246 -0.4681363  -0.01911592  0.6933284   1.4219775
0:  -0.32900095 -0.25042868]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.5266676 -2.9542046 -3.0241785 -2.8406076 -2.648076  -2.6332283
0:  -2.6961708 -2.919756  -3.4440985 -3.9829378 -4.6646214 -5.467664
0:  -5.984865  -6.1592875 -5.94977   -5.3992243 -4.8687143 -4.5079603
0:  -6.9520025 -7.176366 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.682789 24.317173 24.965054 25.421612 25.607342 25.45443  25.635498
0:  25.681095 25.75584  25.68586  25.164837 24.443947 23.738499 23.450302
0:  23.631104 24.17033  24.856247 25.33107  24.616817 24.745434]
0: validation loss for strategy=forecast at epoch 29 : 0.3235372304916382
0: validation loss for velocity_u : 0.16180385649204254
0: validation loss for velocity_v : 0.2812767028808594
0: validation loss for specific_humidity : 0.18319503962993622
0: validation loss for velocity_z : 0.5029168725013733
0: validation loss for temperature : 0.12611611187458038
0: validation loss for total_precip : 0.685914933681488
0: 30 : 19:57:43 :: batch_size = 96, lr = 1.0017555672365157e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 30, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9158, -0.9215, -0.9315, -0.9482, -0.9639, -0.9780, -0.9971, -1.0192, -1.0461, -1.0733, -1.0946, -1.1059,
0:         -1.1032, -1.0968, -1.1007, -1.1214, -1.1571, -1.1951, -0.9232, -0.9232, -0.9243, -0.9340, -0.9504, -0.9727,
0:         -0.9999], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2618, 0.2337, 0.2103, 0.1948, 0.1864, 0.1940, 0.2185, 0.2603, 0.3207, 0.3829, 0.4428, 0.4903, 0.5190, 0.5341,
0:         0.5335, 0.5196, 0.4926, 0.4497, 0.2363, 0.2002, 0.1730, 0.1598, 0.1571, 0.1694, 0.1990], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3667,  0.4622,  0.5472,  0.6042,  0.5933,  0.5400,  0.4868,  0.4142,  0.2892,  0.0820, -0.1386, -0.3173,
0:         -0.4152, -0.4777, -0.5056, -0.5149, -0.5223, -0.5267,  0.2656,  0.3598,  0.4146,  0.4597,  0.4719,  0.4470,
0:          0.4159], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0433,  0.0179,  0.0046, -0.1410, -0.1421, -0.2138, -0.2800, -0.1454, -0.1189,  0.1304,  0.2551,  0.2275,
0:          0.2110, -0.0483, -0.0726, -0.0439, -0.0042,  0.1735,  0.0422, -0.0362, -0.0814, -0.2072, -0.2127, -0.2194,
0:         -0.1951], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.2054, 1.2025, 1.1295, 1.0567, 1.0483, 1.0751, 1.1199, 1.1443, 1.1767, 1.2207, 1.2846, 1.3637, 1.3920, 1.3820,
0:         1.3211, 1.2548, 1.2538, 1.3282, 1.4923, 1.6730, 1.7767, 1.7816, 1.7185, 1.6520, 1.6053], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1361, -0.1630, -0.1782, -0.1828, -0.2050, -0.1489, -0.1934, -0.2039, -0.1887, -0.1548, -0.1770, -0.1793,
0:         -0.1922, -0.1875, -0.1127, -0.1606, -0.1805, -0.1688, -0.1513, -0.1606, -0.1653, -0.1174, -0.1185, -0.0998,
0:         -0.1045], device='cuda:0')
0: [DEBUG] Epoch 30, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan, -0.2413,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2413,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan, -0.2413,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan,     nan,     nan,
0:         -0.2413,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan, -0.2413,     nan,     nan,
0:             nan,     nan,     nan, -0.2389,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan,
0:             nan, -0.2413,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2389,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366, -0.2389, -0.2413,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,
0:             nan,     nan, -0.2413,     nan,     nan,     nan,     nan, -0.2413, -0.2413,     nan,     nan,     nan,
0:             nan, -0.2413,     nan, -0.2413,     nan,     nan,     nan, -0.2413,     nan, -0.2413,     nan,     nan,
0:             nan,     nan,     nan, -0.2413,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2413])
0: [DEBUG] Epoch 30, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6675, -0.6601, -0.6154, -0.5445, -0.4781, -0.4401, -0.4506, -0.4640, -0.5091, -0.5334, -0.5490, -0.5831,
0:         -0.6338, -0.6973, -0.7594, -0.7825, -0.7638, -0.7112, -0.6021, -0.6560, -0.6628, -0.6230, -0.5628, -0.5119,
0:         -0.4875], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3629, 0.3645, 0.3352, 0.2979, 0.2690, 0.2554, 0.2512, 0.2582, 0.2749, 0.2776, 0.2762, 0.2691, 0.2387, 0.1968,
0:         0.1561, 0.1484, 0.1786, 0.2406, 0.3553, 0.3703, 0.3472, 0.3108, 0.2798, 0.2597, 0.2561], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6487, -0.6482, -0.6401, -0.6225, -0.6047, -0.5970, -0.5999, -0.6169, -0.6372, -0.6583, -0.6703, -0.6700,
0:         -0.6526, -0.6244, -0.6047, -0.5832, -0.5794, -0.5797, -0.6471, -0.6346, -0.6172, -0.5965, -0.5812, -0.5794,
0:         -0.5851], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2905, 0.3294, 0.4139, 0.3704, 0.2363, 0.3703, 0.4398, 0.4139, 0.5135, 0.4964, 0.3772, 0.3320, 0.3953, 0.3939,
0:         0.3370, 0.4970, 0.5837, 0.4347, 0.3611, 0.2737, 0.2481, 0.2163, 0.1230, 0.2858, 0.3913], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.6276, 1.6151, 1.5818, 1.5432, 1.5389, 1.5886, 1.6718, 1.7449, 1.7888, 1.8030, 1.7977, 1.7799, 1.7554, 1.7208,
0:         1.6720, 1.6208, 1.5850, 1.5684, 1.5639, 1.5589, 1.5565, 1.5603, 1.5764, 1.5980, 1.6089], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2418, -0.2439, -0.2546, -0.2466, -0.2447, -0.2382, -0.2399, -0.2390, -0.2336, -0.2465, -0.2441, -0.2399,
0:         -0.2443, -0.2417, -0.2351, -0.2334, -0.2420, -0.2514, -0.2430, -0.2361, -0.2416, -0.2359, -0.2339, -0.2408,
0:         -0.2352], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2109173685312271; velocity_v: 0.3067324757575989; specific_humidity: 0.1755504459142685; velocity_z: 0.5785704851150513; temperature: 0.14931720495224; total_precip: 0.6428170204162598; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19749434292316437; velocity_v: 0.31567248702049255; specific_humidity: 0.17974092066287994; velocity_z: 0.5417107939720154; temperature: 0.14735741913318634; total_precip: 0.5786658525466919; 
0: epoch: 30 [1/5 (20%)]	Loss: 0.61074 : 0.30115 :: 0.20960 (2.60 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20644596219062805; velocity_v: 0.2930922508239746; specific_humidity: 0.182729110121727; velocity_z: 0.5090693235397339; temperature: 0.17081816494464874; total_precip: 0.7099548578262329; 
0: epoch: 30 [2/5 (40%)]	Loss: 0.70995 : 0.31181 :: 0.20376 (15.79 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18872575461864471; velocity_v: 0.3219752907752991; specific_humidity: 0.18326319754123688; velocity_z: 0.6788110733032227; temperature: 0.1510571539402008; total_precip: 0.6553977131843567; 
0: epoch: 30 [3/5 (60%)]	Loss: 0.65540 : 0.32914 :: 0.20837 (15.82 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20406746864318848; velocity_v: 0.35202333331108093; specific_humidity: 0.19097299873828888; velocity_z: 0.7047497034072876; temperature: 0.1646413952112198; total_precip: 0.7521044015884399; 
0: epoch: 30 [4/5 (80%)]	Loss: 0.75210 : 0.36034 :: 0.20723 (16.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.90734863e-06
0:  2.86102295e-06 1.33514404e-05 5.62667847e-05 1.03950500e-04
0:  2.00271606e-04 2.59399414e-04 3.33786011e-05 7.72476196e-05
0:  1.04904175e-04 1.37329102e-04 1.17301941e-04 6.67572021e-05
0:  3.43322754e-05 2.19345093e-05 3.14712524e-05 1.04904175e-05
0:  7.62939453e-06 1.62124634e-05 2.47955322e-05 2.09808350e-05
0:  9.53674316e-06 3.81469727e-06 2.86102295e-06 3.81469727e-06
0:  4.76837158e-06 5.72204590e-06 4.76837158e-06 4.76837158e-06
0:  1.04904175e-05 1.14440918e-05 2.09808350e-05 5.91278076e-05
0:  8.39233398e-05 5.62667847e-05 9.05990601e-05 7.91549683e-05
0:  4.00543213e-05 1.43051147e-05 1.43051147e-05 2.95639038e-05
0:  9.72747803e-05 1.06811523e-04 1.53541565e-04 1.23977661e-04
0:  1.37329102e-04 1.94549561e-04 2.47955322e-04 2.67982483e-04
0:  2.51770020e-04 3.17573547e-04 3.76701355e-04 3.07083130e-04
0:  2.70843506e-04 3.12805176e-04 3.07083130e-04 2.67028809e-04
0:  2.44140625e-04 2.53677368e-04 2.96592712e-04 4.15802002e-04
0:  4.74929810e-04 3.75747681e-04 2.30789185e-04 1.61170959e-04
0:  6.58035278e-05 4.48226929e-05 5.05447388e-05 1.05857849e-04
0:  1.09672546e-04 6.67572021e-05 3.05175781e-05 2.19345093e-05
0:  1.43051147e-05 4.10079956e-05 5.43594360e-05 3.71932983e-05
0:  4.19616699e-05 4.57763672e-05 4.76837158e-05 1.23977661e-05
0:  1.71661377e-05 6.77108765e-05 5.62667847e-05 5.24520874e-05
0:  8.39233398e-05 5.91278076e-05 4.00543213e-05 2.19345093e-05
0:  7.62939453e-06 2.86102295e-06 2.86102295e-06 9.53674316e-07
0:  9.53674316e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 3.71932983e-05 1.02043152e-04
0:  1.05857849e-04 5.05447388e-05 2.38418579e-05 4.67300415e-05
0:  0.00000000e+00 0.00000000e+00 4.76837158e-06 5.72204590e-06
0:  2.00271606e-05 9.82284546e-05 1.50680542e-04 1.63078308e-04
0:  1.82151794e-04 1.36375427e-04 2.38418579e-05 3.24249268e-05
0:  4.57763672e-05 6.77108765e-05 6.10351562e-05 2.00271606e-05
0:  1.62124634e-05 1.14440918e-05 1.81198120e-05 8.58306885e-06
0:  1.52587891e-05 1.14440918e-05 1.71661377e-05 1.23977661e-05
0:  1.90734863e-06 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 9.53674316e-07 2.67028809e-05
0:  6.96182251e-05 8.01086426e-05 6.38961792e-05 2.95639038e-05
0:  7.62939453e-06 1.14440918e-05 1.33514404e-05 3.52859497e-05
0:  7.91549683e-05 9.53674316e-05 1.21116638e-04 1.02996826e-04
0:  1.13487244e-04 1.68800354e-04 2.50816345e-04 2.81333923e-04
0:  3.31878662e-04 3.96728516e-04 3.86238098e-04 3.71932983e-04
0:  3.22341919e-04 2.71797180e-04 2.38418579e-04 2.01225281e-04
0:  1.86920166e-04 1.80244446e-04 1.90734863e-04 2.01225281e-04
0:  1.86920166e-04 1.70707703e-04 1.01089478e-04 4.76837158e-05
0:  5.43594360e-05 3.81469727e-05 3.52859497e-05 5.81741333e-05
0:  1.23023987e-04 9.72747803e-05 4.57763672e-05 3.24249268e-05
0:  2.19345093e-05 2.57492065e-05 4.29153442e-05 8.86917114e-05
0:  1.13487244e-04 1.07765198e-04 8.01086426e-05 8.20159912e-05
0:  4.57763672e-05 5.72204590e-05 6.10351562e-05 6.29425049e-05
0:  7.24792480e-05 5.81741333e-05 4.00543213e-05 9.53674316e-06]
0: Target values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.29153442e-06 4.29153442e-06 3.81469727e-06
0:  1.85966492e-05 2.81333923e-05 3.05175781e-05 2.09808350e-05
0:  1.76429749e-05 2.05039978e-05 2.28881836e-05 2.14576721e-05
0:  2.81333923e-05 2.52723694e-05 1.66893005e-05 1.57356262e-05
0:  1.28746033e-05 1.04904175e-05 1.09672546e-05 1.33514404e-05
0:  1.09672546e-05 7.62939453e-06 8.58306885e-06 2.00271606e-05
0:  3.24249268e-05 2.86102295e-05 1.76429749e-05 1.00135803e-05
0:  4.76837158e-06 2.86102295e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 1.43051147e-06 1.90734863e-06
0:  1.00135803e-04 1.24931335e-04 1.02519989e-04 8.63075256e-05
0:  8.58306885e-05 9.20295715e-05 2.62260437e-05 1.04904175e-05
0:  4.76837158e-06 4.76837158e-07 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 1.90734863e-06 1.90734863e-06 5.24520874e-06
0:  5.72204590e-06 1.00135803e-05 9.05990601e-06 9.05990601e-06
0:  7.15255737e-06 7.15255737e-06 9.53674316e-07 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 2.57492065e-05 4.33921814e-05
0:  4.00543213e-05 4.86373901e-05 3.71932983e-05 7.43865967e-05
0:  4.10079956e-05 2.62260437e-05 1.52587891e-05 1.23977661e-05
0:  1.47819519e-05 2.86102295e-05 4.43458557e-05 2.95639038e-05
0:  5.00679016e-05 2.90870667e-05 3.24249268e-05 6.10351562e-05
0:  4.67300415e-05 1.38282776e-05 1.14440918e-05 2.38418579e-06
0:  4.76837158e-07 4.76837158e-07 1.33514404e-05 6.86645508e-05
0:  7.96318054e-05 6.48498535e-05 4.67300415e-05 3.38554382e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.28881836e-05
0:  5.34057617e-05 2.81333923e-05 2.95639038e-05 1.76429749e-05
0:  1.33514404e-05 1.19209290e-05 1.04904175e-05 1.00135803e-05
0:  1.33514404e-05 1.14440918e-05 1.23977661e-05 1.33514404e-05
0:  8.58306885e-06 9.53674316e-06 8.58306885e-06 7.15255737e-06
0:  4.76837158e-06 5.72204590e-06 7.62939453e-06 1.52587891e-05
0:  1.57356262e-05 1.52587891e-05 1.43051147e-05 5.24520874e-06
0:  3.81469727e-06 1.90734863e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 0.00000000e+00
0:  2.24113464e-05 7.72476196e-05 9.39369202e-05 8.44001770e-05
0:  7.43865967e-05 2.38418579e-05 5.72204590e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  3.81469727e-06 3.29017639e-05 5.72204590e-05 8.20159912e-05
0:  1.00612640e-04 8.48770142e-05 5.38825989e-05 5.10215759e-05
0:  3.00407410e-05 1.62124634e-05 1.47819519e-05 1.19209290e-05
0:  1.85966492e-05 3.43322754e-05 3.24249268e-05 1.57356262e-05]
0: Prediction values (first 20):
0: [-1.9714651 -2.1877604 -2.0620956 -1.6108894 -1.1755133 -0.9610138
0:  -1.1274204 -1.3929901 -2.0858212 -2.6816902 -3.2513895 -4.028865
0:  -4.601774  -5.09262   -5.278738  -4.964984  -4.332075  -3.542616
0:  -4.218005  -4.4781895]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.475, max = 0.609, mean = -0.393
0:          sample (first 20): tensor([-0.7420, -0.7596, -0.7494, -0.7127, -0.6774, -0.6600, -0.6735, -0.6950, -0.7513, -0.7997, -0.8459, -0.9091,
0:         -0.9556, -0.9954, -1.0106, -0.9851, -0.9337, -0.8696, -0.7273, -0.7988])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.5328045 13.61151   13.8800335 14.167941  14.39397   14.517057
0:  14.72423   14.926883  14.977068  15.094086  15.046438  14.868702
0:  14.740235  14.707201  14.97589   15.452487  15.972398  16.348778
0:  14.967454  15.185231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.49131298 0.53464127 0.84774876 1.3672032  1.8274314  2.1143553
0:  2.514621   2.8031213  2.9315958  2.9867175  2.7491717  2.2687063
0:  1.7804513  1.5672693  1.6868384  2.1119473  2.610818   2.9248364
0:  1.6485014  1.6750648 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3010216 -5.763191  -5.846427  -5.7146473 -5.6292353 -5.7937155
0:  -5.748248  -5.821862  -5.859476  -5.8717747 -6.1124077 -6.4042196
0:  -6.6136255 -6.42286   -5.84779   -4.937231  -4.094348  -3.5387144
0:  -5.5420094 -5.9423914]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.42268   13.422352  13.609377  13.8874035 14.037712  13.87871
0:  14.034534  14.10038   14.199087  14.25403   13.983158  13.573388
0:  13.158986  13.061159  13.331791  13.934885  14.627573  15.076309
0:  12.25419   12.09873  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9195669 4.116751  4.5174074 5.125361  5.6700826 6.067364  6.258159
0:  6.483158  6.4958024 6.6558566 6.8637233 6.9043636 6.911466  6.8401246
0:  6.877928  7.1890836 7.7812386 8.515875  7.7801867 7.9845448]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9625192  -1.9169545  -1.5471778  -0.94514275 -0.41525316 -0.10086012
0:   0.00872946  0.04040146 -0.20822573 -0.43196106 -0.745172   -1.2266021
0:  -1.6210771  -1.8192048  -1.69027    -1.1294713  -0.36377287  0.3806386
0:  -1.252233   -1.2276578 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.166758  -11.152655  -10.770697   -9.944885   -9.008242   -8.256079
0:   -7.790236   -7.5053773  -7.8037705  -8.098951   -8.545618   -9.187286
0:   -9.551733   -9.786684   -9.644899   -9.157219   -8.626566   -8.122265
0:   -8.968624   -9.072271 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.47119    -2.6196465  -2.4180894  -1.8292603  -1.16608    -0.6531272
0:  -0.49596167 -0.47945547 -0.92514086 -1.3184562  -1.6807084  -2.2253966
0:  -2.58775    -2.8716817  -2.9170823  -2.520989   -1.8375211  -1.0070329
0:  -0.943933   -0.9693837 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.548829  9.043438  9.979965 11.13078  12.176104 12.965362 13.833866
0:  14.649826 15.364548 16.127205 16.736366 17.12782  17.616966 18.250555
0:  19.240177 20.485458 21.710354 22.571571 20.289923 20.628166]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.672642 14.751989 14.796406 14.651133 14.390858 13.953821 14.067283
0:  14.143101 14.31041  14.379168 13.958084 13.548371 13.145735 13.338085
0:  14.060247 15.213051 16.391642 17.21369  17.687292 18.07032 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.8729806 -4.8629823 -4.4801106 -3.7264476 -2.956101  -2.3551755
0:  -2.0920568 -1.996779  -2.3561177 -2.6752315 -3.040464  -3.6056342
0:  -4.0262218 -4.3636956 -4.475331  -4.195637  -3.7423558 -3.2353997
0:  -5.3192773 -5.4732404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.840257  6.9150224 7.1911764 7.5674834 7.8244553 7.9073567 8.001172
0:  8.016157  7.870891  7.7161202 7.359984  6.8848066 6.497652  6.32239
0:  6.525238  7.0327373 7.6263137 8.096583  6.991053  7.2477427]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.973459  -8.895781  -8.357168  -7.4500566 -6.5514445 -5.928407
0:  -5.60049   -5.575649  -6.039284  -6.517257  -7.0972896 -7.836414
0:  -8.268631  -8.557169  -8.493071  -8.118853  -7.7144713 -7.3140407
0:  -9.551065  -9.577654 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.903479 11.976935 12.087149 12.130697 12.074263 11.925962 12.256105
0:  12.595758 12.932014 13.164072 12.911228 12.519163 12.099758 12.187601
0:  12.72785  13.639963 14.572388 15.265337 15.963446 16.116043]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4090705  1.4998794  1.834033   2.1976347  2.4047265  2.3976433
0:  2.5597367  2.6354778  2.623469   2.5579953  2.1397772  1.511879
0:  0.93747663 0.69827557 0.88870287 1.5205002  2.2441146  2.8716013
0:  1.0188556  1.3107543 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.831488 19.570354 19.377    19.169443 18.901144 18.56562  18.684872
0:  18.828844 19.071764 19.306786 19.200336 18.983051 18.714682 18.748446
0:  19.116034 19.59811  19.880022 19.73939  16.594795 16.399757]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4752593  -1.6083112  -1.403686   -0.85421085 -0.30496168  0.04558516
0:   0.0794487  -0.0574894  -0.61537933 -1.1526656  -1.694488   -2.4317594
0:  -2.9811416  -3.4484477  -3.5781484  -3.3059573  -2.7645135  -2.0946727
0:  -2.8638492  -2.9846225 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.47699  15.166113 15.139662 15.300657 15.55518  15.612042 16.040598
0:  16.241528 16.311592 16.063625 15.29074  14.443772 13.674253 13.405702
0:  13.711601 14.385677 15.111065 15.544302 14.294685 13.93973 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.711903 17.70118  17.884138 18.14857  18.339363 18.317606 18.56947
0:  18.711348 18.792732 18.815601 18.494406 18.012049 17.53465  17.333813
0:  17.53007  18.07858  18.766682 19.28481  17.010412 17.079964]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.029661 26.219944 26.568727 26.9243   27.158255 27.17502  27.439299
0:  27.625721 27.832417 27.988485 27.893108 27.726046 27.687004 27.891352
0:  28.463533 29.230097 30.03317  30.58464  27.847042 28.058971]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.88306    7.8951035  8.168788   8.586651   8.940226   9.14549
0:   9.459333   9.745014   9.956517  10.201151  10.266846  10.137215
0:  10.028822  10.030102  10.207382  10.54772   10.790796  10.813284
0:   8.27408    8.32585  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.463624   -1.6066275  -1.4713354  -1.0319891  -0.57809067 -0.20342064
0:  -0.03676987  0.18480682  0.06844902 -0.01495504 -0.21050262 -0.6594639
0:  -1.1023755  -1.5020266  -1.6313658  -1.3027878  -0.6221638   0.17354822
0:  -1.1409912  -1.2677336 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.165928  -8.157972  -7.7872963 -7.058367  -6.311081  -5.8220196
0:  -5.639903  -5.66399   -6.148427  -6.5679226 -7.034421  -7.684306
0:  -8.112383  -8.446853  -8.498495  -8.160126  -7.682539  -7.156499
0:  -8.092361  -8.156646 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-22.457413 -22.493889 -22.1875   -21.652687 -21.065346 -20.617302
0:  -19.886303 -19.308437 -18.936897 -18.73302  -19.072006 -19.764748
0:  -20.432877 -20.723152 -20.594704 -20.17007  -19.888542 -19.888283
0:  -20.077324 -20.004387]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.734398 9.720963 9.722294 9.49575  9.11467  8.580526 8.742787 9.015665
0:  9.415916 9.695346 9.32338  8.845646 8.186712 8.128757 8.49842  9.157099
0:  9.537073 9.403613 8.376717 8.247688]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.192438  10.958045  10.912896  10.987092  11.0325    10.887297
0:  11.152888  11.294708  11.426043  11.369423  10.8525095 10.20072
0:   9.570454   9.428654   9.795034  10.627548  11.5337925 12.154524
0:  10.20303    9.973498 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4789033 1.5245914 1.8473029 2.4196062 2.9527485 3.3359854 3.4005358
0:  3.4554498 3.1519177 2.955919  2.7699146 2.34228   1.9966841 1.6631546
0:  1.6079607 2.0615144 2.9482837 3.9985948 3.4941769 3.6270819]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.038599   -2.1813745  -2.010087   -1.5689287  -1.167314   -0.99300575
0:  -1.0570173  -1.2085571  -1.6851807  -2.0475187  -2.4167085  -2.9878764
0:  -3.393764   -3.7268586  -3.7713876  -3.3345923  -2.6009927  -1.770886
0:  -1.9961343  -1.539115  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.107156   8.322702   8.761883   9.386127   9.8514    10.103058
0:  10.114517  10.136179   9.864029   9.659246   9.392075   8.856877
0:   8.333522   7.805622   7.575914   7.785083   8.340015   8.988094
0:   7.3743715  7.3735085]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.808426  13.863853  14.156492  14.464453  14.557081  14.434531
0:  14.4442425 14.508569  14.646722  14.713289  14.643397  14.398903
0:  14.313325  14.509771  15.133987  16.073643  16.989494  17.69577
0:  13.533905  13.702537 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.423687  16.240015  16.150948  15.958422  15.661161  15.116949
0:  15.020218  14.874229  14.807371  14.685846  14.098683  13.399402
0:  12.615909  12.266745  12.334706  12.828911  13.406444  13.82308
0:  12.1602955 12.0367365]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1901884 2.167541  2.4671707 2.94407   3.3594801 3.5777655 3.7041554
0:  3.6676724 3.4076166 3.098027  2.6310081 2.0001817 1.4898396 1.1918931
0:  1.2403584 1.5395803 1.9107366 2.1719165 1.410541  1.4588437]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.321125 17.577507 18.160946 18.90654  19.571604 20.007591 20.651175
0:  21.245647 21.834267 22.437965 22.795145 22.965399 23.17772  23.563412
0:  24.215862 25.038052 25.80944  26.148834 22.243004 22.10952 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5049887  -0.55178595 -0.19477272  0.52288866  1.2196288   1.7232218
0:   1.9524574   2.1169043   1.9461765   1.8614383   1.7650084   1.3729701
0:   1.0604777   0.75815153  0.68723726  1.1205134   1.89078     2.783423
0:   2.0960708   2.037066  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.993595 21.873262 21.832876 21.717056 21.404175 20.836514 20.513271
0:  20.116531 19.648405 19.112747 18.10904  16.903202 15.68671  14.855395
0:  14.49132  14.607773 14.938703 15.328484 12.786164 12.853983]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.423096  -4.265858  -3.7436786 -2.9405828 -2.1888785 -1.7009706
0:  -1.4581919 -1.4649215 -1.886035  -2.3733096 -2.971727  -3.7047505
0:  -4.171911  -4.375462  -4.2005353 -3.650021  -3.0149188 -2.488028
0:  -4.002851  -3.8012166]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.0678625  0.86745024 1.0768223  1.6852036  2.5007997  3.3214855
0:  4.2489443  5.147563   5.8119617  6.4166174  6.795308   6.9641666
0:  7.184321   7.514058   8.048709   8.788702   9.498098   9.978421
0:  7.8694253  8.103886  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.2905674 4.322068  4.6730647 5.3257422 6.000714  6.5553875 6.9960475
0:  7.3839507 7.460254  7.609477  7.6698723 7.5097294 7.4290447 7.344754
0:  7.480974  7.981131  8.647308  9.2871895 8.141016  8.156081 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.718084 20.394314 21.260622 22.196611 22.962769 23.447308 24.134775
0:  24.658218 25.100376 25.381584 25.192932 24.635395 23.965935 23.398142
0:  23.099096 22.992134 22.962547 22.772655 18.357368 17.820562]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1076446 3.3300967 3.8618963 4.5080233 5.0156283 5.261512  5.6248813
0:  5.87101   6.0670176 6.2469616 6.1986675 6.0299273 5.923707  6.1230936
0:  6.699588  7.617574  8.589821  9.352537  7.549362  7.673991 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1632357 6.494583  7.111485  7.87765   8.415622  8.666674  8.628788
0:  8.665454  8.466742  8.447154  8.428881  8.096998  7.7389574 7.2965956
0:  7.1059465 7.4166303 8.180548  9.091848  6.9540663 7.074318 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.15011692 -0.05832338  0.359509    1.0208869   1.6658363   2.1602278
0:   2.4638348   2.737895    2.7319093   2.768475    2.7133837   2.4413238
0:   2.2121387   2.1244192   2.260882    2.7656322   3.4032736   3.9576824
0:   3.3820326   3.5112696 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.459511  -15.410339  -14.866684  -13.848938  -12.79583   -12.016254
0:  -11.620121  -11.542568  -12.01041   -12.499449  -13.077019  -13.7603245
0:  -14.1299515 -14.281738  -14.07707   -13.525656  -12.905972  -12.309505
0:  -12.494381  -12.421494 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3271623 -3.4667106 -3.299725  -2.7206845 -2.0425687 -1.4707513
0:  -1.2777386 -1.1890292 -1.6071887 -1.9707179 -2.3255858 -2.842749
0:  -3.1925626 -3.4840274 -3.5258412 -3.1897502 -2.5650363 -1.8101001
0:  -1.8577628 -2.0107808]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.959044 20.83169  20.824516 20.86287  20.86658  20.64315  20.877434
0:  20.957016 21.080513 21.038385 20.526508 19.996138 19.468742 19.411776
0:  19.796457 20.548805 21.412193 22.002987 20.988327 21.043997]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-19.922764 -20.011631 -19.594193 -18.610996 -17.475529 -16.582771
0:  -16.072487 -15.804347 -16.05931  -16.375343 -16.77766  -17.392956
0:  -17.791645 -18.147282 -18.16367  -17.854511 -17.331894 -16.719032
0:  -16.501892 -16.618984]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.2222724 -4.138616  -3.7686496 -3.1287675 -2.5334125 -2.1548972
0:  -2.1412263 -2.261376  -2.8483987 -3.3830056 -3.9550052 -4.742748
0:  -5.3352    -5.844416  -6.0355134 -5.7433553 -5.136831  -4.3765025
0:  -4.9698043 -4.789823 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.32464266  0.04725027  0.04006815  0.3615942   0.93947506  1.4810448
0:   2.70215     3.8387618   5.0040226   5.995127    6.477201    6.799856
0:   7.0603056   7.858688    9.049391   10.678951   12.300194   13.51681
0:  10.870411   10.67348   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.252863   -2.3525467  -2.0719585  -1.4363585  -0.8086667  -0.37714767
0:  -0.23681736 -0.2277174  -0.6498313  -1.0878086  -1.6156778  -2.416315
0:  -3.0687523  -3.6343155  -3.9004946  -3.6527882  -3.1019297  -2.4118462
0:  -4.441986   -4.7139025 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.455742   7.3984914  7.72481    8.313357   8.988313   9.525377
0:  10.451408  11.254981  12.106314  12.835625  13.239573  13.54834
0:  13.9690895 14.74275   15.931353  17.47623   19.07218   20.451494
0:  19.92676   20.239822 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.805868 28.934437 29.013098 29.008545 29.08499  29.03992  29.70309
0:  30.22191  30.790241 31.142817 30.908575 30.728113 30.611374 30.944403
0:  31.737955 32.76201  33.797897 34.531162 34.029854 34.189724]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.5391164 5.8846035 6.406329  7.0644345 7.68438   8.169779  8.451216
0:  8.617657  8.439851  8.310141  8.142935  7.852501  7.7011905 7.641488
0:  7.8411617 8.284606  8.81121   9.289495  8.02643   8.295131 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.381365  -10.390526   -9.924856   -9.045677   -8.035385   -7.165422
0:   -6.4705195  -6.1250176  -6.34436    -6.7916265  -7.561191   -8.522299
0:   -9.375965  -10.061617  -10.5641365 -10.859228  -11.154011  -11.420845
0:  -14.950104  -14.686844 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.760663 -15.007333 -14.795484 -14.037745 -13.124484 -12.395413
0:  -11.986562 -11.822522 -12.218822 -12.646363 -13.187287 -13.914145
0:  -14.369908 -14.733216 -14.709254 -14.319253 -13.760041 -13.170852
0:  -13.874351 -14.251065]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.42575788 -0.3865199   0.10123825  0.9047446   1.6623952   2.1952682
0:   2.5369475   2.6640549   2.4693856   2.306583    2.0458002   1.629137
0:   1.3891559   1.3099608   1.5066543   2.0148666   2.6022832   3.0890627
0:   1.7170937   1.9198523 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.120156  9.025673  9.1229    9.445698  9.661257  9.687866  9.412489
0:  9.159057  8.569183  8.1199665 7.6960506 6.9863033 6.29796   5.5582952
0:  5.105097  5.101572  5.5189376 6.0961895 3.8063047 3.735795 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.4036884 -7.2159224 -6.599128  -5.7094755 -4.9755535 -4.623762
0:  -4.591666  -4.903407  -5.654649  -6.33943   -7.103382  -7.9790688
0:  -8.577849  -8.90295   -8.851313  -8.332806  -7.6889076 -7.0242014
0:  -7.48106   -7.0323715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.995714  4.8916316 4.9700766 5.1323013 5.2020764 5.1400137 5.0778646
0:  4.9613056 4.654823  4.3762226 3.9940355 3.4969082 3.1590395 3.0121198
0:  3.2416105 3.8193877 4.5766277 5.256056  4.084584  4.2405562]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.28917  16.23731  16.36825  16.536451 16.613413 16.447319 16.58563
0:  16.619303 16.656597 16.608707 16.217497 15.710661 15.219719 15.063189
0:  15.291393 15.905401 16.649174 17.19106  15.669632 15.755323]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.812517  -8.950472  -8.719799  -8.032545  -7.293511  -6.708616
0:  -6.531711  -6.5408816 -6.988938  -7.34306   -7.6481147 -8.057663
0:  -8.323978  -8.539877  -8.531836  -8.151291  -7.4461417 -6.566302
0:  -6.9506774 -7.278214 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.633545 29.152887 29.742798 30.244555 30.636059 30.94807  31.729675
0:  32.475296 33.25361  33.858303 34.03997  34.114113 34.41549  35.108315
0:  36.33531  37.81145  39.37304  40.663403 38.034393 38.707424]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.178405   6.2471623  6.5065017  6.805157   7.0281606  7.154988
0:   7.5879493  8.0009985  8.470544   8.982869   9.236574   9.420576
0:   9.668122  10.2685375 11.1967125 12.30784   13.246899  13.781017
0:  13.894051  14.165617 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.385566 10.158988 10.309427 10.702091 11.091114 11.416433 11.921358
0:  12.39776  12.772412 13.032855 12.992882 12.660057 12.353891 12.306378
0:  12.627994 13.221989 13.919771 14.380713 13.194023 13.332602]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.4081273 6.8620048 7.6269603 8.510026  9.133475  9.446691  9.633982
0:  9.708549  9.490899  9.263714  8.777721  8.0218115 7.294735  6.7345505
0:  6.6032352 6.9059997 7.447761  7.931104  5.5799384 5.9728236]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.385422 21.345526 21.520912 21.663818 21.570713 21.152042 20.827326
0:  20.405237 19.913105 19.366543 18.559214 17.625582 16.828104 16.371227
0:  16.426891 16.868177 17.472689 17.909588 15.231738 15.215323]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.062781  13.4869795 14.044259  14.600914  14.956078  15.023518
0:  15.224178  15.280697  15.268564  15.111124  14.635307  14.057013
0:  13.555997  13.391944  13.6993    14.362955  15.185953  15.873737
0:  15.101065  15.443823 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.5626135  9.619878   9.857438  10.144725  10.26053   10.147301
0:  10.162258  10.096718   9.936376   9.746154   9.264761   8.644049
0:   8.055917   7.7693357  7.8174787  8.15831    8.517802   8.702436
0:   6.4898934  6.5704355]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.521595 31.745771 31.942854 32.00223  31.828247 31.325397 31.146048
0:  30.820934 30.585709 30.276997 29.564075 28.884089 28.32238  28.14598
0:  28.468538 29.074091 29.749279 30.25702  29.081562 29.324306]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.787462   9.926304  10.296209  10.725033  11.065274  11.148025
0:  11.162267  10.987829  10.562355  10.227257   9.793301   9.306417
0:   9.036866   8.96189    9.149278   9.573722   9.9239645 10.116939
0:   8.938375   9.10685  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.2573719  0.08244514 0.16612816 0.2970004  0.52001715 0.7332964
0:  1.6952901  2.6789632  3.725406   4.463695   4.413171   4.148532
0:  3.5717866  3.6468794  4.214123   5.096835   5.774218   5.9868073
0:  6.676964   6.4676695 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [42.783085 42.63453  42.32018  42.104843 41.959263 41.711536 42.500908
0:  43.15266  44.00393  44.49812  44.120678 43.820847 43.672924 44.036316
0:  45.009552 46.114933 47.072197 47.457172 45.38905  45.248425]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.1079884  6.143819   6.4544554  6.9015546  7.313404   7.5504403
0:   7.9161286  8.228113   8.428799   8.573408   8.484032   8.182322
0:   7.9048104  7.8617425  8.1703005  8.848183   9.638782  10.313906
0:   9.513184   9.691584 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.749794 40.796383 40.58774  40.28674  40.008232 39.566708 40.171894
0:  40.674538 41.337914 41.638653 40.971848 40.414272 39.808586 39.916115
0:  40.693844 41.729523 42.700996 43.004276 42.951107 43.195286]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.074171   7.352132   7.84076    8.377724   8.729694   8.817569
0:   9.068501   9.233497   9.4130335  9.636902   9.633779   9.523087
0:   9.401985   9.54621   10.04086   10.87542   11.848711  12.736116
0:  11.4275    11.788521 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.167187 42.98819  42.207256 41.2712   40.37391  39.330166 39.58279
0:  39.77932  40.264755 40.255207 39.4441   38.710552 37.93489  37.961895
0:  38.74353  39.755646 40.64138  40.586266 42.18586  42.50514 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.5866137 4.6767883 5.058874  5.640088  6.086981  6.3286867 6.595734
0:  6.7462883 6.6480207 6.5226927 6.135387  5.527239  5.007944  4.7614374
0:  4.8984184 5.4002347 5.9558287 6.362438  4.459978  4.5619497]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.15613   10.863186  11.687078  12.5541315 13.332121  13.886222
0:  14.687416  15.439524  16.098494  16.725916  17.177765  17.424063
0:  17.837324  18.443502  19.263496  20.222332  21.046131  21.512142
0:  17.636042  17.762123 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.31528  17.364616 17.541088 17.661974 17.53635  17.085732 16.8632
0:  16.491741 16.144815 15.779359 15.140118 14.428409 13.801631 13.545074
0:  13.784882 14.465798 15.318905 16.090054 14.408578 14.507726]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2864413  -3.3524365  -3.168757   -2.8461118  -2.643746   -2.6031623
0:  -2.7999434  -2.8947902  -3.1469054  -3.1774974  -3.1258044  -3.2542548
0:  -3.3308177  -3.3373742  -3.0734715  -2.365065   -1.3329043  -0.24963903
0:  -1.4124055  -0.98690605]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.144323 23.048304 22.915133 22.66074  22.388361 21.878668 21.953308
0:  21.785276 21.572508 21.084162 20.052519 19.038513 18.144106 17.91934
0:  18.3473   19.265312 20.39613  21.315    20.25552  20.507603]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.45825   14.529795  14.711592  14.946938  14.909117  14.563461
0:  13.987679  13.39254   12.5680065 11.854593  11.149619  10.271995
0:   9.518872   8.832032   8.464014   8.481633   8.794135   9.131243
0:   6.582743   6.312627 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.904773  -5.9392686 -5.586225  -4.985664  -4.4517007 -4.157884
0:  -3.9734406 -3.835832  -3.8597922 -3.8591294 -3.9706302 -4.3144803
0:  -4.61904   -4.8294244 -4.8059487 -4.354923  -3.7273898 -3.0903478
0:  -3.9202533 -3.8815827]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.384372  -5.477038  -5.1273446 -4.390098  -3.5637984 -2.8435807
0:  -2.271793  -1.9945865 -2.28126   -2.6970935 -3.367681  -4.213879
0:  -4.8116164 -5.1823373 -5.2120004 -4.917563  -4.583711  -4.259187
0:  -6.2734666 -6.3505654]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.841483  3.2176683 3.965211  4.834319  5.4937234 5.8559017 6.014984
0:  6.123158  6.006998  5.9016256 5.675615  5.157181  4.690448  4.3191233
0:  4.271135  4.6784787 5.3361096 5.9982257 3.1339886 3.3961344]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.986233 -10.096489  -9.772671  -9.073259  -8.403124  -7.959005
0:   -7.846273  -8.004597  -8.65102   -9.261208  -9.89529  -10.66119
0:  -11.169771 -11.433413 -11.355421 -10.775696 -10.024563  -9.210226
0:  -10.557177 -10.200321]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0470624 -4.092096  -3.8786016 -3.468511  -3.1476855 -3.0214715
0:  -3.1737275 -3.4405894 -4.2005258 -4.981212  -5.823315  -6.8030066
0:  -7.457808  -7.9131513 -8.035982  -7.7604747 -7.3658586 -6.9236455
0:  -8.576174  -8.844921 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.9935     7.149853   7.54433    8.038873   8.268484   8.089813
0:  7.4721823  6.639426   5.4162984  4.3315744  3.3186631  2.2438908
0:  1.3752861  0.64264774 0.23442936 0.3192668  0.7527156  1.3274603
0:  0.5953479  0.77858543]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.8777275 11.969175  12.348781  12.817076  13.108217  13.110493
0:  13.169392  13.135702  13.006971  12.888145  12.53507   12.030516
0:  11.611229  11.408207  11.576856  12.125668  12.803801  13.341356
0:  11.615961  11.740729 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.55798    9.887572  10.460002  11.117306  11.681347  12.077896
0:  12.620459  13.037041  13.299673  13.551687  13.501314  13.319372
0:  13.205814  13.39133   13.890393  14.59261   15.206363  15.523169
0:  13.180437  13.3507595]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.30203  14.391572 14.509026 14.518666 14.503267 14.449817 15.045738
0:  15.678512 16.416424 17.012794 17.101925 17.130962 17.11664  17.595667
0:  18.475447 19.617355 20.68144  21.348614 21.254642 21.421556]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.253645   -1.4066453  -1.2339125  -0.79189825 -0.44141674 -0.2915082
0:  -0.4936309  -0.6520424  -1.0522847  -1.2280326  -1.3257494  -1.6817098
0:  -2.0244112  -2.3480382  -2.397941   -1.8575168  -0.8054271   0.4328456
0:   0.02754164  0.17755365]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.164682   9.102943   9.326216   9.716455  10.074914  10.234453
0:  10.439177  10.488118  10.309221  10.132925   9.714383   9.124275
0:   8.579627   8.196519   8.146088   8.493026   9.024919   9.536158
0:   7.690415   7.7812004]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6517372 2.4465194 2.5963478 2.8971486 3.162325  3.220994  3.4757218
0:  3.6520855 3.8247972 3.9926789 3.9749572 3.7730694 3.6596162 3.823453
0:  4.306555  5.191386  6.1221943 6.831888  4.5309424 4.472284 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.316363   7.3461666  7.6239867  8.074692   8.550617   8.931462
0:   9.601971  10.215103  10.754434  11.183381  11.248915  11.129198
0:  10.972998  11.032234  11.391138  11.968708  12.523691  12.932657
0:  10.924339  10.720123 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.880077   -9.898277   -9.531008   -8.752684   -7.918184   -7.357639
0:   -6.9618497  -6.8227277  -7.1984553  -7.5780673  -8.178595   -8.984045
0:   -9.522739   -9.868761   -9.829966   -9.476292   -9.184425   -8.971867
0:  -10.461674  -10.646227 ]
0: validation loss for strategy=forecast at epoch 30 : 0.3391546308994293
0: validation loss for velocity_u : 0.14696945250034332
0: validation loss for velocity_v : 0.24734292924404144
0: validation loss for specific_humidity : 0.17749221622943878
0: validation loss for velocity_z : 0.526221513748169
0: validation loss for temperature : 0.12154287099838257
0: validation loss for total_precip : 0.8153589367866516
0: 31 : 20:01:42 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 31, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0390, 0.0539, 0.0677, 0.0826, 0.1008, 0.1235, 0.1506, 0.1796, 0.2073, 0.2310, 0.2488, 0.2600, 0.2651, 0.2652,
0:         0.2609, 0.2531, 0.2434, 0.2322, 0.0813, 0.0974, 0.1094, 0.1191, 0.1286, 0.1411, 0.1592], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5359, 0.5000, 0.4695, 0.4440, 0.4213, 0.4009, 0.3811, 0.3597, 0.3354, 0.3065, 0.2736, 0.2373, 0.1990, 0.1615,
0:         0.1269, 0.0966, 0.0729, 0.0548, 0.5348, 0.4922, 0.4580, 0.4293, 0.4050, 0.3852, 0.3675], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5004, -0.4653, -0.4208, -0.3747, -0.3127, -0.2762, -0.1978, -0.0944, -0.0039,  0.1314,  0.2569,  0.3713,
0:          0.4511,  0.5475,  0.5580,  0.5908,  0.5896,  0.5550, -0.5087, -0.4916, -0.4518, -0.4050, -0.3512, -0.3137,
0:         -0.2719], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2746,  0.3044,  0.2978,  0.2492,  0.2138,  0.1232,  0.0337, -0.0149, -0.0856, -0.1209, -0.1320, -0.1861,
0:         -0.2325, -0.2944, -0.3474, -0.3319, -0.3375, -0.3397,  0.2238,  0.2735,  0.3066,  0.3110,  0.3254,  0.2680,
0:          0.1906], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.1652, 1.1523, 1.1387, 1.1217, 1.0988, 1.0705, 1.0388, 1.0036, 0.9673, 0.9322, 0.9049, 0.8862, 0.8705, 0.8539,
0:         0.8377, 0.8242, 0.8162, 0.8140, 0.8164, 0.8231, 0.8318, 0.8413, 0.8510, 0.8561, 0.8554], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2393, -0.2393, -0.2381, -0.2381, -0.2393, -0.2393, -0.2300, -0.2196, -0.2370, -0.2393, -0.2393, -0.2393,
0:         -0.2393, -0.2393, -0.2393, -0.2393, -0.2243, -0.2381, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393,
0:         -0.2393], device='cuda:0')
0: [DEBUG] Epoch 31, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2393, -0.2393,     nan,     nan,     nan, -0.2393,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,
0:             nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,
0:             nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,
0:         -0.2393,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2370,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2358,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 31, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0395,  0.0098, -0.0108, -0.0220, -0.0324, -0.0540, -0.0238, -0.0043,  0.0174,  0.0182, -0.0241, -0.0725,
0:         -0.1117, -0.0973, -0.0479,  0.0154,  0.0735,  0.0863,  0.0866,  0.0971,  0.1157,  0.1075,  0.0586,  0.0256,
0:          0.0248], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4555, 0.4787, 0.4838, 0.4751, 0.4427, 0.3962, 0.3305, 0.2919, 0.2808, 0.2870, 0.2955, 0.3020, 0.2782, 0.2205,
0:         0.1623, 0.1369, 0.1572, 0.2059, 0.4314, 0.4571, 0.4647, 0.4561, 0.4226, 0.3714, 0.3159], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6846, -0.6790, -0.6689, -0.6499, -0.6352, -0.6290, -0.6305, -0.6372, -0.6457, -0.6552, -0.6583, -0.6560,
0:         -0.6419, -0.6219, -0.6079, -0.5944, -0.5894, -0.5853, -0.6741, -0.6657, -0.6513, -0.6335, -0.6187, -0.6122,
0:         -0.6090], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1693, -0.0632,  0.0464, -0.0475, -0.1511,  0.0071,  0.0063, -0.0516,  0.0008, -0.0480, -0.0270,  0.0047,
0:          0.0085, -0.0200, -0.1221,  0.0018,  0.1940,  0.1677, -0.0207, -0.0797, -0.1016, -0.1423, -0.2112, -0.0509,
0:         -0.0529], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([2.4097, 2.4423, 2.4283, 2.3856, 2.3559, 2.3600, 2.3876, 2.4043, 2.3941, 2.3588, 2.3193, 2.2905, 2.2745, 2.2679,
0:         2.2499, 2.2213, 2.1959, 2.1846, 2.1936, 2.2190, 2.2505, 2.2866, 2.3196, 2.3382, 2.3322], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2226, -0.2265, -0.2354, -0.2350, -0.2298, -0.2296, -0.2284, -0.2270, -0.2190, -0.2255, -0.2219, -0.2258,
0:         -0.2295, -0.2319, -0.2263, -0.2219, -0.2267, -0.2327, -0.2202, -0.2181, -0.2241, -0.2198, -0.2198, -0.2292,
0:         -0.2214], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23133140802383423; velocity_v: 0.3434155583381653; specific_humidity: 0.16175143420696259; velocity_z: 0.5270826816558838; temperature: 0.14371496438980103; total_precip: 0.3838515877723694; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20133136212825775; velocity_v: 0.30630403757095337; specific_humidity: 0.19928033649921417; velocity_z: 0.5245582461357117; temperature: 0.16135573387145996; total_precip: 0.49315178394317627; 
0: epoch: 31 [1/5 (20%)]	Loss: 0.43850 : 0.27182 :: 0.21030 (2.57 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20635278522968292; velocity_v: 0.2943632900714874; specific_humidity: 0.18672949075698853; velocity_z: 0.6025442481040955; temperature: 0.152093306183815; total_precip: 0.7224259972572327; 
0: epoch: 31 [2/5 (40%)]	Loss: 0.72243 : 0.32675 :: 0.20588 (16.04 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20354408025741577; velocity_v: 0.30368661880493164; specific_humidity: 0.18495389819145203; velocity_z: 0.6514074206352234; temperature: 0.15727499127388; total_precip: 0.6590153574943542; 
0: epoch: 31 [3/5 (60%)]	Loss: 0.65902 : 0.32597 :: 0.20498 (15.89 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20606663823127747; velocity_v: 0.2723272442817688; specific_humidity: 0.21274907886981964; velocity_z: 0.597071647644043; temperature: 0.16833281517028809; total_precip: 0.9362756609916687; 
0: epoch: 31 [4/5 (80%)]	Loss: 0.93628 : 0.36463 :: 0.20650 (16.24 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 1.90734863e-06 1.04904175e-05
0:  6.96182251e-05 1.90734863e-04 1.39236450e-04 2.76565552e-05
0:  3.81469727e-06 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  9.53681592e-07 1.04904175e-05 2.95639038e-05 2.38418579e-05
0:  2.95639038e-05 3.81469727e-05 2.00271606e-05 9.53681592e-07
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 9.53681592e-07
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 1.90734863e-06
0:  3.81469727e-06 9.53681592e-07 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 4.76837158e-06 3.81469727e-06 2.09808350e-05
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 2.81333923e-04 2.42233276e-04
0:  4.10079956e-05 1.26838684e-04 4.19616699e-05 2.38418579e-05
0:  9.53681592e-07 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  9.53681592e-07 9.53674316e-06 2.00271606e-05 1.90734863e-05
0:  3.24249268e-05 1.14440918e-05 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  9.53681592e-07 4.76837158e-06 6.67572021e-06 1.90734863e-06
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 2.86102295e-06]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7684443e-07 0.0000000e+00 1.9073486e-06 4.7684443e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 1.9073486e-06 7.1525574e-06 1.2874603e-05 9.0599060e-06
0:  1.2874603e-05 1.2874603e-05 9.0599060e-06 3.8146973e-06 1.4305042e-06
0:  2.3841858e-06 1.2397766e-05 1.8596649e-05 1.2397766e-05 3.3378601e-06
0:  6.1988831e-06 4.2915344e-06 2.6702881e-05 3.6239624e-05 3.1471252e-05
0:  3.6716461e-05 5.0544739e-05 1.2874603e-05 4.7684443e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 4.7684443e-07 1.9073486e-06 7.1525574e-06
0:  4.2915344e-06 4.2915344e-06 4.7684443e-07 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 9.5367432e-07 3.3378601e-06
0:  3.8146973e-06 5.7220459e-06 2.7179718e-05 2.0027161e-05 1.4781952e-05
0:  7.1525574e-06 2.8610229e-06 4.7684443e-07 7.1525574e-06 1.4305115e-05
0:  6.6757202e-06 4.7684443e-07 0.0000000e+00 9.5367432e-07 4.7683716e-06]
0: Prediction values (first 20):
0: [-1.2119417  -1.237814   -0.89292336 -0.35372877  0.07665825  0.2469716
0:   0.4436493   0.50439644  0.36431265  0.24005365 -0.04930019 -0.52284
0:  -0.83952427 -0.86442757 -0.5296178   0.1957593   0.99204254  1.6144056
0:   0.17993498  0.37514257]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.653, max = 2.962, mean = 0.064
0:          sample (first 20): tensor([-0.6170, -0.6192, -0.5901, -0.5448, -0.5085, -0.4942, -0.4776, -0.4725, -0.4843, -0.4948, -0.5191, -0.5590,
0:         -0.5856, -0.5877, -0.5596, -0.4985, -0.4315, -0.3791, -0.5119, -0.5346])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.15126   -9.073107  -8.57352   -7.7149477 -6.86928   -6.3073473
0:  -6.0968046 -6.1228075 -6.651593  -7.1153107 -7.614997  -8.247044
0:  -8.59434   -8.775297  -8.651288  -8.132541  -7.5134087 -6.901208
0:  -8.287655  -8.240223 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7510056  -2.585627   -2.024878   -1.1386981  -0.29811     0.28008413
0:   0.542377    0.64333344  0.35966825  0.13470125 -0.11020851 -0.5699258
0:  -0.876699   -1.14955    -1.1617541  -0.75852823 -0.11834192  0.5978751
0:  -0.50509644 -0.442039  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.2451844 7.341226  7.672737  8.097711  8.398134  8.528431  8.489529
0:  8.42462   8.073058  7.745633  7.3225336 6.731102  6.2796516 5.967912
0:  6.0411015 6.4758863 7.1091785 7.6901827 5.9909406 6.2066565]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.960264 22.923239 23.125837 23.396532 23.39016  22.958406 22.589878
0:  22.21187  21.769043 21.298588 20.62013  19.736357 18.99983  18.500544
0:  18.566578 19.137714 20.052195 20.921564 17.37829  17.43734 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.175342  -6.0973277 -5.6826835 -4.9618974 -4.297055  -3.8617759
0:  -3.6559243 -3.635507  -4.036855  -4.4398627 -4.963409  -5.6591887
0:  -6.1624427 -6.5198865 -6.5628686 -6.259028  -5.822108  -5.356788
0:  -6.2861953 -6.1035438]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.71021  24.548664 24.565199 24.669819 24.753386 24.637281 24.946331
0:  25.07847  25.299566 25.336254 24.935902 24.51931  24.226799 24.291992
0:  24.88368  25.82322  26.846426 27.739202 25.743849 25.678308]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7740006  -5.4509196  -4.67571    -3.4699292  -2.3144107  -1.425839
0:  -0.8787098  -0.69346666 -1.05938    -1.5340266  -2.1623101  -2.9403052
0:  -3.4611888  -3.7225528  -3.6006484  -3.0351071  -2.311707   -1.6412358
0:  -2.5184398  -2.514236  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.0867963 7.2758107 7.753632  8.347332  8.788185  8.9962845 9.184694
0:  9.295552  9.262802  9.172705  8.858492  8.367565  7.9149523 7.656112
0:  7.768291  8.2170315 8.785179  9.24982   7.7172832 7.9726315]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.189774  13.984031  14.081337  14.272151  14.439314  14.418664
0:  14.56646   14.5724325 14.494202  14.313068  13.825365  13.219775
0:  12.704004  12.501093  12.733547  13.311741  14.020058  14.610907
0:  11.822144  11.655256 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.805775   8.876959   9.22813    9.711742  10.067575  10.170176
0:  10.2792    10.184151   9.878429   9.503606   8.878753   8.10512
0:   7.4600935  7.1344886  7.1973934  7.630163   8.170592   8.577536
0:   7.0924907  7.147005 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.6819024  -0.5079398   0.12065268  1.0544524   1.9549351   2.6415377
0:   3.1859603   3.5299077   3.5802999   3.6239536   3.536273    3.2318692
0:   3.077879    3.0426831   3.2703671   3.8377786   4.484338    5.037814
0:   3.2794943   3.2990746 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.838506 11.031919 11.516805 12.063974 12.41086  12.430847 12.588105
0:  12.576626 12.517191 12.401714 11.964027 11.377678 10.830605 10.60609
0:  10.81003  11.412779 12.119668 12.663828 11.188509 11.282387]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2370753 1.5739512 2.2830071 3.2049356 4.0114636 4.5973754 4.919584
0:  5.22306   5.2154865 5.249741  5.1733117 4.727665  4.25671   3.757385
0:  3.5219047 3.8061051 4.5203023 5.449927  4.3053226 4.5679665]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.632202  12.403347  12.300333  12.286998  12.256698  12.006579
0:  12.216212  12.276291  12.445787  12.53178   12.243717  11.951636
0:  11.70642   11.959812  12.677791  13.855396  15.1677265 16.236816
0:  16.111929  16.337402 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.295197  4.315796  4.639823  5.185922  5.6382656 5.896131  6.028576
0:  6.030812  5.7957115 5.5179944 5.111665  4.547858  4.132276  3.9106107
0:  4.0797853 4.6692767 5.4670973 6.206135  5.2219033 5.3520756]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.424202 17.442028 17.57446  17.669092 17.607225 17.323368 17.169645
0:  16.940006 16.703058 16.41466  15.955849 15.387443 15.010206 14.968847
0:  15.34536  16.029152 16.796963 17.370651 15.288305 15.372819]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.946686   9.195656   9.70726   10.216741  10.422861  10.297656
0:  10.262407  10.284799  10.3604145 10.573214  10.730414  10.779825
0:  10.9969635 11.511192  12.454147  13.729069  15.022725  16.022116
0:  14.401926  14.750385 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.88369  44.37729  44.89686  45.415226 45.739723 45.664944 46.10384
0:  46.283863 46.56262  46.62948  46.076374 45.378647 44.781986 44.34188
0:  44.293304 44.37467  44.429394 44.25701  40.090782 40.33262 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.290813 23.116465 23.295753 23.561077 23.651157 23.429888 23.231895
0:  22.8856   22.50431  22.12284  21.575012 20.825148 20.264387 19.881683
0:  19.982353 20.48469  21.196508 21.767958 19.032965 18.984259]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.466812 44.05065  43.992344 43.632885 43.129078 42.412766 42.87979
0:  43.361153 44.180763 44.629173 44.170963 43.800114 43.326378 43.488045
0:  44.2731   45.166298 45.930943 45.861023 45.329487 46.2289  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.651775 18.706358 19.076885 19.546906 19.736967 19.626854 19.477179
0:  19.366617 19.197292 19.088064 18.769688 18.187744 17.596552 17.047058
0:  16.943882 17.213152 17.726215 18.1339   15.351814 15.132219]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.297121  -8.022198  -7.4313226 -6.787435  -6.3335834 -6.17683
0:  -5.983524  -5.942056  -6.0819592 -6.2841916 -6.7883267 -7.5400376
0:  -8.211594  -8.562977  -8.563581  -8.187601  -7.851004  -7.6084256
0:  -8.85111   -8.588669 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.143993 12.304502 12.605009 12.838089 12.848362 12.616205 12.561094
0:  12.395952 12.163338 11.864639 11.194111 10.382769  9.580478  9.197687
0:   9.237296  9.572686  9.880321  9.954749  7.137734  7.1281  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.9135494  7.4340725  8.214955   9.049381   9.694173  10.058444
0:  10.4584675 10.656641  10.682442  10.641315  10.298731   9.831895
0:   9.465635   9.47006    9.885583  10.648424  11.465202  12.101088
0:  11.080975  11.343189 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.180813 14.364121 14.824586 15.412945 15.896322 16.07029  16.476677
0:  16.706625 16.882408 16.904322 16.553968 16.102312 15.699873 15.683048
0:  16.074606 16.781492 17.478462 17.800463 14.509627 14.346857]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.51587963  0.5642109   0.891767    1.4420199   1.917458    2.2009697
0:   2.1851988   2.1332893   1.7400279   1.4474139   1.1426692   0.6112871
0:   0.18283892 -0.18785429 -0.2544961   0.17202282  0.94466114  1.7913132
0:   0.94616747  0.8040042 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.731583   -2.8635306  -2.67098    -2.2723947  -1.9636855  -1.8199201
0:  -1.4145789  -1.1246219  -0.85520935 -0.7697263  -1.1205468  -1.6945996
0:  -2.2663026  -2.4062152  -2.100367   -1.3708262  -0.5894103   0.0053153
0:  -1.8725123  -1.853169  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.19752741 -0.5658126  -0.57348776 -0.42711687 -0.3212762  -0.4317603
0:  -0.43352222 -0.46422672 -0.54154444 -0.6002288  -0.90221596 -1.3665228
0:  -1.8013725  -1.9398403  -1.6661568  -0.9390135  -0.06151628  0.7967763
0:  -0.9046612  -1.2165394 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.932116 13.067732 13.470203 13.921154 14.220062 14.256893 14.427605
0:  14.45141  14.483059 14.500288 14.268898 13.925367 13.661554 13.711847
0:  14.161772 14.996298 15.990797 16.917038 16.282536 16.616333]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.98203  31.282982 31.577856 31.795416 32.021877 32.00243  32.660923
0:  33.233234 33.89861  34.376892 34.2293   34.24439  34.20937  34.812347
0:  35.823105 36.904247 37.68799  37.777657 34.40659  34.654686]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.129303  -9.004257  -8.473494  -7.6159167 -6.8050437 -6.268436
0:  -6.055699  -6.1524434 -6.745773  -7.3007855 -7.952708  -8.680725
0:  -9.136965  -9.389252  -9.318476  -8.931042  -8.493443  -8.065319
0:  -8.357218  -8.128544 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.20112371 -0.36267424 -0.20009375  0.1777668   0.49169636  0.62034035
0:   0.84869385  0.96295357  0.90904     0.8529005   0.5389943   0.08319426
0:  -0.30274868 -0.3974533  -0.13091898  0.40145922  0.8906679   1.1494107
0:  -0.4565344  -0.4576192 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.322456  -10.082111   -9.483429   -8.640322   -7.8471265  -7.4315248
0:   -7.3331122  -7.5382237  -8.237484   -8.911461   -9.720831  -10.604977
0:  -11.272934  -11.715015  -11.719025  -11.40028   -10.935558  -10.566814
0:  -10.733295  -10.581266 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.005159  -8.956032  -8.476316  -7.599297  -6.611709  -5.814812
0:  -5.2869954 -4.980165  -5.2123275 -5.494079  -5.9393363 -6.6161838
0:  -7.0771585 -7.4661    -7.5307612 -7.249391  -6.8537526 -6.3961854
0:  -7.7185206 -7.835358 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.7795677  0.65076685 0.73971224 1.0318909  1.3304787  1.5596638
0:  1.9018826  2.2366123  2.427291   2.5828037  2.5358691  2.3747249
0:  2.2777128  2.4576497  2.974628   3.6962478  4.4057674  4.945304
0:  3.417762   3.4430454 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.326853  14.943035  14.6469965 14.406399  14.097483  13.613115
0:  13.693256  13.731818  13.845571  13.887886  13.476425  12.974485
0:  12.424385  12.36695   12.765188  13.561613  14.464923  15.085201
0:  13.288267  13.028389 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.9152002   1.8952084   2.164545    2.634583    3.0203576   3.1710837
0:   3.04853     2.7154074   1.9540153   1.2348933   0.41461134 -0.54134846
0:  -1.3384752  -1.956253   -2.2714663  -2.1572495  -1.841033   -1.4096327
0:  -2.7889462  -2.7632275 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.681656   -5.4131575  -4.8396072  -3.9194908  -2.8993187  -1.974412
0:  -1.2644968  -0.6992979  -0.6864376  -0.77275085 -1.0449266  -1.50667
0:  -1.8103924  -2.0188227  -1.8921528  -1.5040874  -0.9606762  -0.48282766
0:  -2.5974092  -2.2573347 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.082727  -4.04481   -3.6464772 -2.8570628 -2.1418233 -1.7314019
0:  -1.8618665 -2.272325  -3.2759128 -4.3251987 -5.4422135 -6.7620835
0:  -7.8420177 -8.762293  -9.230623  -9.02231   -8.400333  -7.5352683
0:  -7.757486  -7.895226 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.201928   8.215354   8.465162   8.827219   9.097079   9.139468
0:   9.414129   9.588805   9.751776   9.828299   9.590954   9.178871
0:   8.811326   8.742246   9.1225195  9.864237  10.719424  11.410593
0:   9.919584  10.08947  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.813078 19.665352 19.33106  18.922136 18.647919 18.263985 18.891909
0:  19.371487 20.046957 20.492928 20.256886 20.146923 20.015047 20.630333
0:  21.86502  23.480122 25.108585 26.152433 27.936346 28.372257]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.708361 13.267487 13.132765 13.150791 13.152279 13.06574  13.092299
0:  13.145742 13.131022 13.159794 13.052463 12.713716 12.53677  12.556257
0:  12.873482 13.436753 14.081789 14.525065 12.493599 12.348003]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.621832 16.869745 17.098757 17.148226 17.063457 16.794216 17.141275
0:  17.505795 17.960516 18.2784   17.961117 17.602104 17.116737 17.18207
0:  17.723799 18.625576 19.523832 20.0571   20.20878  20.374287]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.8103895 3.127711  3.7086623 4.3734665 4.850104  5.1001263 5.163911
0:  5.2808266 5.1946473 5.206419  5.1451154 4.8340764 4.5378366 4.2601147
0:  4.23586   4.5948534 5.214972  5.8516846 4.873477  5.1413956]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.123808 19.541744 20.144102 20.681507 20.89143  20.864355 20.866806
0:  20.882292 20.819895 20.805998 20.601862 20.22094  19.958838 19.853777
0:  20.150618 20.725708 21.33396  21.751564 18.11817  18.439133]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.592999 16.766653 17.081045 17.390831 17.544739 17.471731 17.520987
0:  17.45301  17.259983 17.040178 16.508305 15.827608 15.167377 14.74411
0:  14.678505 14.95911  15.36125  15.654417 12.785948 12.786772]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.11401987  0.17362547  0.51897144  1.1701112   1.8003387   2.176187
0:   2.2010703   1.9382796   1.1353955   0.34426785 -0.45650625 -1.4551668
0:  -2.1985874  -2.7775383  -3.0659256  -2.9174604  -2.501051   -1.9511242
0:  -2.8963633  -3.0725737 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.5392313 4.3322377 4.3861785 4.5987544 4.737626  4.6026597 4.69116
0:  4.6607757 4.621577  4.5373216 4.130618  3.6119747 3.0350008 2.822045
0:  2.9881446 3.5437477 4.1691713 4.5977917 3.2231448 3.036117 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.459618 35.60483  35.606815 35.456287 35.17849  34.53152  34.518738
0:  34.30463  34.11291  33.705048 32.67413  31.659918 30.646818 30.239496
0:  30.462564 31.12257  31.935555 32.44722  31.056183 31.0813  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.558853 24.91681  24.253483 23.660536 23.046991 22.165234 21.92963
0:  21.456167 21.098969 20.52119  19.334    18.195415 17.018713 16.38499
0:  16.32645  16.685698 17.302624 17.7224   18.073015 17.842869]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.319318   4.632559   5.2500825  5.982294   6.7422585  7.4302845
0:   8.438943   9.4633665 10.555142  11.616145  12.399906  13.064732
0:  13.700024  14.645238  15.897518  17.309433  18.577955  19.430475
0:  18.767155  19.531183 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.702008  4.962655  5.5359306 6.2545314 6.841868  7.1919274 7.475976
0:  7.536068  7.3542585 7.106147  6.6599364 6.0744925 5.665606  5.546076
0:  5.8456125 6.5200844 7.326301  8.023712  6.708904  6.851698 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.124985 25.28368  25.642763 25.96734  26.072842 25.917196 25.903078
0:  25.926483 25.976763 26.051348 25.908478 25.591795 25.324373 25.2191
0:  25.523779 26.08934  26.741653 27.188263 23.34166  23.325829]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.8677173  -4.9722066  -4.6285625  -4.01216    -3.4026961  -2.9700494
0:  -2.524345   -2.1521945  -1.9583268  -1.744483   -1.6971931  -1.8800235
0:  -1.9561753  -1.7898712  -1.315649   -0.52060986  0.28143167  0.91439486
0:  -0.34348726  0.0121913 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.078444 25.062468 25.183573 25.333729 25.402403 25.14258  25.1971
0:  24.893024 24.316984 23.381731 21.762949 19.885464 17.984915 16.617512
0:  15.974369 16.030441 16.53761  17.139585 16.04429  16.09511 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1897125 2.3175507 2.6953888 3.32506   3.9150789 4.3143883 4.438322
0:  4.5516634 4.3180704 4.1983004 4.0446978 3.6555154 3.2783625 2.9027858
0:  2.734495  2.9995685 3.608639  4.2634335 1.9275975 1.917377 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.0235195  2.3731236  2.9494262  3.590887   3.9938416  4.142392
0:   4.127101   4.237377   4.2901926  4.543059   4.840542   4.9749966
0:   5.169937   5.453064   6.076103   7.1665254  8.60046   10.05413
0:   9.452943   9.933715 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.477611   5.4777174  5.758332   6.2702675  6.801885   7.257577
0:   7.6827164  8.027984   8.129763   8.261361   8.289226   8.202224
0:   8.271309   8.447834   8.917263   9.597644  10.322736  10.869216
0:  10.099441  10.325859 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.4271297  6.4873247  6.725337   7.1273584  7.543729   7.8452287
0:   8.509047   9.059721   9.623099  10.079702  10.197039  10.249523
0:  10.365444  10.986874  12.103418  13.688122  15.430635  16.931837
0:  18.800716  19.324808 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.958704   -2.056964   -1.8166919  -1.3016496  -0.7994437  -0.43454075
0:  -0.19587421 -0.02271414 -0.14110518 -0.3360877  -0.74307156 -1.4555564
0:  -2.1454587  -2.687098   -2.8715997  -2.620173   -2.0913734  -1.5395455
0:  -2.5765843  -2.4522338 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4689262  2.606153   2.959735   3.4276724  3.7879348  3.9215863
0:  4.0830593  4.16704    4.057349   3.9124534  3.4958835  2.8572598
0:  2.2782526  1.9401894  1.9036441  2.2256     2.5899901  2.8409054
0:  0.98642635 1.15486   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.972183 19.019539 19.253515 19.477045 19.521297 19.282978 19.20768
0:  19.039047 18.822123 18.553705 17.998812 17.32212  16.708649 16.409697
0:  16.49108  16.877111 17.36544  17.65822  15.50025  15.578531]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.7697668   3.2170568   2.8993063   2.9226627   2.9795532   2.9393277
0:   2.6125798   2.338346    1.7334013   1.3225436   0.9850607   0.40486717
0:  -0.09590101 -0.5945573  -0.82205105 -0.52597904  0.20564175  1.1181321
0:   0.53735304  0.29096127]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.834667 40.77327  40.353127 39.808037 39.313984 38.741985 39.32631
0:  39.81681  40.44119  40.676838 39.970924 39.409023 38.8383   39.083263
0:  40.06282  41.361084 42.65022  43.17376  43.84333  44.04327 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0786757  -4.0040927  -3.4366717  -2.4331145  -1.3321185  -0.3332529
0:   0.5273056   1.2901874   1.6497254   1.9474797   2.0333815   1.7345295
0:   1.4178433   1.0229626   0.7151871   0.7109451   0.8332572   0.98156834
0:  -1.7799029  -1.891129  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.4597926 7.713049  8.244118  8.773749  8.867857  8.456588  7.811966
0:  7.110678  6.325246  5.7624846 5.2015886 4.408136  3.694841  3.0990088
0:  2.8663008 3.1184034 3.6450224 4.124911  2.0198045 2.0736628]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0897121  -3.1832771  -3.0317688  -2.4385157  -1.7103271  -1.0389938
0:  -0.57045937 -0.14701748 -0.20074177 -0.26696157 -0.46782255 -0.8383436
0:  -1.0764542  -1.1746101  -0.9132395  -0.3203969   0.5221553   1.3827901
0:   0.48974705  0.39572763]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.01671839  0.02690792  0.41714048  1.1054792   1.775814    2.3086066
0:   2.565216    2.8290763   2.763636    2.7958832   2.8209352   2.5740075
0:   2.3915367   2.1866732   2.2108083   2.683567    3.5001957   4.40745
0:   3.7988033   3.8296714 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.326025   -3.9802065  -3.263523   -2.4038973  -1.7201328  -1.249753
0:  -0.8980484  -0.5489316  -0.38323212 -0.20796251 -0.10665035 -0.26767874
0:  -0.3228407  -0.21863556  0.24567366  1.159864    2.288239    3.469792
0:   2.0051622   2.3626642 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [48.80196  48.952675 48.69328  48.460518 48.135105 47.7527   48.39846
0:  48.863754 49.689568 50.069836 49.683426 49.29724  49.057842 49.38533
0:  50.520645 51.695007 52.73637  53.040066 52.345833 52.877945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.886917 27.869692 27.95515  28.133192 28.364851 28.4288   29.14174
0:  29.678905 30.269438 30.632307 30.388918 30.168571 29.917639 30.184677
0:  30.954489 31.931627 32.905304 33.448048 32.249165 32.194603]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9717526 -2.1850724 -2.0858197 -1.6361089 -1.2432151 -1.0408902
0:  -1.3065944 -1.6897521 -2.5837588 -3.410605  -4.269381  -5.3067374
0:  -6.183699  -6.966107  -7.396415  -7.3250384 -6.912125  -6.3552613
0:  -6.846476  -6.941252 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.404763  6.476862  6.840853  7.3983006 7.8350787 8.062077  8.161782
0:  8.165926  7.8909774 7.561966  7.0447903 6.3174915 5.7416253 5.3804965
0:  5.44069   5.9341803 6.609854  7.2274756 5.9494047 6.1020513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.71443   -14.502346  -13.847998  -12.678343  -11.252607   -9.867935
0:   -8.62047    -7.6943555  -7.4450526  -7.2681203  -7.3416305  -7.5140853
0:   -7.459542   -7.291721   -6.8038516  -6.071838   -5.2975364  -4.5419316
0:   -5.818353   -5.6835046]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.038365  -5.10095   -4.8793187 -4.4278665 -4.011846  -3.8077264
0:  -3.8242736 -3.9409556 -4.3681693 -4.692846  -5.008394  -5.3712583
0:  -5.5224223 -5.477451  -5.141479  -4.520454  -3.8493876 -3.3169618
0:  -6.230889  -6.44194  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.87211  25.644732 25.563934 25.578493 25.576578 25.385054 25.467085
0:  25.451004 25.452946 25.369387 25.04863  24.669874 24.502045 24.538654
0:  24.956848 25.649097 26.468887 27.159945 24.679909 24.956524]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.11055    -2.1275449  -1.7721415  -1.024128   -0.23591995  0.43134928
0:   0.8249345   1.0384393   0.7845316   0.44854975 -0.06341219 -0.8424082
0:  -1.5864902  -2.2244258  -2.530973   -2.362473   -1.8766403  -1.3098292
0:  -1.3430185  -1.1619692 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.9261079 1.0667324 1.5252967 2.2770677 3.0385964 3.6330202 4.1226416
0:  4.4893312 4.532909  4.576214  4.5110693 4.255382  4.211355  4.2993526
0:  4.700285  5.440448  6.245481  6.8723416 5.3894887 5.255038 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.070519  -5.9665637 -5.463431  -4.622232  -3.787828  -3.199616
0:  -2.8700824 -2.7626042 -3.1072288 -3.4312034 -3.839974  -4.4059415
0:  -4.755671  -4.990825  -4.954641  -4.606807  -4.184749  -3.7624917
0:  -4.615296  -4.6422887]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.065116   7.2299986  7.6111417  7.9596214  8.024854   7.8228006
0:   7.605389   7.4422603  7.2876368  7.286134   7.2558336  7.093973
0:   7.105061   7.3666716  8.124341   9.356546  10.835661  12.212387
0:  10.375406  10.787191 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.96135  40.25793  40.42792  40.568886 40.606716 40.351135 40.819622
0:  41.034374 41.242035 41.13664  40.330925 39.583652 38.86679  38.850178
0:  39.472942 40.36289  41.23917  41.526794 39.778378 39.71134 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-21.285364 -21.553864 -21.394382 -20.793118 -20.073534 -19.439323
0:  -18.564077 -17.88755  -17.364254 -17.018023 -17.041622 -17.353754
0:  -17.466879 -17.211308 -16.554277 -15.615086 -14.932817 -14.575508
0:  -14.934725 -14.94253 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.072063  -5.8997893 -5.3347306 -4.4127812 -3.4913735 -2.8157678
0:  -2.483109  -2.3862967 -2.7713895 -3.1713243 -3.6252866 -4.2594867
0:  -4.735563  -5.100199  -5.145473  -4.7653337 -4.0656943 -3.2299638
0:  -3.3811793 -3.0773044]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2511926  -3.3581223  -3.0755515  -2.357246   -1.5896277  -0.9454131
0:  -0.6160717  -0.35626173 -0.5050478  -0.60636806 -0.71465254 -1.1029382
0:  -1.3474975  -1.5825124  -1.5311332  -1.047421   -0.26566172  0.6072011
0:  -0.00791836 -0.05829334]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.3380733   1.2906928   1.4750104   1.9294782   2.3394775   2.5876694
0:   2.453666    2.2775006   1.6933193   1.1983466   0.76027584  0.10011292
0:  -0.43288183 -0.93919706 -1.2018085  -0.9373183  -0.2540493   0.6514745
0:  -0.3162589  -0.4506631 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9334717 -4.884305  -4.4979935 -3.8540506 -3.3028479 -3.00453
0:  -2.8734865 -2.9126072 -3.2340689 -3.53308   -3.9628506 -4.548567
0:  -4.9990854 -5.2554493 -5.2391357 -4.9281387 -4.62271   -4.4394817
0:  -5.5905704 -5.5262156]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.21035337 -0.3109579  -0.03385639  0.66129637  1.4152503   2.0139747
0:   2.1910071   2.2324777   1.6983356   1.1246543   0.45976782 -0.55994654
0:  -1.4597569  -2.3329582  -2.8526545  -2.809435   -2.2902508  -1.5132971
0:  -2.2831373  -2.6332045 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.464478  -11.45295   -11.01552   -10.171854   -9.321611   -8.7234
0:   -8.50499    -8.518227   -8.9923     -9.432325   -9.8977165 -10.519499
0:  -10.902224  -11.130447  -11.074207  -10.6364765 -10.098652   -9.542219
0:  -10.545937  -10.561943 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.648148 17.6238   17.89761  18.29302  18.621199 18.72885  19.170603
0:  19.555876 20.007845 20.422295 20.48134  20.421726 20.363874 20.53649
0:  21.089262 21.900923 22.81268  23.51021  21.072405 21.079042]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.618183 24.587322 24.799484 25.143143 25.442749 25.501198 25.849693
0:  26.017807 26.15857  26.236721 25.906593 25.524372 25.1846   25.111462
0:  25.480007 26.148003 26.941456 27.554367 24.826263 24.727795]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.270126 30.484333 30.836933 30.998878 30.861568 30.41328  30.148388
0:  29.858803 29.471127 29.033333 28.169212 27.015656 25.921799 25.118713
0:  24.917107 25.20502  25.703308 26.210613 26.49723  26.530727]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.9646974 6.1136026 6.4884744 7.0604715 7.527156  7.79559   7.826051
0:  7.9073586 7.7375145 7.694554  7.6399574 7.2894745 6.9113574 6.50655
0:  6.353034  6.727127  7.5971503 8.633396  7.2706337 7.3196683]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.200872  6.144041  6.229591  6.2782755 6.249479  5.965553  6.143985
0:  6.223873  6.4238176 6.5097165 6.140569  5.6715107 5.164971  5.1288757
0:  5.579465  6.5589185 7.6727223 8.641619  6.89257   6.984853 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.302525   -2.1893582  -1.723      -0.8932557  -0.08778     0.5121808
0:   0.9529333   1.1536303   0.92803764  0.65630484  0.19364214 -0.41757488
0:  -0.8063884  -0.9965291  -0.87376976 -0.3995595   0.06665468  0.47447348
0:  -0.41400433 -0.514205  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.79518  18.965612 19.578426 20.327888 20.919611 21.169136 21.608704
0:  21.969898 22.292135 22.459505 22.226803 21.736809 21.320742 21.16423
0:  21.450325 22.023615 22.662067 23.087954 19.057365 19.142921]
0: validation loss for strategy=forecast at epoch 31 : 0.3816208243370056
0: validation loss for velocity_u : 0.1499396562576294
0: validation loss for velocity_v : 0.2796192169189453
0: validation loss for specific_humidity : 0.1952446550130844
0: validation loss for velocity_z : 0.5987498760223389
0: validation loss for temperature : 0.12543617188930511
0: validation loss for total_precip : 0.9407352805137634
0: 32 : 20:05:41 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 32, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3667, 0.3708, 0.3738, 0.3761, 0.3774, 0.3801, 0.3841, 0.3864, 0.3870, 0.3822, 0.3721, 0.3604, 0.3463, 0.3332,
0:         0.3240, 0.3192, 0.3208, 0.3261, 0.4026, 0.4038, 0.4024, 0.3995, 0.3963, 0.3943, 0.3952], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2718,  0.2614,  0.2375,  0.2014,  0.1569,  0.1078,  0.0562,  0.0046, -0.0445, -0.0887, -0.1259, -0.1553,
0:         -0.1750, -0.1843, -0.1850, -0.1805, -0.1730, -0.1651,  0.2237,  0.2288,  0.2208,  0.2001,  0.1672,  0.1232,
0:          0.0727], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0195, -0.2000, -0.3877, -0.5094, -0.5603, -0.5700, -0.5714, -0.5734, -0.5719, -0.5544, -0.5021, -0.3993,
0:         -0.2392, -0.0143,  0.2628,  0.5433,  0.7890,  1.0031,  0.3328,  0.1437, -0.0704, -0.2948, -0.4424, -0.5172,
0:         -0.5502], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0425,  0.0145,  0.1178,  0.1963,  0.2167,  0.2285,  0.2877,  0.3188,  0.3619,  0.3640,  0.3124,  0.2823,
0:          0.2726,  0.3124,  0.3543,  0.3629,  0.3479,  0.3425,  0.0586,  0.1092,  0.1801,  0.2554,  0.3081,  0.3629,
0:          0.4307], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.9639, 1.1218, 1.2909, 1.4518, 1.5885, 1.6906, 1.7685, 1.8438, 1.9069, 1.9547, 1.9680, 1.9447, 1.8844, 1.7706,
0:         1.6177, 1.4374, 1.2447, 1.0621, 0.9004, 0.7678, 0.6672, 0.6029, 0.5695, 0.5535, 0.5471], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2291, -0.2269, -0.2313, -0.2313, -0.2313,
0:         -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2280, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313,
0:         -0.2313], device='cuda:0')
0: [DEBUG] Epoch 32, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2313,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1700, -0.2007, -0.2050,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2226,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0243,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2215,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1996,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0572,     nan,     nan,     nan,     nan,     nan, -0.1174,     nan, -0.1492,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1240,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1415,     nan,     nan, -0.1908,     nan,     nan,
0:             nan,     nan,     nan, -0.1809,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2269,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0747,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1963,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0287,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 32, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3246, -0.3088, -0.2791, -0.2513, -0.2464, -0.2585, -0.2602, -0.2631, -0.2561, -0.2472, -0.2406, -0.2465,
0:         -0.2379, -0.2039, -0.1445, -0.0683,  0.0016,  0.0444, -0.2247, -0.2044, -0.1771, -0.1713, -0.1913, -0.2171,
0:         -0.2324], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4243, 0.4202, 0.3938, 0.3560, 0.3140, 0.2787, 0.2428, 0.2182, 0.2138, 0.2134, 0.2186, 0.2241, 0.2039, 0.1670,
0:         0.1281, 0.1174, 0.1485, 0.2198, 0.4212, 0.4386, 0.4336, 0.4151, 0.3869, 0.3529, 0.3213], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5944, -0.5931, -0.5895, -0.5821, -0.5773, -0.5793, -0.5877, -0.6032, -0.6174, -0.6294, -0.6382, -0.6408,
0:         -0.6362, -0.6294, -0.6264, -0.6185, -0.6181, -0.6068, -0.5999, -0.5951, -0.5894, -0.5782, -0.5742, -0.5753,
0:         -0.5817], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1152,  0.1655,  0.3383,  0.1997,  0.0268,  0.2070,  0.2984,  0.2335,  0.2490,  0.2329,  0.1214,  0.0756,
0:          0.1440,  0.1703,  0.2001,  0.3967,  0.5039,  0.3405,  0.1674,  0.0227,  0.0724, -0.0258, -0.0899,  0.1129,
0:          0.2204], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.6990, 1.7192, 1.7151, 1.6925, 1.6714, 1.6738, 1.6953, 1.7229, 1.7522, 1.7779, 1.8026, 1.8315, 1.8602, 1.8841,
0:         1.8993, 1.9044, 1.9121, 1.9255, 1.9462, 1.9660, 1.9872, 2.0083, 2.0321, 2.0547, 2.0581], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1298, -0.1296, -0.1420, -0.1518, -0.1466, -0.1540, -0.1465, -0.1486, -0.1384, -0.1403, -0.1399, -0.1425,
0:         -0.1382, -0.1520, -0.1529, -0.1459, -0.1475, -0.1386, -0.1364, -0.1425, -0.1402, -0.1401, -0.1419, -0.1469,
0:         -0.1425], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1881304234266281; velocity_v: 0.3280821144580841; specific_humidity: 0.1859102100133896; velocity_z: 0.6235151886940002; temperature: 0.15071676671504974; total_precip: 0.6348713040351868; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21682630479335785; velocity_v: 0.2951743006706238; specific_humidity: 0.22967471182346344; velocity_z: 0.5890582203865051; temperature: 0.7514957785606384; total_precip: 0.68166184425354; 
0: epoch: 32 [1/5 (20%)]	Loss: 0.65827 : 0.37107 :: 0.20443 (2.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23387590050697327; velocity_v: 0.3154970705509186; specific_humidity: 0.210993230342865; velocity_z: 0.751043975353241; temperature: 0.15310713648796082; total_precip: 1.1562080383300781; 
0: epoch: 32 [2/5 (40%)]	Loss: 1.15621 : 0.43506 :: 0.21351 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2329278439283371; velocity_v: 0.3481452167034149; specific_humidity: 0.19467930495738983; velocity_z: 0.6682239174842834; temperature: 0.20818528532981873; total_precip: 0.6974477767944336; 
0: epoch: 32 [3/5 (60%)]	Loss: 0.69745 : 0.35584 :: 0.20793 (16.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21016830205917358; velocity_v: 0.28562772274017334; specific_humidity: 0.1836930215358734; velocity_z: 0.5141072273254395; temperature: 0.16964726150035858; total_precip: 0.47441574931144714; 
0: epoch: 32 [4/5 (80%)]	Loss: 0.47442 : 0.27213 :: 0.21089 (15.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  2.38418579e-06 4.29153442e-06 9.05990601e-06 9.05990601e-06
0:  7.62939453e-06 3.81469727e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  5.24520874e-06 1.00135803e-05 1.47819519e-05 2.62260437e-05
0:  4.05311548e-05 5.72204590e-05 2.95639038e-05 3.19480896e-05
0:  3.95774841e-05 3.67164612e-05 1.76429749e-05 1.00135803e-05
0:  1.43051147e-05 3.95774841e-05 5.43594360e-05 6.91413879e-05
0:  3.33786011e-05 5.96046448e-05 7.62939453e-05 4.81605530e-05
0:  3.52859497e-05 4.24385071e-05 4.72068787e-05 4.33921778e-05
0:  4.38690222e-05 5.43594360e-05 4.14848364e-05 4.33921778e-05
0:  4.33921778e-05 5.34057617e-05 4.91142273e-05 4.29153442e-05
0:  3.76701355e-05 4.24385071e-05 7.72476196e-05 8.91685486e-05
0:  8.72612000e-05 7.67707825e-05 7.43865967e-05 8.58306885e-05
0:  9.48905945e-05 8.15391541e-05 5.81741333e-05 3.67164612e-05
0:  3.29017639e-05 6.96182251e-05 5.81741333e-05 9.34600830e-05
0:  1.54972076e-04 6.48498535e-05 4.19616699e-05 6.00814819e-05
0:  1.31130219e-04 1.43051147e-04 1.85966492e-04 4.62532080e-05
0:  9.44137573e-05 1.24454498e-04 1.42574310e-04 1.23500824e-04
0:  1.48773193e-04 2.35080719e-04 2.01702118e-04 2.29835510e-04
0:  2.43663788e-04 1.92165375e-04 1.42097473e-04 1.43527985e-04
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 9.53674316e-07 4.76837158e-07
0:  2.38418579e-06 2.86102295e-06 5.72204590e-06 5.72204590e-06
0:  7.15255737e-06 5.24520874e-06 1.90734863e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.76837158e-07 4.76837158e-07
0:  5.24520874e-06 1.09672546e-05 1.57356262e-05 4.33921778e-05
0:  5.10215759e-05 6.34193420e-05 5.24520874e-05 3.38554382e-05
0:  4.14848364e-05 3.24249268e-05 1.90734863e-05 1.14440918e-05
0:  3.62396240e-05 3.24249268e-05 4.72068787e-05 5.29289246e-05
0:  6.96182251e-05 9.53674316e-05 8.72612000e-05 6.19888306e-05
0:  5.10215759e-05 5.00679016e-05 4.86373901e-05 4.52995300e-05
0:  4.10079992e-05 4.76837158e-05 3.19480896e-05 4.43458557e-05
0:  4.86373901e-05 5.62667847e-05 7.00950623e-05 6.34193420e-05
0:  4.76837158e-05 8.86917114e-05 1.23500824e-04 1.33514404e-04
0:  7.91549683e-05 6.86645508e-05 9.01222229e-05 9.25064087e-05
0:  1.01566315e-04 9.82284546e-05 6.67572021e-05 4.57763635e-05
0:  4.81605530e-05 8.77380371e-05 8.29696655e-05 1.13010406e-04
0:  1.69754028e-04 8.53538513e-05 6.24656677e-05 7.77244568e-05]
0: Target values (first 200):
0: [1.71661377e-04 7.15255737e-05 1.76429749e-05 2.19345093e-05
0:  1.90734863e-05 1.19209290e-05 4.29153442e-06 5.24520874e-06
0:  1.19209290e-05 1.43051147e-05 6.67572021e-06 3.33786011e-06
0:  2.86102295e-06 1.43051147e-06 4.76837158e-07 1.90734863e-06
0:  3.33786011e-06 2.90870667e-05 6.62803650e-05 1.20639801e-04
0:  1.98841095e-04 1.71661377e-04 6.15119934e-05 2.14576721e-05
0:  1.90734863e-06 1.43051147e-06 1.90734863e-06 3.81469727e-06
0:  1.00135803e-05 1.38282776e-05 1.28746033e-05 1.52587891e-05
0:  2.05039978e-05 2.33650208e-05 2.28881836e-05 2.19345093e-05
0:  7.15255737e-06 3.33786011e-06 4.76837158e-06 6.19888306e-06
0:  5.24520874e-06 5.72204590e-06 6.67572021e-06 3.33786011e-06
0:  2.86102295e-06 9.05990601e-06 9.05990601e-06 9.05990601e-06
0:  6.67572021e-06 6.19888306e-06 8.10623169e-06 8.58306885e-06
0:  9.05990601e-06 1.62124634e-05 3.76701355e-05 4.91142273e-05
0:  4.86373901e-05 4.33921778e-05 3.91006470e-05 2.28881836e-05
0:  1.09672546e-05 9.05990601e-06 8.58306885e-06 8.58306885e-06
0:  9.05990601e-06 1.00135803e-05 1.09672546e-05 1.09672546e-05
0:  1.09672546e-05 1.62124634e-05 1.47819519e-05 1.57356262e-05
0:  2.24113464e-05 1.62124634e-05 1.28746033e-05 2.47955322e-05
0:  1.62124634e-05 1.09672546e-05 1.57356262e-05 8.10623169e-06
0:  1.04904175e-05 9.05990601e-06 7.62939453e-06 4.29153442e-06
0:  8.58306885e-06 1.66893005e-05 2.33650208e-05 2.38418579e-05
0:  1.85966492e-05 2.09808350e-05 1.95503235e-05 3.29017639e-05
0:  3.95774841e-05 5.29289246e-05 5.96046448e-05 7.20024109e-05
0:  8.48770142e-05 9.63211060e-05 1.14440918e-04 8.72612000e-05
0:  6.48498535e-05 5.86509705e-05 4.95910645e-05 6.34193420e-05
0:  6.62803650e-05 5.72204590e-05 6.34193420e-05 5.34057617e-05
0:  6.35623932e-04 1.27315521e-04 7.39097595e-05 2.52723694e-05
0:  2.71797180e-05 2.05039978e-05 5.72204590e-06 5.24520874e-06
0:  1.19209290e-05 1.52587891e-05 1.14440918e-05 8.58306885e-06
0:  5.72204590e-06 1.43051147e-06 9.53674316e-07 4.76837158e-07
0:  9.53674316e-07 1.85966492e-05 8.29696655e-05 1.46865845e-04
0:  1.62124634e-04 1.23500824e-04 1.85966492e-05 4.29153442e-06
0:  2.86102295e-06 2.86102295e-06 6.19888306e-06 2.57492065e-05
0:  3.19480896e-05 2.19345093e-05 5.57899475e-05 5.81741333e-05
0:  2.28881836e-05 2.43186951e-05 2.43186951e-05 4.95910645e-05
0:  4.43458557e-05 3.76701355e-05 1.28746033e-05 5.72204590e-06
0:  4.76837158e-06 4.29153442e-06 5.24520874e-06 5.72204590e-06
0:  5.24520874e-06 1.09672546e-05 8.10623169e-06 7.15255737e-06
0:  4.29153442e-06 3.33786011e-06 3.81469727e-06 3.81469727e-06
0:  4.76837158e-06 6.67572021e-06 2.76565552e-05 3.62396240e-05
0:  3.86238135e-05 3.29017639e-05 4.38690222e-05 2.28881836e-05
0:  1.09672546e-05 7.62939453e-06 7.15255737e-06 7.62939453e-06
0:  9.05990601e-06 1.00135803e-05 1.04904175e-05 1.23977661e-05
0:  1.28746033e-05 1.66893005e-05 1.19209290e-05 1.28746033e-05
0:  2.57492065e-05 2.43186951e-05 2.05039978e-05 2.28881836e-05
0:  1.62124634e-05 1.09672546e-05 1.43051147e-05 1.14440918e-05
0:  1.38282776e-05 1.76429749e-05 8.10623169e-06 5.72204590e-06
0:  1.57356262e-05 2.43186951e-05 2.86102295e-05 2.38418579e-05
0:  2.05039978e-05 2.76565552e-05 3.00407410e-05 3.91006470e-05]
0: Prediction values (first 20):
0: [15.227488 15.319914 15.49901  15.666414 15.764801 15.591249 15.922678
0:  16.05954  16.228422 16.195509 15.568224 14.868609 14.079739 13.865494
0:  14.138395 14.878138 15.707718 16.306208 15.453157 15.644375]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.919, max = 2.537, mean = 0.267
0:          sample (first 20): tensor([0.6705, 0.6782, 0.6930, 0.7068, 0.7149, 0.7006, 0.7280, 0.7393, 0.7532, 0.7505, 0.6987, 0.6409, 0.5757, 0.5580,
0:         0.5805, 0.6417, 0.7102, 0.7597, 0.7008, 0.7342])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1955276  -1.1333814  -0.7574773  -0.0535121   0.7037978   1.2900767
0:   1.6148977   1.7994738   1.5361676   1.2698693   0.9014516   0.3903103
0:   0.03831482 -0.19736242 -0.15569401  0.18578291  0.59911823  0.97258663
0:   0.3605914   0.35505104]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.701947   5.988293   6.619395   7.2417765  7.692851   7.825214
0:   8.189203   8.610174   9.228228   9.827518  10.2661915 10.510153
0:  10.871273  11.443611  12.43951   13.839373  15.405146  16.933481
0:  14.725231  15.301922 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.217469  -12.067275  -11.410764  -10.237758   -8.913224   -7.740279
0:   -6.9813824  -6.458954   -6.4926014  -6.5485525  -6.684863   -6.9979477
0:   -7.1404037  -7.1556673  -6.893178   -6.26641    -5.470625   -4.7265253
0:   -6.174114   -6.171365 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.240322 18.538116 18.923485 19.206429 19.311745 19.193336 19.44772
0:  19.66018  19.923656 20.136196 19.95551  19.718452 19.500763 19.640606
0:  20.1507   20.977379 21.83065  22.445013 20.633486 20.989918]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.301432  8.387487  8.777722  9.276538  9.623507  9.73271   9.774359
0:  9.821312  9.710009  9.631135  9.428395  8.983442  8.585634  8.284209
0:  8.357164  8.7553425 9.317476  9.707445  7.10615   7.0353003]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7374458  -2.655037   -2.2456112  -1.6065226  -1.0886855  -0.90584326
0:  -0.91198206 -1.2247782  -1.8628068  -2.559538   -3.385789   -4.3084297
0:  -4.9644585  -5.309019   -5.26001    -4.854541   -4.4007063  -4.048271
0:  -6.1427383  -6.1731405 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.846195 31.583862 31.270798 31.00496  30.822575 30.405483 30.825848
0:  30.998493 31.366367 31.484032 30.91056  30.494537 30.142845 30.455204
0:  31.37527  32.646984 33.867783 34.59556  33.637848 33.758144]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.598314 16.694765 16.95274  17.27269  17.471498 17.461243 17.706797
0:  17.864002 17.984573 18.08511  17.825237 17.472887 17.104702 17.082005
0:  17.469296 18.186472 19.019825 19.691437 17.941599 18.028355]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.533768  -9.811768  -9.6038475 -9.100458  -8.455971  -7.954758
0:  -7.3991723 -7.0447383 -7.0386395 -7.045218  -7.311733  -7.765549
0:  -7.9234757 -7.7524543 -7.1266427 -6.241095  -5.4231696 -4.840215
0:  -6.056974  -5.903193 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5454073 3.3919175 3.5133665 3.7876503 4.0102625 4.0002837 4.161499
0:  4.205837  4.1948557 4.0913744 3.73587   3.2267175 2.7752795 2.6356635
0:  2.8756623 3.4882712 4.1748285 4.7493916 2.283165  2.1864104]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.291234   -3.0978222  -2.6010656  -1.7643805  -0.9084501  -0.18664217
0:   0.3127246   0.68707705  0.616045    0.56110764  0.37763834 -0.0047965
0:  -0.2482624  -0.41822433 -0.31646872  0.15309143  0.8197098   1.5212383
0:   0.86604214  0.97415733]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.5564604  -0.0811739  -0.51868486 -0.7799306  -1.1559658  -1.685389
0:  -2.246252   -2.682108   -3.2446365  -3.6402187  -4.178443   -5.0356092
0:  -6.010664   -6.9175262  -7.6098824  -7.823223   -7.7278633  -7.447131
0:  -8.452299   -8.960699  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.321747 22.150343 22.191639 22.344196 22.454456 22.303606 22.581371
0:  22.650724 22.717987 22.606455 22.02991  21.319164 20.700157 20.502857
0:  20.875473 21.695183 22.761894 23.691174 22.302498 22.16496 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.948706  11.706326  11.705486  11.812322  11.884827  11.770737
0:  12.038894  12.209118  12.48139   12.673791  12.496251  12.2658615
0:  12.055032  12.298523  13.028532  14.08911   15.198952  16.060148
0:  15.55035   15.662358 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.13267  22.470465 23.007301 23.451271 23.683552 23.597786 23.628109
0:  23.550016 23.458342 23.272144 22.866322 22.318153 21.9473   21.826612
0:  22.13078  22.760565 23.491793 24.100574 21.46311  21.734558]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.15921354 -0.24345493 -0.11390734  0.36096478  0.8694463   1.2898602
0:   1.378552    1.4042954   0.9441452   0.5581188   0.17974472 -0.37558174
0:  -0.78863144 -1.1389713  -1.2259274  -0.89051914 -0.3178048   0.37382793
0:  -0.58456993 -0.7441201 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4529638  -1.3676353  -0.9323497  -0.27725935  0.20499468  0.3442936
0:   0.3281455   0.08230639 -0.47319412 -1.0301585  -1.7449026  -2.6600275
0:  -3.36757    -3.78228    -3.807602   -3.3873186  -2.856907   -2.454527
0:  -3.936574   -3.8780947 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.17656  28.207146 28.359896 28.453955 28.37939  28.137411 27.87895
0:  27.677816 27.322708 27.025803 26.633764 26.032694 25.556637 25.192205
0:  25.104212 25.252634 25.474997 25.611923 22.892113 22.60244 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.318875  -4.4408717 -4.209948  -3.6124082 -2.988853  -2.5152373
0:  -2.3227787 -2.288993  -2.7189078 -3.1661572 -3.7048068 -4.4108825
0:  -4.887442  -5.2459636 -5.253943  -4.895868  -4.383139  -3.8122
0:  -4.666118  -4.7930837]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.130093  13.684612  13.5625725 13.594074  13.488136  13.053246
0:  12.730688  12.283932  11.824753  11.366946  10.709539   9.894768
0:   9.181002   8.76893    8.7867565  9.269551   9.992448  10.652754
0:   8.669994   8.473    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.933034  10.942782  11.054672  11.158821  11.157415  10.954184
0:  11.169106  11.334757  11.444357  11.47209   11.034851  10.4601
0:   9.762148   9.479908   9.558764   9.945245  10.317186  10.453184
0:   8.6035595  8.591415 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.3800507 -7.4023037 -7.0772066 -6.415118  -5.8200283 -5.4607773
0:  -5.511877  -5.6831236 -6.2999372 -6.8242283 -7.349828  -8.064462
0:  -8.608039  -8.9838085 -8.998111  -8.541981  -7.876011  -7.1239033
0:  -7.4145985 -7.5041647]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3971877 4.3108926 4.4232388 4.673431  4.7798104 4.673078  4.548668
0:  4.3752785 4.068485  3.7758348 3.3342319 2.731238  2.2751875 2.1524692
0:  2.5226283 3.4443383 4.6460304 5.7517385 6.276733  6.712387 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9435358 -3.9120932 -3.5714674 -2.8713102 -2.090232  -1.5158596
0:  -1.0289116 -0.712049  -0.7917309 -0.8641739 -1.1112924 -1.5196762
0:  -1.7682047 -1.9259291 -1.7934046 -1.4122324 -1.0426307 -0.7824969
0:  -1.8740482 -1.908637 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.277204  12.353567  12.631342  13.044964  13.461788  13.891621
0:  14.349188  14.914486  15.3137455 15.743769  16.068476  16.21889
0:  16.433184  16.701273  17.27314   18.019463  18.764267  19.26707
0:  17.058455  17.200388 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.564372  8.407754  8.425753  8.586573  8.683416  8.664941  8.637884
0:  8.583348  8.30772   8.027832  7.5933247 6.9440007 6.3593574 5.901878
0:  5.750287  5.9205513 6.238903  6.5355062 4.611793  4.5798345]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.649454 -15.509346 -14.927237 -13.883131 -12.867966 -12.198221
0:  -11.880402 -11.946592 -12.560654 -13.164316 -13.886837 -14.61698
0:  -15.051249 -15.214216 -15.080214 -14.663485 -14.334396 -14.120035
0:  -15.321501 -15.098743]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4106202  -2.1800103  -1.5653877  -0.73013544 -0.05266333  0.27686834
0:   0.22831011 -0.07791471 -0.88285875 -1.750185   -2.8372865  -4.231626
0:  -5.5348086  -6.7031856  -7.4555726  -7.637326   -7.418633   -7.008119
0:  -7.115335   -7.162561  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.340542  11.582129  12.119978  12.669699  12.995661  13.055069
0:  13.038484  12.997735  12.812437  12.594123  12.185413  11.505814
0:  10.856031  10.345706  10.197409  10.44558   10.918715  11.378895
0:   8.5716505  8.715476 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.8591247  7.239063   7.8439097  8.538245   9.127478   9.51103
0:   9.968616  10.302967  10.3604145 10.384385  10.057716   9.559731
0:   9.135397   8.947072   9.081846   9.430899   9.710598   9.755923
0:   8.163203   8.350264 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.504885 23.838137 24.261923 24.645496 24.895346 24.921375 25.311804
0:  25.640759 26.010838 26.220013 26.025167 25.690296 25.434929 25.380322
0:  25.78085  26.511194 27.397673 28.268665 25.37473  25.49197 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.883789 13.920129 14.335098 14.846342 15.184282 15.150633 15.233688
0:  15.164066 15.168053 15.234621 15.117214 14.876387 14.7348   14.856534
0:  15.406488 16.362757 17.456238 18.394958 16.38411  16.336374]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.44975  33.076797 32.445965 31.861727 31.380468 30.630318 30.739088
0:  30.78774  31.049452 31.141356 30.512249 30.089764 29.52019  29.494432
0:  29.982439 30.644817 31.280897 31.378115 32.25796  32.203884]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1451325  -4.010818   -3.5012689  -2.597217   -1.6641846  -0.88843155
0:  -0.3129816   0.12355328  0.25505638  0.3924079   0.44430208  0.30789804
0:   0.25348234  0.31085062  0.6073904   1.250668    2.0549436   2.797312
0:   1.6310015   1.7024117 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.751624    4.4001226   4.4077883   4.648266    4.8053427   4.7097244
0:   4.4955997   4.083723    3.3709333   2.7099438   1.8662424   0.8837738
0:   0.07046127 -0.44083214 -0.5738354  -0.32508135 -0.00575161  0.18778515
0:  -0.37362957 -0.6112561 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.880842 17.265982 16.70542  16.125498 15.485504 14.605394 14.319077
0:  13.954529 13.696362 13.356154 12.491762 11.528606 10.449051  9.888589
0:   9.812498 10.180305 10.681023 10.957174  8.813555  8.319944]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.120224   6.363017   6.89924    7.5552387  8.022098   8.229784
0:   8.489783   8.71757    8.834647   8.950896   8.866657   8.699476
0:   8.637418   8.923468   9.632046  10.690924  11.815419  12.682707
0:  12.64173   12.962215 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.66719   15.8276825 16.31668   16.86393   17.285059  17.561146
0:  17.996689  18.370285  18.674915  19.008942  19.161476  19.18734
0:  19.418499  19.914555  20.816372  22.013123  23.19312   24.180004
0:  22.17786   22.46186  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6429005 2.465724  2.4956994 2.6600933 2.7751436 2.7441096 3.0068974
0:  3.2132006 3.4205332 3.6025238 3.520402  3.3304005 3.2259912 3.5819924
0:  4.453356  5.677691  6.9879107 7.9949474 8.329533  8.978214 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.096169  11.506905  12.193279  12.904734  13.416264  13.7185955
0:  14.176712  14.639432  15.10264   15.561748  15.760074  15.766019
0:  15.834552  16.147924  16.825287  17.742085  18.700653  19.426662
0:  17.19005   17.699854 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.01481056 -0.05958366  0.13547945  0.59624004  1.0375557   1.2536197
0:   1.3823786   1.3483129   0.9905815   0.6697078   0.25241804 -0.3048749
0:  -0.70925045 -0.898211   -0.841516   -0.57087564 -0.3841424  -0.3899336
0:  -2.125339   -2.140678  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.644285  12.018097  12.587374  13.130871  13.493322  13.60767
0:  13.854183  13.968473  14.052965  14.151915  14.020691  13.783415
0:  13.611115  13.7361965 14.206806  14.998014  15.850819  16.49771
0:  15.060043  15.458996 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.198715  11.237308  11.505671  11.838551  12.04034   12.022249
0:  12.341894  12.558529  12.787822  12.947527  12.669441  12.221491
0:  11.77389   11.769253  12.222425  13.1158085 14.074423  14.753327
0:  13.139696  13.151665 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.866597  -12.977233  -12.6733465 -11.876834  -10.982003  -10.234315
0:  -10.004745   -9.966362  -10.5193205 -11.054154  -11.608014  -12.346744
0:  -12.885566  -13.344161  -13.510346  -13.198995  -12.547078  -11.705817
0:  -12.660426  -12.95762  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.28864574 0.4838519  0.8649106  1.4057555  1.9021268  2.2205567
0:  2.3069894  2.3231564  2.0011754  1.7515616  1.463326   1.0495081
0:  0.76626396 0.5724673  0.65925837 1.0636077  1.6603141  2.2775164
0:  0.88278675 1.1115546 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.946552  -10.193302  -10.058447   -9.485731   -8.826241   -8.426231
0:   -8.365253   -8.49461    -9.073292   -9.620035  -10.221024  -10.976244
0:  -11.723118  -12.365568  -12.6559105 -12.568361  -12.083405  -11.495451
0:  -12.057735  -12.354553 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.06916   -11.118786  -10.711065   -9.836288   -8.812742   -7.947204
0:   -7.484836   -7.183146   -7.403998   -7.6081758  -7.7967143  -8.176527
0:   -8.35796    -8.492834   -8.352722   -7.8791337  -7.179898   -6.4222913
0:   -6.5712714  -6.504639 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8138318  -2.8487153  -2.4820762  -1.8052459  -1.20404    -0.8580284
0:  -0.6922159  -0.67505074 -1.017384   -1.326839   -1.7770047  -2.4838748
0:  -3.0547366  -3.4806838  -3.5985055  -3.2801347  -2.7711387  -2.2706294
0:  -4.218023   -4.311443  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.52236  32.352398 31.94669  31.325764 30.71656  29.955944 30.168629
0:  30.325634 30.654392 30.744843 30.064957 29.500355 28.906868 29.012108
0:  29.7196   30.721645 31.634167 31.939949 31.908466 31.96038 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.302029 21.45635  21.828905 22.325098 22.780884 23.036991 23.662401
0:  24.186646 24.700342 25.08205  25.019844 24.768705 24.467445 24.465101
0:  24.873539 25.63329  26.560137 27.251005 25.123772 25.119453]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.351601   -8.69536    -8.574707   -7.976228   -7.2067027  -6.664167
0:   -6.5220103  -6.6027694  -7.2318234  -7.8106256  -8.446453   -9.229469
0:   -9.841352  -10.343676  -10.536264  -10.356283   -9.962618   -9.584469
0:   -9.724773  -10.336264 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0990648  -2.9900293  -2.5607023  -1.9039335  -1.3049307  -0.90783596
0:  -0.5968785  -0.3599739  -0.37198544 -0.4070282  -0.57890224 -1.0203896
0:  -1.3441114  -1.5220585  -1.4143858  -0.87140846 -0.13624048  0.67056894
0:  -0.42732048 -0.23864079]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8262725  -0.6993761  -0.28505802  0.47456217  1.271586    2.0144343
0:   2.6266327   3.161828    3.3475084   3.5251346   3.5612352   3.4028854
0:   3.30835     3.3147397   3.5705142   4.118436    4.7888293   5.4504166
0:   4.209603    4.3511424 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.193963  2.2051392 2.499979  3.022246  3.4197748 3.5637376 3.589621
0:  3.5479832 3.2441275 2.9860497 2.5798507 1.9688177 1.4484386 1.0868821
0:  1.0285249 1.3817272 1.8817549 2.3384147 1.0737805 1.0923009]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.277005  -9.501451  -9.361592  -8.753951  -7.890815  -7.039338
0:  -6.4080687 -6.022316  -6.266057  -6.59991   -7.1164093 -7.902874
0:  -8.443453  -8.976926  -9.189158  -9.018912  -8.530411  -7.8946047
0:  -9.109218  -9.423297 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.079375  -13.571958  -13.629548  -13.203282  -12.622968  -12.137879
0:  -11.727796  -11.5793085 -11.881513  -12.125354  -12.448115  -12.831723
0:  -12.713038  -12.256146  -11.339602  -10.057857   -8.759609   -7.5931225
0:   -5.3476686  -4.2430277]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.502037  13.537823  13.6916    13.777978  13.789545  13.596441
0:  13.919594  14.2117405 14.602993  14.945563  14.812122  14.596858
0:  14.275906  14.4362545 14.954542  15.824445  16.651848  17.126446
0:  16.273277  16.460352 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9660306  -2.16886    -1.9496169  -1.3373418  -0.5794444   0.10567522
0:   0.7811985   1.3653946   1.602478    1.7695088   1.8186665   1.63766
0:   1.6164446   1.6908407   1.9063044   2.2589679   2.4666228   2.4563432
0:  -1.2514338  -1.5845618 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.31240797 -0.24339437  0.12702036  0.86217356  1.6047082   2.2053216
0:   2.4566946   2.6722667   2.4399931   2.2652829   2.0584335   1.5638084
0:   1.1403613   0.6595807   0.39854813  0.5843959   1.1410127   1.8291955
0:   0.13286448 -0.02865362]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.156206 16.224903 16.248014 16.170166 16.10369  16.010834 16.728102
0:  17.515156 18.463642 19.219137 19.30302  19.34709  19.229681 19.6958
0:  20.607615 21.764704 22.838097 23.455038 24.570026 24.700424]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9923835 -2.2825704 -2.2360215 -1.898128  -1.5529237 -1.3217473
0:  -1.14714   -1.1317267 -1.4555411 -1.7831893 -2.2733579 -2.9170575
0:  -3.3417096 -3.475243  -3.2488828 -2.6524644 -2.0230823 -1.5510106
0:  -2.9372783 -2.684401 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.829426 22.882984 22.731445 22.35738  21.916056 21.371956 21.800327
0:  22.259056 22.933289 23.331177 22.886955 22.398914 21.706985 21.667707
0:  22.15342  22.903767 23.55994  23.677448 24.193066 24.23993 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.200163  10.436188  11.016004  11.770116  12.371799  12.736172
0:  13.182972  13.500107  13.673603  13.809164  13.645607  13.226858
0:  12.799614  12.497068  12.488818  12.808712  13.2161045 13.499249
0:  10.719376  10.644758 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.325581  -8.190094  -7.648672  -6.7458286 -5.859112  -5.3263645
0:  -5.115238  -5.215763  -5.8804665 -6.5223284 -7.3127375 -8.276544
0:  -8.973223  -9.47734   -9.573706  -9.254445  -8.887191  -8.556995
0:  -9.63366   -9.620889 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7761989  -2.7045512  -2.2516065  -1.4812722  -0.66830873 -0.04154825
0:   0.37061167  0.5965953   0.44726992  0.31794834  0.10900545 -0.26684952
0:  -0.42725372 -0.5045066  -0.31056738  0.11770725  0.5838418   0.92945004
0:  -0.6243081  -0.5569196 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.843833  11.660665  11.600828  11.568746  11.384453  11.051424
0:  10.778527  10.571983  10.339044  10.143413   9.810993   9.17654
0:   8.471217   7.810202   7.3946905  7.344649   7.580446   7.897776
0:   5.9693575  6.0744643]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -2.9333768  -3.8517642  -4.475492   -4.6396585  -4.587506   -4.552912
0:   -4.399155   -4.4324756  -4.862722   -5.356111   -6.176393   -7.2499166
0:   -8.184637   -8.976818   -9.499496   -9.649393   -9.512058   -9.135979
0:  -11.239933  -11.582935 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.615476  11.8011875 12.299608  12.864241  13.195847  13.209946
0:  13.195269  13.157532  13.08006   13.059763  12.916965  12.57421
0:  12.301182  12.161636  12.439739  13.150195  14.162204  15.171871
0:  12.262419  12.453474 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.153276  14.068468  14.171766  14.355574  14.525288  14.536203
0:  14.963808  15.273184  15.589075  15.7990055 15.58676   15.310923
0:  15.093216  15.321337  16.037247  17.1241    18.335667  19.283854
0:  18.648691  18.782236 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7555075 3.1455965 3.5995977 4.0481296 4.417255  4.6157866 5.134878
0:  5.5145836 5.679643  5.640238  5.182016  4.583604  4.012727  3.87757
0:  4.0928116 4.5308    4.8739605 5.0219665 3.5380483 3.4286404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.523682  -9.088976  -9.331194  -9.264204  -9.155996  -9.15645
0:   -8.861279  -8.681232  -8.580961  -8.601543  -9.029196  -9.65852
0:  -10.325512 -10.54476  -10.396507  -9.943722  -9.566195  -9.482077
0:  -10.833866 -11.266222]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.134607 12.379725 12.676607 12.884613 12.94618  12.752382 12.988775
0:  13.180845 13.37048  13.442633 13.037098 12.604322 12.084526 12.031111
0:  12.310307 12.852009 13.364183 13.544014 12.058232 12.215504]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.374727  14.424381  14.727968  15.082461  15.2375965 15.091789
0:  14.938953  14.673493  14.313378  13.903646  13.278755  12.461292
0:  11.690895  11.144501  10.9739895 11.159703  11.515919  11.727095
0:   9.373813   9.31068  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9728103 -4.1615577 -4.0394835 -3.5216832 -2.962891  -2.5258985
0:  -2.4894938 -2.5819755 -3.1813025 -3.7185035 -4.22533   -4.8830314
0:  -5.2788377 -5.545977  -5.4631276 -4.868463  -3.9292855 -2.8507466
0:  -3.9409995 -4.4083953]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.7510366 5.0182705 5.5820055 6.2305155 6.7685194 7.041457  7.2854753
0:  7.4194574 7.282825  7.1776905 6.8482904 6.2881846 5.8576913 5.5180206
0:  5.4831457 5.7305965 6.0957613 6.366588  4.57296   4.8491993]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.3451948   2.2752633   2.5308313   3.0204415   3.4319944   3.6388023
0:   3.7389996   3.7237043   3.4009366   3.0345953   2.514628    1.7696075
0:   1.1736774   0.76343584  0.68774605  0.97317743  1.380805    1.7314601
0:  -0.26043224 -0.29178572]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.355463 30.210749 30.107515 30.06045  29.947937 29.459845 29.431116
0:  29.073872 28.742336 28.227913 27.2504   26.396425 25.765991 25.79257
0:  26.46638  27.548302 28.672094 29.471737 29.017338 29.072243]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.01388  21.82708  21.751402 21.674173 21.637472 21.318703 21.51658
0:  21.50467  21.642597 21.575085 21.082102 20.598803 20.136217 20.051773
0:  20.414066 21.143383 22.046164 22.809767 21.93618  21.957975]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.4662485 7.5505943 7.9872723 8.681632  9.305759  9.688667  9.819565
0:  9.859045  9.476108  9.130851  8.62994   7.8105392 7.0615253 6.2846994
0:  5.814694  5.7711334 5.9983068 6.236752  2.8259518 2.5622056]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.849348   -3.8228264  -3.3933372  -2.5259342  -1.5928965  -0.7746682
0:  -0.30026197  0.11124706  0.09109545  0.15470171  0.23021269  0.06023312
0:  -0.01622629 -0.17041254 -0.1836729   0.10459805  0.63810253  1.283133
0:   0.14998388  0.10108137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6518593  -3.5325851  -3.2286959  -2.8484344  -2.54533    -2.4067698
0:  -1.9527254  -1.6130304  -1.2709756  -1.0310979  -1.1652045  -1.3929024
0:  -1.6017618  -1.2126026  -0.36509228  0.8045254   1.8231983   2.4719162
0:   4.8493342   5.266     ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.884905  7.1160116 7.598282  8.062247  8.195733  7.944534  7.642556
0:  7.197071  6.568926  6.027605  5.309738  4.4031563 3.6243513 3.1910012
0:  3.1793513 3.632094  4.294828  4.8918433 2.6576002 2.5956566]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.576624  13.549028  13.616968  13.74501   13.670771  13.321665
0:  12.660311  11.960444  10.969074  10.135724   9.360699   8.397747
0:   7.524792   6.65       6.017083   5.8163004  5.9323034  6.209887
0:   4.017173   3.6252608]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.6742816 -7.534776  -6.9991097 -6.1026254 -5.2098756 -4.584536
0:  -4.3899016 -4.4513173 -5.032964  -5.561397  -6.1164603 -6.7339168
0:  -7.0875125 -7.2842307 -7.216371  -6.8111053 -6.2844234 -5.770206
0:  -6.267966  -6.1556945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.228061   5.350157   5.7762036  6.292428   6.8235435  7.243933
0:   7.808463   8.354076   8.819915   9.272591   9.561616   9.63623
0:   9.904104  10.349655  11.050684  11.966304  12.91156   13.676269
0:  12.141943  12.418688 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.47577  18.732462 19.347406 20.100958 20.712254 20.947193 21.051792
0:  20.970425 20.679525 20.437986 20.048424 19.619696 19.339027 19.243307
0:  19.5488   20.05741  20.563883 20.934048 17.819687 17.681896]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.4543686 -5.8517466 -5.861903  -5.4216843 -4.8398523 -4.329995
0:  -3.8776846 -3.578888  -3.6063862 -3.6754885 -3.8991346 -4.2068634
0:  -4.275916  -4.128724  -3.658904  -2.9228182 -2.213212  -1.6561937
0:  -3.3237362 -3.639    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.868629  15.811163  15.88439   15.936188  15.961813  15.8129635
0:  16.001852  16.05183   16.100079  16.005215  15.499905  14.899481
0:  14.395412  14.320129  14.750244  15.604612  16.661493  17.656376
0:  16.404947  16.672    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.970519   6.9451747  7.126444   7.512144   7.876888   8.076279
0:   8.653051   9.109599   9.556398   9.833639   9.648914   9.346313
0:   8.957134   9.036108   9.510061  10.28757   11.054886  11.4446945
0:  10.976622  11.169752 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.587866   7.771611   8.27826    8.957473   9.53812    9.871254
0:  10.2830515 10.506606  10.624073  10.657044  10.416844  10.040085
0:   9.709827   9.665004   9.964043  10.540742  11.159466  11.590918
0:   9.262277   9.197608 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [52.116013 52.510082 52.862373 53.16643  53.32821  53.278793 53.740788
0:  54.032215 54.381104 54.513195 54.0981   53.646187 53.365746 53.42033
0:  53.97274  54.599075 55.16958  55.301006 50.89222  51.271698]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.682284  15.03034   15.525114  15.942238  16.017822  15.807724
0:  15.492716  15.289118  15.053223  14.982109  14.952229  14.763191
0:  14.724392  14.852894  15.4088955 16.446009  17.791513  19.140533
0:  17.285082  17.895458 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.2988396  5.414289   5.8920054  6.595542   7.330931   7.847453
0:   8.305059   8.6105     8.618264   8.685006   8.618515   8.423886
0:   8.39656    8.4341345  8.699717   9.165591   9.699707  10.172472
0:   8.8274765  9.115893 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.583879  5.2366095 5.078837  5.1308765 5.111541  4.9751396 4.935899
0:  4.9162993 4.9171095 5.018238  5.037097  4.8353877 4.6395044 4.623494
0:  4.907447  5.578007  6.443882  7.2191873 6.314296  6.4635563]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.37671  22.28567  22.20668  22.095572 21.920391 21.469833 21.52383
0:  21.406704 21.319008 21.11455  20.44021  19.77365  19.097897 18.884802
0:  19.099464 19.678045 20.268461 20.584679 19.195623 19.154856]
0: validation loss for strategy=forecast at epoch 32 : 0.3017832338809967
0: validation loss for velocity_u : 0.16083328425884247
0: validation loss for velocity_v : 0.3036786615848541
0: validation loss for specific_humidity : 0.14392443001270294
0: validation loss for velocity_z : 0.5504385828971863
0: validation loss for temperature : 0.10270386189222336
0: validation loss for total_precip : 0.5491207242012024
0: 33 : 20:09:42 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 33, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7247, -0.7355, -0.7461, -0.7562, -0.7658, -0.7748, -0.7827, -0.7899, -0.7962, -0.8016, -0.8059, -0.8096,
0:         -0.8127, -0.8152, -0.8172, -0.8191, -0.8208, -0.8226, -0.7468, -0.7564, -0.7657, -0.7744, -0.7825, -0.7898,
0:         -0.7963], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.3441, -1.3657, -1.3851, -1.4018, -1.4161, -1.4280, -1.4375, -1.4450, -1.4506, -1.4544, -1.4573, -1.4591,
0:         -1.4605, -1.4619, -1.4637, -1.4661, -1.4696, -1.4744, -1.3562, -1.3758, -1.3931, -1.4081, -1.4206, -1.4311,
0:         -1.4397], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4881, -0.4911, -0.4941, -0.4971, -0.4991, -0.4879, -0.4751, -0.4625, -0.4497, -0.4370, -0.4147, -0.3853,
0:         -0.3557, -0.3260, -0.2965, -0.2682, -0.2416, -0.2153, -0.5006, -0.5021, -0.5036, -0.4926, -0.4815, -0.4705,
0:         -0.4596], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3267, 0.3267, 0.3246, 0.3246, 0.3267, 0.3351, 0.3457, 0.3625, 0.3772, 0.3898, 0.3962, 0.3941, 0.3856, 0.3730,
0:         0.3562, 0.3372, 0.3225, 0.3099, 0.3962, 0.3983, 0.3983, 0.4004, 0.4067, 0.4151, 0.4256], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2597, -0.2731, -0.2874, -0.3026, -0.3189, -0.3359, -0.3540, -0.3726, -0.3919, -0.4119, -0.4321, -0.4527,
0:         -0.4731, -0.4939, -0.5145, -0.5350, -0.5556, -0.5756, -0.5955, -0.6153, -0.6345, -0.6535, -0.6718, -0.6900,
0:         -0.7074], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1677, -0.1742, -0.1786, -0.1829, -0.1873, -0.1894, -0.1894, -0.1894, -0.1894, -0.1960, -0.1894, -0.1829,
0:         -0.1829, -0.1829, -0.1851, -0.1851, -0.1851, -0.1894, -0.2199, -0.2177, -0.2155, -0.2155, -0.2155, -0.2134,
0:         -0.2134], device='cuda:0')
0: [DEBUG] Epoch 33, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
0:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2416, -0.2438,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,
0:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2416,     nan,     nan, -0.2416, -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2416, -0.2416,     nan,     nan,     nan,
0:         -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2438,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2438,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438, -0.2438,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2427,     nan,     nan,     nan,     nan, -0.2427,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2427,     nan,     nan,     nan, -0.2427,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2416, -0.2438,     nan, -0.2438,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
0:         -0.2427,     nan,     nan])
0: [DEBUG] Epoch 33, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5722, -0.5651, -0.5416, -0.4999, -0.4645, -0.4440, -0.4319, -0.4198, -0.4410, -0.4519, -0.4772, -0.5339,
0:         -0.5845, -0.6397, -0.6755, -0.6721, -0.6485, -0.6094, -0.5056, -0.5395, -0.5477, -0.5368, -0.5196, -0.4958,
0:         -0.4743], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3959, -0.3810, -0.4050, -0.4646, -0.5574, -0.6620, -0.7899, -0.8737, -0.8848, -0.8548, -0.8012, -0.7609,
0:         -0.7799, -0.8611, -0.9602, -1.0016, -0.9631, -0.8769, -0.5161, -0.4830, -0.4738, -0.5092, -0.5887, -0.7001,
0:         -0.8006], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3367, -0.3433, -0.3565, -0.3659, -0.3748, -0.3806, -0.3948, -0.4078, -0.4225, -0.4355, -0.4499, -0.4621,
0:         -0.4740, -0.4857, -0.5008, -0.5105, -0.5217, -0.5175, -0.3594, -0.3662, -0.3772, -0.3813, -0.3899, -0.4018,
0:         -0.4090], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0041,  0.0926,  0.1178,  0.0189, -0.0934, -0.0297, -0.0189, -0.0661,  0.0046, -0.0015, -0.0631, -0.0113,
0:          0.0946,  0.0822,  0.0037,  0.1079,  0.1898,  0.0811,  0.0976,  0.0345, -0.0637, -0.1440, -0.1894, -0.0811,
0:         -0.0318], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.0022, -1.0707, -1.0788, -1.0255, -0.9573, -0.9219, -0.9405, -0.9875, -1.0248, -1.0290, -1.0116, -0.9770,
0:         -0.9550, -0.9360, -0.9246, -0.9186, -0.9397, -0.9751, -1.0130, -1.0404, -1.0545, -1.0681, -1.0838, -1.1006,
0:         -1.0941], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2327, -0.2335, -0.2449, -0.2422, -0.2441, -0.2412, -0.2452, -0.2510, -0.2436, -0.2343, -0.2319, -0.2309,
0:         -0.2355, -0.2443, -0.2392, -0.2405, -0.2456, -0.2511, -0.2284, -0.2274, -0.2348, -0.2335, -0.2281, -0.2389,
0:         -0.2377], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2163155972957611; velocity_v: 0.3176615834236145; specific_humidity: 0.18363125622272491; velocity_z: 0.6313720345497131; temperature: 0.1395769864320755; total_precip: 0.6654055118560791; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19827193021774292; velocity_v: 0.2822257876396179; specific_humidity: 0.20335283875465393; velocity_z: 0.7017016410827637; temperature: 0.15016278624534607; total_precip: 0.8041486740112305; 
0: epoch: 33 [1/5 (20%)]	Loss: 0.73478 : 0.34035 :: 0.20902 (2.79 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18771930038928986; velocity_v: 0.3004333972930908; specific_humidity: 0.16533316671848297; velocity_z: 0.6264833211898804; temperature: 0.15029989182949066; total_precip: 0.5976518988609314; 
0: epoch: 33 [2/5 (40%)]	Loss: 0.59765 : 0.30442 :: 0.20840 (15.84 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21415570378303528; velocity_v: 0.3190775513648987; specific_humidity: 0.18975284695625305; velocity_z: 0.8045070767402649; temperature: 0.15777714550495148; total_precip: 0.8185339570045471; 
0: epoch: 33 [3/5 (60%)]	Loss: 0.81853 : 0.38224 :: 0.21178 (15.87 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18696711957454681; velocity_v: 0.30108141899108887; specific_humidity: 0.19606348872184753; velocity_z: 0.49465620517730713; temperature: 0.14670123159885406; total_precip: 0.6684377789497375; 
0: epoch: 33 [4/5 (80%)]	Loss: 0.66844 : 0.29854 :: 0.20395 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [2.95639038e-05 3.57627869e-05 4.19616699e-05 4.38690222e-05
0:  4.52995300e-05 4.67300415e-05 6.91413879e-05 9.15527344e-05
0:  1.12056732e-04 1.05381012e-04 9.77516174e-05 8.96453857e-05
0:  8.91685486e-05 8.82148743e-05 8.82148743e-05 7.67707825e-05
0:  6.58035278e-05 5.48362732e-05 4.81605530e-05 4.14848364e-05
0:  3.43322754e-05 2.67028809e-05 1.95503235e-05 1.23977661e-05
0:  9.53674316e-06 7.15255737e-06 4.76837158e-06 5.24520874e-06
0:  5.24520874e-06 5.72204590e-06 8.58306885e-06 1.14440918e-05
0:  1.43051147e-05 1.52587891e-05 1.62124634e-05 1.71661377e-05
0:  1.43051147e-05 1.14440918e-05 8.58306885e-06 9.05990601e-06
0:  9.53674316e-06 1.04904175e-05 8.58306885e-06 6.67572021e-06
0:  4.76837158e-06 3.33786011e-06 2.38418579e-06 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 1.90734863e-06
0:  2.86102295e-06 7.15255737e-06 1.09672546e-05 1.52587891e-05
0:  2.09808350e-05 2.67028809e-05 3.24249268e-05 3.57627869e-05
0:  3.95774841e-05 4.29153442e-05 4.24385071e-05 4.24385071e-05
0:  4.24385071e-05 4.91142273e-05 5.53131104e-05 6.15119934e-05
0:  7.20024109e-05 8.24928284e-05 9.34600830e-05 9.96589661e-05
0:  1.05857849e-04 1.12533569e-04 1.16825104e-04 1.21593475e-04
0:  1.26361847e-04 1.32083893e-04 1.37805939e-04 1.43527985e-04
0:  1.50680542e-04 1.57356262e-04 1.64031982e-04 1.51157379e-04
0:  1.38282776e-04 1.25408173e-04 1.02519989e-04 7.91549683e-05
0:  5.57899475e-05 4.43458557e-05 3.29017639e-05 2.14576721e-05
0:  1.57356262e-05 1.00135803e-05 4.76837158e-06 5.24520874e-06
0:  5.24520874e-06 5.72204590e-06 5.24520874e-06 4.29153442e-06
0:  3.81469727e-06 3.33786011e-06 2.86102295e-06 1.90734863e-06
0:  3.81469690e-05 4.38690222e-05 4.95910645e-05 5.05447388e-05
0:  5.14984131e-05 5.29289246e-05 5.76972961e-05 5.67436218e-05
0:  5.57899475e-05 6.81877136e-05 8.01086426e-05 8.72612000e-05
0:  9.20295715e-05 9.96589661e-05 1.06811523e-04 9.25064087e-05
0:  7.82012939e-05 6.48498535e-05 5.34057617e-05 4.05311548e-05
0:  2.71797180e-05 2.09808350e-05 1.47819519e-05 8.10623169e-06
0:  6.67572021e-06 6.19888306e-06 5.72204590e-06 5.24520874e-06
0:  4.76837158e-06 4.76837158e-06 7.62939453e-06 1.00135803e-05
0:  1.23977661e-05 1.43051147e-05 1.62124634e-05 1.81198120e-05
0:  1.47819519e-05 1.14440918e-05 7.62939453e-06 6.19888306e-06
0:  4.29153442e-06 3.33786011e-06 3.33786011e-06 3.81469727e-06
0:  3.81469727e-06 2.86102295e-06 1.90734863e-06 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 1.90734863e-06
0:  2.86102295e-06 8.10623169e-06 1.28746033e-05 1.76429749e-05
0:  2.33650208e-05 2.86102295e-05 3.43322754e-05 3.67164612e-05
0:  3.91006470e-05 4.19616699e-05 4.48226929e-05 4.86373901e-05
0:  5.24520874e-05 6.43730164e-05 7.58171082e-05 8.63075256e-05
0:  9.77516174e-05 1.09672546e-04 1.21116638e-04 1.26838684e-04
0:  1.32083893e-04 1.37805939e-04 1.41143799e-04 1.44004822e-04
0:  1.46865845e-04 1.51634216e-04 1.56402588e-04 1.61170959e-04
0:  1.63555145e-04 1.63555145e-04 1.64031982e-04 1.46389008e-04
0:  1.29222870e-04 1.13010406e-04 8.86917114e-05 6.38961792e-05]
0: Target values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 2.86102295e-06
0:  2.86102295e-06 3.81469727e-06 4.76837158e-06 4.76837158e-06
0:  5.72204590e-06 5.72204590e-06 5.72204590e-06 5.72204590e-06
0:  7.62939453e-06 8.58306885e-06 1.04904175e-05 1.23977661e-05
0:  1.43051147e-05 1.62124634e-05 1.90734863e-05 2.19345093e-05
0:  2.38418579e-05 2.76565552e-05 3.05175781e-05 3.43322754e-05
0:  3.43322754e-05 3.33786011e-05 3.33786011e-05 3.14712524e-05
0:  2.95639038e-05 2.76565552e-05 2.67028809e-05 2.57492065e-05
0:  2.47955322e-05 2.67028809e-05 2.76565552e-05 2.95639038e-05
0:  2.86102295e-05 2.67028809e-05 2.57492065e-05 2.57492065e-05
0:  2.57492065e-05 2.57492065e-05 2.47955322e-05 2.28881836e-05
0:  2.19345093e-05 2.00271606e-05 1.90734863e-05 1.71661377e-05
0:  1.52587891e-05 1.33514404e-05 1.14440918e-05 1.04904175e-05
0:  1.04904175e-05 9.53674316e-06 8.58306885e-06 8.58306885e-06
0:  7.62939453e-06 6.67572021e-06 5.72204590e-06 4.76837158e-06
0:  9.53674316e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 2.86102295e-06 2.86102295e-06
0:  2.86102295e-06 2.86102295e-06 2.86102295e-06 3.81469727e-06
0:  3.81469727e-06 4.76837158e-06 5.72204590e-06 5.72204590e-06
0:  6.67572021e-06 8.58306885e-06 9.53674316e-06 1.04904175e-05
0:  1.23977661e-05 1.43051147e-05 1.62124634e-05 1.90734863e-05
0:  2.19345093e-05 2.38418579e-05 2.67028809e-05 2.95639038e-05
0:  3.14712524e-05 3.33786011e-05 3.52859497e-05 3.81469690e-05
0:  3.71932983e-05 3.62396240e-05 3.52859497e-05 3.33786011e-05
0:  3.14712524e-05 3.05175781e-05 2.95639038e-05 2.95639038e-05
0:  2.95639038e-05 2.86102295e-05 2.76565552e-05 2.76565552e-05
0:  2.76565552e-05 2.76565552e-05 2.76565552e-05 2.67028809e-05
0:  2.57492065e-05 2.57492065e-05 2.57492065e-05 2.57492065e-05]
0: Prediction values (first 20):
0: [18.285156 18.298223 18.597866 18.867062 18.920523 18.641382 18.50422
0:  18.306862 18.127272 17.936441 17.534351 17.002735 16.623821 16.497751
0:  16.939558 17.804852 18.890867 19.925932 17.798456 17.815664]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.533, max = 2.877, mean = -0.107
0:          sample (first 20): tensor([0.9232, 0.9243, 0.9490, 0.9713, 0.9757, 0.9526, 0.9413, 0.9250, 0.9101, 0.8944, 0.8611, 0.8172, 0.7859, 0.7755,
0:         0.8120, 0.8835, 0.9732, 1.0588, 1.0311, 1.0360])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.5985026 -4.4582076 -3.9195151 -3.1099057 -2.4085994 -1.975739
0:  -1.783567  -1.7191577 -1.9509664 -2.133244  -2.3628182 -2.8150582
0:  -3.1183372 -3.2956042 -3.1850896 -2.6268897 -1.8904672 -1.1811466
0:  -2.3442502 -2.2588992]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.625749 13.784401 14.201702 14.682059 14.986824 15.021124 15.237717
0:  15.346157 15.411898 15.469109 15.28418  14.914177 14.649387 14.617865
0:  14.918333 15.613224 16.442646 17.198313 15.315992 15.527136]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.4984717 -5.4217286 -4.888224  -4.108292  -3.4576201 -3.1081614
0:  -2.928206  -3.0136313 -3.462006  -3.8060794 -4.2594686 -4.833035
0:  -5.1824946 -5.2789483 -5.102632  -4.675801  -4.353705  -4.1639237
0:  -5.996486  -5.956549 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.066418 24.917505 24.614801 24.148108 23.697908 23.088488 23.265915
0:  23.340607 23.52562  23.529413 22.920261 22.365952 21.762545 21.71133
0:  22.110174 22.839857 23.571377 23.941545 23.645681 23.644737]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.7661214 -4.1799445 -4.2463946 -3.9145484 -3.4973903 -3.1564994
0:  -3.2049708 -3.262752  -3.7464833 -4.0905776 -4.415871  -4.9958816
0:  -5.458714  -5.9435334 -6.164334  -5.802072  -4.958137  -3.876248
0:  -4.8983407 -5.3115363]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.69422007 0.7681184  1.1529984  1.7998209  2.4093838  2.8268552
0:  2.9720657  2.9191859  2.4915009  2.1105943  1.7143965  1.2236414
0:  0.909101   0.7022214  0.81862164 1.2384272  1.7857375  2.2956648
0:  0.7740159  0.88100195]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.26696587 -0.7802782  -1.0243511  -0.9075589  -0.69878435 -0.54454947
0:  -0.61940336 -0.65372324 -1.0304365  -1.3016453  -1.5771027  -2.1198134
0:  -2.6015372  -3.0560117  -3.2838283  -3.0367007  -2.4137855  -1.668663
0:  -3.3044772  -3.7150426 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.39393   -12.671999  -12.436499  -11.636837  -10.673771   -9.891764
0:   -9.435242   -9.376026   -9.804161  -10.216171  -10.7339325 -11.372404
0:  -11.827526  -12.0380745 -11.941704  -11.437651  -10.860247  -10.226223
0:  -10.535566  -10.367674 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.962297 30.615849 30.107666 29.554253 29.02183  28.198874 28.246542
0:  28.102066 28.097513 27.835646 26.793518 25.88454  24.895397 24.554905
0:  24.741646 25.2731   25.73525  25.711592 25.643515 25.501196]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.07537   -6.123062  -5.7620945 -5.035162  -4.2623987 -3.684143
0:  -3.402605  -3.2788324 -3.5685434 -3.9003487 -4.294011  -4.90193
0:  -5.3224907 -5.5911818 -5.517006  -5.02969   -4.372017  -3.6491175
0:  -4.885721  -4.927266 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.992839  12.322056  12.828875  13.316816  13.649202  13.7225895
0:  13.921387  13.990534  14.033347  14.058271  13.843052  13.506432
0:  13.192581  13.119948  13.394635  14.020693  14.77927   15.486864
0:  13.574088  13.896425 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.22012    -7.242846   -6.9369216  -6.355247   -5.677773   -5.1299453
0:  -4.4624305  -3.9678588  -3.614059   -3.3165908  -3.2166905  -3.199985
0:  -3.0866513  -2.6827006  -2.0176673  -1.1198626  -0.2987938   0.34675932
0:  -1.4950056  -1.3963523 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.636782 19.665144 19.907644 20.252254 20.385643 20.15844  20.121843
0:  19.970955 19.771423 19.56424  19.023098 18.320606 17.596165 17.15738
0:  17.121857 17.456814 17.86239  18.04733  14.284984 13.999308]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.810556  13.864555  14.037794  14.216425  14.35634   14.265528
0:  14.59029   14.790695  15.060539  15.270351  15.116484  14.8533325
0:  14.589579  14.648483  15.080442  15.891964  16.851103  17.634695
0:  16.373158  16.46337  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.269513  10.608613  11.084294  11.581053  11.890284  11.989664
0:  11.999088  11.995132  11.76424   11.590071  11.27861   10.825113
0:  10.438448  10.173048  10.278835  10.703722  11.2577915 11.680766
0:   9.772124  10.126468 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.35916  19.102146 19.140024 19.13582  18.8964   18.298796 17.730272
0:  17.118664 16.59565  16.16465  15.662666 15.017027 14.530857 14.2827
0:  14.532154 15.162689 15.995237 16.733961 13.361284 13.198957]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.3822484  7.871142   8.655108   9.587142  10.248316  10.415514
0:  10.057245   9.195895   7.643283   5.8889284  3.9052515  1.8209729
0:   0.12957   -1.0971379 -1.5606208 -1.3280444 -0.6650939  0.0375061
0:  -1.3990102 -1.4043489]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2435398   0.85571337  0.77392817  0.89629793  0.90029716  0.66611004
0:   0.4182787   0.16031694 -0.11540174 -0.18138504 -0.19209862 -0.30308485
0:  -0.342731   -0.31134367 -0.17844391  0.1410265   0.48785686  0.7023244
0:  -1.8382902  -2.139379  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.185698   -2.3624625  -2.1997619  -1.6246209  -0.9273963  -0.33246708
0:  -0.13066053 -0.05248547 -0.4975829  -0.9074979  -1.3231382  -1.9734902
0:  -2.4775844  -2.9328132  -3.0937028  -2.7561154  -1.9848218  -0.99008083
0:  -1.4217453  -1.8158412 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.3119283   1.0915594   1.0643964   1.0393529   0.9006152   0.48347425
0:   0.4559312   0.35897636  0.35978842  0.34737396 -0.07254744 -0.5982132
0:  -1.1439519  -1.1095481  -0.53236294  0.60364866  1.855402    2.9796503
0:   3.444295    3.36846   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.006216    0.23698187 -0.22278929 -0.4313264  -0.60253954 -0.8583932
0:  -0.7183604  -0.6050601  -0.4065857  -0.31051683 -0.56677055 -1.031434
0:  -1.5138922  -1.4619322  -0.9499841   0.03735495  1.1314697   1.9277067
0:   0.7474179   1.0457859 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.980196  11.344344  11.97536   12.622727  13.003971  13.084185
0:  13.0901785 13.15358   13.075951  13.077267  12.945393  12.530783
0:  12.128785  11.775637  11.768717  12.154232  12.780288  13.35651
0:  10.120707  10.30414  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1361642  -2.2095876  -2.0505586  -1.7291942  -1.4336715  -1.3159785
0:  -0.8983207  -0.5615182  -0.2596326  -0.00926828 -0.09414577 -0.30633974
0:  -0.53227806 -0.3180127   0.25324392  1.1337581   1.9422045   2.448739
0:   1.9347854   1.9682775 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.893867  11.730018  11.671437  11.699818  11.504866  11.036063
0:  10.2216     9.396189   8.289009   7.374972   6.50422    5.5157404
0:   4.612788   3.819617   3.408715   3.5802598  4.252623   5.099803
0:   3.9134274  3.5960746]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.392353 22.415277 22.522161 22.682272 22.780777 22.806623 23.209139
0:  23.57219  23.986746 24.272507 24.128204 23.836731 23.559052 23.61788
0:  24.105923 24.802101 25.545864 26.12803  24.176962 24.300016]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.071203 25.209543 25.148964 24.894169 24.59625  24.104647 24.643661
0:  25.070604 25.658928 25.943285 25.424208 24.96318  24.414186 24.64276
0:  25.421164 26.499826 27.423216 27.641243 27.777395 27.882864]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.727721  -11.056424  -10.923362  -10.27501    -9.481145   -8.8552885
0:   -8.562674   -8.516689   -8.986882   -9.434247   -9.931077  -10.533454
0:  -10.918891  -11.178714  -11.058953  -10.581401   -9.852944   -9.038815
0:   -8.918006   -8.877157 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.48360872 0.56052256 1.0254974  1.7514396  2.3453684  2.6728134
0:  2.8019109  2.8877234  2.724842   2.6598358  2.556828   2.1890054
0:  1.9269896  1.7492175  1.8523693  2.404605   3.1577013  3.8564749
0:  1.694902   1.3911614 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.878523  11.012428  11.3834305 11.82712   12.1531315 12.262399
0:  12.392546  12.439737  12.315144  12.202631  11.90925   11.455116
0:  11.090172  10.834354  10.912916  11.279932  11.775417  12.14826
0:  10.19076   10.3719225]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.895351 10.098727 10.569948 11.11055  11.489671 11.649302 11.825388
0:  11.928593 11.908466 11.892801 11.710455 11.344257 11.089973 10.981096
0:  11.229208 11.803247 12.484518 13.024727 11.367273 11.644955]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.941944 29.539585 29.27378  29.121002 28.986965 28.72828  29.11987
0:  29.417881 29.833313 30.0784   29.676731 29.232502 28.789486 28.834553
0:  29.492779 30.583973 31.820415 32.852894 31.243874 31.144444]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.918968 17.955536 17.97857  17.967594 17.9463   17.738909 18.162098
0:  18.448116 18.920805 19.308086 19.33239  19.425722 19.664541 20.46897
0:  21.76824  23.36096  24.8888   25.896338 24.542765 24.747705]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.33846807  0.25004816  0.45688486  0.769824    0.8730173   0.70720005
0:   0.66373014  0.5125146   0.3333497   0.17976904 -0.23387861 -0.8150139
0:  -1.3444257  -1.4827495  -1.1664214  -0.4169383   0.39438677  1.0874815
0:  -0.97443676 -1.085444  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.428627 24.301819 24.284544 24.331152 24.413843 24.26404  24.725039
0:  24.96022  25.222153 25.246683 24.663527 24.049925 23.359835 23.17682
0:  23.494707 24.182463 24.951357 25.403152 24.246628 24.160238]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.998128  8.766599  8.848401  8.972727  8.950421  8.694063  8.600108
0:  8.415501  8.226596  8.057311  7.645747  7.0913744 6.582372  6.4052677
0:  6.5613995 7.0958652 7.6618443 8.066839  6.9884176 6.921643 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2944055 -3.5059018 -3.3941722 -2.9401212 -2.479281  -2.2019248
0:  -2.2825007 -2.4533086 -3.072083  -3.610911  -4.1313243 -4.813353
0:  -5.2841935 -5.651069  -5.7138076 -5.354116  -4.7493663 -4.067803
0:  -5.13209   -5.238296 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.021498  -12.891691  -12.337555  -11.381325  -10.4096985  -9.761253
0:   -9.413347   -9.340761   -9.79237   -10.246233  -10.942999  -11.866739
0:  -12.603369  -13.18722   -13.376318  -13.142946  -12.732414  -12.272978
0:  -13.707167  -13.825167 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.9496374 -5.829591  -5.339843  -4.570014  -3.8357248 -3.305274
0:  -3.0951242 -3.0179434 -3.3121586 -3.5298643 -3.736927  -4.0483074
0:  -4.165494  -4.145179  -3.810958  -3.1669874 -2.401987  -1.7129793
0:  -2.286748  -1.8523674]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.373605 -12.73994  -12.585628 -11.955541 -11.40146  -11.222508
0:  -11.384873 -11.877332 -12.831901 -13.785047 -14.800906 -15.957177
0:  -16.731197 -17.163849 -17.122524 -16.706966 -16.317112 -15.998311
0:  -16.925236 -17.27573 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [42.29649  42.7127   42.90673  42.873566 42.660248 42.07908  42.300236
0:  42.341404 42.41479  42.164066 41.030846 39.8386   38.51124  37.729286
0:  37.571896 37.824768 38.210865 38.306004 39.06693  39.184097]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.601025   8.813902   9.251711   9.824408  10.221895  10.358687
0:  10.281242  10.151822   9.718309   9.403546   9.042942   8.500133
0:   8.059839   7.6555676  7.564116   7.8513618  8.38739    8.940007
0:   6.8299594  6.986317 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.1804657  6.098444   6.230812   6.513982   6.797086   7.0103784
0:   7.706127   8.369645   8.99765    9.532749   9.570841   9.430692
0:   9.171755   9.31048    9.76643   10.472504  11.070905  11.296791
0:  10.158405  10.310559 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.1258144 -5.1487083 -4.77267   -4.0639687 -3.292282  -2.634509
0:  -2.2205687 -1.9341111 -2.1409874 -2.431532  -2.8999157 -3.6901512
0:  -4.3555202 -4.984703  -5.2758656 -5.1240044 -4.6906815 -4.1142406
0:  -5.2709894 -5.406622 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.6431274 3.5189805 3.7214859 4.119235  4.505493  4.705449  4.917967
0:  5.0715494 5.059875  5.066148  4.968892  4.744488  4.679965  4.8268976
0:  5.30134   6.1138587 6.9882917 7.7372875 6.1258645 6.1020164]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.814306   4.6374297  4.706494   4.896279   4.925501   4.687357
0:  4.4691944  4.1853886  3.757926   3.3459287  2.7689977  1.9950585
0:  1.3264914  0.94112444 0.9477978  1.4414091  2.1460419  2.792356
0:  1.2692709  1.3300657 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9517188 -4.0662265 -3.849132  -3.3275285 -2.838718  -2.568315
0:  -2.6063056 -2.7483087 -3.2527041 -3.664765  -4.042232  -4.591518
0:  -4.914332  -5.1495442 -5.1296535 -4.6893525 -4.048021  -3.3443189
0:  -4.3615193 -4.4933743]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.728453  11.080092  11.783169  12.584007  13.175387  13.453342
0:  13.770825  13.987995  14.132817  14.314783  14.2775135 14.049952
0:  13.846283  13.738916  13.854448  14.230825  14.6166315 14.910801
0:  12.594937  12.685608 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.584403  2.639791  2.8909655 3.1852467 3.3209105 3.1681073 3.3927631
0:  3.5606933 3.7719984 3.9057295 3.5296168 3.0266128 2.3737817 2.229074
0:  2.5337343 3.196812  3.876031  4.257254  2.3837633 2.4952803]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3516078  -3.3161826  -2.9031234  -2.1772728  -1.484355   -0.98324966
0:  -0.84271526 -0.77095985 -1.1087761  -1.3516455  -1.5967174  -2.0854
0:  -2.44552    -2.8134046  -2.9154086  -2.5859685  -1.9448557  -1.1934404
0:  -2.55096    -2.6271272 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.2483   26.26781  26.197765 26.049582 25.810938 25.321335 25.465208
0:  25.379995 25.389666 25.140339 24.286325 23.527254 22.875923 22.855026
0:  23.40643  24.29043  25.15711  25.72348  26.449778 26.589687]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.61293    8.843863   9.397922  10.187361  10.791195  11.093112
0:  11.093942  10.971036  10.508354  10.109373   9.609594   8.83694
0:   8.103642   7.375578   6.978712   7.0234294  7.378325   7.801826
0:   5.5889583  5.6161995]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.472783   4.675699   5.2272425  6.0355988  6.7226048  7.167336
0:   7.3834224  7.5283356  7.395489   7.3845205  7.354614   7.1726875
0:   7.095078   7.1197433  7.4499316  8.159361   9.0955     9.947686
0:   9.576881  10.324268 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1050291  -1.3710842  -1.2405119  -0.7198262  -0.19828749  0.14322042
0:   0.09758377 -0.01271677 -0.5278897  -0.9339714  -1.3286934  -2.033965
0:  -2.6508303  -3.2755213  -3.6255636  -3.3236904  -2.5241146  -1.4818869
0:  -3.5080671  -4.05505   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.4545236 5.5477886 5.9375734 6.5183983 6.997541  7.316617  7.644137
0:  7.8109374 7.7752256 7.724015  7.4627953 7.0225887 6.7150455 6.582009
0:  6.816201  7.337858  7.8817387 8.3079    7.2567163 7.2826204]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.8917806 3.3205407 3.9911902 4.779043  5.408909  5.8356295 6.0687485
0:  6.214629  6.1060476 6.072523  5.996996  5.8046474 5.7280116 5.763051
0:  6.0711474 6.6403794 7.300353  7.917822  6.697796  7.1255956]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8557816 -2.164688  -2.149705  -1.8371997 -1.5496202 -1.4608274
0:  -1.7292538 -2.0993853 -2.9054203 -3.6490602 -4.406086  -5.4018593
0:  -6.18682   -6.906536  -7.2955875 -7.1602974 -6.692891  -6.0333843
0:  -6.761222  -7.027622 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6827555 -3.724863  -3.4220686 -2.8707857 -2.4030848 -2.1599073
0:  -2.0050955 -2.013609  -2.3015757 -2.5494184 -2.9072046 -3.4141731
0:  -3.7236938 -3.8157253 -3.6148844 -3.0681715 -2.4380407 -1.8733053
0:  -2.3892012 -2.110549 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.359663 16.559525 16.88319  17.173424 17.375237 17.378883 17.705887
0:  17.977652 18.265259 18.404346 18.159523 17.800354 17.463966 17.45483
0:  17.882814 18.636242 19.524033 20.263302 18.03056  18.251781]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.914524 12.375297 13.096979 13.860514 14.531041 14.988064 15.527496
0:  15.99222  16.429413 16.828947 17.046705 17.13817  17.398266 17.916842
0:  18.88003  20.159231 21.588848 22.887186 21.502636 22.30679 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3421326  -1.6052217  -1.3414288  -0.46144104  0.45325708  1.1104784
0:   1.2701902   1.1044316   0.44183826 -0.27172327 -1.0773811  -2.2474809
0:  -3.3673358  -4.4480405  -5.183618   -5.1228604  -4.3466096  -3.108973
0:  -2.112084   -1.8660665 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.149877 -11.213519 -10.93715  -10.162856  -9.229616  -8.507825
0:   -8.150607  -7.998558  -8.491622  -8.944404  -9.508439 -10.208964
0:  -10.672372 -11.063645 -11.082995 -10.736493 -10.19992   -9.637927
0:   -9.084223  -9.35354 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.797873 22.799158 22.873062 22.955576 22.917564 22.66514  22.751461
0:  22.724411 22.638912 22.459534 21.918322 21.297306 20.803997 20.78417
0:  21.275    22.170061 23.184355 23.96133  22.080765 22.106888]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.6489487 -6.7942114 -6.5093155 -5.908761  -5.278905  -4.824662
0:  -4.621686  -4.5901947 -4.9755354 -5.3733044 -5.8250313 -6.3826
0:  -6.7348213 -6.941889  -6.868187  -6.472558  -5.9860277 -5.5257683
0:  -6.496458  -6.4221478]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.066389 13.381869 13.820417 14.129095 14.176998 13.928547 14.097504
0:  14.25402  14.495223 14.752799 14.506493 14.173279 13.720553 13.736486
0:  14.126441 14.79571  15.316643 15.51482  14.827635 14.927222]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.621907  5.5860133 5.7672486 6.1245155 6.4793215 6.601271  7.00333
0:  7.238263  7.3985567 7.482345  7.239886  6.9184265 6.647175  6.743594
0:  7.1917105 7.9971375 8.831605  9.393535  8.087499  7.9169173]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.9714308   0.75957537  0.8638358   1.3217864   1.7713752   2.0740519
0:   2.010519    1.9222231   1.4211602   1.0023375   0.58778715 -0.08749866
0:  -0.6883192  -1.2787962  -1.5870752  -1.4073653  -0.824543   -0.07617998
0:  -1.1564021  -1.5191431 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.858812 26.61491  26.441818 26.375326 26.399015 26.190039 26.64094
0:  26.826725 27.11013  27.133934 26.628986 26.194956 25.919842 26.18977
0:  27.046257 28.237795 29.473248 30.33363  29.780146 29.77938 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.762992  14.593772  14.652127  14.911116  15.153278  15.239452
0:  15.512165  15.714533  15.852587  15.884586  15.634144  15.251165
0:  14.91711   14.8461275 15.137228  15.764292  16.586824  17.33838
0:  15.395952  15.44474  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [42.205654 42.41285  42.439957 42.438023 42.369156 41.93904  42.207043
0:  42.21718  42.3897   42.32412  41.70671  41.21412  40.893375 40.983585
0:  41.51988  42.18029  42.80423  43.05516  42.177616 42.318253]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.142735  -10.259666   -9.937023   -9.172756   -8.318703   -7.66713
0:   -7.4539657  -7.5714755  -8.282381   -9.040756   -9.870226  -10.841509
0:  -11.534195  -12.011934  -12.14418   -11.775753  -11.157455  -10.451639
0:  -11.585608  -11.826248 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.369221   7.598014   7.9123807  8.355649   8.7535715  8.891686
0:   9.321081   9.618996   9.830086  10.053963   9.944251   9.767937
0:   9.580803   9.600483   9.884078  10.201464  10.373664  10.26849
0:   9.746889  10.0498   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.432243  13.201424  13.331785  13.679438  13.927418  13.879
0:  14.004799  14.038561  14.105104  14.206146  14.1059885 13.92193
0:  13.778215  13.871183  14.32992   15.116145  15.9723215 16.650787
0:  14.12261   13.985388 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.875825 -12.33985  -12.357732 -11.781074 -11.022519 -10.368121
0:  -10.290799 -10.383868 -11.160296 -11.902999 -12.639007 -13.569066
0:  -14.282596 -14.848437 -15.066337 -14.685983 -13.870837 -12.822213
0:  -12.632306 -13.230815]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.686604 33.643143 33.579857 33.58216  33.53743  33.219074 33.53734
0:  33.70382  34.027744 34.11053  33.57299  33.08854  32.656013 32.625687
0:  33.09483  33.86036  34.66621  35.210564 35.86707  36.02177 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.16207   -6.1837115 -5.787852  -5.030646  -4.269362  -3.733644
0:  -3.520944  -3.465465  -3.8493295 -4.1810765 -4.548157  -5.0741124
0:  -5.360413  -5.46621   -5.269486  -4.6485343 -3.9123235 -3.2288632
0:  -4.7745156 -4.8248534]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.6631556  5.0140886  5.710223   6.5585823  7.278521   7.770261
0:   8.216608   8.511714   8.597409   8.685422   8.640637   8.386997
0:   8.276089   8.330006   8.654495   9.290936  10.012132  10.615773
0:   8.946278   9.215865 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.565594 34.836193 35.35343  35.906113 36.18607  36.10793  36.109585
0:  36.111294 36.202152 36.253933 36.027897 35.58117  35.198376 34.89181
0:  34.966644 35.24876  35.538692 35.64031  31.75121  31.64812 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.302462    2.3776364   2.7841733   3.2887676   3.5619838   3.5111187
0:   3.359416    3.161803    2.8210537   2.5355322   2.141347    1.4863853
0:   0.9139967   0.49659777  0.3996048   0.78481627  1.3660331   1.9034872
0:  -0.35728025 -0.4023409 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.31232   -8.120582  -7.505747  -6.5613136 -5.648428  -4.9930735
0:  -4.697482  -4.600747  -4.9375    -5.213358  -5.4911814 -5.903797
0:  -6.0809946 -6.149987  -5.895547  -5.3034525 -4.576667  -3.868825
0:  -4.751981  -4.429061 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.0510626 6.0486913 6.274517  6.618597  6.967762  7.121161  7.6600866
0:  8.070505  8.461238  8.688979  8.428329  7.9963603 7.475016  7.4012594
0:  7.752179  8.474522  9.216164  9.689088  7.534458  7.284646 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.668135  8.615657  8.825091  9.219414  9.537209  9.62738   9.573259
0:  9.439583  8.969202  8.511314  7.9239244 7.132804  6.4319043 5.8481264
0:  5.5555105 5.5720625 5.7466464 5.7991962 4.2528496 4.1840954]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.791164 43.518044 43.105522 42.97828  42.941418 42.680008 43.21734
0:  43.556927 44.125587 44.348236 43.91938  43.55651  43.260242 43.45737
0:  44.19451  45.042896 45.791706 45.961613 44.3972   44.236324]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.852576  -6.160684  -6.062057  -5.557442  -4.9987025 -4.5810237
0:  -4.3133426 -4.249088  -4.62577   -5.0331883 -5.6439776 -6.480554
0:  -7.140572  -7.609862  -7.754165  -7.538298  -7.2335877 -6.916162
0:  -8.198748  -8.19895  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4948459  -1.6657405  -1.4475188  -0.85971117 -0.1614871   0.40216255
0:   0.63090944  0.6712556   0.20062351 -0.3096819  -0.8431535  -1.5136046
0:  -1.9542012  -2.2821536  -2.2888646  -2.0206933  -1.572453   -1.1534104
0:  -2.4975219  -2.4596782 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.664827 16.533195 16.66241  16.8806   16.960102 16.787733 16.74382
0:  16.678381 16.615637 16.610538 16.471361 16.220423 16.047983 16.136812
0:  16.621506 17.463446 18.45498  19.30102  17.714201 17.729961]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.808259  4.581634  4.529215  4.419054  4.147115  3.7277038 3.633374
0:  3.5933077 3.6235595 3.735984  3.5905142 3.3640158 3.1401238 3.3485389
0:  3.8677585 4.668962  5.363449  5.794306  4.453267  4.609709 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.511058  11.280687  11.260872  11.339682  11.329254  11.133408
0:  10.908258  10.559607   9.908667   9.149065   8.128765   6.9255137
0:   5.8499937  5.0839047  4.794745   4.947217   5.263764   5.547131
0:   3.1117551  2.8854468]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.906666  19.189247  19.75125   20.39247   20.851627  20.954018
0:  21.159912  21.107542  20.909445  20.509985  19.659014  18.553095
0:  17.42446   16.575418  16.137922  16.104986  16.252117  16.32799
0:  12.7045555 12.379658 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.514709  -5.5625386 -5.268007  -4.616388  -3.9169936 -3.3844972
0:  -3.2387624 -3.219646  -3.701778  -4.133469  -4.5729985 -5.1991477
0:  -5.6336417 -6.0455146 -6.241018  -6.0798545 -5.7206044 -5.2697253
0:  -6.381085  -6.383118 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.406826   -3.1515112  -2.5605955  -1.7645245  -1.0935354  -0.682796
0:  -0.50847197 -0.49445677 -0.834445   -1.112566   -1.4336176  -1.8655071
0:  -2.0679016  -2.0659018  -1.745451   -1.0939989  -0.33311462  0.35985136
0:  -0.6888509  -0.4307065 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.5378075  -4.416916   -3.9145584  -3.3125534  -2.7757378  -2.3665571
0:  -1.7808313  -1.2340951  -0.7345772  -0.32638836 -0.1754551  -0.16092968
0:   0.05056286  0.5474682   1.4826093   2.6959887   3.8781142   4.8758736
0:   2.8140297   3.2768757 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.7874107 4.87277   5.3439674 6.063818  6.674979  7.0124645 7.3029647
0:  7.345661  7.0483212 6.6049805 5.83866   4.849785  4.046276  3.6331034
0:  3.7500653 4.3795753 5.2106786 5.9392843 6.13959   6.372409 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.82253  17.061161 17.46239  17.84011  18.07383  18.094574 18.27251
0:  18.40453  18.532476 18.714067 18.71121  18.622414 18.564138 18.686789
0:  19.121601 19.81185  20.627201 21.332048 19.227108 19.596972]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.850296 37.506184 36.924847 36.33559  35.821953 35.076435 35.1594
0:  35.11842  35.2073   35.032482 34.08806  33.297935 32.43189  32.162987
0:  32.442184 32.96021  33.46974  33.536625 33.71073  33.557808]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.2751017  6.199045   6.4071684  6.7138696  7.0543036  7.3200183
0:   8.194366   9.048756  10.021773  10.914568  11.296604  11.572239
0:  11.728193  12.393159  13.469357  14.836055  16.045074  16.770836
0:  17.378159  17.544048 ]
0: validation loss for strategy=forecast at epoch 33 : 0.3824182152748108
0: validation loss for velocity_u : 0.1796916127204895
0: validation loss for velocity_v : 0.28207674622535706
0: validation loss for specific_humidity : 0.19427043199539185
0: validation loss for velocity_z : 0.5288738012313843
0: validation loss for temperature : 0.11942195892333984
0: validation loss for total_precip : 0.9901747703552246
0: 34 : 20:13:37 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 34, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.6875, 1.6780, 1.6610, 1.6395, 1.6178, 1.5997, 1.5878, 1.5826, 1.5833, 1.5873, 1.5908, 1.5895, 1.5781, 1.5530,
0:         1.5121, 1.4563, 1.3896, 1.3171, 1.6860, 1.6699, 1.6484, 1.6256, 1.6054, 1.5908, 1.5830], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.7667, 2.9162, 3.0580, 3.1912, 3.3153, 3.4297, 3.5326, 3.6222, 3.6957, 3.7526, 3.7928, 3.8168, 3.8231, 3.8086,
0:         3.7700, 3.7051, 3.6147, 3.5024, 2.7962, 2.9438, 3.0832, 3.2129, 3.3324, 3.4406, 3.5353], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4880, -0.5116, -0.5244, -0.5348, -0.5423, -0.5462, -0.5492, -0.5461, -0.5432, -0.5403, -0.5364, -0.5367,
0:         -0.5447, -0.5513, -0.5723, -0.5953, -0.6160, -0.6375, -0.4943, -0.5150, -0.5283, -0.5417, -0.5465, -0.5514,
0:         -0.5512], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2112, 0.1869, 0.2112, 0.2819, 0.3858, 0.4963, 0.5847, 0.6356, 0.6444, 0.6245, 0.5869, 0.5449, 0.5162, 0.5096,
0:         0.5383, 0.6024, 0.6952, 0.7969, 0.1803, 0.2223, 0.3107, 0.4278, 0.5449, 0.6378, 0.6930], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.6044,  0.5829,  0.5608,  0.5387,  0.5168,  0.4951,  0.4735,  0.4512,  0.4270,  0.3990,  0.3654,  0.3235,
0:          0.2714,  0.2080,  0.1346,  0.0532, -0.0321, -0.1172, -0.1990, -0.2749, -0.3436, -0.4045, -0.4580, -0.5057,
0:         -0.5495], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2343, -0.2343, -0.2343, -0.2354, -0.2320, -0.2309, -0.2286, -0.2263, -0.2229, -0.2332, -0.2354, -0.2343,
0:         -0.2343, -0.2286, -0.2241, -0.2218, -0.2218, -0.2218, -0.2309, -0.2332, -0.2332, -0.2332, -0.2309, -0.2275,
0:         -0.2252], device='cuda:0')
0: [DEBUG] Epoch 34, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.1764,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1776,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1776,     nan,     nan,     nan, -0.1345,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1526, -0.1674,     nan,     nan, -0.2195,     nan, -0.1934,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0233,     nan,     nan, -0.0845,     nan,     nan,     nan,     nan, -0.0879,     nan,
0:             nan, -0.1129,     nan, -0.1095,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1560,
0:             nan,     nan,     nan, -0.1299,     nan,     nan,     nan,     nan,     nan, -0.1038,     nan, -0.1129,
0:             nan,     nan,     nan,     nan, -0.1889,     nan,     nan,     nan,     nan,     nan, -0.1866,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2059,     nan,     nan, -0.2332,     nan,     nan,
0:             nan,     nan,     nan, -0.1345,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0278,     nan,     nan,     nan,     nan,     nan,     nan,  0.0380,     nan, -0.0426,
0:             nan,     nan,     nan,     nan, -0.0085,  0.0289,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1549,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1730,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1991,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 34, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.8520, 1.8442, 1.8443, 1.8524, 1.8511, 1.8369, 1.8503, 1.8527, 1.8574, 1.8513, 1.8164, 1.7798, 1.7622, 1.7719,
0:         1.8287, 1.9078, 1.9914, 2.0592, 1.8368, 1.8419, 1.8519, 1.8506, 1.8184, 1.7917, 1.7686], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7340, 0.7797, 0.7943, 0.7917, 0.7664, 0.7398, 0.7078, 0.6939, 0.6902, 0.6981, 0.7220, 0.7624, 0.7910, 0.8026,
0:         0.8096, 0.8162, 0.8378, 0.8804, 0.7162, 0.7619, 0.7759, 0.7638, 0.7390, 0.7110, 0.6964], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2037, -0.2025, -0.2072, -0.2105, -0.2125, -0.2233, -0.2430, -0.2649, -0.2935, -0.3155, -0.3389, -0.3555,
0:         -0.3633, -0.3666, -0.3711, -0.3710, -0.3747, -0.3645, -0.2313, -0.2337, -0.2418, -0.2398, -0.2467, -0.2591,
0:         -0.2719], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1624, -0.0352,  0.1461,  0.1008,  0.0398,  0.1424,  0.1686,  0.1503,  0.1619,  0.1874,  0.2913,  0.3950,
0:          0.4895,  0.5434,  0.5563,  0.7397,  0.8885,  0.7244, -0.1328, -0.1121, -0.0320, -0.0591, -0.0634,  0.0458,
0:          0.0780], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0405, -0.0456, -0.0533, -0.0557, -0.0533, -0.0470, -0.0477, -0.0482, -0.0464, -0.0357, -0.0215, -0.0013,
0:          0.0100,  0.0204,  0.0276,  0.0397,  0.0523,  0.0674,  0.0751,  0.0735,  0.0631,  0.0526,  0.0513,  0.0628,
0:          0.0923], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1657, -0.1695, -0.1837, -0.1889, -0.1901, -0.1899, -0.1922, -0.1919, -0.1846, -0.1608, -0.1638, -0.1652,
0:         -0.1784, -0.1907, -0.1945, -0.1939, -0.1929, -0.1954, -0.1509, -0.1528, -0.1668, -0.1693, -0.1742, -0.1942,
0:         -0.1999], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2476198971271515; velocity_v: 0.3253748416900635; specific_humidity: 0.20261885225772858; velocity_z: 0.5664141178131104; temperature: 0.17398561537265778; total_precip: 0.5921527147293091; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21547900140285492; velocity_v: 0.3045015037059784; specific_humidity: 0.18178784847259521; velocity_z: 0.606787383556366; temperature: 0.16735795140266418; total_precip: 0.7281358242034912; 
0: epoch: 34 [1/5 (20%)]	Loss: 0.66014 : 0.32469 :: 0.20783 (2.63 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21349485218524933; velocity_v: 0.3419851064682007; specific_humidity: 0.1803409904241562; velocity_z: 0.6162227988243103; temperature: 0.1608119010925293; total_precip: 0.8640276789665222; 
0: epoch: 34 [2/5 (40%)]	Loss: 0.86403 : 0.36097 :: 0.21668 (15.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18164899945259094; velocity_v: 0.2578427791595459; specific_humidity: 0.21044155955314636; velocity_z: 0.4644877016544342; temperature: 0.13800306618213654; total_precip: 0.5148985981941223; 
0: epoch: 34 [3/5 (60%)]	Loss: 0.51490 : 0.26075 :: 0.20636 (15.92 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21779298782348633; velocity_v: 0.34299248456954956; specific_humidity: 0.19154587388038635; velocity_z: 0.6163718104362488; temperature: 0.17988859117031097; total_precip: 0.5220311880111694; 
0: epoch: 34 [4/5 (80%)]	Loss: 0.52203 : 0.31035 :: 0.20553 (16.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 0.00000000e+00 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 9.53674316e-07 1.43051147e-06 1.43051147e-06
0:  1.43051147e-06 2.38418579e-06 4.29153442e-06 4.76837158e-06
0:  5.24520874e-06 5.24520874e-06 4.29153442e-06 5.24520874e-06
0:  5.72204590e-06 6.19888306e-06 7.15255737e-06 6.67572021e-06
0:  9.05990601e-06 9.53674316e-06 9.05990601e-06 9.05990601e-06
0:  6.67572021e-06 6.67572021e-06 5.24520874e-06 6.19888306e-06
0:  8.10623169e-06 1.00135803e-05 1.19209290e-05 1.52587891e-05
0:  2.28881836e-05 2.71797180e-05 3.09944153e-05 2.95639038e-05
0:  2.62260437e-05 2.90870667e-05 3.24249268e-05 3.95774841e-05
0:  4.14848364e-05 4.43458557e-05 4.05311548e-05 8.82148743e-05
0:  6.91413879e-05 8.48770142e-05 8.72612000e-05 8.20159912e-05
0:  8.15391541e-05 6.81877136e-05 7.86781311e-05 8.44001770e-05
0:  7.34329224e-05 7.34329224e-05 7.39097595e-05 6.67572021e-05
0:  5.62667847e-05 4.62532080e-05 4.43458557e-05 3.19480896e-05
0:  2.28881836e-05 2.19345093e-05 2.05039978e-05 2.00271606e-05
0:  2.57492065e-05 3.29017639e-05 3.91006470e-05 4.81605530e-05
0:  9.15527344e-05 1.22547150e-04 1.62601471e-04 1.91688538e-04
0:  2.54631042e-04 2.85625458e-04 4.92572784e-04 8.71181488e-04
0:  6.51836395e-04 6.50882721e-04 1.08480453e-03 1.09481812e-03
0:  1.05047226e-03 9.23156680e-04 7.56740570e-04 7.58647919e-04
0:  6.83784485e-04 6.36100769e-04 6.34193420e-04 6.05583191e-04
0:  6.01768494e-04 5.87940216e-04 5.45978604e-04 4.55856323e-04
0:  3.92436981e-04 2.95639067e-04 2.11238861e-04 1.28269196e-04
0:  6.48498535e-05 7.39097595e-05 8.72612000e-05 7.05718994e-05
0:  6.38961792e-05 5.81741333e-05 7.72476196e-05 5.86509705e-05
0:  8.63075256e-05 8.48770142e-05 4.72068787e-05 1.28746033e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 9.53674316e-07 2.38418579e-06
0:  2.86102295e-06 3.33786011e-06 2.38418579e-06 2.38418579e-06
0:  3.81469727e-06 6.19888306e-06 8.58306885e-06 8.58306885e-06
0:  6.19888306e-06 4.76837158e-06 2.38418579e-06 2.86102295e-06
0:  5.24520874e-06 7.15255737e-06 9.05990601e-06 1.19209290e-05
0:  1.47819519e-05 1.85966492e-05 2.28881836e-05 2.47955322e-05
0:  2.81333923e-05 3.43322754e-05 3.76701355e-05 4.05311548e-05
0:  5.10215759e-05 5.81741333e-05 5.43594360e-05 7.43865967e-05
0:  7.00950623e-05 9.72747803e-05 1.01566315e-04 9.48905945e-05
0:  9.53674316e-05 8.24928284e-05 8.34465027e-05 7.15255737e-05
0:  7.62939453e-05 7.67707825e-05 6.81877136e-05 5.72204590e-05
0:  5.00679016e-05 4.05311548e-05 3.05175781e-05 2.57492065e-05
0:  1.76429749e-05 1.62124634e-05 1.71661377e-05 2.24113464e-05
0:  2.81333923e-05 3.76701355e-05 4.14848364e-05 4.48226929e-05
0:  8.82148743e-05 1.69754028e-04 2.57968903e-04 2.63690948e-04
0:  3.25679779e-04 4.12940979e-04 5.39779663e-04 9.51290131e-04
0:  6.96182251e-04 6.32762909e-04 6.47068024e-04 6.57558441e-04
0:  6.14166260e-04 6.20841980e-04 6.69002533e-04 6.70909882e-04
0:  6.38961792e-04 6.41822815e-04 6.59465790e-04 6.30855560e-04
0:  5.81741333e-04 4.95910645e-04 4.47750092e-04 3.50475311e-04]
0: Target values (first 200):
0: [8.10623169e-06 6.67572021e-06 7.62939453e-06 2.38418579e-05
0:  3.09944153e-05 2.67028809e-05 2.38418579e-05 1.52587891e-05
0:  2.57492065e-05 4.33921778e-05 4.33921778e-05 3.38554382e-05
0:  2.14576721e-05 1.38282776e-05 8.10623169e-06 6.19888306e-06
0:  9.53674316e-06 1.28746033e-05 1.43051147e-05 2.57492065e-05
0:  3.14712524e-05 2.95639038e-05 3.57627869e-05 4.91142273e-05
0:  6.00814819e-05 6.05583191e-05 5.91278076e-05 6.00814819e-05
0:  6.77108765e-05 5.43594360e-05 3.14712524e-05 1.90734863e-05
0:  2.38418579e-05 3.91006470e-05 4.29153442e-05 4.43458557e-05
0:  6.24656677e-05 8.48770142e-05 1.01566315e-04 1.09195709e-04
0:  1.02996826e-04 9.53674316e-05 7.67707825e-05 5.00679016e-05
0:  3.29017639e-05 3.09944153e-05 3.33786011e-05 4.33921778e-05
0:  4.00543213e-05 3.29017639e-05 3.24249268e-05 2.62260437e-05
0:  2.38418579e-05 2.05039978e-05 2.05039978e-05 2.00271606e-05
0:  1.57356262e-05 1.47819519e-05 1.09672546e-05 1.43051147e-05
0:  1.95503235e-05 1.90734863e-05 1.76429749e-05 1.47819519e-05
0:  1.19209290e-05 1.00135803e-05 8.58306885e-06 1.81198120e-05
0:  5.10215759e-05 7.24792480e-05 9.87052917e-05 1.11579895e-04
0:  6.24656677e-05 3.29017639e-05 9.05990601e-06 1.81198120e-05
0:  1.00135803e-05 2.28881836e-05 1.66893005e-05 1.52587891e-05
0:  1.47819519e-05 1.38282776e-05 2.19345093e-05 3.09944153e-05
0:  3.48091125e-05 4.38690222e-05 5.67436218e-05 5.72204590e-05
0:  6.62803650e-05 8.20159912e-05 1.11579895e-04 1.02519989e-04
0:  1.18732452e-04 1.59740448e-04 1.54972076e-04 1.23977661e-04
0:  8.44001770e-05 7.20024109e-05 7.05718994e-05 5.67436218e-05
0:  5.38825989e-05 4.62532080e-05 5.29289246e-05 4.86373901e-05
0:  5.76972961e-05 4.86373901e-05 5.81741333e-05 7.34329224e-05
0:  8.10623169e-06 6.67572021e-06 6.67572021e-06 1.52587891e-05
0:  2.28881836e-05 1.19209290e-05 1.19209290e-05 1.14440918e-05
0:  2.24113464e-05 4.14848364e-05 4.67300415e-05 3.57627869e-05
0:  2.38418579e-05 9.53674316e-06 7.15255737e-06 6.19888306e-06
0:  9.05990601e-06 9.05990601e-06 1.52587891e-05 2.62260437e-05
0:  4.05311548e-05 7.05718994e-05 8.77380371e-05 1.01089478e-04
0:  1.14917755e-04 1.22070312e-04 1.23500824e-04 1.24454498e-04
0:  1.29222870e-04 1.08242035e-04 8.34465027e-05 6.72340393e-05
0:  3.91006470e-05 3.19480896e-05 3.52859497e-05 3.19480896e-05
0:  4.05311548e-05 3.33786011e-05 5.00679016e-05 8.29696655e-05
0:  8.15391541e-05 7.10487366e-05 5.24520874e-05 6.10351562e-05
0:  4.24385071e-05 4.43458557e-05 4.43458557e-05 4.00543213e-05
0:  3.29017639e-05 2.57492065e-05 3.24249268e-05 2.71797180e-05
0:  2.47955322e-05 2.14576721e-05 1.76429749e-05 1.66893005e-05
0:  1.04904175e-05 9.53674316e-06 1.19209290e-05 1.23977661e-05
0:  1.76429749e-05 1.71661377e-05 1.85966492e-05 1.71661377e-05
0:  1.47819519e-05 1.38282776e-05 1.04904175e-05 3.19480896e-05
0:  6.43730164e-05 1.08718872e-04 9.29832458e-05 8.91685486e-05
0:  4.05311548e-05 1.71661377e-05 9.05990601e-06 2.57492065e-05
0:  1.23977661e-05 9.53674316e-06 8.58306885e-06 7.62939453e-06
0:  1.04904175e-05 1.66893005e-05 2.71797180e-05 3.62396240e-05
0:  4.48226929e-05 6.24656677e-05 7.53402710e-05 7.58171082e-05
0:  6.86645508e-05 8.48770142e-05 1.17301941e-04 1.02996826e-04]
0: Prediction values (first 20):
0: [27.814405 27.171194 26.411812 25.647602 25.081547 24.359402 24.6043
0:  24.688726 24.962465 24.944815 24.153536 23.526884 22.868061 22.912174
0:  23.599052 24.634844 25.692955 26.219221 27.744648 27.586878]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -0.231, max = 4.255, mean = 1.862
0:          sample (first 20): tensor([1.7106, 1.6575, 1.5947, 1.5316, 1.4848, 1.4251, 1.4454, 1.4523, 1.4750, 1.4735, 1.4081, 1.3563, 1.3019, 1.3055,
0:         1.3623, 1.4479, 1.5353, 1.5788, 1.7414, 1.7677])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.659407 15.960796 15.366225 14.855907 14.304749 13.610506 13.316193
0:  13.033218 12.800498 12.553328 12.089613 11.48289  10.986671 11.083719
0:  11.682401 12.771557 14.035572 14.83125  12.414622 12.214247]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.855688 22.16932  22.652933 23.127909 23.40845  23.52856  23.640404
0:  23.934092 24.169733 24.499887 24.730474 24.687754 24.64352  24.589373
0:  24.80539  25.28614  25.890955 26.361647 23.670095 23.55399 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2645898  -2.7015462  -2.7170863  -2.4316335  -2.1876335  -2.17375
0:  -1.9089756  -1.7498364  -1.5025244  -1.2459412  -1.2959156  -1.5201192
0:  -1.805934   -1.7406015  -1.3260078  -0.47727585  0.4750743   1.2533994
0:  -0.20371914 -0.41763115]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.6427197 -7.6293864 -7.2747426 -6.5559316 -5.8719115 -5.4780755
0:  -5.4186826 -5.586332  -6.223829  -6.837045  -7.547997  -8.35088
0:  -8.92396   -9.324266  -9.353003  -9.037319  -8.618536  -8.291718
0:  -8.571651  -8.700232 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.46281  18.691118 18.996052 19.089151 18.82384  18.241503 17.89682
0:  17.532055 17.194166 16.777208 15.946377 14.859491 13.837658 13.224239
0:  13.257391 13.867491 14.803909 15.678129 13.584156 13.664621]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.9491863   0.9460821   1.2843027   1.9246631   2.4706683   2.8019187
0:   2.7575097   2.6917384   2.271025    1.9780412   1.6855264   1.1311731
0:   0.5991812   0.05288124 -0.20283842  0.1003418   0.85862255  1.8226199
0:   0.34211588  0.09285069]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.4952984 6.3801637 6.488656  6.7209396 6.8599563 6.781697  6.970894
0:  7.0816526 7.193     7.284447  7.0386524 6.639088  6.217643  6.12977
0:  6.4129524 7.06086   7.7707973 8.298317  7.054232  7.148806 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.9953313 1.0041695 1.4224229 2.160139  2.8717318 3.3127713 3.8013215
0:  4.0482726 4.046187  4.060897  3.8831732 3.540869  3.305566  3.274214
0:  3.5439591 3.991668  4.4393225 4.7074404 4.504669  4.816117 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.870731  8.700057  8.727703  8.782993  8.754563  8.529055  8.620074
0:  8.6419    8.693552  8.665664  8.253086  7.706967  7.08226   6.804324
0:  6.846146  7.1830387 7.4551854 7.527144  4.956973  4.7408066]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.058651 20.411985 20.963009 21.42559  21.638866 21.605286 21.650219
0:  21.61304  21.479298 21.281311 20.72123  19.97688  19.291996 18.958473
0:  19.183556 19.842625 20.701084 21.503778 18.603874 18.921764]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.571352 16.57584  16.862183 17.187613 17.348776 17.274202 17.275675
0:  17.212612 17.133442 17.109121 16.933224 16.56547  16.292456 16.187403
0:  16.467491 17.082958 17.83625  18.460783 16.598024 16.596474]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.479952  14.340863  14.495437  14.75312   14.784512  14.521361
0:  14.190002  13.90144   13.527893  13.271336  12.925173  12.366755
0:  11.826372  11.381779  11.329566  11.745672  12.487484  13.250129
0:  10.43531   10.5686035]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.38225  18.100561 17.974785 17.856281 17.48165  16.725227 16.28857
0:  15.825651 15.64546  15.599325 15.3169   15.034035 14.745983 14.902367
0:  15.47764  16.334055 17.334688 18.0345   16.87522  17.16564 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.86445    9.954906  10.316357  10.741795  10.9768    10.955212
0:  11.007103  11.074132  11.213673  11.404348  11.4397545 11.30805
0:  11.181308  11.22011   11.63102   12.418709  13.433929  14.413166
0:  12.382965  12.53643  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8390727  -1.9039769  -1.6024971  -1.0094018  -0.475245   -0.17114162
0:  -0.13724613 -0.1728549  -0.55626345 -0.8328686  -1.1461344  -1.6983566
0:  -2.1144238  -2.5113773  -2.623087   -2.272767   -1.650888   -0.9332776
0:  -2.3120728  -2.465806  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.552462  -9.354963  -8.68083   -7.724382  -6.8796973 -6.3481526
0:  -5.9804378 -5.8884826 -6.1447034 -6.3017306 -6.5887904 -7.007198
0:  -7.214413  -7.1759257 -6.849953  -6.214254  -5.63674   -5.173077
0:  -5.997623  -5.7649446]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.56482935 -0.64726543 -0.43855572  0.02701998  0.42427588  0.6187
0:   0.64176226  0.57986975  0.17131376 -0.20650196 -0.72260857 -1.4238982
0:  -1.9701743  -2.371591   -2.46283    -2.179606   -1.7925477  -1.467237
0:  -2.7975106  -2.9567027 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.597803  13.715432  13.872665  13.973846  13.962412  13.777013
0:  13.672409  13.510989  13.267286  13.031738  12.7118025 12.3419075
0:  12.125414  12.160389  12.471579  13.011799  13.632954  14.107256
0:  11.725372  11.916986 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3093357 6.206281  6.4465895 6.8494387 7.074759  6.983364  6.719965
0:  6.339845  5.742159  5.1981664 4.5810375 3.8038342 3.201754  2.8344464
0:  2.8625655 3.3987012 4.179886  4.9591904 3.511178  3.4198427]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.704027  -14.016602  -13.933928  -13.336344  -12.516834  -11.761715
0:  -11.362036  -11.1257    -11.4585705 -11.84617   -12.281607  -12.895462
0:  -13.245327  -13.50359   -13.500652  -13.171516  -12.706764  -12.14764
0:  -12.209038  -12.575367 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.83594  21.80376  21.94154  22.111761 22.180557 22.091747 22.206312
0:  22.29728  22.38497  22.5304   22.475689 22.28409  22.188213 22.241182
0:  22.595312 23.229664 23.9423   24.490902 22.843    22.83873 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.787519   8.891945   9.2740345  9.739599  10.040548  10.124084
0:  10.165442  10.130474   9.92811    9.738199   9.380894   8.848881
0:   8.3886795  8.093917   8.17556    8.598535   9.201568   9.759616
0:   8.150489   8.271023 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.165089 26.729988 26.31929  26.00548  25.795103 25.296852 25.47738
0:  25.376215 25.45561  25.279755 24.536446 23.918884 23.316984 23.26411
0:  23.72752  24.575975 25.492184 25.999537 25.350897 25.16095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.977978 36.588604 36.165176 35.693737 35.1363   34.318893 33.922134
0:  33.355556 32.816414 32.111652 30.910439 29.837536 28.908749 28.596195
0:  29.020844 29.78973  30.689373 31.326862 31.714684 31.650936]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.9627476 1.1242385 1.5501795 2.1275182 2.6874707 3.0656934 3.3977435
0:  3.5661004 3.4980206 3.349972  3.0262206 2.5927126 2.2978373 2.242742
0:  2.457956  2.9030044 3.3286593 3.585095  1.2584119 1.0860715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.322392  8.659554  9.244799 10.011591 10.664789 11.003887 11.463118
0:  11.727208 11.859751 11.810906 11.390499 10.713732 10.051122  9.75996
0:   9.932171 10.645821 11.66357  12.644334 11.362087 11.303227]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2969837   1.5281849   2.1761043   3.0965705   3.9191275   4.4752727
0:   4.7403097   4.7996798   4.4405737   3.9908152   3.3404095   2.3728025
0:   1.4657807   0.63758993  0.16265583  0.20989418  0.648376    1.2113752
0:  -1.2387199  -1.2546287 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.018305 22.40086  22.953396 23.379614 23.43905  23.17712  22.862867
0:  22.645351 22.426224 22.25903  22.018553 21.48894  21.06264  20.759876
0:  20.879631 21.421967 22.16112  22.793682 19.091158 19.267519]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.424035  10.5872965 10.855013  11.071737  11.153255  11.002171
0:  11.264884  11.454649  11.73997   12.015057  11.925827  11.817654
0:  11.743582  12.143688  12.982014  14.181066  15.51689   16.63306
0:  17.824644  18.181725 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.0087905 7.069246  7.432447  7.9143977 8.226706  8.261232  8.323007
0:  8.284079  8.132927  8.027495  7.721228  7.2377596 6.7904844 6.533066
0:  6.641968  7.1671667 7.9032564 8.57884   7.240367  7.3301   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.5734315 6.2880816 7.170623  8.018314  8.653796  8.981445  9.175728
0:  9.069711  8.612266  7.9079604 6.874393  5.638176  4.533641  3.7004771
0:  3.2443595 3.0824075 2.9833584 2.8184361 1.1666799 1.1275187]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.226822 20.143171 20.022005 19.774755 19.497143 19.0481   19.255589
0:  19.39719  19.64384  19.725471 19.224272 18.673294 18.062551 17.924664
0:  18.246456 18.958603 19.705315 20.242332 20.107008 20.171684]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4747567  -2.3181796  -1.8611617  -1.2388759  -0.74666977 -0.510242
0:  -0.32005644 -0.21552658 -0.32432747 -0.43889618 -0.74655914 -1.2596273
0:  -1.7018657  -1.9083562  -1.8272691  -1.4001222  -0.94157267 -0.6353049
0:  -2.0458455  -2.0251174 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.1525812 1.2222047 1.4758816 1.6624885 1.7493129 1.731533  2.1704092
0:  2.6596446 3.174628  3.6090875 3.5932078 3.3950605 3.149858  3.4403207
0:  4.1286006 5.134837  5.953467  6.4250298 6.32554   6.511398 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.74943  20.068659 20.162521 19.928122 19.488739 18.776459 18.771284
0:  18.714613 18.754078 18.660927 17.910204 17.177696 16.351723 16.188728
0:  16.508947 17.225405 17.935427 18.27433  17.654427 17.909855]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.0635777   1.9358463   2.1155066   2.4707818   2.7448974   2.839075
0:   2.9159346   2.860034    2.558895    2.244748    1.7254009   0.9822211
0:   0.4085555   0.07211351  0.0871439   0.44832897  0.82698154  1.1080475
0:  -0.30755186 -0.21763134]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.574963 40.13497  40.301353 40.14294  39.74865  39.14273  39.554432
0:  39.903484 40.41702  40.571827 39.85931  39.09432  38.262623 38.156864
0:  38.645264 39.32915  39.887547 39.66101  37.28726  37.446915]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.30733  20.230154 20.42428  20.623194 20.596577 20.201235 19.748785
0:  19.07946  18.228262 17.300621 16.105637 14.779831 13.654901 12.904932
0:  12.789957 13.19491  13.910149 14.591803 12.332182 12.186117]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.174301  10.739695  10.541115  10.405497  10.179192   9.663576
0:   9.385675   9.012184   8.672105   8.3303795  7.709406   6.971386
0:   6.293199   5.9517865  6.081449   6.643611   7.3357654  7.9156213
0:   5.797621   5.304078 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.8572273 -5.8674726 -5.45695   -4.6979    -3.885397  -3.233714
0:  -2.8592033 -2.585782  -2.7155304 -2.8139343 -2.9495516 -3.3021145
0:  -3.521247  -3.718586  -3.728116  -3.3989105 -2.9161148 -2.3895397
0:  -4.0365486 -4.0217724]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.492157  5.2263126 5.1738367 5.4226995 5.683655  5.8404655 5.665813
0:  5.476535  4.8455777 4.3026457 3.7547762 2.9638674 2.274098  1.5832734
0:  1.2063737 1.3369422 1.9495807 2.7954342 1.5247941 1.2893405]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.6922922   2.3876777   2.3041139   2.4660907   2.5667074   2.5217881
0:   2.239814    1.9664602   1.337676    0.79437876  0.2201724  -0.5971556
0:  -1.265235   -1.8471355  -2.093278   -1.8697376  -1.3520565  -0.75773954
0:  -1.6674027  -1.8973947 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6981587  -5.318481   -4.522678   -3.5525823  -2.74573    -2.2624784
0:  -1.9475312  -1.856174   -2.046063   -2.1284833  -2.3106914  -2.6167626
0:  -2.8038173  -2.8098292  -2.5275245  -1.957169   -1.354774   -0.8442092
0:  -1.2008967  -0.60193014]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.095736  -10.254591   -9.909857   -9.025116   -7.9840064  -7.04907
0:   -6.62365    -6.374974   -6.739514   -7.103186   -7.502648   -8.184561
0:   -8.717007   -9.251337   -9.50485    -9.22661    -8.50915    -7.5035386
0:   -8.077385   -8.47551  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.841347  -8.931341  -8.571543  -7.8547497 -7.095936  -6.487296
0:  -6.129383  -5.91185   -6.1390123 -6.3892717 -6.7318707 -7.2965727
0:  -7.6301994 -7.848955  -7.7920647 -7.404399  -6.952224  -6.551134
0:  -8.286717  -8.343227 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.328014 24.64975  25.1208   25.506962 25.686905 25.516289 25.724651
0:  25.720997 25.758488 25.72116  25.223423 24.659588 24.136402 24.034805
0:  24.425076 25.164288 26.024132 26.710953 24.932884 25.199284]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.80753  19.82103  20.074675 20.358536 20.473831 20.307266 20.380516
0:  20.297974 20.209599 20.057167 19.549007 18.89814  18.322662 18.10699
0:  18.379692 19.044075 19.851841 20.440418 18.002903 17.981358]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6853313 -5.927144  -5.8401356 -5.2807035 -4.587618  -3.9575129
0:  -3.8076448 -3.7380314 -4.199972  -4.5649395 -4.8679447 -5.315525
0:  -5.630332  -5.937764  -6.0011587 -5.563838  -4.708827  -3.6194324
0:  -4.084613  -4.5283985]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.761099  16.18778   16.869581  17.55343   17.988823  18.129196
0:  18.209154  18.279528  18.228254  18.176596  17.98979   17.598658
0:  17.246014  17.010027  17.104753  17.538767  18.129267  18.647963
0:  15.083958  15.3442745]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.837059   9.896586  10.048186  10.147311  10.188198  10.089293
0:  10.4031315 10.695367  11.018675  11.27556   11.153385  10.92891
0:  10.706209  10.876424  11.394638  12.24914   13.147329  13.831505
0:  12.618635  12.817235 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.5154314 4.706487  5.084964  5.5938964 5.975488  6.1361737 6.090855
0:  5.9025335 5.411238  4.995202  4.5578613 4.0517597 3.7752414 3.6995215
0:  3.922595  4.466574  5.111449  5.687358  4.5584574 4.71181  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.0324955 10.225574   9.90287    9.849694   9.807568   9.5919285
0:   9.718806   9.828188  10.092753  10.357351  10.38693   10.263578
0:  10.2765255 10.581168  11.390648  12.599634  13.930557  14.972689
0:  11.989946  11.418479 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.40782976  0.2993412   0.21007442  0.1729312   0.16389322 -0.00446987
0:   0.17165565  0.3233919   0.22350502  0.06961823 -0.54149055 -1.2300773
0:  -1.7960496  -1.7720933  -1.2080417  -0.30603218  0.6033859   1.1955724
0:   1.9217257   2.1617498 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.905292   7.77033    7.9047236  8.123495   8.275016   8.307818
0:   8.602451   8.8601885  9.085863   9.320994   9.251364   9.033246
0:   8.860833   9.0687895  9.624685  10.466135  11.254686  11.776705
0:   9.713099   9.872929 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.141212  -8.897564  -9.20929   -8.925043  -8.355667  -7.709657
0:  -7.3937507 -7.058884  -7.2334037 -7.3798547 -7.5496955 -8.050228
0:  -8.439751  -8.812508  -8.949347  -8.6071415 -7.862103  -6.9650435
0:  -7.9911466 -8.39492  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.323593 23.28305  24.138916 24.688698 25.126472 25.442265 26.626724
0:  27.763313 28.999882 30.109974 30.625069 31.038914 31.452362 32.504704
0:  33.9297   35.477184 36.61081  36.852352 35.631927 36.382576]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.064802  7.195062  7.483123  7.791545  7.958783  7.9234486 8.032458
0:  8.04514   7.9753766 7.8706274 7.5108957 7.00009   6.5621924 6.486038
0:  6.7883596 7.442668  8.162883  8.698727  7.827661  8.080713 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.2725496 6.524154  6.995537  7.5591273 8.0114975 8.290127  8.446522
0:  8.56332   8.411549  8.2077265 7.8651094 7.30671   6.824016  6.4021497
0:  6.2637486 6.437215  6.831478  7.2293587 5.0266504 5.2199907]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.732872  13.979992  14.459108  15.008259  15.4391365 15.671667
0:  16.085424  16.368944  16.614819  16.77439   16.639559  16.309479
0:  16.049618  16.102581  16.534378  17.29051   18.110382  18.721312
0:  17.077288  17.23634  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.0456457 -5.716942  -4.9030285 -3.7824755 -2.860404  -2.344901
0:  -2.328001  -2.5490637 -3.2891693 -4.030451  -4.871497  -5.9708376
0:  -6.8568583 -7.529826  -7.6928368 -7.207178  -6.32689   -5.383109
0:  -5.778858  -5.7662287]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-20.18716  -20.223307 -19.750425 -18.935305 -18.13069  -17.478949
0:  -16.795195 -16.366495 -16.206484 -16.172487 -16.455408 -16.901875
0:  -17.157022 -16.962105 -16.272106 -15.365223 -14.606649 -14.195261
0:  -12.283737 -12.129602]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.04855  21.678745 22.385801 23.033958 23.522774 23.719606 24.168976
0:  24.57504  25.006157 25.394796 25.533754 25.58575  25.688568 26.085657
0:  26.809187 27.762022 28.791477 29.518124 27.439001 27.74919 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.901883  4.067589  4.5421696 5.1603026 5.616529  5.8476667 6.2427316
0:  6.595064  6.896313  7.174993  7.128347  6.813182  6.4170985 6.3166103
0:  6.539343  7.117903  7.75268   8.162295  6.024314  6.2013288]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1879792  -1.2247334  -1.034709   -0.6109352  -0.281497   -0.18640852
0:   0.01203775  0.05253696 -0.03397655 -0.1941843  -0.6299963  -1.2755208
0:  -1.8658547  -2.076096   -1.8837762  -1.3422008  -0.75392246 -0.35922098
0:  -2.2518287  -2.6497588 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0227075  -0.8529825  -0.28383636  0.7393675   1.7805514   2.704419
0:   3.178997    3.517477    3.3725352   3.2704277   3.1530051   2.7855034
0:   2.4733      2.0957155   1.9940631   2.3578827   3.1803699   4.1784205
0:   1.9856691   1.653213  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.442162 -10.60568  -10.342533  -9.656006  -8.905825  -8.303787
0:   -8.026503  -8.00144   -8.537783  -9.102091  -9.746189 -10.525349
0:  -10.970467 -11.221817 -11.095823 -10.630133 -10.014135  -9.384758
0:   -9.283499  -9.276102]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.739231  13.68433   13.680366  13.653882  13.613569  13.375099
0:  13.752962  14.0207    14.4137125 14.703381  14.42128   14.176914
0:  13.891802  14.226036  15.123127  16.521767  18.044928  19.28411
0:  21.031944  21.30889  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.735126 19.94807  20.321295 20.72651  20.87767  20.736078 20.722445
0:  20.544188 20.276243 19.921085 19.232407 18.348818 17.55207  17.072813
0:  17.08344  17.588657 18.346338 19.022099 17.112886 17.32358 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.046154  10.481365   9.935606   9.308581   8.48506    7.4852567
0:   7.029897   6.620841   6.353616   6.0407295  5.261552   4.2849255
0:   3.216184   2.7300112  2.7029486  3.102028   3.572839   3.7368178
0:   1.49787    1.4027848]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.4556346 6.4574265 6.8346086 7.39994   7.8426123 8.053449  8.378176
0:  8.579066  8.672525  8.715166  8.403511  7.819832  7.244315  6.9013453
0:  6.884132  7.24251   7.6691084 7.9576526 4.789111  4.745275 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.32573  30.449596 30.4241   30.255713 30.103245 29.812998 30.385044
0:  30.865307 31.440044 31.840776 31.550743 31.383224 31.148258 31.485086
0:  32.2267   33.104824 33.823082 34.038483 31.36347  31.450558]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.764849  -6.735501  -6.281135  -5.632172  -5.028826  -4.613402
0:  -4.2021356 -3.9155068 -3.9076219 -3.9487443 -4.233087  -4.7937245
0:  -5.151751  -5.327604  -5.1593976 -4.6125197 -4.0510297 -3.5416083
0:  -5.065992  -4.9056773]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.91834164  0.8386297   1.079061    1.596551    2.0663896   2.3323483
0:   2.2580385   2.10957     1.5571685   1.1014056   0.64574003 -0.02483988
0:  -0.57538605 -1.0769081  -1.3186502  -1.0424423  -0.41257572  0.3333311
0:  -1.041636   -1.3671513 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.155255  10.270044  10.623119  11.191187  11.637796  11.923521
0:  12.035214  12.130915  11.917533  11.718838  11.3563175 10.722458
0:  10.113023   9.528222   9.320336   9.556288  10.085613  10.621955
0:   8.287493   8.119704 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.29039  11.559366 12.048191 12.642125 13.072195 13.2684   13.649078
0:  13.856013 13.980183 14.011089 13.685634 13.165064 12.650604 12.469286
0:  12.667408 13.122633 13.608652 13.826903 11.350157 11.399607]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.82972   -3.845282  -3.571197  -3.0703778 -2.5754247 -2.2774396
0:  -1.9834771 -1.8242478 -1.9018083 -1.9785028 -2.2183328 -2.5525355
0:  -2.729652  -2.653893  -2.2733617 -1.6283345 -1.0250454 -0.5769882
0:  -2.5918021 -2.7146173]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.012028   7.9889045  8.263963   8.653481   8.9616785  9.062159
0:   9.508027   9.873873  10.248684  10.547695  10.427273  10.105158
0:   9.721485   9.677226  10.022383  10.712897  11.407675  11.890272
0:   8.883974   8.625207 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.106087 17.28248  17.616653 17.955673 18.057642 17.77609  17.795086
0:  17.6904   17.58765  17.434973 16.95405  16.405579 15.966931 15.94311
0:  16.392853 17.23708  18.155651 18.80731  17.111227 17.133867]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.885909  6.9453273 7.469496  8.168669  8.655716  8.799105  8.812721
0:  8.643553  8.237733  7.8531065 7.279     6.444181  5.72525   5.2673426
0:  5.187255  5.603839  6.202251  6.7130947 4.564773  4.447465 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.0781975 9.06741   9.327602  9.721842  9.976656  9.9633465 9.907738
0:  9.740337  9.390682  9.050888  8.56021   7.8972445 7.2892613 6.8583975
0:  6.7375717 7.002385  7.46984   7.898838  5.8337708 5.9842405]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3744068  -2.3885765  -2.1194887  -1.4974289  -0.78051805 -0.17088413
0:   0.13738394  0.3871231   0.21050358  0.09008837 -0.03615189 -0.36079693
0:  -0.5738983  -0.7547512  -0.73192406 -0.3102808   0.38046026  1.1553965
0:   0.35683823  0.40188932]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.642275 14.11236  14.927467 15.700289 16.093693 16.064966 15.893993
0:  15.898195 15.896799 16.128798 16.327694 16.196941 16.000069 15.707153
0:  15.696915 16.149286 17.013916 18.016808 14.845469 15.159113]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.5762944   5.3973694   5.530756    5.880545    6.214697    6.312936
0:   6.4843106   6.596678    6.4851675   6.3644924   5.9467087   5.3294306
0:   4.722154    4.2360187   3.943256    3.9192538   3.92949     3.850688
0:   0.28660297 -0.10843801]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.717922 24.805485 24.931305 24.82631  24.445341 23.808487 23.491978
0:  23.138435 22.890394 22.636223 21.973204 21.186737 20.382143 20.056313
0:  20.32142  21.127008 22.222794 23.186243 22.229647 22.557116]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.45787   -1.9950423 -2.1592498 -2.0796876 -2.0830207 -2.308185
0:  -2.4286141 -2.5882163 -2.7899194 -2.9390273 -3.2978282 -3.849053
0:  -4.34215   -4.602793  -4.5699134 -4.1245723 -3.596745  -3.2054362
0:  -4.8881135 -5.1593947]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.844779  -6.8797474 -6.5903854 -6.037696  -5.4595776 -5.041917
0:  -4.8528643 -4.7592077 -5.0728064 -5.3204412 -5.6207314 -6.078127
0:  -6.359991  -6.581835  -6.5432453 -6.1812086 -5.6894045 -5.14802
0:  -6.715269  -6.889495 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.957375 23.85242  23.893566 23.9503   23.909662 23.532228 23.52715
0:  23.30307  23.103645 22.810413 21.991123 21.056479 20.073193 19.465128
0:  19.293552 19.597692 20.040142 20.369318 18.513994 18.336033]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.90333414 -0.6933265  -0.0725441   0.8074527   1.6361966   2.2274666
0:   2.6828313   2.944394    2.9794831   3.049776    3.049549    2.9574018
0:   3.041811    3.3718696   3.9550707   4.747717    5.4914045   5.963938
0:   4.5555477   4.6345963 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.884424 25.054823 25.342829 25.544989 25.626991 25.47917  25.587349
0:  25.541918 25.45904  25.277264 24.75451  24.15028  23.670134 23.544905
0:  23.880856 24.547794 25.388489 26.055729 24.225378 24.43457 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8414087 -6.9408207 -6.6452866 -5.881996  -5.084379  -4.4736357
0:  -4.2527294 -4.197442  -4.689749  -5.2141905 -5.8192415 -6.656049
0:  -7.3024254 -7.819441  -8.057964  -7.830495  -7.3918457 -6.809515
0:  -7.6309156 -7.790143 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.6269126 4.5707183 4.8655987 5.3619084 5.8329754 6.116383  6.6901765
0:  7.056917  7.139472  6.878376  5.9800835 4.841252  3.6703382 3.0949128
0:  3.2355464 3.9728453 4.908784  5.6341915 4.9107265 4.878933 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4775367 1.4109554 1.5840311 1.8859682 2.1794116 2.2603054 2.5431943
0:  2.7444284 2.890062  3.0193346 2.8864224 2.6332486 2.3849802 2.4662218
0:  2.872214  3.5828834 4.3584156 4.972497  3.1501598 3.0975034]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6925244 -5.637482  -5.177881  -4.282043  -3.2931094 -2.508102
0:  -2.0214467 -1.781158  -2.0378766 -2.3446202 -2.7360425 -3.2570071
0:  -3.5706248 -3.7611604 -3.6486478 -3.2688231 -2.8240924 -2.459434
0:  -3.107718  -3.1307845]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.9763584 5.1096115 5.5754757 6.157429  6.5493674 6.620345  6.7762346
0:  6.7653103 6.636533  6.440498  5.9590497 5.1977034 4.4852057 4.091502
0:  4.024104  4.3854175 4.8217754 5.125564  2.189259  2.140231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.245502  17.440018  17.89304   18.345888  18.50589   18.38296
0:  18.071192  17.8918    17.665323  17.619719  17.549965  17.207981
0:  16.842535  16.479136  16.481993  16.940996  17.736607  18.489336
0:  15.3803425 15.33808  ]
0: validation loss for strategy=forecast at epoch 34 : 0.27854040265083313
0: validation loss for velocity_u : 0.16963408887386322
0: validation loss for velocity_v : 0.2564220726490021
0: validation loss for specific_humidity : 0.16049852967262268
0: validation loss for velocity_z : 0.5001494288444519
0: validation loss for temperature : 0.09788644313812256
0: validation loss for total_precip : 0.48665204644203186
0: 35 : 20:17:41 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 35, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9145, -0.9246, -0.9320, -0.9372, -0.9404, -0.9424, -0.9438, -0.9456, -0.9481, -0.9518, -0.9569, -0.9630,
0:         -0.9702, -0.9777, -0.9852, -0.9925, -0.9993, -1.0055, -0.9267, -0.9378, -0.9451, -0.9489, -0.9500, -0.9490,
0:         -0.9474], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0103, -0.0080, -0.0052, -0.0026, -0.0015, -0.0022, -0.0047, -0.0091, -0.0151, -0.0218, -0.0282, -0.0338,
0:         -0.0375, -0.0375, -0.0330, -0.0226, -0.0060,  0.0170, -0.0412, -0.0332, -0.0250, -0.0177, -0.0121, -0.0091,
0:         -0.0088], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2896, -0.2643, -0.2542, -0.2454, -0.2363, -0.2273, -0.2136, -0.1997, -0.1856, -0.1718, -0.1568, -0.1417,
0:         -0.1265, -0.1116, -0.1025, -0.0937, -0.0849, -0.0760, -0.3077, -0.2839, -0.2767, -0.2719, -0.2673, -0.2624,
0:         -0.2536], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4588, -0.4698, -0.4819, -0.4918, -0.4962, -0.4951, -0.4896, -0.4830, -0.4819, -0.4896, -0.5072, -0.5314,
0:         -0.5556, -0.5710, -0.5677, -0.5413, -0.4940, -0.4292, -0.4962, -0.5138, -0.5259, -0.5270, -0.5171, -0.4940,
0:         -0.4632], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.8986, 0.9023, 0.9073, 0.9132, 0.9190, 0.9245, 0.9287, 0.9317, 0.9327, 0.9320, 0.9288, 0.9236, 0.9165, 0.9082,
0:         0.8996, 0.8916, 0.8847, 0.8796, 0.8761, 0.8742, 0.8733, 0.8724, 0.8713, 0.8698, 0.8677], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0408, -0.0271, -0.0180, -0.0089,  0.0025,  0.0093,  0.0002, -0.0112, -0.0203, -0.0795, -0.0659, -0.0522,
0:         -0.0340, -0.0180,  0.0002,  0.0207,  0.0412,  0.0618, -0.1639, -0.1616, -0.1525, -0.1433, -0.1319, -0.1137,
0:         -0.0932], device='cuda:0')
0: [DEBUG] Epoch 35, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.1832,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2003,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,     nan,
0:             nan,     nan, -0.2527,     nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1741,     nan, -0.2334,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2026,     nan, -0.2425,     nan,     nan,     nan,     nan,     nan,     nan, -0.2299,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2277,     nan,     nan,     nan,     nan,     nan,
0:         -0.2470,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2505,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2516,     nan,
0:             nan,     nan, -0.2527,     nan, -0.2527,     nan,     nan, -0.2527,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 35, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2399, -0.2229, -0.1855, -0.1384, -0.1073, -0.0910, -0.0768, -0.0747, -0.0847, -0.1025, -0.1428, -0.2003,
0:         -0.2500, -0.2712, -0.2624, -0.2236, -0.1812, -0.1504, -0.1623, -0.1690, -0.1538, -0.1384, -0.1384, -0.1321,
0:         -0.1238], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0652, -0.0236, -0.0213, -0.0520, -0.1089, -0.1780, -0.2658, -0.3256, -0.3325, -0.3061, -0.2613, -0.2217,
0:         -0.2339, -0.2873, -0.3485, -0.3671, -0.3328, -0.2593, -0.0647, -0.0170, -0.0030, -0.0231, -0.0713, -0.1594,
0:         -0.2458], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5873, -0.5835, -0.5758, -0.5636, -0.5462, -0.5314, -0.5254, -0.5204, -0.5214, -0.5297, -0.5393, -0.5485,
0:         -0.5454, -0.5392, -0.5395, -0.5287, -0.5214, -0.5029, -0.6046, -0.5961, -0.5840, -0.5677, -0.5555, -0.5463,
0:         -0.5355], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5136, 0.6096, 0.7592, 0.7422, 0.6331, 0.6278, 0.6139, 0.5271, 0.4379, 0.4064, 0.3959, 0.4054, 0.4126, 0.2801,
0:         0.2505, 0.4279, 0.4299, 0.2890, 0.7924, 0.6509, 0.6241, 0.5738, 0.4872, 0.4877, 0.5079], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.1699, 0.1590, 0.1522, 0.1370, 0.1237, 0.1096, 0.1025, 0.0961, 0.0943, 0.0961, 0.0965, 0.0918, 0.0830, 0.0780,
0:         0.0659, 0.0564, 0.0508, 0.0559, 0.0706, 0.0929, 0.1124, 0.1256, 0.1302, 0.1221, 0.0993], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2310, -0.2337, -0.2447, -0.2380, -0.2359, -0.2324, -0.2268, -0.2315, -0.2225, -0.2314, -0.2304, -0.2273,
0:         -0.2333, -0.2356, -0.2298, -0.2287, -0.2289, -0.2345, -0.2274, -0.2251, -0.2333, -0.2281, -0.2236, -0.2327,
0:         -0.2338], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.22028139233589172; velocity_v: 0.34691378474235535; specific_humidity: 0.17462393641471863; velocity_z: 0.6391617655754089; temperature: 0.20458436012268066; total_precip: 0.4065936505794525; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2067973017692566; velocity_v: 0.332466185092926; specific_humidity: 0.189634770154953; velocity_z: 0.6361268162727356; temperature: 0.15178614854812622; total_precip: 0.658447265625; 
0: epoch: 35 [1/5 (20%)]	Loss: 0.53252 : 0.31234 :: 0.20872 (2.84 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19728533923625946; velocity_v: 0.3512800335884094; specific_humidity: 0.17732369899749756; velocity_z: 0.6515154838562012; temperature: 0.1600600928068161; total_precip: 0.646038830280304; 
0: epoch: 35 [2/5 (40%)]	Loss: 0.64604 : 0.32883 :: 0.21221 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19338540732860565; velocity_v: 0.29346829652786255; specific_humidity: 0.19198614358901978; velocity_z: 0.5498485565185547; temperature: 0.16825240850448608; total_precip: 0.38998815417289734; 
0: epoch: 35 [3/5 (60%)]	Loss: 0.38999 : 0.26322 :: 0.21056 (15.89 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19689182937145233; velocity_v: 0.30009883642196655; specific_humidity: 0.19519440829753876; velocity_z: 0.5864031910896301; temperature: 0.13763821125030518; total_precip: 0.5375189781188965; 
0: epoch: 35 [4/5 (80%)]	Loss: 0.53752 : 0.29152 :: 0.20915 (16.38 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 4.76837158e-07 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 2.00271606e-05 1.11579895e-04 2.33173370e-04
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 4.29153442e-06 4.29153442e-06
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 2.8610229e-06 0.0000000e+00 0.0000000e+00
0:  8.5830688e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 1.4305115e-06 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  3.8146973e-06 1.6212463e-05 4.8160557e-05 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 8.9168549e-05 8.2969666e-05 0.0000000e+00 0.0000000e+00
0:  7.1525574e-06 1.1920929e-05 0.0000000e+00 3.0040741e-05 3.0040741e-05
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [28.003769 27.961884 27.97968  28.048264 28.069782 27.879387 28.012695
0:  28.051197 28.119911 28.111897 27.771294 27.334152 26.959663 26.87841
0:  27.173798 27.738213 28.446476 28.95805  27.555878 27.553822]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.651, max = 2.208, mean = 0.259
0:          sample (first 20): tensor([1.8159, 1.8123, 1.8139, 1.8198, 1.8216, 1.8052, 1.8167, 1.8200, 1.8259, 1.8252, 1.7960, 1.7584, 1.7263, 1.7193,
0:         1.7447, 1.7931, 1.8539, 1.8979, 1.8698, 1.8937])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.13733  21.803364 21.437735 21.100342 20.821411 20.344929 20.748186
0:  20.960932 21.290073 21.323536 20.535717 19.777596 18.960032 18.914131
0:  19.521679 20.511108 21.441591 21.79035  22.44026  22.714945]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.1491485 5.379875  5.9131117 6.6072636 7.1400456 7.406353  7.562265
0:  7.5929394 7.347322  7.155444  6.809565  6.2588763 5.803838  5.490878
0:  5.531639  5.9641137 6.5515623 7.089656  4.9076333 5.1434803]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.913513  -16.832714  -16.144604  -15.054627  -13.792794  -12.642436
0:  -11.192012  -10.022727   -9.079269   -8.253004   -7.817271   -7.743437
0:   -7.385356   -6.66475    -5.491226   -4.1454325  -3.189928   -2.8436728
0:   -4.5551653  -4.319933 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.743856  4.654544  4.482773  4.3269954 4.1785984 4.072307  4.7192793
0:  5.518113  6.293199  6.9339614 6.8799596 6.6418753 6.2030344 6.4212255
0:  6.974262  7.7412243 8.271313  8.2747    7.813259  7.977689 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4719863  -2.3480706  -1.8184247  -0.929873   -0.04288769  0.6301813
0:   1.052669    1.2840486   1.0779614   0.8627248   0.51653194 -0.11419582
0:  -0.6077552  -1.0374436  -1.1618476  -0.8052597  -0.14562368  0.632535
0:  -0.10235262  0.0091114 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.30906   3.3904946 3.8309357 4.6145544 5.3747115 5.9795237 6.298334
0:  6.5527506 6.3724346 6.218074  5.9468026 5.3441935 4.779116  4.169858
0:  3.8561225 4.0287113 4.612775  5.3431463 3.113209  3.0087197]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.143409 12.239539 12.553623 12.930649 13.193258 13.342535 13.627966
0:  13.903481 14.151852 14.38549  14.394201 14.309193 14.339018 14.71398
0:  15.593521 16.768444 18.046478 19.094017 17.987415 18.41412 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.2809305  8.135532   8.390948   8.782942   8.991553   8.862107
0:   8.9613     8.9154415  8.916624   8.864064   8.462497   7.9018397
0:   7.382254   7.2498627  7.637055   8.529978   9.579193  10.53244
0:   8.47916    8.46274  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.343777 30.411354 30.28341  29.929653 29.520168 28.922043 29.326092
0:  29.647251 30.128284 30.316086 29.613522 28.961304 28.239979 28.303997
0:  29.055061 30.170464 31.207134 31.620224 33.246956 33.605312]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.5898123 5.167213  6.0614367 7.0237207 7.747168  8.097533  8.47107
0:  8.613022  8.616167  8.595106  8.319049  7.886552  7.497484  7.4661713
0:  7.818253  8.528971  9.292467  9.852921  7.874512  8.208836 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.3529806  -0.18779135 -0.03996992 -0.03815937 -0.15138292 -0.610373
0:  -0.46933174 -0.582211   -0.42411327 -0.3555894  -0.73176    -0.9887333
0:  -1.2450008  -0.7011194   0.45045042  2.1017876   3.7702703   5.122027
0:   9.323145   10.061584  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.09926  31.965952 31.73187  31.37356  31.018482 30.437088 30.385002
0:  30.044626 29.563618 28.732536 27.226612 25.888319 24.69366  24.297726
0:  24.69142  25.480385 26.322216 26.802189 26.568106 26.48392 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.7529187   0.81400156  1.1716557   1.7328224   2.2299142   2.5123396
0:   2.6216602   2.5508752   2.1590242   1.747942    1.2416401   0.57583666
0:   0.10820532 -0.22307205 -0.2542162   0.04713631  0.43484354  0.75998116
0:  -0.87149954 -1.0547934 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3948007 -5.715298  -5.637321  -5.081571  -4.395535  -3.816362
0:  -3.6546474 -3.5954146 -4.0312057 -4.3917527 -4.713308  -5.239079
0:  -5.620642  -5.9714713 -6.0486274 -5.6214643 -4.7917    -3.7564502
0:  -4.019916  -4.281838 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.211605  8.066198  8.3117485 8.886969  9.377023  9.674424  9.623588
0:  9.524746  9.082693  8.76202   8.453067  7.885333  7.391435  6.9436646
0:  6.8038273 7.092347  7.7508225 8.371303  7.088957  7.0743456]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.7547    -11.851267  -11.547291  -10.835123  -10.034433   -9.405178
0:   -9.058386   -8.927454   -9.329109   -9.758867  -10.312567  -11.087793
0:  -11.557608  -11.873838  -11.828835  -11.390385  -10.888958  -10.32605
0:  -11.0601635 -11.313784 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.2837124   0.5676155   1.2066016   1.8736444   2.2437615   2.171244
0:   1.8846574   1.232058    0.24917603 -0.7135811  -1.7968278  -2.9131804
0:  -3.783248   -4.192277   -4.2319994  -3.8532887  -3.4436512  -3.1693935
0:  -4.291749   -4.1860695 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.139224  8.782907  8.701998  8.657812  8.483093  8.06255   8.08308
0:  8.033144  8.042541  7.9543924 7.412952  6.7137523 5.975175  5.7657557
0:  6.032658  6.750593  7.4428654 7.811354  6.03062   5.7359786]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.437649  8.437068  8.63486   8.991537  9.192032  9.174583  8.855081
0:  8.547836  7.946728  7.4847918 7.046735  6.3953533 5.7571454 5.1129494
0:  4.6910896 4.699498  5.0898643 5.620788  2.6009476 2.1151943]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.690191  -16.616135  -16.03486   -15.032949  -13.992449  -13.1142025
0:  -12.747574  -12.6238785 -12.973143  -13.386448  -13.759354  -14.210829
0:  -14.370472  -14.330887  -13.965242  -13.292229  -12.50045   -11.6043415
0:  -12.89168   -12.695391 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.303223  -8.276924  -7.813375  -6.9401526 -6.0498796 -5.3926406
0:  -5.021252  -4.922544  -5.2976804 -5.7215123 -6.263025  -6.957726
0:  -7.3629613 -7.5872827 -7.4633985 -6.9414105 -6.3569274 -5.8247886
0:  -5.514337  -5.2838855]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0254602  -0.8178706  -0.2751646   0.4360175   0.9381075   1.11099
0:   1.0697627   0.9687891   0.65875864  0.45604944  0.21050215 -0.2678094
0:  -0.6353078  -0.889832   -0.8328862  -0.2871809   0.51656437  1.3073373
0:   0.18037653  0.34116554]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.228584  11.019344  12.550627  14.090106  15.132439  15.448888
0:  15.57108   15.4788475 15.441756  15.463718  15.238241  14.552309
0:  13.771328  12.928251  12.5077915 12.673276  13.303917  14.140345
0:  11.089684  11.165508 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7993054 -4.828235  -4.5424943 -3.9810185 -3.4445224 -3.0868278
0:  -3.033833  -3.0957713 -3.5927043 -4.0514836 -4.572656  -5.226294
0:  -5.681558  -5.995958  -6.0223737 -5.7444005 -5.365483  -5.012379
0:  -5.7740855 -5.9023185]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.295701  -8.300491  -7.957782  -7.3728523 -6.83535   -6.4792747
0:  -6.3288097 -6.304042  -6.562623  -6.792137  -7.081768  -7.5421076
0:  -7.7922955 -7.858453  -7.610381  -6.9410677 -6.143711  -5.384117
0:  -6.287327  -6.0648937]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.822735  -6.5357704 -7.0792103 -7.345642  -7.400058  -7.310875
0:  -6.712599  -6.0439034 -5.4678354 -4.8631487 -4.4247    -4.0685883
0:  -3.4185658 -2.302391  -0.7690878  1.0531249  2.7904868  4.0596633
0:   3.607814   4.1395435]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.33993   14.091246  13.909601  13.613445  13.163797  12.409403
0:  12.289345  12.127756  12.153795  12.144283  11.538962  10.898762
0:  10.097357   9.921774  10.26827   11.0503235 11.860113  12.345818
0:  12.1443615 12.059565 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.5547185  -4.614556   -4.347569   -3.8436074  -3.3644576  -3.1081843
0:  -2.7470164  -2.5478225  -2.3888354  -2.2385721  -2.2526293  -2.3417473
0:  -2.337984   -2.0164647  -1.4212952  -0.54704237  0.29044628  0.8935356
0:  -0.83383226 -0.8857293 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.982391 18.084133 18.386496 18.693468 18.91566  19.020737 19.32584
0:  19.571587 19.78067  19.936975 19.825731 19.583483 19.47614  19.61387
0:  20.161964 20.995493 21.94399  22.778452 20.188997 20.37996 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.886393 14.120046 14.668505 15.199246 15.386324 15.137786 14.90841
0:  14.542548 14.159786 13.821609 13.265699 12.483982 11.72764  11.212088
0:  11.045973 11.259958 11.669011 11.970558  8.907285  9.128241]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.08091  31.131653 31.070953 30.795204 30.404171 29.754684 29.991978
0:  30.216084 30.566113 30.705189 29.961746 29.0117   27.772507 27.013012
0:  26.776878 26.931221 27.156322 27.043633 24.396976 24.40623 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.2197576  8.064748   9.109584  10.125834  10.923765  11.4652605
0:  11.864235  12.094023  11.957146  11.702864  11.249552  10.65276
0:  10.152376   9.840378   9.859798  10.105334  10.402749  10.597529
0:   6.379946   6.6036572]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.0299363   0.75513506  0.8720956   1.4299984   2.0192466   2.3736496
0:   2.473507    2.3894677   1.8411531   1.2815261   0.5798011  -0.3834901
0:  -1.2826209  -2.0393052  -2.482679   -2.400333   -2.0222545  -1.5495763
0:  -2.5153766  -2.8813767 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.633022 13.443749 13.6164   13.94809  14.16024  14.046115 14.111828
0:  14.052542 14.014138 13.964916 13.629932 13.187732 12.783299 12.660514
0:  12.968358 13.685974 14.617132 15.396885 13.428867 13.373827]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.730806 24.379015 23.910082 23.331839 22.677223 21.758705 21.517197
0:  21.112993 20.866219 20.431492 19.44166  18.501429 17.559534 17.188139
0:  17.408089 18.051184 18.828915 19.337185 19.260265 19.089968]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.8276563 -4.664923  -4.148103  -3.3070111 -2.4724507 -1.8116527
0:  -1.425364  -1.1381783 -1.3039908 -1.4569116 -1.7216897 -2.308086
0:  -2.782823  -3.2004466 -3.3672767 -3.071319  -2.444303  -1.7362962
0:  -1.8380351 -1.9655213]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.0964885 6.162796  6.5309143 7.1381836 7.646859  7.949767  7.982793
0:  7.9619117 7.5554266 7.2323713 6.8433714 6.2853937 5.8737164 5.540256
0:  5.5300064 5.7993693 6.1582313 6.414409  3.9608521 3.905519 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.615173  -4.8285794 -4.6348796 -3.9538817 -3.1599317 -2.5296206
0:  -2.38374   -2.4453216 -3.1508079 -3.8983846 -4.7582736 -5.867724
0:  -6.8178096 -7.68018   -8.2178135 -8.201887  -7.726624  -6.978311
0:  -8.348152  -9.069145 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.45898   4.2924695 4.399624  4.8652825 5.3606963 5.769895  5.8176494
0:  5.8553247 5.435074  5.0754824 4.665077  3.9663477 3.28624   2.5969028
0:  2.2307525 2.4462724 3.2114096 4.239848  2.968041  2.5217638]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.340027 14.712105 15.366934 16.063562 16.56824  16.81378  17.192078
0:  17.482328 17.700775 17.846651 17.631035 17.14062  16.583149 16.18764
0:  16.064117 16.184343 16.292559 16.171177 12.357371 12.505574]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.120918 16.78747  17.6723   18.533497 19.179522 19.59824  20.080217
0:  20.46298  20.774292 21.033314 21.059141 20.866707 20.813831 20.943293
0:  21.473831 22.239939 23.002268 23.509588 21.24052  21.446898]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.27734  36.9197   36.40266  35.85347  35.272404 34.422348 34.372757
0:  34.1894   34.194122 33.997852 33.073326 32.161335 31.180656 30.71476
0:  30.798153 31.155647 31.503502 31.392906 30.083565 29.857784]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.510407  -2.570747  -2.3629193 -1.8905897 -1.3433571 -0.8952284
0:  -0.7261734 -0.6182747 -0.9529319 -1.2905698 -1.685152  -2.2968645
0:  -2.7759137 -3.195374  -3.3238254 -3.0942101 -2.579557  -1.9912195
0:  -3.0354924 -3.0931296]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.215831  15.227779  15.498979  15.8251915 16.02704   16.00351
0:  16.068157  16.039051  15.943403  15.838436  15.500988  15.003309
0:  14.547562  14.343189  14.514193  15.039793  15.690738  16.261261
0:  13.545208  13.535242 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.932641   -1.1181989  -0.8691025  -0.1745739   0.5949607   1.2195382
0:   1.4740634   1.6396961   1.3011527   1.0445404   0.72633505  0.09978724
0:  -0.46609545 -1.1050649  -1.4888425  -1.3717675  -0.8466029  -0.14627409
0:  -2.10743    -2.3988185 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.09433603  0.13144064  0.58962965  1.3945622   2.277883    3.1323056
0:   4.1189585   4.987547    5.65074     6.342226    6.986263    7.612999
0:   8.565831    9.763756   11.279158   12.949085   14.535367   15.868166
0:  15.501356   16.241888  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5384917  -1.6683965  -1.4211869  -0.8023429  -0.14288521  0.34292173
0:   0.5342927   0.572906    0.15015125 -0.21926165 -0.64690876 -1.2302132
0:  -1.6273456  -1.9211545  -1.8958087  -1.4506955  -0.7844157  -0.08925724
0:  -1.8430257  -2.0436864 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1860323  -3.9944     -3.365816   -2.3950672  -1.4881716  -0.83201694
0:  -0.34523058 -0.06432533 -0.15666485 -0.2835822  -0.55711746 -1.0401163
0:  -1.379097   -1.5413208  -1.4111729  -0.92170525 -0.35492563  0.11992693
0:  -1.9037609  -1.8347769 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.262928   -8.570097   -8.468817   -7.872126   -7.1502995  -6.5123515
0:   -6.3867593  -6.4019227  -6.9953074  -7.5611978  -8.126385   -8.933531
0:   -9.590448  -10.197916  -10.48929   -10.265009   -9.559465   -8.596779
0:   -8.9652405  -9.334934 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5180416 3.5580401 3.9233158 4.523921  5.0580235 5.3965535 5.669151
0:  5.8071156 5.636657  5.483145  5.133154  4.5881433 4.1416035 3.8295283
0:  3.7934992 4.0833073 4.47032   4.8080435 2.6872373 2.7346234]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.968205  -11.98673   -11.593074  -10.815749  -10.061059   -9.437864
0:   -8.897975   -8.353988   -7.919263   -7.3900433  -6.7936783  -6.309353
0:   -5.67815    -4.832984   -3.7310047  -2.2855816  -0.7424736   0.6962571
0:    1.6668725   2.0380409]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.816429  -9.790131  -9.508831  -9.097389  -8.764458  -8.586489
0:  -8.045105  -7.534942  -7.041639  -6.6101336 -6.6269717 -6.9205728
0:  -7.237885  -7.1320195 -6.584844  -5.6442065 -4.6465774 -3.9312005
0:  -4.354193  -4.174518 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.481977 11.598841 12.007658 12.579286 13.144783 13.524435 14.027754
0:  14.399871 14.665166 14.912253 14.907368 14.736229 14.579639 14.498473
0:  14.671669 15.019607 15.449726 15.715374 14.774952 14.72694 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.486621  -2.540988  -2.2059965 -1.6021042 -1.1081061 -0.8604369
0:  -0.8820524 -0.93889   -1.2743511 -1.5193262 -1.8009119 -2.335556
0:  -2.7724571 -3.1536074 -3.2619982 -2.9003735 -2.2694645 -1.6005564
0:  -3.7965918 -4.004904 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.054512 17.216412 17.687061 18.26128  18.60537  18.645119 18.692257
0:  18.73254  18.625277 18.542345 18.26586  17.738672 17.338627 17.037458
0:  17.079819 17.414173 17.798761 18.024136 14.885153 14.545538]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.0448866 -5.501354  -5.530799  -5.0865316 -4.5271597 -4.096258
0:  -4.11833   -4.257657  -4.9034348 -5.4725866 -6.0205946 -6.781623
0:  -7.3519354 -7.876524  -8.114924  -7.8146667 -7.145109  -6.287204
0:  -7.111368  -7.4912243]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.375667  -10.210986   -9.563808   -8.555442   -7.622547   -7.0137367
0:   -6.766881   -6.7633457  -7.224328   -7.650977   -8.17087    -8.847074
0:   -9.285327   -9.539707   -9.477228   -9.094784   -8.735169   -8.47354
0:   -9.6937     -9.692594 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.9624577 -6.927126  -6.4991302 -5.7644033 -4.9965677 -4.446863
0:  -4.095046  -3.9692574 -4.3053274 -4.6430187 -5.090766  -5.6834984
0:  -5.992614  -6.167844  -6.046666  -5.7031937 -5.4053907 -5.1658497
0:  -6.481595  -6.3922462]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.40213  25.008694 24.44434  23.786976 23.180931 22.339676 22.528961
0:  22.592232 22.912422 22.992886 22.245646 21.612709 20.829794 20.76839
0:  21.31956  22.211267 23.047626 23.270767 23.940685 23.791748]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.480951  4.4171495 4.62918   5.0246572 5.3288383 5.4714513 5.6459885
0:  5.785911  5.730817  5.6795197 5.4546638 5.022798  4.68826   4.507335
0:  4.6121264 5.0709505 5.6784654 6.198056  3.8932595 3.966771 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.326557  16.652523  17.05426   17.312283  17.37768   17.200144
0:  17.186842  17.15716   17.081007  16.956825  16.55066   16.00282
0:  15.490168  15.2697525 15.420071  15.842934  16.324205  16.68072
0:  14.481646  14.566104 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.670631 24.543299 24.605091 24.737625 24.776875 24.429193 24.28845
0:  23.891344 23.43862  22.895578 21.950268 20.973618 20.102304 19.606203
0:  19.664549 20.171778 20.91022  21.551249 19.812016 19.654549]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.0549   20.576492 20.038353 19.424845 18.887705 18.173779 18.22564
0:  18.225864 18.40539  18.400959 17.726402 17.132397 16.461185 16.34156
0:  16.681904 17.271727 17.712254 17.720573 17.450794 17.179443]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.255956  14.735971  15.367327  15.972618  16.288973  16.1491
0:  15.920858  15.487055  14.88521   14.1143055 13.088958  12.008377
0:  11.20446   10.926561  11.394037  12.521573  14.001711  15.362185
0:  14.303142  14.590977 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.548447  18.626175  17.94698   17.368275  16.697052  15.780706
0:  15.11611   14.350877  13.616816  12.847523  11.726015  10.350113
0:   8.980469   7.99564    7.571369   7.8120823  8.521082   9.281791
0:   9.019037   9.357577 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.76678  27.875595 27.965141 27.930714 27.845188 27.550137 28.029118
0:  28.352974 28.681988 28.7213   27.981066 27.249287 26.466387 26.420536
0:  27.053568 28.156893 29.305548 30.022387 30.937641 30.978592]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8449898 -2.6511612 -2.0520592 -1.1896172 -0.3241825  0.4023304
0:   1.1829557  1.8730445  2.3324406  2.7392895  2.9048288  2.8059363
0:   2.808686   2.9685717  3.4557428  4.193696   4.9767227  5.5759397
0:   5.083068   5.636044 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4754405  -2.4399772  -2.015308   -1.2777028  -0.5731063  -0.07128096
0:   0.15175056  0.28368044  0.04060841 -0.11783981 -0.3113222  -0.72019196
0:  -0.9749441  -1.2144275  -1.1900125  -0.76540995 -0.12790823  0.5255389
0:  -1.5742979  -1.9818597 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.201961  -10.25562    -9.866088   -9.028081   -8.115742   -7.386345
0:   -7.1475563  -7.1187973  -7.678026   -8.241302   -8.804685   -9.547095
0:  -10.041612  -10.450391  -10.543086  -10.204141   -9.587703   -8.817808
0:   -9.35031    -9.579627 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [  1.8594017    1.7654595    1.9832623    2.1509583    1.9809959
0:    1.2248292    0.11414146  -1.5889583   -3.7558622   -5.915513
0:   -8.169902   -10.395402   -12.049794   -13.102321   -13.460507
0:  -13.2120905  -12.818189   -12.2918005  -11.183631   -10.986695  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.015846  17.359215  17.93155   18.386059  18.620655  18.6054
0:  18.617355  18.541645  18.332445  17.975018  17.302448  16.380444
0:  15.620233  15.1270895 15.149307  15.5858135 16.277874  17.008373
0:  14.172937  14.553036 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.855538   -2.538055   -1.9226127  -1.0716457  -0.35640383  0.1158762
0:   0.3356223   0.48061848  0.4374752   0.5186844   0.6447892   0.6780839
0:   0.80319643  1.0363479   1.5243201   2.34994     3.378246    4.415379
0:   3.5324447   4.297963  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.92037    -9.201256   -9.027012   -8.322956   -7.4817786  -6.8166146
0:   -6.665129   -6.7477984  -7.421064   -8.080143   -8.7157345  -9.52136
0:  -10.070925  -10.528093  -10.672155  -10.337492   -9.670855   -8.787874
0:   -9.750721  -10.242058 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.2912426 6.600529  7.1897554 7.9300947 8.525194  8.846435  8.938141
0:  8.855352  8.456161  8.152407  7.8225174 7.3814445 7.0885963 6.9606028
0:  7.1717505 7.739446  8.529598  9.317093  8.332841  8.698744 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2634702  2.107154   2.173842   2.3808246  2.4410095  2.2759447
0:  2.258872   2.0956616  1.7923021  1.5112233  1.0564532  0.47975016
0:  0.09942436 0.12975264 0.6155815  1.3881116  2.0936093  2.4808502
0:  2.7837021  3.1745896 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.680306   -2.6992927  -2.357739   -1.7657342  -1.243547   -0.95591307
0:  -0.72755146 -0.60469675 -0.73198605 -0.86902    -1.2122073  -1.7986636
0:  -2.3057828  -2.6434388  -2.6742702  -2.3129811  -1.8383145  -1.4402871
0:  -2.7243552  -2.7043834 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.477804  15.419674  15.751572  16.439686  16.99783   17.409557
0:  17.816523  18.038733  17.989931  17.89269   17.375027  16.519268
0:  15.747776  14.973602  14.454805  14.246611  14.24334   14.289085
0:   8.847907   9.5948715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.44439  37.396828 37.365147 37.358643 37.354458 37.115562 37.49191
0:  37.694004 37.991463 38.07168  37.588528 37.0554   36.63093  36.617493
0:  37.104168 37.86569  38.651443 39.118134 36.776337 36.938606]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3422775 -3.4808054 -3.2045326 -2.5678988 -1.8976669 -1.3999014
0:  -1.1536674 -1.0743408 -1.5030231 -1.922986  -2.419776  -3.1340513
0:  -3.517397  -3.7474823 -3.5519066 -2.8926668 -2.0462646 -1.1689434
0:  -1.3405542 -1.340375 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.3012724   4.2503033   4.475751    5.0440216   5.663842    5.996125
0:   5.9199653   5.5244536   4.473076    3.4325478   2.3144832   1.0848718
0:   0.06603384 -0.8389363  -1.4237523  -1.6231732  -1.561171   -1.35881
0:  -4.365355   -4.8586936 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.54903555  0.54455805  0.8709674   1.4242349   1.8772082   2.1026502
0:   2.2747188   2.3423247   2.1801476   2.0045476   1.6727374   1.1536446
0:   0.7274523   0.48158598  0.49758053  0.834743    1.2208304   1.4680495
0:  -0.89904785 -1.272922  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.790394  -12.935703  -12.651329  -11.927574  -11.08691   -10.342377
0:  -10.0068245  -9.854727  -10.275028  -10.705629  -11.161997  -11.781122
0:  -12.114883  -12.3311405 -12.210205  -11.725313  -11.028259  -10.219467
0:  -10.298796  -10.535448 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.762976  7.7431197 7.882942  8.132531  8.205097  8.00033   7.496264
0:  6.850912  5.844256  4.94664   4.0628834 3.1189625 2.3730187 1.8061385
0:  1.6190467 1.877408  2.4441862 3.0537965 1.504066  1.2675991]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.044723   9.223852   9.695303  10.29642   10.691357  10.751795
0:  10.621501  10.3579645  9.860348   9.390223   8.828826   8.066891
0:   7.465224   7.070897   7.125223   7.6665707  8.482397   9.253045
0:   8.263373   8.581382 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4757738 1.6187181 2.0114403 2.4253411 2.6205773 2.500763  2.3457808
0:  2.1283069 1.8672776 1.7132444 1.5069451 1.1615629 0.8720679 0.8446851
0:  1.1024146 1.7489314 2.5085988 3.1900573 1.4881554 1.8390341]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.658454   -2.604848   -2.1404772  -1.4731717  -1.0041018  -0.8713589
0:  -0.7937021  -0.88315344 -1.1369138  -1.359014   -1.7887545  -2.4230146
0:  -2.9986725  -3.2913017  -3.2304678  -2.7238598  -2.0952005  -1.6101189
0:  -3.0677013  -3.0138998 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.7259598  -3.6129808  -3.1590643  -2.3224134  -1.490262   -0.84857035
0:  -0.63672876 -0.68188286 -1.3030224  -1.8848448  -2.479866   -3.1522942
0:  -3.5652313  -3.8605824  -3.8347578  -3.4027138  -2.7399416  -1.9582314
0:  -2.6935024  -2.837988  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.7788663 8.04469   8.574223  9.156836  9.50134   9.533376  9.601373
0:  9.527081  9.276178  9.000867  8.429766  7.6666727 7.0182967 6.7239017
0:  6.847013  7.377555  7.972172  8.392929  5.5884867 5.5795546]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.720106 25.792545 25.692251 25.532053 25.333809 24.875149 25.174177
0:  25.28018  25.52255  25.552244 24.947294 24.388212 23.760626 23.655743
0:  24.03265  24.744326 25.498238 25.921268 25.889866 25.821789]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.187019  2.1109495 2.4479368 3.0414946 3.5678158 3.876005  4.1726503
0:  4.401056  4.4580417 4.530607  4.4561787 4.1136274 3.7983532 3.6644273
0:  3.8110437 4.357082  5.069008  5.6556683 3.179759  3.1826491]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.802043   9.55677    9.326454   9.023455   8.41919    7.5746546
0:  6.7715726  6.0143657  5.1316805  4.2588444  3.1832018  2.0226755
0:  1.0611992  0.58098125 0.751379   1.4925923  2.4641395  3.2818851
0:  1.3300376  0.6415901 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6128569  2.7447054  3.2421541  3.8019729  4.0977526  4.0188303
0:  3.8180337  3.5856578  3.2078683  2.9058836  2.5163293  1.8894677
0:  1.3677664  1.0161972  0.96698713 1.4108067  2.0291348  2.5673604
0:  0.22797012 0.33229208]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.0588   19.308916 19.645857 19.949682 20.135834 20.050241 20.415104
0:  20.65628  21.008167 21.221767 21.030968 20.714725 20.452137 20.513954
0:  21.019222 21.923643 23.071562 24.062277 23.296572 23.562403]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.06566954  0.43293238  1.1251512   1.9816461   2.6962078   3.1016555
0:   3.20165     3.0986698   2.6178968   2.1453233   1.5649381   0.8017502
0:   0.19198799 -0.28394508 -0.40582323 -0.11188269  0.40937376  0.9695535
0:  -1.2159877  -1.2849936 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.565472 11.857016 12.416388 13.188225 13.777044 14.159449 14.266247
0:  14.458464 14.389421 14.47809  14.533798 14.227808 13.845918 13.323713
0:  13.059193 13.283345 13.987364 14.849516 12.05087  12.119822]
0: validation loss for strategy=forecast at epoch 35 : 0.32554471492767334
0: validation loss for velocity_u : 0.1764153242111206
0: validation loss for velocity_v : 0.29811665415763855
0: validation loss for specific_humidity : 0.17573688924312592
0: validation loss for velocity_z : 0.4825970232486725
0: validation loss for temperature : 0.12556642293930054
0: validation loss for total_precip : 0.6948360800743103
0: 36 : 20:21:32 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 36, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8578, -0.8658, -0.8774, -0.8981, -0.9110, -0.9073, -0.8721, -0.8308, -0.7850, -0.7447, -0.7140, -0.7023,
0:         -0.7131, -0.7386, -0.7857, -0.8446, -0.9123, -0.9754, -0.8389, -0.8573, -0.8761, -0.8989, -0.9111, -0.9031,
0:         -0.8621], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1771, -0.1851, -0.1845, -0.1899, -0.1909, -0.1956, -0.1886, -0.1773, -0.1682, -0.1598, -0.1590, -0.1608,
0:         -0.1773, -0.1936, -0.2117, -0.2189, -0.2078, -0.1874, -0.2002, -0.2035, -0.1987, -0.1965, -0.1897, -0.1903,
0:         -0.1837], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([3.2551, 2.9393, 2.6438, 2.4902, 2.3821, 2.3082, 2.1560, 1.8696, 1.5809, 1.3281, 1.0949, 0.9500, 0.8314, 0.7662,
0:         0.7299, 0.7389, 0.8289, 0.9193, 3.2272, 2.9929, 2.6417, 2.3642, 2.2483, 2.1168, 1.9681], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2161, -0.0535, -0.2823, -0.5539, -0.5396, -0.3637,  0.0664, -0.0414, -0.1437,  0.0488,  0.3579,  0.5086,
0:          0.6043,  0.6395,  0.5119,  0.3447,  0.0477, -0.0854, -1.6484, -0.4901, -1.0049, -1.4372, -0.8586, -0.1085,
0:          0.1885], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.3003, 0.4152, 0.5612, 0.5708, 0.5287, 0.4809, 0.4581, 0.3693, 0.2404, 0.1329, 0.1012, 0.1368, 0.1854, 0.2920,
0:         0.4265, 0.6163, 0.7783, 0.8416, 0.7557, 0.6021, 0.5347, 0.5206, 0.5333, 0.4476, 0.3543], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 2.8595,  1.4923,  0.9534,  0.8590,  1.2271,  1.9811,  1.1159, -0.0863, -0.1604,  5.2376,  3.8143,  3.0339,
0:          2.8678,  2.9383,  1.3466, -0.0277, -0.1556, -0.1664,  5.6929,  5.8901,  4.7512,  4.1967,  2.1962,  0.2567,
0:         -0.1496], device='cuda:0')
0: [DEBUG] Epoch 36, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,  1.4529,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.4276,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  1.9979,     nan,     nan,     nan,  1.2235,  0.8172,     nan,     nan,     nan,     nan,  1.8640,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.5246,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  5.3798,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  5.6355,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  1.4864,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1269,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  4.2732,     nan,     nan,     nan,     nan,
0:             nan,     nan,  1.4075,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  1.8951,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  9.0940,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  5.0822,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  6.0646,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          2.1974,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          3.0686,     nan,     nan])
0: [DEBUG] Epoch 36, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7825, -0.8075, -0.8181, -0.7987, -0.7824, -0.7757, -0.7955, -0.8364, -0.9167, -0.9994, -1.0839, -1.1622,
0:         -1.2053, -1.2202, -1.1915, -1.1303, -1.0501, -0.9761, -0.7161, -0.7871, -0.8334, -0.8437, -0.8362, -0.8190,
0:         -0.8126], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1634,  0.1581,  0.1226,  0.0687,  0.0167, -0.0316, -0.0727, -0.1003, -0.1112, -0.1181, -0.1125, -0.0943,
0:         -0.0811, -0.0886, -0.1059, -0.0996, -0.0700, -0.0273,  0.1738,  0.1848,  0.1534,  0.0987,  0.0385, -0.0165,
0:         -0.0548], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([3.6215, 3.4365, 3.1504, 2.8011, 2.4588, 2.1826, 2.0276, 1.9493, 1.9523, 1.9795, 2.0245, 2.0525, 2.1120, 2.2150,
0:         2.2942, 2.4668, 2.6554, 2.8132, 3.5982, 3.4316, 3.2009, 2.8823, 2.5821, 2.3555, 2.2271], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3125, -0.3042, -0.6619, -1.1643, -1.6974, -2.0977, -2.1598, -1.7741, -1.3865, -1.3539, -1.4312, -1.5194,
0:         -1.3533, -0.9774, -0.6867, -0.2143,  0.1894,  0.1762, -0.1295, -0.1626, -0.5199, -1.0015, -1.5428, -1.8897,
0:         -1.9787], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.9175, 0.8014, 0.7268, 0.7141, 0.7835, 0.9137, 1.0586, 1.1780, 1.2558, 1.2915, 1.3017, 1.2874, 1.2466, 1.1620,
0:         1.0281, 0.8818, 0.7706, 0.7367, 0.7814, 0.8507, 0.8783, 0.8178, 0.6696, 0.4865, 0.3014], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([2.4119, 2.5565, 2.7227, 2.8372, 2.9791, 3.0558, 3.0893, 3.0566, 2.9201, 2.3001, 2.4197, 2.5437, 2.6526, 2.7644,
0:         2.8350, 2.8639, 2.8723, 2.7906, 2.1151, 2.1748, 2.2161, 2.2874, 2.3389, 2.4581, 2.4826], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.22425344586372375; velocity_v: 0.37518689036369324; specific_humidity: 0.1763981133699417; velocity_z: 0.6228069067001343; temperature: 0.14755046367645264; total_precip: 0.5618845224380493; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19403746724128723; velocity_v: 0.28727519512176514; specific_humidity: 0.1681099683046341; velocity_z: 0.5590476393699646; temperature: 0.14386804401874542; total_precip: 0.41345930099487305; 
0: epoch: 36 [1/5 (20%)]	Loss: 0.48767 : 0.28938 :: 0.20843 (2.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20426057279109955; velocity_v: 0.31854256987571716; specific_humidity: 0.18855898082256317; velocity_z: 0.5034341216087341; temperature: 0.14046117663383484; total_precip: 0.37168964743614197; 
0: epoch: 36 [2/5 (40%)]	Loss: 0.37169 : 0.25374 :: 0.20745 (15.84 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18487045168876648; velocity_v: 0.35257774591445923; specific_humidity: 0.1810804158449173; velocity_z: 0.5627338290214539; temperature: 0.16208229959011078; total_precip: 0.8019198775291443; 
0: epoch: 36 [3/5 (60%)]	Loss: 0.80192 : 0.33964 :: 0.20703 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20673619210720062; velocity_v: 0.35742682218551636; specific_humidity: 0.1826525777578354; velocity_z: 0.5190826058387756; temperature: 0.15253500640392303; total_precip: 0.5009925365447998; 
0: epoch: 36 [4/5 (80%)]	Loss: 0.50099 : 0.28553 :: 0.20971 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0.]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 3.8623810e-05 5.7697296e-05 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 1.7166138e-05 5.1498413e-05 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5762787e-05 6.7710876e-05
0:  7.4386597e-05 4.9591064e-05 1.1920929e-05 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [ 8.255181   8.379301   8.995888   9.756945  10.329266  10.552829
0:  10.8882885 11.211306  11.617182  12.07608   12.365669  12.460739
0:  12.617805  12.979165  13.763264  14.998592  16.44931   17.794123
0:  15.378894  15.770922 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.614, max = 1.713, mean = -0.158
0:          sample (first 20): tensor([0.0884, 0.0985, 0.1486, 0.2104, 0.2568, 0.2750, 0.3022, 0.3285, 0.3614, 0.3987, 0.4222, 0.4299, 0.4427, 0.4720,
0:         0.5357, 0.6360, 0.7538, 0.8630, 0.3126, 0.3218])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.224133 10.824703 10.592119 10.330562 10.019284  9.478858  9.672736
0:   9.822229 10.222427 10.633633 10.48406  10.265415  9.939987 10.206169
0:  10.959191 12.162014 13.350264 14.117046 13.173058 13.42087 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.39579582  0.6816058   1.2689652   1.9782181   2.4924743   2.6062596
0:   2.7104056   2.591919    2.3249667   2.0577397   1.51022     0.79971075
0:   0.06067514 -0.24790907 -0.05653381  0.8065057   2.0141873   3.2229168
0:   5.0454865   6.0887012 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.128292 26.07362  26.13168  26.1399   26.060955 25.764378 25.848343
0:  25.794075 25.753323 25.611164 25.044765 24.419868 23.860985 23.691128
0:  24.100761 24.925663 25.978157 26.942371 26.368721 26.366142]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.9762564 -7.2479496 -7.0541744 -6.480277  -5.8940973 -5.527646
0:  -5.475774  -5.5824366 -6.0272303 -6.365196  -6.6810627 -7.0752497
0:  -7.30375   -7.493866  -7.5368004 -7.385172  -7.2111053 -7.087533
0:  -9.54568   -9.9197855]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.02817059  0.15611935  0.66991186  1.4020243   2.0045686   2.3663907
0:   2.4874425   2.5973113   2.4063463   2.3026004   2.1200187   1.6602592
0:   1.2378168   0.8398223   0.72079086  1.0492954   1.6972456   2.385016
0:   0.90434027  1.0142369 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.4455   29.119194 28.537565 27.842201 27.15893  26.246021 26.288078
0:  26.298752 26.522215 26.541792 25.735506 24.988342 24.06685  23.742708
0:  23.93005  24.400421 24.836172 24.815464 25.532183 25.493141]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.7341385 15.163956  15.810057  16.479412  17.181913  17.866623
0:  18.926085  19.99111   21.043234  22.017849  22.568369  23.066467
0:  23.563683  24.375046  25.558445  26.871307  28.106024  29.010798
0:  28.313686  28.648762 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.160427  3.1747844 3.530043  4.094699  4.510708  4.6897173 4.791215
0:  4.8499103 4.695506  4.5726953 4.300806  3.808958  3.3736317 3.1176324
0:  3.1857738 3.6154804 4.167693  4.5981894 2.987401  2.915879 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.419378  10.512134  10.749501  11.022723  11.292906  11.368092
0:  11.986056  12.4950485 13.155592  13.7274685 13.81922   13.838724
0:  13.772663  14.209982  15.017075  16.178818  17.401382  18.309072
0:  17.947258  18.433138 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.5658226  -2.5557547  -2.1064463  -1.2880964  -0.50360584  0.07098198
0:   0.46330786  0.63995314  0.4025879   0.10210609 -0.40937614 -1.1689816
0:  -1.7548761  -2.1917143  -2.2836094  -1.9537954  -1.4845762  -1.0460801
0:  -2.4999266  -2.5192213 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.08646  23.590603 24.250118 24.765697 24.979393 24.80854  24.780846
0:  24.752151 24.876583 25.040955 25.046104 24.840054 24.768745 24.822609
0:  25.244629 25.991821 26.934895 27.863651 25.249584 25.530807]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.744171 29.480328 30.109564 30.57289  30.826557 30.802511 30.831654
0:  30.766804 30.571074 30.455732 30.214857 29.937849 29.832996 29.951578
0:  30.339579 30.903618 31.42474  31.749193 28.707514 29.521631]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.9899297 7.330113  7.952999  8.692932  9.260693  9.557243  9.724138
0:  9.706787  9.499662  9.343274  9.132477  8.858327  8.670439  8.624559
0:  8.743565  9.041935  9.36199   9.589975  6.8322353 6.1883955]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.75674   -9.65508   -9.143699  -8.185779  -7.1278563 -6.2337294
0:  -5.5609436 -5.102872  -5.2173777 -5.2893753 -5.5545664 -6.006768
0:  -6.267109  -6.4284306 -6.241065  -5.7115545 -5.108197  -4.4943275
0:  -5.3187027 -5.6035156]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.192536   7.2498727  7.571056   8.032956   8.419893   8.632105
0:   8.94853    9.163681   9.233048   9.2645645  9.024611   8.6482525
0:   8.346055   8.320565   8.646098   9.287935   9.949268  10.417183
0:   9.040319   9.2189045]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.465145 24.982466 25.744339 26.332382 26.35709  25.816149 25.215786
0:  24.805655 24.431667 24.334408 24.203674 23.619951 22.927355 22.123945
0:  21.485416 21.197708 21.07131  20.836536 13.913348 13.544737]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.291218  17.460087  17.893677  18.36525   18.537256  18.328812
0:  17.977337  17.652203  17.261368  17.016218  16.72289   16.198616
0:  15.656121  15.104189  14.880667  15.075195  15.6154375 16.21252
0:  13.417422  13.540163 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.8348563   2.714746    2.9809961   3.4152493   3.629469    3.5025256
0:   3.2480042   2.745486    1.9329529   1.1719589   0.19078922 -0.9337244
0:  -1.949831   -2.6314287  -2.9195566  -2.7771816  -2.570386   -2.4938097
0:  -3.3144665  -3.3057594 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.5490885 12.453154  12.689736  13.071273  13.389729  13.482033
0:  13.595307  13.543352  13.199111  12.904497  12.408952  11.770377
0:  11.352306  11.112904  11.20709   11.593386  12.101389  12.557561
0:  10.841701  10.972291 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.074379  8.022407  8.221603  8.598124  8.896582  9.023786  9.107267
0:  9.053007  8.750475  8.430925  7.961671  7.3500996 6.875259  6.6296616
0:  6.7937613 7.3457174 8.045768  8.634646  7.3256025 7.5092344]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.319263 29.519344 29.329796 28.757347 28.034191 27.107262 27.245926
0:  27.336685 27.674492 27.757969 27.012085 26.423141 25.759178 25.94374
0:  26.787498 27.961857 29.001186 29.326443 30.561535 30.878563]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.5722556 2.5391617 2.6781874 2.8810701 3.004952  2.9470668 3.1117392
0:  3.2365062 3.3108363 3.2919517 2.9362898 2.4616423 1.9454794 1.7812662
0:  1.9639444 2.470096  2.9950023 3.344795  1.368998  1.3247619]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.3871965  -2.3623276  -1.9165401  -1.2124462  -0.6422391  -0.36618042
0:  -0.1563034  -0.02104616 -0.02348661  0.02584219 -0.02396107 -0.34084082
0:  -0.53133774 -0.55422163 -0.29707813  0.39677143  1.1858239   1.8525076
0:   0.10638714  0.23158121]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5597749  3.5355828  3.8516626  4.306568   4.6369724  4.6918173
0:  4.7467055  4.6552367  4.3798013  4.105327   3.6286952  2.9911308
0:  2.448822   2.207971   2.2835038  2.7257693  3.2471013  3.6490386
0:  0.72438526 0.6830797 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.6383615 -5.679189  -5.3593106 -4.7606654 -4.1500525 -3.704307
0:  -3.3144813 -3.06813   -3.2149968 -3.4375272 -3.9238372 -4.622749
0:  -5.084391  -5.3740735 -5.2483573 -4.8314815 -4.401027  -4.0499053
0:  -5.6554046 -5.651282 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.97817   -5.293099  -5.25217   -4.743295  -4.1024146 -3.5711484
0:  -3.4684005 -3.5417132 -4.2224784 -4.9005456 -5.6373286 -6.5279174
0:  -7.2159743 -7.748422  -7.926945  -7.5754    -6.828362  -5.899256
0:  -5.9837785 -6.218967 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.61922  31.79768  31.993034 32.09882  32.115543 31.878975 32.264782
0:  32.504856 32.82603  32.983845 32.500347 31.992405 31.45745  31.359272
0:  31.783352 32.533504 33.328007 33.83109  32.29433  32.446686]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.3955073   2.3707001   2.6185033   3.1635091   3.639173    3.931971
0:   3.8851776   3.7979836   3.3206956   2.9071739   2.453048    1.746192
0:   1.0548749   0.3833747   0.00790691  0.1700964   0.79690886  1.6021848
0:  -0.16336298 -0.4582634 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.975103 24.448875 25.186527 25.9256   26.357533 26.449072 26.554792
0:  26.73453  26.928694 27.305344 27.574787 27.596376 27.641518 27.712864
0:  28.101418 28.81054  29.727571 30.552418 26.448965 26.867014]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.166781  -6.3492093 -6.1786675 -5.6200633 -4.978952  -4.4326596
0:  -4.1688333 -3.9633183 -4.215401  -4.429402  -4.688461  -5.1478896
0:  -5.4149604 -5.6161847 -5.5588565 -5.1341195 -4.5252204 -3.897037
0:  -4.49124   -4.6015353]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.1881    9.479807 10.197365 11.046033 11.671625 11.939497 12.124317
0:  12.247193 12.287999 12.402042 12.432843 12.26984  12.220427 12.348366
0:  12.91394  13.938143 15.283096 16.640028 15.007877 15.500286]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.039656  -6.963244  -6.500918  -5.674687  -4.8521953 -4.2305093
0:  -4.0077596 -3.9355016 -4.349008  -4.685982  -5.015572  -5.4699655
0:  -5.712857  -5.8289704 -5.6775174 -5.1209197 -4.38458   -3.644878
0:  -4.9367146 -5.038408 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.259342    0.91206074 -0.1548605  -0.6360903  -0.63908863 -0.27394485
0:   0.8372741   2.1847174   3.5669405   4.929558    6.064623    6.9725513
0:   7.9685993   9.358487   11.207328   13.247156   15.175268   16.486536
0:  15.975264   16.636782  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.688343 36.66501  36.722786 36.73164  36.658195 36.188343 36.051205
0:  35.708405 35.42265  35.088333 34.29043  33.418076 32.536522 31.907251
0:  31.6387   31.654596 31.838596 31.947258 29.839762 29.602137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.04552555 -0.07832527  0.29099894  1.107379    1.9677153   2.6233253
0:   3.0766358   3.2244089   2.846565    2.4121358   1.7995148   1.0007486
0:   0.47562265  0.11483765  0.14947224  0.5372591   0.9797654   1.3255458
0:  -1.0714192  -1.2935295 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.413404  5.6010184 6.04588   6.5731964 6.953191  7.1619997 7.608112
0:  7.9547696 8.201638  8.304731  8.027537  7.4521837 6.9502997 6.787881
0:  7.104881  7.747197  8.464259  8.940209  7.380093  7.4311004]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.9153748  8.106595   8.672888   9.380211  10.088629  10.556187
0:  11.116217  11.578797  11.919276  12.238505  12.316013  12.202532
0:  12.1528    12.279888  12.712482  13.457195  14.330597  15.037436
0:  13.819481  14.066624 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.45839  19.587381 19.751877 19.757175 19.591972 19.135798 19.05122
0:  18.839874 18.626348 18.30237  17.462517 16.633125 15.792856 15.521568
0:  15.805071 16.469572 17.209198 17.723995 17.624285 17.762196]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.597842   9.992365  10.597514  11.282314  11.739745  11.960911
0:  11.939259  11.979952  11.822926  11.836185  11.864709  11.688608
0:  11.561068  11.419382  11.5483265 12.079898  12.944662  13.87016
0:  12.566971  12.865886 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.873152  10.2637825 10.993104  11.691097  12.028262  11.9696665
0:  11.878214  11.859615  11.860072  12.034206  12.155802  12.061363
0:  12.0587845 12.146385  12.575241  13.423719  14.462982  15.4547615
0:  12.940885  13.44492  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.698013   -9.139499   -9.1739855  -8.780334   -8.380773   -8.208492
0:   -8.538942   -9.08303   -10.136699  -11.153022  -12.111627  -13.144887
0:  -13.855866  -14.30981   -14.378814  -13.913466  -13.174607  -12.292165
0:  -13.04906   -13.346738 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.958179   -5.8454995  -5.3140216  -4.376229   -3.3339477  -2.297645
0:  -1.1971555  -0.04208469  0.8602805   1.7313519   2.3388782   2.5585518
0:   2.7081137   2.8082314   3.076151    3.6119654   4.1784678   4.6829596
0:   2.7722168   3.1589463 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.511642 24.850805 24.961987 24.934053 24.68602  24.256626 24.723822
0:  25.065298 25.426643 25.424274 24.583895 23.729197 22.98491  23.135723
0:  24.01301  25.251772 26.521425 27.043625 28.304266 28.49717 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.504162 19.684195 20.087608 20.457777 20.565275 20.415194 20.3121
0:  20.114536 19.915672 19.76288  19.454792 19.049585 18.823938 18.819382
0:  19.218435 19.895288 20.60494  21.130037 17.817114 17.995804]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-17.663101  -18.039644  -17.882685  -17.328688  -16.82579   -16.542831
0:  -16.00062   -15.616095  -15.171755  -14.836483  -14.90855   -15.318253
0:  -15.705076  -15.751846  -15.349974  -14.642057  -14.06085   -13.803301
0:  -14.3260765 -14.19702  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.931302  -6.3041105 -6.2517443 -5.750474  -5.1739063 -4.7581234
0:  -4.7727604 -4.9047365 -5.530323  -6.099557  -6.6547937 -7.5163136
0:  -8.1924    -8.820972  -9.1244    -8.7678995 -7.955266  -6.897129
0:  -8.397889  -8.825708 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.375248  -6.426056  -6.0685472 -5.3463955 -4.6011076 -4.0637503
0:  -3.8408852 -3.7860298 -4.1895447 -4.5754223 -5.002206  -5.596058
0:  -5.9609356 -6.1962404 -6.1362467 -5.7309675 -5.2249155 -4.7146187
0:  -6.381979  -6.523512 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.996666 15.320523 15.773488 16.04106  16.054968 15.969851 16.157494
0:  16.51519  16.97757  17.430632 17.538216 17.486965 17.494164 17.880041
0:  18.807312 20.012981 21.20047  22.027647 19.939846 20.51954 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.445555 11.500105 11.700179 11.998699 12.167948 12.169821 11.876976
0:  11.514988 10.793275 10.177674  9.61095   9.032107  8.667625  8.440642
0:   8.667002  9.22267   9.948583 10.670631  9.622663 10.009373]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.767771  -10.834639  -10.487806   -9.668995   -8.72085    -7.910708
0:   -7.5282226  -7.432077   -8.0015545  -8.661959   -9.444254  -10.318407
0:  -10.887102  -11.200825  -11.112555  -10.627201   -9.94557    -9.21529
0:   -8.570244   -8.501259 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.170721  2.62875   3.3625045 4.142934  4.5757475 4.575622  4.32476
0:  3.9121435 3.2996182 2.8708837 2.4552598 1.9880128 1.6628547 1.5558143
0:  1.7493844 2.2880166 2.987197  3.6058316 2.8723865 3.1627183]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.620504 13.746224 14.227486 14.672752 14.671822 14.198582 13.516476
0:  12.987676 12.471952 12.31904  12.297139 12.067305 11.883673 11.640003
0:  11.644897 12.062613 12.77516  13.510435  9.883154  9.656595]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.9543576e+00 -2.5943942e+00 -1.8384371e+00 -8.4489632e-01
0:  -1.8053055e-03  5.1907921e-01  8.0464554e-01  8.4858179e-01
0:   5.1085186e-01  7.7613831e-02 -5.7360983e-01 -1.4762964e+00
0:  -2.2607679e+00 -2.8381934e+00 -3.0269618e+00 -2.7196231e+00
0:  -2.2066884e+00 -1.7260671e+00 -2.6782794e+00 -2.6149278e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.5878015  4.8400354  5.2743516  5.8780775  6.476156   7.020011
0:   7.8829727  8.706935   9.417864  10.094205  10.410873  10.606657
0:  10.780386  11.190844  11.930441  12.915195  13.964756  14.898957
0:  12.422482  12.791833 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2302094  -1.9949522  -1.421915   -0.6134949   0.07912016  0.5082588
0:   0.5506935   0.48107767 -0.01869059 -0.42782068 -0.8471179  -1.4594316
0:  -1.8792515  -2.2041245  -2.1535368  -1.6221056  -0.7428646   0.22658205
0:  -0.48993826 -0.26504326]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [41.692055 41.609447 41.164837 40.692802 40.31239  39.808834 40.275078
0:  40.67571  41.263115 41.51577  40.901676 40.394493 39.83445  39.864437
0:  40.561287 41.405083 42.21182  42.40606  41.30298  41.5867  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.571024 15.018574 15.701149 16.409544 17.027853 17.500717 18.404335
0:  19.270437 20.191889 21.082172 21.55382  21.917812 22.289963 23.099695
0:  24.331116 25.80113  27.188633 28.099741 27.754532 28.6269  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4149654 2.153716  2.1980731 2.4570534 2.5952454 2.5045347 2.4505816
0:  2.2623537 1.9889696 1.7348523 1.3329802 0.7691078 0.3257475 0.1377039
0:  0.3166008 0.9124055 1.6494589 2.2529066 1.059773  0.9610815]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.106575 17.925817 17.812195 17.651585 17.476168 17.027048 17.063478
0:  16.967821 16.999718 16.935066 16.438564 15.945999 15.409559 15.321903
0:  15.786623 16.63613  17.65946  18.449947 18.180103 18.183577]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.466764  13.602137  14.036055  14.47602   14.692455  14.59979
0:  14.572005  14.469155  14.32051   14.146126  13.736317  13.188305
0:  12.707958  12.454294  12.6706705 13.296822  14.062981  14.783075
0:  11.880531  11.979912 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.845484 19.58712  19.518377 19.50976  19.404207 18.99881  19.10771
0:  19.132324 19.302422 19.408699 19.017097 18.490387 17.847555 17.587559
0:  17.764292 18.354265 19.061209 19.533192 18.74369  18.87248 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.27072   11.114348  11.12253   11.148752  11.066412  10.782751
0:  10.708001  10.529772  10.287928   9.913145   9.156113   8.208538
0:   7.2250023  6.5594516  6.27225    6.3746605  6.576481   6.705682
0:   3.4377112  3.1028597]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.444033 18.270592 18.129463 17.948257 17.851013 17.632938 18.052773
0:  18.369705 18.795757 19.028563 18.73549  18.451399 18.171015 18.366796
0:  19.001455 19.876461 20.70578  21.214735 19.348398 19.186407]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.313133 19.048359 18.891808 18.743408 18.483923 17.963049 17.739527
0:  17.355343 17.010225 16.54108  15.693092 14.771315 13.882423 13.421017
0:  13.43195  13.84487  14.465939 14.956406 13.333143 13.168695]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.846779   9.803564  10.011346  10.331359  10.5776205 10.643589
0:  10.876558  11.029373  11.152517  11.239727  11.066344  10.728222
0:  10.444234  10.464729  10.809986  11.453844  12.122367  12.587315
0:  10.308223  10.278063 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.0681605 -6.9884667 -6.480016  -5.6226754 -4.7420707 -4.109792
0:  -3.829556  -3.789435  -4.2116904 -4.634088  -5.123443  -5.7208695
0:  -6.089316  -6.2517695 -6.0832534 -5.5526767 -4.9457116 -4.4109206
0:  -5.3174615 -5.332291 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.143372   -0.84000444 -0.08529377  0.9546013   1.9384599   2.6300435
0:   3.3936653   3.9783623   4.2695217   4.4321475   4.2089396   3.7160754
0:   3.3272069   3.2162554   3.6106112   4.3612413   5.2337227   5.9093814
0:   5.1934557   5.35863   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.51341  24.373167 24.157135 23.861755 23.513609 23.020344 23.453773
0:  23.873804 24.486708 24.843756 24.338322 23.696098 22.764086 22.431946
0:  22.588036 23.071198 23.501663 23.504633 23.349407 23.449091]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.723608 24.811388 25.155249 25.6763   26.187363 26.41631  26.878645
0:  27.024406 27.097878 27.006327 26.529102 26.056755 25.741365 25.857634
0:  26.528988 27.616804 28.93708  30.102917 29.549265 29.674774]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.788923 12.744333 13.051294 13.482904 13.81812  13.92981  14.313862
0:  14.584911 14.790571 14.911123 14.634617 14.198822 13.769329 13.706048
0:  14.123611 14.931509 15.900209 16.710566 15.559048 15.67852 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.0358076  -4.7602854  -4.094736   -3.1945658  -2.2828798  -1.5592871
0:  -0.8074999  -0.20025873  0.17452574  0.52877617  0.7004566   0.6433091
0:   0.70056915  0.94693804  1.4543014   2.1845195   2.8441324   3.2667847
0:   1.7080097   2.01357   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.659283 37.72409  37.361126 36.75867  36.07628  35.24445  35.459747
0:  35.64343  35.959396 35.895576 34.88201  34.013477 33.14033  33.117634
0:  33.90796  34.916412 35.847538 36.064465 38.11895  38.22648 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.338985  9.370382  9.756142 10.231476 10.517712 10.463442 10.45332
0:  10.315132 10.180354 10.093494  9.884273  9.494978  9.222078  9.139416
0:   9.353792  9.953115 10.708274 11.374412  9.293102  9.338886]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.376501 15.320919 15.46607  15.706406 15.932128 16.025745 16.62331
0:  17.132912 17.637676 18.06729  18.022976 17.872158 17.670967 17.919575
0:  18.547962 19.468243 20.304642 20.74057  18.411335 18.621033]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.686365 11.709782 11.942436 12.203572 12.316998 12.245775 12.517077
0:  12.699526 12.840666 12.930164 12.599092 12.076933 11.516041 11.386476
0:  11.643094 12.263206 12.923946 13.346615 11.852217 11.903666]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.10641  36.56122  36.886906 37.027306 37.00771  36.657337 36.86981
0:  36.925117 37.099445 37.136543 36.648052 36.16026  35.70253  35.605377
0:  35.881905 36.383884 36.885143 37.107452 35.889538 36.220932]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.836807 29.83612  29.754578 29.458    29.060684 28.437855 28.543613
0:  28.582993 28.76921  28.81697  28.26344  27.68694  27.100334 27.077248
0:  27.626255 28.56882  29.636677 30.381847 30.435425 30.731396]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.559437  9.7344   10.230573 10.80077  11.151414 11.206082 11.172298
0:  11.126535 10.977657 10.901202 10.74385  10.389207 10.108486  9.885674
0:   9.990301 10.464386 11.172411 11.857111 10.386692 10.555473]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.588089  11.964186  12.618547  13.399708  13.933461  14.245817
0:  14.339723  14.5033865 14.455468  14.554035  14.595687  14.371229
0:  14.226067  14.070568  14.335913  14.986988  15.865565  16.561327
0:  14.176819  14.341257 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.3597145 13.260246  13.094065  12.703702  12.226602  11.663964
0:  11.821878  12.037136  12.379166  12.555193  12.123776  11.640327
0:  11.052238  11.100294  11.619525  12.500103  13.330765  13.803546
0:  14.520618  14.596363 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.121258 19.434374 19.965776 20.576796 21.17644  21.672132 22.558697
0:  23.316015 23.978458 24.33987  24.14169  23.615482 23.14187  23.16387
0:  23.816668 24.868965 26.051487 26.850872 24.104837 24.241215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.3485179   1.28648     1.5320663   1.9118996   2.1402123   2.0693493
0:   1.9101207   1.6476002   1.209312    0.86296654  0.4538679  -0.12606382
0:  -0.5765362  -0.78020525 -0.6930833  -0.23020744  0.32381678  0.7609582
0:  -0.55413675 -0.4421425 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4590917 1.145433  1.2949991 1.7166467 2.108953  2.2881787 2.6182203
0:  2.9089704 3.3562865 3.9363265 4.4286523 4.7731657 5.1357737 5.6508226
0:  6.3503814 7.322341  8.281934  9.040811  6.874588  6.8706913]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.9246607  7.0275207  7.233304   7.407156   7.472266   7.342082
0:   7.657883   7.9599953  8.359571   8.692648   8.621465   8.3964
0:   8.13604    8.331762   8.932117   9.926985  10.999758  11.828917
0:  11.517276  11.709904 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.539202 23.183578 22.77535  22.323635 21.90733  21.448385 21.947224
0:  22.360723 22.899136 23.037561 22.254005 21.2887   20.245604 19.872915
0:  20.240679 21.00957  21.859634 22.336203 21.740334 21.688583]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.224726 19.089409 18.992752 18.841234 18.453014 17.719742 17.225828
0:  16.650316 16.036726 15.401514 14.430352 13.346117 12.30842  11.695169
0:  11.609423 12.014184 12.637125 13.080732 11.865292 11.581884]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.4211445 -11.430531  -10.931803   -9.949011   -8.853714   -7.9273
0:   -7.3095183  -6.872812   -6.8608613  -6.7964683  -6.676253   -6.652308
0:   -6.36719    -5.9429555  -5.208754   -4.14294    -2.9618325  -1.8818336
0:   -1.676022   -1.3744493]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.130572  7.2181406 7.6776304 8.325387  8.8111315 9.059778  9.329037
0:  9.514459  9.490584  9.362518  8.87505   8.0390005 7.1616626 6.515598
0:  6.285362  6.540155  7.012377  7.3454785 4.2694554 4.180916 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.24399  13.675245 14.391184 15.167274 15.718043 15.941848 16.031445
0:  15.950343 15.647881 15.343814 14.932072 14.33474  13.902081 13.621591
0:  13.670921 14.012461 14.418625 14.671803 11.774391 11.753822]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0186896  -1.9860673  -1.5311837  -0.89336824 -0.31021166  0.08211803
0:   0.505733    0.78706264  0.8727231   0.9829526   0.8726511   0.52259016
0:   0.24722147  0.14127588  0.2520051   0.70197916  1.1405344   1.5419254
0:  -0.5700946  -0.43401814]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0304894 -3.0869212 -2.9646525 -2.587543  -2.2204719 -2.056909
0:  -2.259358  -2.6271453 -3.4905238 -4.255916  -5.0296493 -5.8137774
0:  -6.3244624 -6.577395  -6.460935  -5.911287  -5.190395  -4.4726787
0:  -4.672401  -4.7723365]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.92854  24.051655 24.147938 24.123402 23.98108  23.609291 23.789188
0:  23.872654 24.122967 24.274433 23.952446 23.614048 23.26595  23.370766
0:  23.935505 24.79715  25.761162 26.446922 25.737457 25.8623  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.317296  10.46048   10.825885  11.217535  11.287816  10.969392
0:  10.3295555  9.613151   8.716791   7.9836454  7.3246155  6.5965242
0:   5.985923   5.573408   5.52298    5.923959   6.612112   7.391418
0:   5.5880694  5.5386715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.935952  -10.069209   -9.806052   -9.094689   -8.265609   -7.5786543
0:   -7.278983   -7.168705   -7.595154   -7.974818   -8.35893    -8.860986
0:   -9.081062   -9.16841    -8.93525    -8.277432   -7.434971   -6.5560064
0:   -7.041795   -7.247938 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.64647    -8.906645   -8.810693   -8.319615   -7.7346478  -7.2836905
0:   -7.072194   -7.082282   -7.5990973  -8.183124   -8.91301    -9.810056
0:  -10.463887  -10.855248  -10.824035  -10.392564   -9.901267   -9.446399
0:  -10.312636  -10.436536 ]
0: validation loss for strategy=forecast at epoch 36 : 0.2956504225730896
0: validation loss for velocity_u : 0.16211259365081787
0: validation loss for velocity_v : 0.28907376527786255
0: validation loss for specific_humidity : 0.15801413357257843
0: validation loss for velocity_z : 0.49609804153442383
0: validation loss for temperature : 0.10214249044656754
0: validation loss for total_precip : 0.5664613246917725
0: 37 : 20:25:30 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 37, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9172, -0.4714, -0.3581, -0.4794, -0.7920, -1.1487, -1.4565, -1.7154, -1.7963, -1.7982, -1.8038, -1.7900,
0:         -1.7838, -1.7674, -1.7517, -1.7293, -1.7038, -1.6797, -0.6707, -0.3408, -0.2977, -0.5002, -0.8538, -1.1793,
0:         -1.4906], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0807,  0.0436,  0.1082, -0.0183, -0.3703, -0.5097, -0.3737, -0.2182, -0.1270, -0.1106, -0.0655, -0.0099,
0:          0.0460,  0.1078,  0.1662,  0.2316,  0.2808,  0.3328, -0.1760, -0.1420, -0.0436, -0.0831, -0.3334, -0.4584,
0:         -0.3655], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6447, -0.2946, -0.3453, -0.2013,  0.3513,  0.7745,  0.9015,  0.6858,  0.4916,  0.3968,  0.2545,  0.0211,
0:         -0.2398, -0.4887, -0.6143, -0.6695, -0.6723, -0.6602,  0.3228, -0.1281, -0.4582,  0.0371,  0.4082,  0.7212,
0:          0.8003], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.1199,  0.3308,  0.0320, -0.2246, -0.2223,  0.1529, -0.0797, -0.0307,  0.9991,  0.5919,  0.2419,  0.6934,
0:          0.3091,  0.4654,  0.3388,  0.2612,  0.2726, -0.0159, -1.9716, -0.8905,  0.7105, -0.6488, -0.3546,  1.1131,
0:         -0.3363], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.8866, -0.2945, -0.1370,  0.0118,  0.0084, -0.5252, -0.9714, -1.1791, -1.1694, -1.2577, -1.2983, -1.2081,
0:         -1.2010, -1.1307, -1.1385, -1.1440, -1.1458, -1.1588, -1.1160, -1.1124, -1.0548, -1.0035, -0.9512, -0.9163,
0:         -0.9012], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.2595, -0.2071, -0.2353, -0.1918, -0.2389, -0.2577, -0.2577, -0.2306, -0.2306, -0.2577, -0.2553, -0.2295,
0:         -0.0919, -0.2389, -0.2577, -0.2577, -0.1895, -0.1895, -0.2577, -0.2541, -0.2342, -0.2118, -0.2471, -0.2471,
0:         -0.2541], device='cuda:0')
0: [DEBUG] Epoch 37, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2577, -0.2483,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2577, -0.2577, -0.2577,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2577,     nan,     nan,     nan,     nan,
0:             nan, -0.2577,     nan,     nan,     nan,     nan,     nan, -0.2577,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2577,     nan,     nan,     nan,     nan,     nan,     nan, -0.2577,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1989,     nan,     nan,     nan,     nan, -0.2577,  0.3688,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2577,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2577,
0:             nan,     nan,     nan, -0.2389,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2248,     nan, -0.2577,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2295,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2071,     nan,     nan, -0.1366,     nan,     nan, -0.2577,     nan, -0.2577,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2577,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1472,     nan, -0.2577,     nan,     nan,     nan, -0.2306,
0:             nan, -0.2001,     nan,     nan,     nan,     nan,     nan, -0.2130, -0.2306,     nan,     nan, -0.2236,
0:             nan, -0.2577,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2577,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 37, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5365, -0.5333, -0.5126, -0.4840, -0.4715, -0.4842, -0.5327, -0.6030, -0.7260, -0.8435, -0.9561, -1.0475,
0:         -1.1027, -1.1354, -1.1434, -1.1341, -1.1258, -1.1170, -0.4233, -0.4585, -0.4914, -0.5231, -0.5456, -0.5629,
0:         -0.5902], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0201,  0.0103, -0.0226, -0.0551, -0.0852, -0.1219, -0.1619, -0.1763, -0.1528, -0.1094, -0.0684, -0.0435,
0:         -0.0611, -0.1220, -0.1869, -0.2139, -0.2089, -0.1880,  0.0156,  0.0205,  0.0050, -0.0199, -0.0629, -0.1284,
0:         -0.1789], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5299, -0.5490, -0.5303, -0.4524, -0.3370, -0.2045, -0.0746,  0.0367,  0.1316,  0.2086,  0.2708,  0.3248,
0:          0.3769,  0.4259,  0.4509,  0.4593,  0.4351,  0.3941, -0.5623, -0.5716, -0.5380, -0.4428, -0.3223, -0.1934,
0:         -0.0703], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1390,  0.1815,  0.5953, -0.1019, -0.0052,  0.6033,  0.0554,  0.1344,  0.5100,  0.1746,  0.1994,  0.3351,
0:          0.3547,  0.3824,  0.2805,  0.4729,  0.6206,  0.3493, -0.0047, -0.0591,  0.3194, -0.2036,  0.0059,  0.6269,
0:          0.0293], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.9602, -0.9459, -0.9849, -1.0280, -0.9854, -0.8243, -0.5687, -0.2943, -0.0579,  0.1145,  0.2421,  0.3638,
0:          0.4941,  0.6334,  0.7590,  0.8677,  0.9446,  0.9980,  1.0219,  1.0105,  0.9838,  0.9618,  0.9712,  0.9971,
0:          1.0012], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2509, -0.2557, -0.2663, -0.2545, -0.2584, -0.2565, -0.2577, -0.2647, -0.2536, -0.2593, -0.2602, -0.2533,
0:         -0.2595, -0.2606, -0.2551, -0.2589, -0.2539, -0.2685, -0.2619, -0.2612, -0.2680, -0.2598, -0.2532, -0.2641,
0:         -0.2642], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1865299642086029; velocity_v: 0.31998544931411743; specific_humidity: 0.19236868619918823; velocity_z: 0.6118000745773315; temperature: 0.13850544393062592; total_precip: 0.5323912501335144; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22520051896572113; velocity_v: 0.37373194098472595; specific_humidity: 0.15921516716480255; velocity_z: 0.5389194488525391; temperature: 0.1529712677001953; total_precip: 0.4644625186920166; 
0: epoch: 37 [1/5 (20%)]	Loss: 0.49843 : 0.29069 :: 0.21480 (2.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20729634165763855; velocity_v: 0.32962363958358765; specific_humidity: 0.18000386655330658; velocity_z: 0.5395645499229431; temperature: 0.14691795408725739; total_precip: 0.40222302079200745; 
0: epoch: 37 [2/5 (40%)]	Loss: 0.40222 : 0.26701 :: 0.21135 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22731977701187134; velocity_v: 0.37073835730552673; specific_humidity: 0.20882317423820496; velocity_z: 0.5877565145492554; temperature: 0.14662235975265503; total_precip: 0.7816753387451172; 
0: epoch: 37 [3/5 (60%)]	Loss: 0.78168 : 0.35222 :: 0.21346 (16.44 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24206887185573578; velocity_v: 0.349515438079834; specific_humidity: 0.2082788646221161; velocity_z: 0.6211843490600586; temperature: 0.16869404911994934; total_precip: 0.8705904483795166; 
0: epoch: 37 [4/5 (80%)]	Loss: 0.87059 : 0.37502 :: 0.20768 (16.04 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [2.28881836e-05 2.00271606e-05 1.00135803e-05 9.53674316e-06
0:  1.76429749e-05 3.95774841e-05 1.38282776e-05 4.76837158e-06
0:  8.10623169e-06 2.19345093e-05 3.57627869e-05 4.33921850e-05
0:  7.77244568e-05 2.13623047e-04 4.12940979e-04 5.40256442e-04
0:  6.48498535e-04 8.12530518e-04 6.58035278e-04 6.19888306e-04
0:  6.72817230e-04 3.09944153e-04 3.51428986e-04 9.21249390e-04
0:  2.63690948e-04 1.04904175e-05 1.09672546e-05 1.04904175e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.43051147e-06 2.67028809e-05 8.82148743e-05 2.59399414e-04
0:  3.75747681e-04 4.40597534e-04 6.42299652e-04 7.74860382e-04
0:  8.59260559e-04 7.82489777e-04 7.64369965e-04 4.59671021e-04
0:  4.24861908e-04 2.41756439e-04 2.04563141e-04 1.32560730e-04
0:  7.39097595e-05 3.76701355e-05 2.62260437e-05 1.57356262e-05
0:  1.28746033e-05 7.62939453e-06 3.33786011e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.52587891e-05 1.52587891e-05 1.66893005e-05 1.23977661e-05
0:  1.23977661e-05 1.71661377e-05 2.05039978e-05 1.95503235e-05
0:  1.38282776e-05 1.71661377e-05 4.29153479e-05 8.82148743e-05
0:  2.50339508e-04 4.64439392e-04 7.17639923e-04 7.45296478e-04
0:  7.00950623e-04 6.09874725e-04 4.43935394e-04 4.53948975e-04
0:  6.31332397e-04 4.92095947e-04 2.82764435e-04 4.38690222e-05
0:  4.76837158e-07 5.38825989e-05 1.54018402e-04 5.38825989e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 5.24520874e-06 6.96182251e-05
0:  1.80721283e-04 3.38077516e-04 5.35964966e-04 6.27994537e-04
0:  6.21318817e-04 1.03855133e-03 1.71899796e-03 2.54678726e-03
0:  1.13105774e-03 3.52859497e-04 3.09467316e-04 2.32696533e-04
0:  1.55925751e-04 8.15391541e-05 5.96046448e-05 3.71932983e-05
0:  1.62124634e-05 1.33514404e-05 9.05990601e-06 2.86102295e-06
0:  9.53674316e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Target values (first 200):
0: [4.38690222e-05 4.76837158e-05 4.43458557e-05 2.52723694e-05
0:  1.23977661e-05 5.24520874e-06 2.38418579e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 9.53674316e-07 4.76837158e-06
0:  1.33514404e-05 2.71797180e-05 4.76837158e-05 6.15119934e-05
0:  6.91413879e-05 3.38554382e-05 4.14848291e-05 3.62396240e-05
0:  7.86781311e-05 2.30312347e-04 4.22000885e-04 8.92162323e-04
0:  1.47342682e-03 2.23636627e-03 2.55823135e-03 2.75087357e-03
0:  2.81286240e-03 2.73895264e-03 2.89821625e-03 3.42607498e-03
0:  2.76899338e-03 2.23970413e-03 1.77478790e-03 1.41763687e-03
0:  1.08432770e-03 7.59124756e-04 6.22272491e-04 5.34534454e-04
0:  5.02109528e-04 4.11987305e-04 3.07083130e-04 1.89304352e-04
0:  1.02996826e-04 4.81605493e-05 1.38282776e-05 9.05990601e-06
0:  1.57356262e-05 2.81333923e-05 3.09944153e-05 3.24249268e-05
0:  3.62396240e-05 2.38418579e-05 1.47819519e-05 9.05990601e-06
0:  4.76837158e-06 1.43051147e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.90734863e-06 1.04904175e-05 2.95639038e-05 4.67300415e-05
0:  5.76972961e-05 4.81605493e-05 2.28881836e-05 3.81469727e-06
0:  1.43051147e-06 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  3.48091125e-05 4.10079956e-05 4.10079956e-05 1.66893005e-05
0:  3.33786011e-06 9.53674316e-07 4.76837158e-07 4.76837158e-07
0:  9.53674316e-07 1.43051147e-06 9.53674316e-07 0.00000000e+00
0:  4.76837158e-07 1.43051147e-06 1.90734863e-06 4.29153442e-06
0:  1.14440918e-05 2.28881836e-05 5.34057617e-05 6.72340393e-05
0:  6.43730164e-05 9.34600830e-05 2.14099884e-04 4.08172607e-04
0:  6.60419464e-04 9.51290131e-04 1.25694275e-03 1.63841248e-03
0:  2.07567215e-03 2.59160995e-03 2.60353088e-03 2.79760361e-03
0:  3.17430496e-03 3.22437286e-03 3.10802460e-03 2.87008286e-03
0:  2.52819061e-03 2.12860107e-03 1.65033340e-03 1.25074387e-03
0:  9.47952271e-04 7.36236572e-04 6.34193420e-04 5.37395477e-04
0:  4.48226929e-04 3.25679750e-04 2.15530396e-04 1.18732452e-04
0:  6.29425049e-05 3.00407410e-05 1.71661377e-05 2.71797180e-05
0:  4.62532043e-05 7.15255737e-05 6.62803650e-05 5.72204590e-05
0:  4.62532043e-05 3.67164612e-05 2.62260437e-05 1.57356262e-05
0:  8.58306885e-06 2.86102295e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [22.328161 22.152967 21.893835 21.400946 20.743376 19.949469 19.604902
0:  19.289967 18.93697  18.47624  17.465778 16.39619  15.326143 14.896753
0:  15.048178 15.666349 16.34401  16.764198 16.855463 17.080608]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.389, max = 2.424, mean = 0.585
0:          sample (first 20): tensor([1.3203, 1.3055, 1.2835, 1.2418, 1.1862, 1.1190, 1.0899, 1.0632, 1.0334, 0.9944, 0.9089, 0.8184, 0.7279, 0.6916,
0:         0.7044, 0.7567, 0.8140, 0.8496, 1.2242, 1.2479])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.456724  5.6627407 6.0957603 6.673988  7.0844407 7.2704554 7.278879
0:  7.1367073 6.6594744 6.3023214 5.8538055 5.315239  4.857811  4.560439
0:  4.606717  5.072484  5.7823906 6.5050883 5.7472715 5.979337 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.060955 30.032047 29.91844  29.775393 29.691906 29.42881  29.907696
0:  30.167347 30.61125  30.820606 30.485806 30.255503 30.134377 30.525402
0:  31.475014 32.7242   34.11064  35.153313 35.64888  36.240803]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.2679853  2.9228044  2.9356532  3.0862508  2.9625497  2.4053535
0:   1.6095862  0.5523133 -0.7167487 -1.9273028 -3.181684  -4.3104596
0:  -5.117703  -5.3457217 -5.034573  -4.2440667 -3.356975  -2.7050233
0:  -2.5511327 -3.1244016]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-17.826756 -18.158798 -18.057789 -17.395294 -16.50431  -15.706857
0:  -15.332255 -15.170957 -15.61008  -16.053112 -16.48677  -17.035446
0:  -17.234768 -17.276241 -16.944206 -16.218952 -15.264394 -14.215499
0:  -13.79358  -14.177896]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5247817  -1.6338     -1.4116282  -0.90368795 -0.4057393  -0.13050413
0:  -0.08947849 -0.18735504 -0.68745947 -1.1780953  -1.74404    -2.515283
0:  -3.1001067  -3.5803285  -3.7611718  -3.4624667  -2.9124217  -2.24585
0:  -3.3917117  -3.5409083 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.541989  12.197926  12.117292  12.087305  12.029235  11.769959
0:  11.767281  11.640532  11.469358  11.252476  10.693314  10.012812
0:   9.382109   9.145912   9.338567   9.950477  10.632     11.1806755
0:   9.554804   9.333256 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.3071575 11.221468  11.4766035 11.837999  11.978508  11.804794
0:  11.53186   11.338885  11.087553  10.99898   10.846652  10.398083
0:   9.936874   9.530245   9.490563   9.948037  10.813459  11.705068
0:   9.4621315  9.446544 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.438503 20.310066 20.292475 20.33801  20.455791 20.308561 20.62485
0:  20.721893 20.931684 20.993689 20.661621 20.377764 20.111311 20.251945
0:  20.811707 21.7496   22.885292 23.898674 24.76466  24.955404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.510742  10.683836  11.038071  11.36944   11.566725  11.573807
0:  11.722191  11.727584  11.633181  11.505377  11.152688  10.671235
0:  10.323409  10.306747  10.606304  11.243293  11.9305935 12.523502
0:  10.817921  10.89658  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.480122  6.511251  6.82165   7.186542  7.499139  7.54284   7.6274047
0:  7.617214  7.480979  7.3056374 6.96525   6.5230517 6.17227   6.0324655
0:  6.2091236 6.7322936 7.42445   8.046681  7.0619926 7.2323613]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -5.378195   -5.606879   -5.491063   -4.9644938  -4.331641   -3.791191
0:   -3.6134148  -3.57127    -4.075371   -4.680873   -5.444433   -6.404154
0:   -7.245549   -7.9248323  -8.29442    -8.255778   -7.9592094  -7.5829983
0:   -9.551244  -10.110612 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.349072  -9.436263  -9.070493  -8.297191  -7.491419  -6.9056406
0:  -6.592967  -6.484683  -6.873706  -7.2587376 -7.771083  -8.5176735
0:  -8.994674  -9.340837  -9.354856  -8.965819  -8.46778   -7.94096
0:  -9.140349  -9.310943 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.9788475 1.1972342 1.7253494 2.4564128 3.1165147 3.5428066 3.8442545
0:  4.0366917 3.8887696 3.7442665 3.4300218 2.8991337 2.486507  2.1827087
0:  2.197116  2.5788279 3.087648  3.5254235 2.0376391 2.1170998]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.214301  5.3067846 5.7536077 6.341534  6.6807003 6.6425257 6.533325
0:  6.330816  5.9989266 5.738958  5.353954  4.7797604 4.270793  3.9921868
0:  4.072798  4.59818   5.3156776 5.9508805 4.5685043 4.683976 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.2680683   0.0881176   0.29802084  0.81403303  1.3420858   1.6619358
0:   1.7273226   1.6271415   1.1204581   0.63784075  0.11328459 -0.55559826
0:  -0.99483013 -1.2749786  -1.2753458  -0.87918377 -0.32388878  0.23064566
0:  -1.5730543  -1.6073179 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.498817 20.494574 20.32171  19.942171 19.569662 19.097727 19.63135
0:  20.096962 20.828274 21.363838 21.278183 21.257082 21.216799 22.034195
0:  23.465065 25.269676 27.04842  28.094755 30.80494  31.359846]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.806879  -9.809404  -9.347219  -8.536205  -7.7442174 -7.2169394
0:  -6.917809  -6.8746867 -7.1989617 -7.431912  -7.6887717 -7.9832053
0:  -8.027826  -7.8899302 -7.5432334 -6.9942784 -6.5100064 -6.1627326
0:  -7.7804646 -7.5253654]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.028133  12.077509  12.369251  12.713491  12.8708515 12.748295
0:  12.746805  12.6456375 12.498865  12.371457  11.934713  11.302313
0:  10.643387  10.259865  10.244972  10.6316395 11.169186  11.616718
0:   9.14926    9.143726 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.5629773  -4.540235   -4.0933585  -3.2572198  -2.3795152  -1.668777
0:  -1.3373742  -1.1274829  -1.3715835  -1.5692029  -1.7715774  -2.198832
0:  -2.4880147  -2.7524037  -2.7585502  -2.2596316  -1.4158826  -0.44161892
0:  -1.3112159  -1.311234  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.607432   -3.5505452  -3.2380776  -2.75394    -2.2322516  -1.8241663
0:  -1.0553513  -0.42538118  0.15767622  0.6262789   0.74402905  0.8040719
0:   0.9803238   1.7518854   3.0300558   4.762356    6.528784    8.033129
0:  10.247499   10.708036  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.162354  1.328454  1.7984853 2.5955334 3.3965008 4.037679  4.4006796
0:  4.5583744 4.2319126 3.863205  3.3985968 2.814382  2.37361   2.0902867
0:  2.0885706 2.4431443 2.9664578 3.4741378 2.6052809 2.716658 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.605215  2.6138494 2.8303027 3.3388472 3.939672  4.435502  4.7657986
0:  4.936287  4.6125393 4.2711973 3.8143094 3.288108  2.9788365 2.8126855
0:  2.9625323 3.2365808 3.4895058 3.5936334 2.1845455 2.212356 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.787036  -12.760526  -12.263892  -11.338706  -10.407378   -9.760583
0:   -9.472732   -9.510473  -10.130301  -10.763665  -11.504592  -12.377515
0:  -12.951709  -13.303027  -13.265681  -12.8093195 -12.214696  -11.560737
0:  -11.942004  -11.967824 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [49.265827 49.285103 48.90184  48.45121  47.91425  47.15481  47.38808
0:  47.470757 47.69946  47.436947 46.46857  45.463665 44.373642 44.019054
0:  44.247883 44.533924 44.625164 43.841515 43.55788  43.53489 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.000051 22.186325 22.431616 22.6088   22.627396 22.426434 22.478344
0:  22.466053 22.450897 22.449907 22.13596  21.72473  21.346283 21.241976
0:  21.507023 22.059187 22.694777 23.130354 20.968285 21.145803]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.375896 17.37384  17.7519   18.337212 18.821632 19.057667 19.510677
0:  19.816132 20.062801 20.239079 20.093678 19.78276  19.564137 19.64839
0:  20.144464 20.9208   21.711403 22.228317 19.307137 19.387346]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.340939 16.53857  17.006498 17.411581 17.507797 17.27873  17.148438
0:  17.050142 17.048546 17.173769 17.177067 17.044401 17.02156  17.182951
0:  17.752575 18.675524 19.820618 20.866302 18.485483 18.81413 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.383202   -7.6522164  -7.4847417  -6.9543815  -6.4396033  -6.2087436
0:   -6.252321   -6.577289   -7.3160615  -8.113371   -8.9869     -9.926775
0:  -10.640463  -11.05172   -11.225506  -11.269287  -11.4994545 -11.984518
0:  -15.503998  -16.570154 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.46953964 0.5993552  1.0861778  1.8038845  2.4234102  2.8136675
0:  3.0688176  3.2452424  3.1140537  3.0324914  2.7981148  2.341833
0:  2.0108104  1.7924571  1.8872094  2.351869   2.9239686  3.4290771
0:  2.1558826  2.3178144 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.7036653 -3.7000585 -3.330296  -2.7368007 -2.191321  -1.8568511
0:  -1.5748    -1.4626422 -1.6802363 -1.9108033 -2.3779988 -3.059133
0:  -3.5566764 -3.8584113 -3.848538  -3.492947  -3.1199384 -2.8152738
0:  -4.9201417 -4.846869 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.94768   -13.985822  -13.575105  -12.758172  -11.9526415 -11.446122
0:  -11.2699    -11.384366  -11.96292   -12.548381  -13.223773  -14.045478
0:  -14.561645  -14.825173  -14.684255  -14.175637  -13.705138  -13.3113165
0:  -13.302686  -13.346856 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.830886  -7.943426  -7.5207357 -6.611783  -5.5492344 -4.6391287
0:  -4.0360994 -3.6467323 -3.764946  -3.9307528 -4.1852255 -4.635043
0:  -4.909402  -5.0781517 -5.025053  -4.6356726 -4.1645    -3.7209015
0:  -5.01661   -5.178406 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.450497   9.393287   9.670483  10.067155  10.219357   9.988287
0:   9.615767   9.077366   8.3581085  7.7357225  7.0415664  6.272389
0:   5.6462584  5.3004265  5.299321   5.690681   6.1976757  6.6593957
0:   4.3770666  4.1007695]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.159324  7.9398694 7.951054  8.142713  8.239769  8.085068  8.266117
0:  8.32616   8.406862  8.460075  8.147696  7.6029196 7.014258  6.8098235
0:  6.9895477 7.669981  8.538012  9.238116  8.181165  7.926156 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.6931534 5.6969037 5.9487634 6.341791  6.582365  6.5322475 6.5730877
0:  6.474088  6.334014  6.2219763 5.9187922 5.513842  5.186989  5.255702
0:  5.742076  6.5952773 7.507838  8.155493  6.4354653 6.186404 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.05414   9.027771  9.173334  9.383819  9.543293  9.54818   9.927913
0:  10.232815 10.476955 10.629665 10.318733  9.874842  9.376872  9.293805
0:   9.623588 10.295028 11.015974 11.455509  9.883267  9.893443]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.921074 39.513477 40.253952 41.168976 42.1389   42.88267  44.248035
0:  45.42101  46.643578 47.664738 48.098866 48.537186 48.981667 49.772137
0:  50.847424 51.933765 52.80968  53.067654 49.13847  49.568874]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.513515 22.126503 21.722988 21.320045 20.91935  20.229862 20.335476
0:  20.276709 20.359127 20.242035 19.376835 18.516659 17.488554 17.095028
0:  17.24224  17.782932 18.348919 18.570236 19.094822 18.903421]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5978575  -0.6621485  -0.2860942   0.28212166  0.72084904  0.934113
0:   1.0667377   1.0594625   0.76558733  0.43688345 -0.15817165 -1.0332861
0:  -1.8674097  -2.487565   -2.7206478  -2.3914576  -1.7186546  -0.93965435
0:  -2.5489264  -2.4774504 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.86046  15.975454 16.289568 16.720367 17.02912  17.097647 17.357706
0:  17.48929  17.563675 17.565035 17.272863 16.815077 16.372177 16.168509
0:  16.339342 16.853552 17.49485  17.953207 15.391893 15.491655]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.208717  -18.222565  -17.703169  -16.73121   -15.686386  -14.874685
0:  -14.273447  -13.998299  -14.131079  -14.310772  -14.607958  -15.023895
0:  -15.05378   -14.81105   -14.133802  -13.155146  -12.291264  -11.594006
0:  -12.4671955 -12.161036 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8298197  1.7171807  1.8873601  2.3012137  2.6015337  2.7312355
0:  2.534877   2.4326477  2.0692275  1.950151   1.8930712  1.607976
0:  1.315794   0.9890013  0.95415974 1.4805198  2.5267346  3.7290828
0:  3.4643488  3.6148446 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.814843   6.986917   7.3382215  7.563224   7.594612   7.457513
0:   7.7790923  8.180314   8.637552   9.073652   9.002255   8.671542
0:   8.18886    8.178152   8.539363   9.300477   9.992154  10.435232
0:   9.677126   9.949317 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3732963  -1.4562678  -1.2456393  -0.7300725  -0.19383907  0.21017218
0:   0.5089884   0.64815617  0.40225315  0.17736197 -0.18862104 -0.72226
0:  -1.101584   -1.3341918  -1.2850785  -0.9043021  -0.40639305  0.07111979
0:  -1.0715566  -0.9229293 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.631294  -8.493244  -7.8711915 -6.8256545 -5.7848115 -5.012014
0:  -4.5881658 -4.4962487 -4.9413295 -5.441137  -6.0526395 -6.7977023
0:  -7.282747  -7.534077  -7.4535823 -7.0030417 -6.458113  -5.96514
0:  -7.2562165 -7.2390304]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.156158 -15.246473 -14.963585 -14.253555 -13.421816 -12.765205
0:  -12.329227 -12.077421 -12.330666 -12.622358 -13.055866 -13.68951
0:  -14.0532   -14.217411 -14.058586 -13.531736 -13.008383 -12.542423
0:  -13.913151 -14.138374]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.491014  16.30761   16.195726  16.081867  15.968252  15.711126
0:  16.077114  16.412437  16.795061  17.047226  16.721426  16.296858
0:  15.7373085 15.692097  16.149313  17.018707  17.973684  18.620344
0:  18.379805  18.424515 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.637005 14.717356 14.986013 15.248564 15.364876 15.21876  15.357531
0:  15.386332 15.406702 15.357367 14.899364 14.361267 13.810581 13.617386
0:  13.800957 14.299406 14.813114 15.09021  12.555702 12.711251]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2855844  -3.081915   -2.5827112  -1.8798637  -1.2303843  -0.84987116
0:  -0.63682127 -0.6147218  -1.0436974  -1.4307442  -1.9922714  -2.655383
0:  -3.0850458  -3.2826595  -3.1186     -2.6906147  -2.3698435  -2.2017746
0:  -3.2049646  -3.0089283 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.774858  2.7494483 3.1267989 3.848867  4.48654   4.903182  5.016453
0:  5.1352344 4.989683  5.032999  5.112317  4.92445   4.7417746 4.54426
0:  4.5748434 5.146281  6.1208134 7.2058606 5.6629577 5.583186 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0667152  2.8225355  2.835527   3.1895401  3.602592   3.9358017
0:  3.7777355  3.6511555  3.057414   2.638657   2.2831302  1.746182
0:  1.2270074  0.7275214  0.52371264 0.8666034  1.7449031  2.836423
0:  1.5823789  1.3078933 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9972544  -4.683421   -3.9893284  -2.9970303  -2.0356255  -1.3005228
0:  -0.776299   -0.45849514 -0.516778   -0.6362128  -0.8562002  -1.2524786
0:  -1.4563179  -1.541769   -1.3673105  -0.8912306  -0.33377028  0.15693283
0:  -1.3462782  -1.1362791 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.9269547  -8.206951   -8.039019   -7.51259    -6.9507785  -6.6259017
0:   -6.5905404  -6.700745   -7.260937   -7.7648177  -8.343243   -9.123112
0:   -9.679131  -10.0958805 -10.226328   -9.946775   -9.537586   -9.149395
0:  -10.45652   -10.743757 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.24173    9.403626   9.878658  10.568079  11.24144   11.794233
0:  12.592749  13.248371  13.671305  13.803689  13.38916   12.587271
0:  11.766199  11.292034  11.298216  11.74531   12.338417  12.7113905
0:  10.001369   9.6746235]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.320024  17.28866   17.595186  18.002964  18.22542   18.068401
0:  17.969648  17.44841   16.298822  14.592472  12.064927   9.216788
0:   6.4595547  4.575017   3.701093   3.8455753  4.4631586  5.0049477
0:   5.801818   5.7917924]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.415125  14.428013  14.550947  14.840038  15.045931  15.075987
0:  14.647125  14.209187  13.376992  12.754387  12.235145  11.517202
0:  10.802853  10.0156355  9.460193   9.4414     9.938569  10.678751
0:   7.145978   6.746904 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.18863  31.784935 31.322521 30.789894 30.237265 29.401602 29.106241
0:  28.731394 28.554712 28.325281 27.62606  27.065197 26.613268 26.585693
0:  27.085356 27.830011 28.542673 28.958496 27.831987 27.688452]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.8608608  -2.1649933  -2.1285868  -1.6692524  -1.0781393  -0.52139044
0:  -0.35601807 -0.21384001 -0.5816331  -0.84510946 -1.0525336  -1.4934645
0:  -1.770031   -2.0748034  -2.1227598  -1.691946   -0.85313845  0.18177986
0:  -0.23606491 -0.33338118]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8934722  -0.9175463  -0.6201277  -0.16705608  0.07534409 -0.01314735
0:  -0.2325201  -0.5220876  -0.98190546 -1.3143005  -1.6836667  -2.1857772
0:  -2.5023751  -2.5711207  -2.2995172  -1.6195102  -0.84646845 -0.25573206
0:  -1.1219001  -1.0073724 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.238948   7.9934845  8.132793   8.539112   8.937262   9.106749
0:   9.457609   9.746361   9.980543  10.120462   9.941049   9.495431
0:   8.993826   8.720385   8.824261   9.2781315  9.820539  10.157206
0:   7.202985   6.8932524]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.127815  4.9622507 5.126809  5.533683  5.9267235 6.181328  6.4947524
0:  6.734891  6.7873387 6.8065386 6.6192713 6.1976175 5.830842  5.6781783
0:  5.790787  6.2144403 6.699987  7.032023  4.4798546 4.2666473]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.1708236  5.5359344  6.065037   6.6640024  7.1670327  7.3906507
0:   7.7467775  7.9487724  8.0816765  8.21799    8.111738   7.9642696
0:   7.8622885  8.109489   8.671808   9.56124   10.521303  11.287109
0:  10.390747  10.969855 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.415955 18.332268 18.378065 18.452045 18.329727 17.955275 17.56617
0:  17.057858 16.383804 15.782708 15.064658 14.240831 13.56395  13.13327
0:  13.100423 13.4153   13.865874 14.224081 12.052511 11.972881]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6839104  2.7825708  3.2019942  3.8041089  4.271207   4.47601
0:  4.5863876  4.546402   4.1897545  3.8865333  3.3694432  2.6490002
0:  2.0299253  1.6192245  1.5614414  1.8820615  2.3124523  2.6867552
0:  0.62303257 0.75791645]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.930963   8.310906   8.876018   9.512733   9.984234  10.246096
0:  10.442577  10.52404   10.370081  10.128107   9.655345   8.961632
0:   8.328656   7.906595   7.9489193  8.421107   9.142669   9.784195
0:   7.2425485  7.154459 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3326793  -1.0964837  -0.42046976  0.56638813  1.4886961   2.1668668
0:   2.5556965   2.8739104   2.8317099   2.8858023   2.880907    2.6024837
0:   2.397534    2.2142525   2.2880416   2.8160367   3.569961    4.307043
0:   2.9566593   2.8245606 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.250912 16.137472 16.171547 16.212791 16.28557  16.203274 16.79695
0:  17.291527 17.867386 18.317356 18.172396 17.911634 17.604256 17.887653
0:  18.695248 20.013977 21.474308 22.632454 21.745472 21.929977]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.70705  29.235577 28.782509 28.335735 27.853512 27.146511 27.081738
0:  26.876446 26.818104 26.684216 26.102654 25.617226 25.266865 25.524277
0:  26.417    27.741417 29.161201 30.18169  28.915272 28.903198]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.3639483  7.7226768  8.410845   9.069231   9.443105   9.504233
0:   9.584804   9.698071   9.869097  10.120274  10.274274  10.197393
0:  10.227755  10.450557  11.047607  12.009783  13.08829   14.00679
0:  11.374799  11.714132 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.133162  18.440395  17.738138  17.108755  16.562014  15.793005
0:  15.702675  15.432484  15.298252  14.984592  14.027935  13.1525955
0:  12.206892  11.839713  12.039227  12.662632  13.457703  14.027408
0:  14.329674  13.99925  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.244936  -18.480045  -18.373613  -17.457624  -16.04376   -14.503275
0:  -12.825741  -11.413387  -10.578354   -9.8779545  -9.549391   -9.412832
0:   -9.129835   -8.745695   -8.06654    -7.180388   -6.3789086  -5.825443
0:   -8.066757   -8.036785 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6459794  -2.5491824  -2.0681462  -1.2320943  -0.4174719   0.16450405
0:   0.40421677  0.40263653 -0.13638067 -0.73940325 -1.4786735  -2.3463616
0:  -2.985599   -3.4079566  -3.496859   -3.2153192  -2.85282    -2.5914054
0:  -6.1911263  -6.955976  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3939357  -5.2619452  -4.629146   -3.6602254  -2.695744   -1.9419112
0:  -1.375937   -1.0017943  -0.9383588  -0.8621249  -0.8767047  -1.0671458
0:  -1.0895562  -0.98704195 -0.6475806  -0.05584049  0.54189205  0.9798722
0:  -0.9238744  -0.70370245]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.418613 26.542274 26.942719 27.476273 27.83654  27.819366 28.030628
0:  27.99137  27.929121 27.728994 27.089062 26.299673 25.508112 25.022873
0:  25.045282 25.492456 26.099943 26.625742 22.95254  22.688993]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.177236 14.248639 14.552037 14.958775 15.208813 15.287086 15.344428
0:  15.409162 15.306521 15.278843 15.133739 14.814213 14.592985 14.496328
0:  14.819361 15.55348  16.477283 17.292515 15.121759 15.182441]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.264992 21.031199 20.945374 20.908669 20.748976 20.300922 20.100197
0:  19.714367 19.28834  18.786602 17.972822 17.111744 16.366446 16.054298
0:  16.274242 16.840069 17.534004 17.986328 14.889696 14.248959]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.240686  7.4024553 7.8960295 8.525315  8.867945  8.778779  8.492496
0:  8.1014    7.4523816 6.9034023 6.227595  5.273321  4.37694   3.611347
0:  3.200653  3.336656  3.80586   4.3256702 2.0345006 1.7477298]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.037494  13.885952  13.786606  13.60511   13.35619   12.94637
0:  13.2234955 13.491795  13.908726  14.199715  13.8907795 13.504202
0:  13.005634  13.082331  13.646939  14.614958  15.575476  16.17253
0:  16.18584   16.13737  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.16268   -8.360674  -8.191309  -7.634849  -6.9867225 -6.506596
0:  -6.409599  -6.457909  -6.9991565 -7.461655  -7.862164  -8.334919
0:  -8.501932  -8.534988  -8.296726  -7.717831  -7.022818  -6.3422656
0:  -7.085078  -7.3318295]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.328997 11.493653 11.888266 12.321704 12.603872 12.706734 12.695499
0:  12.681754 12.42058  12.109058 11.600971 10.852486 10.12509   9.559135
0:   9.395114  9.553463  9.825234  9.983719  8.098506  8.355211]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.303001 10.282197 10.628827 11.216554 11.668453 11.821846 11.806594
0:  11.720928 11.450289 11.284644 11.048849 10.541492 10.017857  9.45665
0:   9.197571  9.427607 10.093838 10.904261  9.266482  9.126047]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.221744 11.578316 12.056101 12.442739 12.644326 12.623873 13.098532
0:  13.550283 14.134951 14.698715 14.842228 14.829803 14.718281 15.021774
0:  15.632624 16.473776 17.11383  17.31069  13.846022 13.939854]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.528225 29.503311 29.506823 29.450237 29.31038  28.893726 29.020805
0:  28.95095  28.960352 28.840551 28.20788  27.524405 26.846062 26.606222
0:  26.907375 27.627377 28.55343  29.264956 27.142174 27.021229]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.00732   15.579334  15.288462  15.061098  14.813307  14.38209
0:  14.44618   14.406717  14.499264  14.5358095 14.181667  13.776544
0:  13.407578  13.452749  13.973764  14.876715  15.917944  16.79854
0:  16.838654  16.882105 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.531388  8.550274  9.021492  9.635918 10.123909 10.409139 10.885669
0:  11.406869 12.032848 12.620241 12.980813 13.102272 13.192762 13.471672
0:  14.159023 15.204772 16.413809 17.536081 15.065704 15.180037]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.272787  -13.573881  -13.419485  -12.742876  -11.898825  -11.172699
0:  -10.658005  -10.491812  -10.867451  -11.35157   -12.010244  -12.73262
0:  -13.146431  -13.287773  -13.0160885 -12.377459  -11.711035  -11.073442
0:  -11.496616  -11.754076 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3195405 -3.5541444 -3.3857226 -2.798572  -2.174933  -1.6809344
0:  -1.5056453 -1.3673692 -1.6338983 -1.8417764 -2.0787892 -2.6071362
0:  -3.04464   -3.449759  -3.5386157 -3.0950618 -2.2412724 -1.2620931
0:  -2.519072  -2.8434215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.3859148  0.5189061  0.9394679  1.5741539  2.127106   2.471631
0:  2.7336607  2.7712293  2.464607   2.20652    1.7928982  1.2939687
0:  0.94510794 0.84684706 1.0826635  1.5966301  2.1116233  2.47534
0:  0.56310177 0.49970198]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7353673  -2.639378   -2.176416   -1.3329744  -0.407166    0.3975749
0:   0.79769087  1.0829883   0.86723137  0.6940422   0.5065627   0.13420439
0:  -0.1145134  -0.32285023 -0.28701496  0.18334341  0.9400525   1.7725763
0:   0.82163334  0.7556925 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.73521  26.706219 26.57367  26.24939  26.016674 25.665398 26.396168
0:  27.101704 28.060078 28.76548  28.683186 28.56662  28.382362 28.820675
0:  29.779129 31.046654 32.210438 32.845238 31.562363 31.703968]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2968621 2.3869376 2.8167176 3.3990853 3.8261921 3.980894  4.0290375
0:  3.971947  3.6414547 3.3724685 2.9626634 2.329145  1.7947116 1.3598723
0:  1.2631378 1.6324029 2.208897  2.8094807 1.6469278 1.7237453]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0709968 -3.409984  -3.3348699 -2.843061  -2.3184419 -1.96596
0:  -1.9829373 -2.0906014 -2.602097  -3.0577178 -3.518639  -4.2637534
0:  -4.923388  -5.5593667 -5.9511037 -5.747255  -5.1262145 -4.2617764
0:  -5.9779887 -6.535176 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.194585   8.4230995  8.928399   9.597176  10.106353  10.346639
0:  10.763334  11.055757  11.295814  11.518202  11.4303875 11.160088
0:  10.853769  10.837999  11.102381  11.662643  12.200179  12.486376
0:  10.967954  11.059459 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.807175 -10.09968   -9.847591  -9.150515  -8.40012   -7.851093
0:   -7.562674  -7.441505  -7.733501  -8.042202  -8.451191  -9.090158
0:   -9.487108  -9.681055  -9.505444  -8.925646  -8.301733  -7.731837
0:   -9.26021   -9.451752]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.964194   6.7670946  6.888203   7.199683   7.5850663  7.897116
0:   8.757092   9.526716  10.4033    11.176359  11.428738  11.540169
0:  11.609329  12.22349   13.344077  14.937999  16.576645  17.882875
0:  18.791275  19.196354 ]
0: validation loss for strategy=forecast at epoch 37 : 0.3424209952354431
0: validation loss for velocity_u : 0.16497501730918884
0: validation loss for velocity_v : 0.31281742453575134
0: validation loss for specific_humidity : 0.15300099551677704
0: validation loss for velocity_z : 0.5542023181915283
0: validation loss for temperature : 0.10267338156700134
0: validation loss for total_precip : 0.7668564915657043
0: 38 : 20:29:31 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 38, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0920, 0.0915, 0.0869, 0.0799, 0.0728, 0.0679, 0.0659, 0.0664, 0.0689, 0.0753, 0.0869, 0.1020, 0.1161, 0.1257,
0:         0.1300, 0.1294, 0.1266, 0.1246, 0.0767, 0.0751, 0.0690, 0.0612, 0.0538, 0.0476, 0.0435], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4175, -0.3819, -0.3499, -0.3244, -0.3079, -0.2995, -0.2974, -0.3018, -0.3102, -0.3207, -0.3316, -0.3404,
0:         -0.3443, -0.3425, -0.3371, -0.3301, -0.3223, -0.3145, -0.4015, -0.3696, -0.3427, -0.3225, -0.3096, -0.3030,
0:         -0.3020], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3033, -0.2813, -0.2684, -0.2641, -0.2684, -0.2983, -0.3347, -0.3832, -0.4258, -0.4671, -0.4853, -0.4937,
0:         -0.4973, -0.4926, -0.4855, -0.4753, -0.4653, -0.4561, -0.2820, -0.2542, -0.2328, -0.2237, -0.2230, -0.2428,
0:         -0.2750], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1600, -0.1236, -0.1257, -0.1257, -0.1300, -0.1943, -0.2951, -0.3744, -0.4279, -0.4365, -0.3336, -0.1386,
0:          0.0200,  0.0650,  0.0029, -0.1214, -0.2222, -0.2007, -0.1943, -0.1472, -0.1214, -0.0786, -0.0614, -0.1214,
0:         -0.2179], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.6003, 0.5944, 0.5870, 0.5775, 0.5651, 0.5493, 0.5310, 0.5096, 0.4867, 0.4652, 0.4480, 0.4372, 0.4322, 0.4298,
0:         0.4234, 0.4095, 0.3869, 0.3561, 0.3189, 0.2791, 0.2386, 0.1971, 0.1571, 0.1237, 0.0983], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1651, -0.1489, -0.1721, -0.1883, -0.2091, -0.2276, -0.2299, -0.1998, -0.1582, -0.1802, -0.1628, -0.1281,
0:         -0.1397, -0.1802, -0.2102, -0.2218, -0.2079, -0.1489, -0.1651, -0.1825, -0.1802, -0.1848, -0.1929, -0.2056,
0:         -0.2230], device='cuda:0')
0: [DEBUG] Epoch 38, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,  0.0721,     nan,     nan, -0.1744,     nan,     nan,     nan,     nan,
0:          0.0201,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.1277,  0.0143,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1697,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1489,     nan,     nan,     nan, -0.0772,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2056, -0.1894,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0957, -0.0563,     nan,     nan,     nan,     nan, -0.0216,     nan,     nan,     nan,     nan,
0:          0.1219,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1096,     nan,
0:             nan,     nan, -0.0401,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0015,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1454,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0448,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0876,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1061,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0656,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 38, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4422, -0.4398, -0.4046, -0.3459, -0.2994, -0.2778, -0.2782, -0.2999, -0.3548, -0.4020, -0.4513, -0.5078,
0:         -0.5392, -0.5529, -0.5396, -0.5033, -0.4654, -0.4380, -0.3622, -0.3962, -0.3994, -0.3752, -0.3572, -0.3369,
0:         -0.3223], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8703, -0.8732, -0.8838, -0.8984, -0.9188, -0.9588, -1.0233, -1.0619, -1.0272, -0.9411, -0.8232, -0.7108,
0:         -0.6581, -0.6716, -0.7097, -0.7035, -0.6301, -0.5237, -0.8947, -0.8844, -0.8725, -0.8721, -0.8868, -0.9423,
0:         -1.0004], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0862, -0.0819, -0.1099, -0.1539, -0.2143, -0.2942, -0.3869, -0.4808, -0.5646, -0.6311, -0.6796, -0.7033,
0:         -0.7042, -0.6951, -0.6872, -0.6753, -0.6759, -0.6627, -0.1035, -0.1089, -0.1475, -0.1997, -0.2763, -0.3658,
0:         -0.4555], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3015, -0.2260, -0.0668, -0.0894, -0.1004,  0.0234,  0.0372,  0.1233,  0.2442,  0.1892,  0.1385,  0.1033,
0:          0.0494, -0.0582, -0.1774, -0.0954,  0.0391,  0.0271, -0.2799, -0.3969, -0.3193, -0.2694, -0.1650,  0.0604,
0:          0.1314], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.8989, 0.8740, 0.8296, 0.7714, 0.7176, 0.6781, 0.6555, 0.6367, 0.6142, 0.5819, 0.5428, 0.5053, 0.4746, 0.4544,
0:         0.4382, 0.4187, 0.3999, 0.3898, 0.3897, 0.3976, 0.4041, 0.3971, 0.3700, 0.3219, 0.2541], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0278, -0.0246, -0.0347, -0.0384, -0.0438, -0.0531, -0.0548, -0.0598, -0.0531, -0.0098, -0.0083, -0.0178,
0:         -0.0325, -0.0453, -0.0574, -0.0558, -0.0539, -0.0641, -0.0082, -0.0083, -0.0224, -0.0304, -0.0429, -0.0527,
0:         -0.0721], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.19741329550743103; velocity_v: 0.3317270874977112; specific_humidity: 0.18174825608730316; velocity_z: 0.608895480632782; temperature: 0.14754311740398407; total_precip: 0.6134130954742432; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20940864086151123; velocity_v: 0.3271214962005615; specific_humidity: 0.17962706089019775; velocity_z: 0.6433234810829163; temperature: 0.14660851657390594; total_precip: 0.5685502290725708; 
0: epoch: 38 [1/5 (20%)]	Loss: 0.59098 : 0.31161 :: 0.21565 (2.53 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2209569215774536; velocity_v: 0.30695927143096924; specific_humidity: 0.18261182308197021; velocity_z: 0.5739518404006958; temperature: 0.14071623980998993; total_precip: 0.7171381115913391; 
0: epoch: 38 [2/5 (40%)]	Loss: 0.71714 : 0.32271 :: 0.21405 (16.04 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23771260678768158; velocity_v: 0.34817245602607727; specific_humidity: 0.20376306772232056; velocity_z: 0.7385334968566895; temperature: 0.1534983068704605; total_precip: 0.8072670102119446; 
0: epoch: 38 [3/5 (60%)]	Loss: 0.80727 : 0.37965 :: 0.21698 (15.81 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2652346193790436; velocity_v: 0.32946640253067017; specific_humidity: 0.22186781466007233; velocity_z: 0.6573927998542786; temperature: 0.2143489271402359; total_precip: 0.8464263081550598; 
0: epoch: 38 [4/5 (80%)]	Loss: 0.84643 : 0.38612 :: 0.21310 (15.85 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0.]
0: Target values (first 200):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
0:  0. 0. 0. 0. 0. 0. 0. 0.]
0: Prediction values (first 20):
0: [-7.284313  -7.296133  -6.8505764 -5.980052  -5.027076  -4.3149004
0:  -4.0862865 -4.163641  -4.912238  -5.6369724 -6.406782  -7.261284
0:  -7.827179  -8.190455  -8.194194  -7.7837133 -7.164604  -6.4887323
0:  -7.162056  -7.037163 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.846, max = 0.916, mean = -0.559
0:          sample (first 20): tensor([-1.0520, -1.0529, -1.0186, -0.9517, -0.8784, -0.8237, -0.8061, -0.8120, -0.8696, -0.9253, -0.9845, -1.0502,
0:         -1.0937, -1.1217, -1.1220, -1.0904, -1.0428, -0.9908, -1.0087, -1.0620])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.253202 10.966524 10.913889 10.899168 10.795662 10.48513  10.457964
0:  10.354409 10.293644 10.225562  9.86264   9.386662  8.94112   8.871455
0:   9.218633  9.987571 10.906872 11.634837 10.669626 10.820347]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.082397 17.312693 17.714352 18.192383 18.554617 18.745974 19.131184
0:  19.431753 19.609653 19.759195 19.623577 19.310942 19.06263  19.031443
0:  19.347326 19.961191 20.64746  21.185778 18.946716 19.156359]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.2183485  7.639618   8.365631   9.141218   9.700435   9.967413
0:  10.213341  10.374434  10.438122  10.496728  10.390711  10.1470375
0:   9.966087   9.9913225 10.45106   11.27828   12.28763   13.182077
0:  12.487649  13.002039 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.117394  14.028906  14.141785  14.275589  14.244325  13.884823
0:  13.940704  13.8396225 13.880249  13.898679  13.496277  13.0472355
0:  12.533381  12.490185  12.820282  13.607948  14.470229  15.078574
0:  13.79376   13.906836 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.20681  32.40306  32.704933 32.976807 33.118362 32.884884 33.01174
0:  32.934753 32.942627 32.861897 32.37439  31.795609 31.28344  31.00025
0:  31.120958 31.451363 31.869425 32.09612  29.27897  29.368454]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.0258746 0.9208765 1.1987672 1.7115688 2.198079  2.5532908 3.0352168
0:  3.475565  3.7008896 3.8414147 3.6511192 3.184829  2.7497969 2.5455108
0:  2.6774998 3.0912454 3.4838445 3.6155608 1.477396  1.6147962]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.73243475  0.6218972   0.7958698   1.1268702   1.369534    1.3725653
0:   1.3381405   1.110755    0.62779045  0.16355991 -0.44359636 -1.1390133
0:  -1.5974936  -1.7113304  -1.4818454  -0.9750676  -0.51328945 -0.29669094
0:  -3.2679057  -3.5857072 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.339302 27.573214 27.81381  27.983171 27.99488  27.742083 27.796085
0:  27.70608  27.698336 27.648685 27.279467 26.844643 26.476765 26.385696
0:  26.681004 27.25769  28.017145 28.660498 26.970116 27.199173]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.093447  -9.297791  -9.088417  -8.42206   -7.6431117 -7.0119233
0:  -6.77327   -6.7039847 -7.1623826 -7.6444926 -8.137148  -8.790199
0:  -9.196161  -9.423735  -9.315828  -8.775866  -8.010809  -7.1920314
0:  -8.044859  -8.295847 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.071194 20.099426 20.187344 20.146637 19.964895 19.635395 19.732254
0:  19.77493  19.879864 19.87966  19.468498 19.058773 18.75776  19.088158
0:  20.052137 21.379753 22.771374 23.763924 24.270988 25.109192]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7906487  2.9083252  3.222184   3.6562922  3.8516011  3.7815042
0:  3.4242852  3.149832   2.6818738  2.4628046  2.309674   1.8861079
0:  1.4750757  0.9835501  0.70267296 0.9040561  1.5488524  2.3694196
0:  0.80310345 0.61433125]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.38488    -2.333109   -1.848814   -0.9680824  -0.088521    0.55142164
0:   0.86781836  1.0406022   0.7657094   0.5578723   0.26146603 -0.27922964
0:  -0.7985196  -1.360342   -1.7118549  -1.6643085  -1.354908   -0.8737469
0:  -1.3315344  -1.246067  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.666476  10.683527  10.853452  11.06192   11.207648  11.1720295
0:  11.363745  11.471608  11.587512  11.64436   11.406313  11.059338
0:  10.733755  10.73823   11.129711  11.877665  12.753131  13.501223
0:  12.124054  12.186993 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.6903577 -4.554972  -3.9941401 -3.135344  -2.3264308 -1.7863746
0:  -1.5289483 -1.5414596 -1.9834142 -2.3736262 -2.8186898 -3.3724437
0:  -3.6462073 -3.7480054 -3.514031  -2.9293141 -2.25351   -1.5916305
0:  -2.2612834 -1.9360824]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.138247  15.053923  15.281755  15.657282  15.7933235 15.5796585
0:  15.196625  14.785454  14.211543  13.732248  13.146301  12.363396
0:  11.633236  11.097409  11.059706  11.585524  12.461275  13.25542
0:  10.636425  10.468236 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -6.1551414  -6.2717824  -6.0749745  -5.530833   -4.9294534  -4.512358
0:   -4.466398   -4.713017   -5.6083493  -6.521522   -7.568109   -8.77227
0:   -9.690001  -10.42461   -10.736017  -10.570597  -10.187473   -9.680521
0:   -9.580708   -9.870947 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.4524798   2.571134    2.9712267   3.5194857   3.8493445   3.797193
0:   3.5475368   3.1601813   2.5448837   2.0392342   1.4999733   0.8146291
0:   0.29545116  0.00538111  0.06293201  0.57153606  1.2830138   1.9334493
0:  -0.19189167 -0.44472456]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.986876 21.422146 22.03909  22.586197 22.915012 22.931042 23.293865
0:  23.526981 23.851719 24.174896 24.100563 24.034737 24.053375 24.449615
0:  25.314377 26.399662 27.529362 28.349144 27.397682 27.716446]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.351192  1.521977  1.9754498 2.5534024 3.0118084 3.223431  3.4501777
0:  3.565009  3.5204854 3.4835432 3.2210162 2.7816055 2.4169362 2.2878582
0:  2.4437175 2.8972964 3.3116944 3.507369  1.5828571 1.8216696]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.918907 19.861637 20.002344 20.204887 20.301373 20.121105 20.25935
0:  20.322016 20.513834 20.76022  20.745123 20.70823  20.720974 21.0084
0:  21.722406 22.74554  23.983547 25.047268 24.654049 24.843166]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.696032   9.734494   9.896273  10.143094  10.281861  10.209081
0:   9.950449   9.579218   8.890834   8.275098   7.654476   6.97683
0:   6.4682465  6.1348777  6.0849824  6.395993   6.8455334  7.3353653
0:   6.3114486  6.2251067]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.412453  -11.734474  -11.622154  -10.976887  -10.195101   -9.456238
0:   -9.223185   -9.12796    -9.662283  -10.2797985 -11.035128  -12.108385
0:  -13.082882  -13.991817  -14.565922  -14.516403  -13.972137  -13.155695
0:  -14.673265  -15.461367 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.5563602 -6.4092264 -5.919545  -5.2261877 -4.625828  -4.260339
0:  -3.9777899 -3.987053  -4.354539  -4.703615  -5.2331047 -5.8555107
0:  -6.285925  -6.363493  -6.047063  -5.3151703 -4.542001  -3.753377
0:  -2.8766432 -2.5497131]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.999858 -16.293911 -16.114296 -15.438012 -14.644034 -13.982745
0:  -13.2832   -12.662423 -12.102511 -11.549893 -11.095571 -10.769151
0:  -10.323127  -9.625577  -8.697763  -7.53918   -6.520257  -5.74395
0:   -5.727623  -5.401437]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.75804  44.12675  44.455315 44.548195 44.267723 43.596546 43.611794
0:  43.7022   44.020878 44.228977 43.750385 43.1242   42.591747 42.531425
0:  43.111897 43.852707 44.35739  44.356956 42.75504  43.04706 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.8853645 -7.8387165 -7.3130207 -6.4757347 -5.7189054 -5.28526
0:  -5.066964  -5.0573187 -5.3408194 -5.6437216 -6.0383615 -6.598428
0:  -6.948476  -7.064354  -6.853326  -6.2506895 -5.624564  -5.154505
0:  -6.5556726 -6.4694395]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.338247  -13.547379  -13.275299  -12.568254  -11.832275  -11.374115
0:  -11.38385   -11.718609  -12.597021  -13.484375  -14.369419  -15.366156
0:  -15.9819145 -16.377499  -16.394264  -15.957508  -15.317505  -14.583729
0:  -15.067274  -15.346989 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.0381465  -0.02322674  0.25881815  0.7671437   1.1334977   1.2521949
0:   1.2071371   1.2126951   1.0368686   0.99706316  0.91647243  0.6086292
0:   0.33824348  0.19806051  0.41839504  1.1375747   2.126939    3.0378814
0:   1.9056497   1.8926442 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.732097  -10.474626  -10.67232   -10.358752   -9.897853   -9.5727625
0:   -9.576045   -9.708957  -10.296487  -10.895352  -11.547782  -12.356286
0:  -12.905791  -13.244878  -13.2160845 -12.670274  -11.834752  -10.946425
0:  -11.816646  -12.022418 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.34704   7.809341  8.592462  9.46776  10.257767 10.839709 11.450304
0:  11.952475 12.331211 12.707958 12.945173 13.109105 13.397182 13.941903
0:  14.87385  16.057638 17.303905 18.282812 16.807974 17.511837]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.935602  6.06451   6.4106846 6.849749  7.160489  7.2481728 7.273561
0:  7.299893  7.1718855 7.125157  7.024791  6.838566  6.8206234 6.915793
0:  7.366042  8.078687  8.9205885 9.6108465 7.9696193 8.322324 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.72212  26.447401 26.17853  25.847754 25.54677  25.017303 25.24973
0:  25.287413 25.452871 25.43094  24.766867 24.213636 23.68121  23.814974
0:  24.561745 25.661194 26.779713 27.49259  26.788612 26.690872]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.68405  21.938616 21.138317 20.053192 18.578758 16.719072 15.352499
0:  14.020746 13.098426 12.376619 11.519205 10.933438 10.603512 10.968678
0:  11.996395 13.369289 14.818354 15.818531 16.385662 16.5484  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.1565762   1.9901767   2.12363     2.5987206   3.1429672   3.5613768
0:   3.576837    3.485529    2.8714418   2.2854743   1.6831937   0.89802074
0:   0.26899767 -0.2800641  -0.5583167  -0.3838067   0.15285635  0.859777
0:  -0.9672103  -1.3175483 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.350157   9.489873   9.841712  10.226004  10.415179  10.297256
0:  10.128237   9.76836    9.239787   8.690485   8.015465   7.256769
0:   6.672049   6.398916   6.4653087  6.827179   7.270919   7.583132
0:   5.510675   5.44295  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.8945312  7.8155107  8.154571   8.758362   9.417127   9.95671
0:  10.581652  11.241588  11.721703  12.2537    12.618212  12.7085
0:  12.769848  12.830881  13.104477  13.552435  14.109451  14.467346
0:  12.953978  13.25738  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0456767  -3.0357127  -2.6378846  -1.8863912  -1.0336547  -0.3673377
0:  -0.02382708  0.12961912 -0.33842325 -0.7636256  -1.2850337  -1.9727607
0:  -2.5117517  -3.0202537  -3.2158713  -2.9991202  -2.4490094  -1.8228717
0:  -4.440286   -4.920398  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.43705  28.779581 29.062304 29.175634 29.070017 28.699947 28.945898
0:  29.046934 29.360178 29.518375 29.022264 28.464912 27.869349 27.78642
0:  28.275135 29.05758  29.851805 30.31805  28.774727 29.01618 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.901626   -5.766436   -5.2279677  -4.3513255  -3.4677196  -2.821405
0:  -2.5458694  -2.4080014  -2.6244712  -2.6946206  -2.6017957  -2.5951982
0:  -2.3604102  -2.0463605  -1.5163512  -0.69153214  0.24933863  1.1458845
0:   0.5956974   0.523273  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.079102  10.957491  11.090322  11.360098  11.5062065 11.390398
0:  11.517181  11.477761  11.510424  11.450456  11.030955  10.520393
0:   9.972065   9.817878  10.063055  10.688762  11.377783  11.927868
0:   9.980909   9.877293 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.167338  -4.15505   -3.760941  -2.9612823 -2.0888877 -1.3908658
0:  -1.0563293 -0.9133563 -1.3026881 -1.6756215 -2.084373  -2.6276717
0:  -2.9417024 -3.1583467 -3.086701  -2.6725001 -2.0845656 -1.4687934
0:  -2.2788835 -2.2689443]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.546299  11.366772  11.145002  10.744747  10.257503   9.663076
0:   9.845848   9.964924  10.200293  10.131355   9.268454   8.260276
0:   7.0697474  6.632304   6.7517905  7.3166666  7.930854   8.207607
0:   7.692525   7.686117 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6832538  -1.6826425  -1.2722774  -0.5354657   0.32216072  1.1101799
0:   1.6949191   2.0216546   1.8338556   1.6219482   1.2944231   0.83922434
0:   0.6554842   0.6338539   1.0017891   1.6725669   2.451012    3.1228573
0:   1.7342992   1.8954024 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.13792562  0.03034019  0.66718054  1.6484871   2.600151    3.2625556
0:   3.6125705   3.711312    3.363474    2.9986482   2.468423    1.6969795
0:   1.016973    0.37699127 -0.03227329 -0.09423208  0.03757191  0.27059174
0:  -1.2208638  -1.1706882 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.170742 21.857979 21.703695 21.614946 21.423586 20.923588 20.829798
0:  20.632957 20.571442 20.437794 19.947334 19.404667 18.877594 18.619255
0:  18.810608 19.340397 20.06985  20.673586 19.469837 19.24929 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.635811  8.57245   8.810234  9.2324505 9.5299    9.576078  9.706594
0:  9.710686  9.618109  9.529229  9.193386  8.694122  8.205073  7.9913697
0:  8.084147  8.490256  8.940838  9.207723  6.599331  6.5051565]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.75055   9.776014  9.962747 10.196955 10.445309 10.624854 11.362839
0:  12.066882 12.940687 13.737332 14.170008 14.537811 14.911326 15.720739
0:  16.909729 18.33268  19.672592 20.615547 17.531975 17.680859]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.7212634  6.7946873  6.901259   7.0074778  7.0656843  7.096413
0:   7.554851   8.063643   8.713841   9.352944   9.7030945 10.078593
0:  10.566803  11.589077  13.128983  15.007065  16.959763  18.665525
0:  21.006586  22.068333 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.227035  2.3359985 2.7279427 3.3189244 3.8413258 4.1293297 4.1687393
0:  4.173023  3.8270183 3.598735  3.3359427 2.8501544 2.452535  2.0914955
0:  1.9606767 2.2308354 2.7261815 3.2622051 2.147963  2.1695552]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.918853  7.872822  8.2214365 8.698138  8.971489  8.884033  8.815481
0:  8.62059   8.373886  8.233855  7.9353476 7.403149  6.96391   6.6880493
0:  6.7696457 7.334422  8.09145   8.759782  7.3024497 7.2899303]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.820635 15.471318 15.223945 14.92207  14.55779  13.920998 13.683724
0:  13.37426  13.15086  12.876363 12.1576   11.408743 10.545878 10.081085
0:  10.090851 10.451243 10.964155 11.330385 11.733066 11.292127]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.296408  13.23132   13.158897  12.910938  12.639577  12.220941
0:  12.499092  12.721227  13.064674  13.264145  12.917727  12.592447
0:  12.225563  12.460149  13.16544   14.2477665 15.294989  16.00368
0:  16.320288  16.296234 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.730923 38.957638 38.81103  38.18038  37.24114  35.93075  35.538124
0:  35.219223 35.226715 35.11571  34.20876  33.222435 32.003345 31.335526
0:  31.299244 31.718555 32.406094 32.71661  32.698227 32.994156]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.395224 15.574088 15.970599 16.403284 16.707184 16.769623 16.897512
0:  16.923996 16.8525   16.800514 16.574778 16.205574 15.96782  15.884978
0:  16.188034 16.779778 17.493067 18.042099 15.454331 15.678764]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.1145616 5.316363  5.735602  6.2639008 6.5972795 6.678913  6.509459
0:  6.318961  5.833092  5.4013987 4.901886  4.19514   3.6089602 3.1658764
0:  3.0842807 3.4673657 4.1050553 4.7020917 2.2152674 1.8623247]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.612978  -9.852575  -9.612717  -8.884005  -8.014719  -7.2394385
0:  -6.865743  -6.617209  -6.90906   -7.2069287 -7.4953685 -7.993442
0:  -8.2631    -8.464138  -8.36046   -7.8464713 -7.100182  -6.254378
0:  -6.350674  -6.4436736]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.6509247  1.6365666  1.9450908  2.4756017  2.972955   3.2592652
0:  3.3084188  3.2537124  2.8803415  2.5554333  2.1581278  1.532218
0:  0.9875126  0.58252907 0.48601818 0.87153244 1.5529618  2.2957478
0:  1.0012345  0.9937291 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [50.866436 51.473423 51.882133 52.29735  52.574768 52.543907 53.31061
0:  53.855865 54.50541  54.749046 54.17508  53.614456 53.118805 53.089848
0:  53.589024 54.15991  54.648605 54.595306 52.106594 52.445503]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.180615   -1.5472021  -1.6271791  -1.3162346  -0.9141588  -0.5962143
0:  -0.5973611  -0.62518644 -1.0707321  -1.4411459  -1.7935433  -2.3527293
0:  -2.7791018  -3.1745086  -3.3063045  -3.0068831  -2.4152017  -1.7090573
0:  -2.945313   -3.4092736 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.002245 20.844969 21.157835 21.774597 22.409546 22.864573 23.548544
0:  24.193232 24.828962 25.356657 25.508204 25.325146 25.074974 24.850029
0:  25.019514 25.463564 26.027824 26.402231 23.602583 23.339378]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.9922185  -0.83936214 -0.2904687   0.5391865   1.2704473   1.6884885
0:   1.7909102   1.7209573   1.328445    1.0134029   0.7018876   0.24431992
0:  -0.03105068 -0.17461634 -0.05771923  0.44339514  1.0616179   1.6047597
0:  -0.16129827 -0.00254917]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.572186 38.951206 38.916245 38.5399   38.113964 37.61343  38.22762
0:  38.906544 39.757618 40.328423 39.94703  39.729267 39.43176  39.894806
0:  41.017757 42.209846 43.15629  43.281174 43.56451  44.221405]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.085653 22.861261 22.641924 22.40542  22.219913 21.92579  22.444433
0:  22.875792 23.413427 23.690468 23.232079 22.632172 21.867985 21.62465
0:  21.945925 22.6197   23.436039 23.824873 22.493359 22.432486]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.684017   -6.255597   -6.5636616  -6.6015806  -6.553196   -6.600089
0:  -6.2090855  -5.8546143  -5.3766327  -4.8784304  -4.657853   -4.5640945
0:  -4.4714785  -3.932047   -2.9917545  -1.6078825  -0.15252066  1.0536113
0:  -0.06387711  0.30912352]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.100244   -3.1280732  -2.7001128  -1.9160762  -1.0492511  -0.40991163
0:  -0.1116724  -0.03251982 -0.497849   -0.9778147  -1.5522819  -2.3038726
0:  -2.833508   -3.2531133  -3.3525577  -3.0899577  -2.6893945  -2.3094745
0:  -4.3553777  -4.616283  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.442903 -13.457255 -12.926599 -11.905105 -10.796451  -9.937352
0:   -9.529987  -9.372641  -9.805802 -10.223346 -10.667848 -11.297685
0:  -11.615741 -11.825381 -11.717137 -11.213044 -10.600118  -9.967164
0:  -11.012147 -11.145255]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3924603 6.537455  6.8804803 7.322648  7.5639343 7.4914327 7.085873
0:  6.3989244 5.2801175 4.2499704 3.3505132 2.6200829 2.3588476 2.4324334
0:  2.9191854 3.714141  4.580425  5.341774  3.265237  3.3102705]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.1846175  -7.0176587  -6.393865   -5.386562   -4.320969   -3.3838878
0:  -2.5955863  -1.9100099  -1.5650682  -1.2570314  -1.0328937  -0.9641199
0:  -0.73136425 -0.37855244  0.22811842  1.0284033   1.8085883   2.3960965
0:   1.459538    1.832289  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.426956 13.945064 13.6196   13.339427 13.025488 12.525702 12.58761
0:  12.586189 12.745272 12.833914 12.436029 11.945001 11.33689  11.222183
0:  11.629023 12.485092 13.516449 14.33252  13.979647 13.973018]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5625205 1.7756009 2.3455887 3.1839442 3.9995458 4.6066685 5.289874
0:  5.814485  6.1697235 6.3867903 6.291382  5.984938  5.6962786 5.655896
0:  5.965659  6.621827  7.402716  8.031936  7.563831  7.9034667]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.07579  28.979897 28.906792 28.796165 28.586323 28.055511 28.062428
0:  27.832695 27.596035 27.167107 26.153114 25.173435 24.226364 23.952702
0:  24.26765  24.96236  25.754333 26.16983  25.651901 25.600582]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.54139  11.706275 12.084503 12.46558  12.711227 12.777384 13.000247
0:  13.166685 13.2746   13.335459 13.105479 12.729259 12.395082 12.319809
0:  12.622631 13.220436 13.883858 14.423595 12.638123 12.834806]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.139069 14.999526 14.897695 14.771279 14.573926 14.104713 14.193678
0:  14.203559 14.290015 14.29993  13.769127 13.219906 12.47418  12.213444
0:  12.40847  12.949728 13.587261 13.944786 14.371229 14.252512]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.197959  -9.710187  -9.779475  -9.493979  -9.370061  -9.625173
0:  -10.256986 -11.236585 -12.631209 -14.016341 -15.440272 -16.967243
0:  -18.123829 -18.878883 -19.060287 -18.701069 -18.176422 -17.603592
0:  -19.215462 -19.633892]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.616819 12.685926 12.911266 13.195367 13.299862 13.116356 13.080826
0:  12.959165 12.801117 12.690187 12.38518  11.93062  11.491731 11.263929
0:  11.360116 11.811218 12.445376 13.001316 10.866787 10.796649]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.7540188  1.7286928  2.1071496  2.6985734  3.137644   3.2724297
0:  3.1109653  2.8884678  2.3789463  1.9996641  1.6380811  1.0799403
0:  0.64803505 0.31440926 0.28089952 0.744236   1.5080166  2.2839935
0:  0.72821426 0.7986121 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.798711   9.1161     9.74276   10.430588  10.819446  10.919748
0:  10.816758  10.805296  10.6703005 10.682573  10.634759  10.344523
0:  10.037601   9.787488   9.914602  10.474909  11.337442  12.174883
0:   9.744665   9.95092  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.8143778 0.8302641 1.1412764 1.7162209 2.315428  2.7647436 3.2338405
0:  3.6216147 3.757051  3.8424795 3.7151532 3.4537137 3.288822  3.3108015
0:  3.6244972 4.1913414 4.7671857 5.173041  4.038602  4.2090845]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.620378 20.795473 21.330702 21.957314 22.33084  22.3467   22.258202
0:  22.171478 22.009623 21.922667 21.714962 21.252851 20.828817 20.46055
0:  20.46961  20.820385 21.320189 21.596619 17.86645  17.270426]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.67062  14.846817 15.179125 15.49687  15.700651 15.640709 15.985943
0:  16.20255  16.51656  16.714573 16.518133 16.167276 15.849495 15.910776
0:  16.447397 17.410713 18.451693 19.276176 16.281206 16.44452 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.000563  -6.867328  -6.464251  -5.661962  -4.7712626 -4.0016465
0:  -3.5545745 -3.2379127 -3.4968047 -3.7019286 -3.9913697 -4.4347034
0:  -4.711801  -4.897351  -4.8496337 -4.5528045 -4.1154885 -3.7436204
0:  -8.693775  -8.984261 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.683775 27.624372 27.541227 27.351675 27.091774 26.597286 26.726648
0:  26.78     27.049416 27.178877 26.767916 26.435617 26.14523  26.28812
0:  26.944551 27.832409 28.737976 29.412804 30.235607 30.46768 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.9380646 -7.912691  -7.5320225 -6.745365  -5.8642797 -5.121524
0:  -4.8024483 -4.6427145 -5.0788155 -5.4845953 -5.9353704 -6.568721
0:  -7.001013  -7.3492374 -7.365163  -6.9422264 -6.252653  -5.332535
0:  -5.1349506 -5.1787157]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2911794 3.3381655 3.7485278 4.3543334 4.9222355 5.311375  5.6131244
0:  5.7717924 5.725022  5.6868773 5.56332   5.3474965 5.350794  5.5670485
0:  6.077558  6.8659997 7.6819577 8.367363  5.9511952 6.0780144]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.0805054 3.9081905 4.01035   4.2725973 4.4586515 4.457807  4.405102
0:  4.185687  3.683233  3.1813452 2.5437474 1.7802491 1.2242537 0.9885874
0:  1.1090126 1.5596209 2.0257044 2.3577752 1.4786496 1.602745 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.128212 42.970856 42.41484  41.828766 41.33101  40.72095  41.20979
0:  41.627598 42.192173 42.435097 41.83437  41.365715 40.91681  41.152824
0:  42.04994  43.11715  44.169987 44.542305 46.533077 46.832794]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.857134 19.833353 19.938072 20.008543 19.884806 19.39154  19.220646
0:  18.847206 18.558573 18.220673 17.506111 16.714304 15.868229 15.391678
0:  15.313168 15.634029 16.208534 16.722603 15.608984 15.580788]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.147321  11.437248  11.91804   12.316474  12.372173  12.072862
0:  11.688407  11.241041  10.675762  10.164599   9.513346   8.720504
0:   8.035689   7.5813546  7.5745144  7.9464974  8.46952    8.891193
0:   7.2668085  7.5850654]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0613704 3.172895  3.5688796 4.174716  4.725807  5.0887275 5.2774506
0:  5.347346  5.064353  4.796652  4.436601  3.9593754 3.5860288 3.2905188
0:  3.247291  3.55501   4.035266  4.5718966 3.4622679 3.5474765]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.962622   6.1357183  6.505368   6.956105   7.252144   7.3177133
0:   7.756876   8.186039   8.732734   9.323366   9.534156   9.555623
0:   9.434482   9.645532  10.127777  10.910564  11.699652  12.185625
0:   9.856668   9.770075 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.701622 18.354918 18.126915 17.934465 17.67612  17.188679 17.113073
0:  16.933815 16.797276 16.607746 15.967798 15.261638 14.467069 14.063603
0:  14.053683 14.459873 14.956413 15.334358 13.748148 13.75646 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.946243 18.317444 18.760822 19.086323 19.060795 18.635838 18.29964
0:  17.936638 17.582088 17.350939 16.984354 16.55508  16.16534  15.960074
0:  16.114922 16.606853 17.31089  17.952152 15.872812 16.132587]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7481694 -4.7046294 -4.2736926 -3.473065  -2.70217   -2.1684203
0:  -1.9356532 -1.9438262 -2.4226003 -2.9311833 -3.5623288 -4.349358
0:  -4.927851  -5.271436  -5.2025256 -4.6986756 -4.027386  -3.3896804
0:  -4.4238887 -4.419867 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4789014 -3.6678205 -3.5191493 -2.997456  -2.4386916 -2.0402384
0:  -2.0612645 -2.1882238 -2.8152509 -3.3857503 -3.907124  -4.6141725
0:  -5.119803  -5.5276227 -5.6053414 -5.154773  -4.343225  -3.367516
0:  -3.4131627 -3.6048923]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.825859  -6.8978243 -6.634693  -6.0352106 -5.4699154 -5.1321554
0:  -5.0436945 -5.1081514 -5.5554075 -5.929739  -6.3389673 -6.853245
0:  -7.154661  -7.286559  -7.144252  -6.6526136 -6.073715  -5.534852
0:  -6.9966364 -7.007779 ]
0: validation loss for strategy=forecast at epoch 38 : 0.31541240215301514
0: validation loss for velocity_u : 0.1543080359697342
0: validation loss for velocity_v : 0.26669809222221375
0: validation loss for specific_humidity : 0.17621026933193207
0: validation loss for velocity_z : 0.5871766209602356
0: validation loss for temperature : 0.10904264450073242
0: validation loss for total_precip : 0.5990391373634338
0: 39 : 20:33:27 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 39, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4044, -0.4022, -0.4004, -0.4092, -0.4104, -0.4222, -0.4407, -0.4616, -0.4854, -0.4997, -0.5073, -0.4255,
0:         -0.2988, -0.2736, -0.2946, -0.3095, -0.3564, -0.4229, -0.4401, -0.4401, -0.4357, -0.4345, -0.4313, -0.4394,
0:         -0.4595], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1068, -0.0770, -0.0426, -0.0169, -0.0041,  0.0020,  0.0088,  0.0110,  0.0065, -0.0033, -0.0711, -0.2193,
0:         -0.3075, -0.2900, -0.2926, -0.3150, -0.2938, -0.2368, -0.1204, -0.0984, -0.0695, -0.0420, -0.0291, -0.0228,
0:         -0.0114], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5783, -0.5701, -0.5718, -0.5737, -0.5826, -0.5948, -0.5856, -0.5588, -0.5650, -0.4889, -0.4347, -0.5464,
0:         -0.6037, -0.6200, -0.6256, -0.6262, -0.6281, -0.6308, -0.5725, -0.5626, -0.5572, -0.5649, -0.5684, -0.5784,
0:         -0.5803], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5312, -0.3608, -0.1571, -0.0951, -0.0309,  0.0067, -0.0575, -0.1239,  0.1218,  0.1196, -0.1704,  0.3543,
0:          0.5070, -0.0841, -0.0597,  0.1572,  0.1262,  0.0908, -0.5512, -0.3785, -0.2191, -0.1283, -0.1018, -0.0996,
0:         -0.0110], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.0116, -0.0604, -0.0624, -0.0035,  0.0810,  0.2070,  0.2817,  0.2968,  0.3226,  0.2843,  0.3606,  0.7075,
0:          0.8656,  0.7911,  0.7493,  0.7533,  0.7789,  0.8703,  0.9898,  1.0599,  1.0512,  0.9825,  0.9066,  0.8697,
0:          0.8492], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381,
0:         -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381,
0:         -0.2381], device='cuda:0')
0: [DEBUG] Epoch 39, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381, -0.2381,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2381,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,
0:         -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2381,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 39, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2144, -0.2171, -0.1934, -0.1538, -0.1250, -0.1205, -0.1390, -0.1608, -0.1986, -0.2252, -0.2520, -0.2955,
0:         -0.3376, -0.3695, -0.3715, -0.3364, -0.2755, -0.2168, -0.2103, -0.2684, -0.2843, -0.2754, -0.2614, -0.2585,
0:         -0.2656], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2006,  0.2135,  0.2163,  0.2155,  0.1969,  0.1640,  0.1105,  0.0648,  0.0395,  0.0300,  0.0309,  0.0370,
0:          0.0165, -0.0296, -0.0832, -0.0946, -0.0602,  0.0005,  0.1766,  0.2157,  0.2345,  0.2320,  0.2080,  0.1673,
0:          0.1196], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5872, -0.5918, -0.5936, -0.5905, -0.5925, -0.6065, -0.6240, -0.6521, -0.6784, -0.6965, -0.7070, -0.7103,
0:         -0.6948, -0.6704, -0.6637, -0.6546, -0.6657, -0.6772, -0.5870, -0.5851, -0.5778, -0.5719, -0.5765, -0.5953,
0:         -0.6160], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1371, 0.2872, 0.2751, 0.2056, 0.2184, 0.3442, 0.3140, 0.2416, 0.4487, 0.4385, 0.1589, 0.2573, 0.4790, 0.2814,
0:         0.0827, 0.2192, 0.2765, 0.1796, 0.2822, 0.2512, 0.0994, 0.0451, 0.1076, 0.2989, 0.3293], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.0254, 0.0169, 0.0273, 0.0508, 0.0878, 0.1179, 0.1378, 0.1593, 0.2142, 0.3312, 0.5092, 0.7173, 0.8963, 1.0083,
0:         1.0462, 1.0444, 1.0398, 1.0626, 1.1181, 1.1934, 1.2722, 1.3424, 1.3870, 1.3996, 1.3804], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2488, -0.2548, -0.2697, -0.2586, -0.2624, -0.2598, -0.2594, -0.2588, -0.2494, -0.2545, -0.2607, -0.2592,
0:         -0.2626, -0.2644, -0.2588, -0.2587, -0.2509, -0.2639, -0.2601, -0.2608, -0.2674, -0.2646, -0.2550, -0.2619,
0:         -0.2601], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.21635498106479645; velocity_v: 0.32035765051841736; specific_humidity: 0.18769748508930206; velocity_z: 0.5089111328125; temperature: 0.16557008028030396; total_precip: 0.5299391150474548; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23316624760627747; velocity_v: 0.3793735206127167; specific_humidity: 0.20567822456359863; velocity_z: 0.6793460845947266; temperature: 0.17509518563747406; total_precip: 0.553715705871582; 
0: epoch: 39 [1/5 (20%)]	Loss: 0.54183 : 0.31083 :: 0.20903 (2.64 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2008713334798813; velocity_v: 0.313448429107666; specific_humidity: 0.17686894536018372; velocity_z: 0.5267043709754944; temperature: 0.15657269954681396; total_precip: 0.7491447329521179; 
0: epoch: 39 [2/5 (40%)]	Loss: 0.74914 : 0.32053 :: 0.20827 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2232016623020172; velocity_v: 0.32739710807800293; specific_humidity: 0.18194341659545898; velocity_z: 0.6861005425453186; temperature: 0.16077697277069092; total_precip: 1.0204106569290161; 
0: epoch: 39 [3/5 (60%)]	Loss: 1.02041 : 0.39825 :: 0.21571 (16.02 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23352546989917755; velocity_v: 0.3718218505382538; specific_humidity: 0.1844276338815689; velocity_z: 0.6573252081871033; temperature: 0.154878169298172; total_precip: 0.6449890732765198; 
0: epoch: 39 [4/5 (80%)]	Loss: 0.64499 : 0.33951 :: 0.21841 (16.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [9.53674316e-07 1.43051147e-06 3.62396240e-05 4.10079956e-05
0:  2.81333923e-05 1.38282776e-05 0.00000000e+00 0.00000000e+00
0:  1.90734863e-06 1.09672546e-05 3.05175781e-05 2.86102295e-05
0:  1.04904175e-05 9.15527344e-05 5.67436218e-05 4.52995300e-05
0:  1.14440918e-05 1.43051147e-06 1.43051147e-06 3.33786011e-06
0:  9.53674316e-06 2.38418579e-06 5.24520874e-06 9.53674316e-06
0:  1.14440918e-05 2.33650208e-05 2.86102295e-05 1.62124634e-05
0:  2.28881836e-05 2.86102295e-05 2.71797180e-05 1.62124634e-05
0:  1.23977661e-05 1.19209290e-05 9.05990601e-06 5.72204590e-06
0:  2.38418579e-06 9.53674316e-07 4.76837158e-07 4.76837158e-07
0:  1.43051147e-06 7.15255737e-06 1.23977661e-05 1.04904175e-05
0:  5.72204590e-06 3.81469727e-06 5.24520874e-06 1.33514404e-05
0:  1.00135803e-05 4.29153442e-06 3.33786011e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  9.53674316e-07 2.38418579e-06 4.10079956e-05 6.81877136e-05
0:  5.05447388e-05 9.53674316e-06 0.00000000e+00 5.72204590e-06
0:  9.53674316e-07 1.43051147e-05 5.29289246e-05 5.48362732e-05
0:  1.47819519e-05 4.91142273e-05 4.91142273e-05 4.19616663e-05
0:  1.33514404e-05 2.86102295e-06 4.29153442e-06 6.67572021e-06
0:  1.09672546e-05 8.58306885e-06 1.43051147e-05 1.38282776e-05
0:  5.24520874e-06 1.14440918e-05 2.76565552e-05 2.67028809e-05
0:  2.57492065e-05 1.23977661e-05 1.19209290e-05 3.24249268e-05
0:  2.33650208e-05 1.19209290e-05 7.62939453e-06 3.81469727e-06
0:  1.43051147e-06 4.76837158e-07 1.90734863e-06 5.72204590e-06
0:  2.38418579e-06 4.76837158e-07 4.29153442e-06 4.29153442e-06
0:  1.43051147e-06 9.53674316e-07 1.90734863e-06 8.10623169e-06
0:  6.19888306e-06 1.43051147e-06 1.43051147e-06 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.90734863e-06 3.33786011e-06
0:  2.38418579e-06 1.43051147e-06 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
0: Target values (first 200):
0: [1.09672546e-04 1.73568726e-04 1.49726868e-04 1.48773193e-04
0:  1.55448914e-04 1.82151794e-04 1.83105469e-04 1.93595886e-04
0:  1.67846680e-04 9.34600830e-05 1.31607056e-04 1.76429749e-04
0:  1.53541565e-04 1.27792358e-04 9.25064087e-05 9.15527344e-05
0:  3.52859497e-05 1.33514404e-05 1.71661377e-05 7.62939453e-06
0:  8.58306885e-06 5.72204590e-06 1.33514404e-05 1.04904175e-05
0:  2.19345093e-05 6.19888306e-05 1.41143799e-04 7.62939453e-05
0:  4.00543213e-05 8.48770142e-05 1.24931335e-04 1.15394592e-04
0:  9.44137573e-05 6.00814819e-05 5.53131104e-05 2.76565552e-05
0:  1.81198120e-05 3.52859497e-05 4.38690222e-05 4.38690222e-05
0:  4.00543213e-05 3.05175781e-05 1.90734863e-05 1.71661377e-05
0:  2.28881836e-05 2.38418579e-05 6.00814819e-05 3.14712524e-05
0:  3.43322754e-05 2.38418579e-05 9.53674316e-07 9.53674316e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  1.43051147e-05 3.14712524e-05 2.95639038e-05 4.29153479e-05
0:  4.86373865e-05 5.05447388e-05 5.14984131e-05 6.77108765e-05
0:  9.25064087e-05 1.14440918e-04 1.04904175e-04 1.38282776e-04
0:  1.86920166e-04 1.85012817e-04 1.00135803e-04 1.58309937e-04
0:  1.95503235e-04 1.27792358e-04 1.58309937e-04 2.08854675e-04
0:  2.50816345e-04 1.34468079e-04 1.55448914e-04 1.38282776e-04
0:  1.18255615e-04 1.00135803e-04 6.38961792e-05 2.86102295e-05
0:  6.19888306e-05 2.57492065e-05 2.38418579e-05 1.81198120e-05
0:  9.53674316e-07 1.90734863e-06 0.00000000e+00 2.86102295e-06
0:  0.00000000e+00 9.53674316e-07 5.72204590e-06 1.04904175e-05
0:  2.67028809e-05 2.28881836e-05 3.71932983e-05 3.33786011e-05
0:  4.57763672e-05 5.81741333e-05 5.24520874e-05 2.09808350e-05
0:  6.38961792e-05 5.24520874e-05 5.81741333e-05 4.10079956e-05
0:  1.44958496e-04 2.25067139e-04 2.67982483e-04 1.65939331e-04
0:  1.74522400e-04 2.05039978e-04 1.96456909e-04 1.36375427e-04
0:  1.12533569e-04 7.43865967e-05 5.43594360e-05 8.10623169e-05
0:  6.19888306e-05 5.05447388e-05 3.62396240e-05 4.10079956e-05
0:  7.62939453e-06 2.09808350e-05 2.95639038e-05 2.86102295e-06
0:  5.72204590e-06 2.00271606e-05 3.91006470e-05 3.62396240e-05
0:  1.62124634e-05 3.14712524e-05 1.00135803e-04 1.35421753e-04
0:  1.27792358e-04 6.29425049e-05 7.91549683e-05 8.86917114e-05
0:  7.34329224e-05 5.05447388e-05 3.33786011e-05 2.38418579e-05
0:  3.62396240e-05 4.95910645e-05 3.71932983e-05 5.91278076e-05
0:  2.57492065e-05 3.71932983e-05 2.76565552e-05 4.19616663e-05
0:  5.81741333e-05 3.52859497e-05 4.38690222e-05 3.33786011e-05
0:  1.71661377e-05 5.72204590e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 9.53674316e-07 9.53674316e-06 8.58306885e-06
0:  1.90734863e-05 4.86373865e-05 6.67572021e-05 6.58035278e-05
0:  8.86917114e-05 8.39233398e-05 6.38961792e-05 6.86645508e-05
0:  7.24792480e-05 1.16348267e-04 7.91549683e-05 1.58309937e-04
0:  2.05993652e-04 1.93595886e-04 1.48773193e-04 1.25885010e-04
0:  1.35421753e-04 1.34468079e-04 1.05857849e-04 1.29699707e-04
0:  1.22070312e-04 6.96182251e-05 1.02996826e-04 5.24520874e-05
0:  3.43322754e-05 3.43322754e-05 1.23977661e-05 1.71661377e-05
0:  2.09808350e-05 1.04904175e-05 2.76565552e-05 1.81198120e-05
0:  6.67572021e-06 1.04904175e-05 0.00000000e+00 1.90734863e-06]
0: Prediction values (first 20):
0: [ 9.369145  9.36113   9.665203 10.107427 10.347297 10.316864 10.132533
0:   9.966993  9.602356  9.382067  9.146953  8.705423  8.376187  8.119654
0:   8.183728  8.71479   9.504016 10.298812  8.502551  8.576595]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.756, max = 0.875, mean = -0.325
0:          sample (first 20): tensor([0.2239, 0.2232, 0.2490, 0.2864, 0.3067, 0.3041, 0.2885, 0.2745, 0.2437, 0.2250, 0.2051, 0.1678, 0.1399, 0.1182,
0:         0.1236, 0.1686, 0.2353, 0.3026, 0.3063, 0.2675])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.275194  -9.3244095 -8.920355  -8.049793  -7.036717  -6.2214417
0:  -5.7875    -5.608214  -5.964144  -6.331243  -6.7121863 -7.146772
0:  -7.331532  -7.3583274 -7.0703034 -6.5481877 -5.929973  -5.363539
0:  -6.0821557 -5.9738708]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.993214 26.126066 26.417515 26.736282 26.864567 26.775494 26.893618
0:  26.967518 27.065874 27.17627  26.981884 26.527493 26.090853 25.83222
0:  25.960693 26.551466 27.44917  28.358393 25.231209 25.388859]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.973083  9.845314  9.926686 10.184617 10.351494 10.413569 10.437027
0:  10.495838 10.409102 10.456838 10.454813 10.247831 10.03987   9.86502
0:   9.9516   10.408773 11.116821 11.865     9.173807  9.018986]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.883047 18.001589 18.299492 18.566158 18.572643 18.29609  18.133198
0:  17.979357 17.748756 17.50891  17.086782 16.476452 15.968424 15.704115
0:  15.931807 16.621052 17.589224 18.411104 16.212114 15.978424]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.35355   10.761023  11.288996  11.895529  12.3863945 12.569616
0:  13.0726    13.363712  13.618544  13.677372  13.306539  12.678072
0:  12.102765  11.950052  12.274924  13.071276  14.107035  15.041576
0:  16.440556  16.61007  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.22434   11.5371475 12.085558  12.601259  12.892586  12.9442215
0:  12.994734  13.001392  12.93666   12.926289  12.78027   12.501398
0:  12.307312  12.269232  12.609917  13.225253  13.978886  14.620572
0:  12.150459  12.428001 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7827744 -5.865396  -5.4386725 -4.637158  -3.875957  -3.4332461
0:  -3.2987123 -3.3759465 -3.8667436 -4.3333554 -4.8823676 -5.622831
0:  -6.1711497 -6.5881953 -6.725497  -6.463474  -6.1535573 -5.8914084
0:  -8.0825    -8.496033 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.978456  12.754129  12.792843  13.0083065 13.134519  13.105474
0:  13.111503  13.104084  12.983108  12.959082  12.834111  12.4828
0:  12.198551  11.975178  11.985943  12.2451515 12.528542  12.664651
0:   9.794012   9.58409  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.022215   8.430175   8.087855   7.831167   7.528937   6.9024396
0:   6.956359   6.830378   6.9835496  7.098228   6.670141   6.1771708
0:   5.625923   5.632884   6.1905932  7.3895397  8.83392   10.045769
0:   9.272662   9.2092285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.503988 22.90589  23.49707  24.049726 24.302212 24.32751  24.40029
0:  24.519012 24.614376 24.781502 24.774557 24.583559 24.465673 24.415247
0:  24.819906 25.528704 26.41081  27.135372 24.562649 24.796398]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.024416  -8.970591  -8.527889  -7.671845  -6.7441087 -6.0040917
0:  -5.6232963 -5.41408   -5.7046084 -5.9613004 -6.225137  -6.594338
0:  -6.7142425 -6.708278  -6.4840655 -5.940408  -5.322164  -4.738793
0:  -5.6933    -5.735625 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.19949  26.947998 26.637188 26.277164 25.923786 25.310661 25.36182
0:  25.244316 25.274803 25.182236 24.480778 23.878372 23.242168 23.158894
0:  23.537811 24.197304 24.828156 25.061981 23.408295 23.356445]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.014974 12.022146 12.333547 12.709426 12.966391 13.018106 13.107784
0:  13.12474  12.956097 12.742514 12.266171 11.597244 11.009182 10.634146
0:  10.633233 10.988242 11.42552  11.753001  8.804189  8.79694 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.7016191   2.7213244   3.1249897   3.707187    4.194123    4.393835
0:   4.5674253   4.5802402   4.3576555   4.142662    3.6814396   2.971106
0:   2.30061     1.8152332   1.6187797   1.8674417   2.2600882   2.6310313
0:  -0.16023731 -0.3200798 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.601009  12.582659  12.804947  13.113798  13.2992    13.207048
0:  13.418072  13.469185  13.617532  13.69697   13.4587965 13.17573
0:  12.93683   13.080247  13.63891   14.546629  15.54561   16.34496
0:  15.645502  15.847134 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.71466  24.626528 24.664919 24.839434 25.05717  25.113216 25.72113
0:  26.151417 26.722076 27.236565 27.222431 27.002064 26.761341 26.770388
0:  27.381367 28.357845 29.46863  30.343733 30.175407 30.174137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.1685514 5.8094783 5.6231403 5.5875716 5.5492125 5.3537235 5.5365977
0:  5.5945873 5.6460567 5.6143513 5.275866  4.8674564 4.5657196 4.8189087
0:  5.550301  6.699533  7.8697085 8.764139  7.7153697 7.4352264]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.17381096 -0.3263402  -0.08143091  0.58671     1.3145399   1.8886318
0:   2.196981    2.366877    2.087162    1.7588143   1.2474632   0.46005583
0:  -0.2704463  -0.95806646 -1.3769422  -1.419313   -1.3102851  -1.1810803
0:  -3.985971   -4.480305  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.84651  14.923509 15.234919 15.613016 15.900887 15.975113 16.250753
0:  16.441246 16.625523 16.776852 16.656847 16.397528 16.188477 16.251707
0:  16.706648 17.44566  18.289738 18.934828 17.409273 17.550259]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.6947756  2.951373   3.4431682  3.9846292  4.493883   4.888999
0:   5.7113934  6.4999313  7.280019   7.9796352  8.280251   8.36997
0:   8.457607   8.896943   9.629992  10.558477  11.323474  11.729742
0:   9.825117   9.987423 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3639035 -5.643151  -5.643788  -5.4258113 -5.2776732 -5.3489394
0:  -5.453608  -5.7300296 -6.290426  -6.8300586 -7.5793757 -8.419474
0:  -9.087137  -9.341591  -9.109104  -8.571789  -8.022589  -7.6492276
0:  -6.124081  -6.0448327]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.4630675  -4.273807   -3.5963464  -2.636497   -1.7575936  -1.1754947
0:  -0.7672949  -0.5473013  -0.66797876 -0.77238894 -0.9937148  -1.4029164
0:  -1.5773349  -1.5836549  -1.2114425  -0.5341468   0.22687435  0.83641386
0:  -0.5450759  -0.25034046]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.934042 29.859924 29.540812 29.03196  28.518705 27.869755 28.263025
0:  28.598131 29.077398 29.320135 28.74862  28.213245 27.583849 27.6633
0:  28.367952 29.394114 30.349861 30.727169 30.611547 30.662006]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.554467   6.520397   6.988488   7.7467737  8.530482   9.143686
0:   9.763433  10.228449  10.468361  10.694288  10.702717  10.505306
0:  10.409966  10.44614   10.786186  11.401654  12.067093  12.527418
0:  10.862875  10.98321  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.57284  25.62865  25.786177 25.975544 26.064682 25.856424 26.08374
0:  26.122217 26.157368 26.023922 25.350698 24.559805 23.639608 23.079863
0:  22.96871  23.195148 23.603191 23.82417  20.8483   20.521095]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.0095887 -7.144501  -6.8793836 -6.2880883 -5.749964  -5.482199
0:  -5.483998  -5.7296576 -6.430967  -7.109739  -7.916513  -8.861435
0:  -9.531088  -9.895729  -9.839882  -9.373062  -8.855642  -8.452913
0:  -9.306599  -9.374109 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.253059  15.115511  15.116663  15.080946  14.861708  14.393177
0:  14.033039  13.5396385 12.928973  12.2946205 11.415059  10.463358
0:   9.674677   9.268716   9.376706   9.779019  10.201264  10.378782
0:   7.1086893  7.1120076]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.151605  -6.137506  -5.673598  -4.835175  -3.9392238 -3.2271218
0:  -2.759729  -2.5104346 -2.6914544 -2.9124312 -3.2252364 -3.702435
0:  -3.992062  -4.1127324 -3.9634519 -3.4458222 -2.8192105 -2.181151
0:  -2.6726527 -2.512917 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.292929  -8.529818  -8.528175  -8.326059  -8.133213  -8.089014
0:  -7.6362557 -7.238793  -6.83054   -6.5334163 -6.66563   -7.0207257
0:  -7.4864936 -7.4915414 -7.1007876 -6.350551  -5.5475383 -5.051183
0:  -6.2008348 -6.4886794]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.470573 24.315334 24.163202 23.987032 23.864695 23.60043  24.225636
0:  24.69353  25.261478 25.536116 25.063091 24.623909 24.120909 24.26043
0:  24.884296 25.74539  26.489521 26.678938 26.67696  26.68991 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.1953354   3.067763    3.3388584   3.920907    4.442585    4.71678
0:   4.6974483   4.638324    4.1972504   3.841525    3.4669771   2.7981122
0:   2.2474422   1.7134228   1.4520235   1.7259974   2.368557    3.1019118
0:   0.22535467 -0.299479  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.9450717 -8.088177  -7.8772607 -7.1975455 -6.4060407 -5.758844
0:  -5.3637986 -5.1166015 -5.371163  -5.633021  -6.0151267 -6.552399
0:  -6.9061465 -7.0683436 -6.963343  -6.547898  -6.1087985 -5.698663
0:  -6.797878  -7.1289086]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.946507 23.900803 23.820297 23.592308 23.19352  22.47078  22.204544
0:  21.866291 21.634346 21.427387 20.85033  20.303082 19.750862 19.657536
0:  20.00449  20.705982 21.53537  22.16103  22.336735 22.401752]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.9836335  7.0637655  7.4325657  7.993278   8.4418     8.61111
0:   8.61067    8.613517   8.3311615  8.236536   8.095879   7.7036896
0:   7.4324718  7.2177997  7.371011   8.084965   9.17817   10.29879
0:   9.5906515 10.304338 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.16523   4.270707  4.7083426 5.419043  6.0093145 6.3657446 6.382974
0:  6.4389033 6.1555166 6.0476832 5.950492  5.551779  5.1871037 4.7669635
0:  4.6326895 5.0849137 6.03934   7.169937  5.750649  5.6951227]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [40.46868  40.568935 40.160046 39.552242 38.8533   37.959713 38.06886
0:  38.163765 38.4704   38.466522 37.57742  36.75708  35.84737  35.546112
0:  35.900597 36.395992 36.93028  36.87892  38.434032 38.75837 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.983439   9.889521  10.128953  10.372316  10.386197  10.170756
0:  10.104856  10.028599  10.054331  10.15073   10.066059   9.846176
0:   9.732708   9.876036  10.443416  11.333172  12.298956  13.124172
0:  10.2754965 10.433794 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.590711  -13.560538  -13.007491  -11.968959  -10.834481   -9.86539
0:   -9.071862   -8.465317   -8.334646   -8.257769   -8.408872   -8.812571
0:   -9.058652   -9.074682   -8.765028   -8.144328   -7.5700054  -7.1779127
0:   -9.783067   -9.857583 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.305765 30.807945 31.30931  31.729761 31.862087 31.743671 31.379276
0:  31.08847  30.543444 30.14423  29.675543 29.062012 28.437832 27.746403
0:  27.264952 26.871056 26.453548 25.899017 23.57059  23.806946]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.3105755 12.619064  13.19299   13.925566  14.5538845 14.965419
0:  15.511169  15.880367  16.135052  16.278328  16.091055  15.753857
0:  15.497496  15.588541  16.092936  16.934586  17.885872  18.670687
0:  17.657463  17.87719  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.105807  -9.40166   -9.207654  -8.513767  -7.6536684 -6.916728
0:  -6.5721836 -6.435871  -6.866761  -7.3200073 -7.8242645 -8.560158
0:  -9.05448   -9.41414   -9.481284  -9.040039  -8.321487  -7.443206
0:  -8.159568  -8.516817 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.382563  6.260374  6.5331845 6.9985805 7.2865124 7.2603726 7.205529
0:  7.0536976 6.837634  6.694764  6.450629  5.9954596 5.649427  5.436363
0:  5.581866  6.152484  6.9429207 7.694768  6.058309  5.9999743]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.976759  -9.689991  -8.902985  -7.8100314 -6.780133  -6.069613
0:  -5.584006  -5.4086194 -5.6504245 -5.9221425 -6.381372  -7.0234494
0:  -7.5086665 -7.7892833 -7.7540593 -7.3733144 -6.950215  -6.5918393
0:  -8.558537  -8.415113 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.7487803  2.7772884  3.1818395  3.8911946  4.4990892  4.8238745
0:   4.8278384  4.7307076  4.248766   3.8195279  3.3078074  2.4903178
0:   1.7581086  1.0408988  0.627048   0.7376275  1.1829276  1.7591467
0:  -3.0251126 -3.495461 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.7558384 5.695081  5.9974256 6.3705935 6.591693  6.555302  6.782056
0:  6.9580045 7.2385817 7.524706  7.4891753 7.171979  6.776186  6.671711
0:  6.866538  7.4144664 7.963465  8.34151   5.220501  5.085372 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.542832   -7.390036   -6.852263   -6.1158824  -5.4367557  -4.931088
0:  -4.444534   -4.0532336  -3.8339438  -3.5945678  -3.4968362  -3.545959
0:  -3.489739   -3.258062   -2.6906676  -1.843648   -0.99632454 -0.27653933
0:  -0.7026992  -0.118011  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.556678 15.505441 15.624207 15.808674 15.907305 15.754613 15.903617
0:  15.93324  15.970352 16.026146 15.733064 15.321881 14.899818 14.768938
0:  15.023337 15.628756 16.365047 16.948818 14.643425 14.517985]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.7152066 6.1402335 5.7554274 5.471381  5.1300807 4.609502  4.5771914
0:  4.576708  4.739293  4.873985  4.6411247 4.2730207 3.89395   3.9555717
0:  4.4434757 5.3526206 6.343992  7.0447397 5.309785  5.1715207]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.321659  11.264047  11.260627  11.073265  10.724737  10.22575
0:  10.2675495 10.330178  10.478762  10.521959  10.057343   9.472526
0:   8.790985   8.644406   9.001933   9.731329  10.437256  10.748034
0:  10.898186  10.977018 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.134981 15.313087 15.693359 16.14428  16.482824 16.632607 16.82016
0:  16.967766 17.127625 17.28771  17.337013 17.288586 17.34524  17.632603
0:  18.308178 19.242962 20.2584   21.179134 18.34284  18.38333 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.669376 11.602376 11.764136 12.001635 12.153345 12.210345 12.508423
0:  12.826181 13.131672 13.405792 13.36271  13.108363 12.806589 12.708324
0:  12.901613 13.316534 13.728562 13.939846 10.091972 10.167755]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.6411767 6.847352  7.3737917 7.9137497 8.114225  7.9531136 7.56954
0:  7.344953  7.0332756 6.9585867 6.8690853 6.4459715 5.970423  5.432598
0:  5.1923375 5.503777  6.2927814 7.247842  4.975293  5.0039015]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5626178 -3.7881026 -3.625898  -3.0307384 -2.4025054 -1.9898791
0:  -2.030295  -2.2565064 -3.0301604 -3.7556767 -4.4587054 -5.332456
0:  -5.970062  -6.460853  -6.601228  -6.1672254 -5.354401  -4.367519
0:  -5.3526034 -5.73064  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.348037  5.5237327 5.9095364 6.3592653 6.6724014 6.728095  6.728308
0:  6.5424423 6.1214967 5.7503505 5.225565  4.646556  4.1955786 3.9935024
0:  4.142432  4.552965  5.066266  5.4278383 4.241129  4.517836 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.005112   -9.195236   -8.895395   -8.099438   -7.2626977  -6.6558747
0:   -6.4621654  -6.6285357  -7.4187083  -8.226656   -9.167158  -10.22283
0:  -11.050388  -11.705397  -11.994799  -11.829813  -11.43347   -10.923666
0:  -10.093405  -10.605434 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.2802727   3.218139    3.509844    4.0988874   4.6713624   5.05698
0:   5.082239    5.015847    4.503271    4.0591903   3.5888765   2.8985932
0:   2.3118584   1.7529497   1.4607859   1.5559707   1.9395556   2.3782237
0:  -0.01141596 -0.07959843]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.617569 21.436714 21.210903 20.948769 20.765564 20.46956  21.0471
0:  21.542831 22.194448 22.641037 22.423069 22.289576 22.116985 22.594929
0:  23.660828 25.038666 26.363052 27.18042  29.488632 29.66185 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.7210445 7.678806  7.871277  8.106389  8.216052  8.065332  8.1891775
0:  8.190332  8.219585  8.225945  7.9145494 7.5088797 7.073944  7.0676327
0:  7.422128  8.144857  8.933287  9.523553  8.397042  8.438427 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.573821  13.928532  14.541884  15.270298  15.765109  15.958488
0:  15.892441  15.740484  15.321808  14.99683   14.624175  14.008838
0:  13.4592085 12.92431   12.731425  12.988743  13.631454  14.380042
0:  11.692003  11.838037 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.419476 37.674225 37.962486 38.12798  38.180576 37.937943 38.204746
0:  38.235504 38.30293  38.144917 37.42071  36.660175 36.026276 35.833744
0:  36.21691  36.943916 37.886467 38.654854 36.920883 37.30286 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.795385  8.674304  8.902351  9.337999  9.695938  9.796381  9.934533
0:  10.01306   9.963118  9.9571    9.796472  9.463685  9.168512  9.05756
0:   9.246146  9.782433 10.48422  11.051929  9.23488   9.301863]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.4654894  6.473897   6.7435536  7.070298   7.325477   7.482754
0:   7.721867   7.9470778  8.061534   8.174992   8.132013   8.045069
0:   8.11635    8.468233   9.240249  10.278331  11.312204  12.15481
0:  11.012357  11.415814 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.785856  10.602533  10.737466  11.0493965 11.266837  11.263793
0:  11.405134  11.441492  11.421562  11.428034  11.169739  10.771208
0:  10.383591  10.219996  10.410071  10.975174  11.640488  12.136904
0:  10.467552  10.432634 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.396585 17.55198  17.81206  18.070686 18.200436 18.070581 18.259975
0:  18.322937 18.36245  18.318687 17.852802 17.264662 16.666092 16.436043
0:  16.62833  17.203775 17.935558 18.4976   16.199877 16.108921]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.4794335 -6.5495095 -6.150988  -5.324631  -4.4335093 -3.7235742
0:  -3.2013288 -2.8688893 -2.927731  -3.0089135 -3.2017093 -3.5770898
0:  -3.8153968 -3.928432  -3.811339  -3.382626  -2.8741326 -2.4105983
0:  -3.5786138 -3.605204 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.317988  6.37512   6.8192005 7.437449  7.9425006 8.169776  8.423117
0:  8.463873  8.2900305 8.016291  7.4647512 6.674199  6.00124   5.6672897
0:  5.684588  6.087763  6.5312405 6.7887053 3.051783  3.0569954]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.197229 14.510363 15.142561 15.954859 16.824585 17.628622 18.570768
0:  19.392948 20.066282 20.709099 21.175653 21.61877  22.382088 23.421297
0:  24.814503 26.223597 27.426682 28.268143 25.611607 25.930012]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2326064  -1.1232123  -0.6690893   0.05941057  0.7291546   1.1616597
0:   1.4514451   1.5424652   1.2580895   0.9991045   0.59975433  0.07487011
0:  -0.22009516 -0.2873621   0.00990582  0.6179476   1.1718822   1.5590134
0:   0.34897566  0.34768343]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5574827e+00 -1.6377544e+00 -1.3791299e+00 -7.4174118e-01
0:  -8.1157684e-04  6.6386604e-01  1.1206827e+00  1.4631367e+00
0:   1.2951808e+00  1.1063871e+00  8.1749868e-01  3.2419682e-01
0:   5.1861286e-02 -1.4691687e-01 -4.1103363e-04  5.1971531e-01
0:   1.2734771e+00  2.0479960e+00  2.2689676e-01  4.3860912e-02]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.237159  12.089708  12.224325  12.397699  12.429962  12.141654
0:  12.0654335 11.864862  11.6845045 11.515565  11.030005  10.376699
0:   9.774595   9.571724   9.81624   10.521276  11.384096  12.096581
0:  10.111137  10.159725 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.16898   9.116479  9.2848    9.592491  9.807852  9.828912  9.713032
0:  9.5542965 9.089866  8.628484  8.039477  7.2470746 6.4896035 5.763913
0:  5.3088408 5.278949  5.5565314 5.980987  3.8308003 3.5651596]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1320992   0.9231596   1.1449647   1.8362975   2.6482146   3.2580569
0:   3.354116    3.15706     2.2929077   1.3707519   0.4088869  -0.76121235
0:  -1.6943526  -2.5027375  -2.946363   -2.8601298  -2.378841   -1.7284799
0:  -2.894609   -3.5922637 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.41551  35.028343 34.275322 33.465862 32.650696 31.762352 31.945442
0:  32.04773  32.371693 32.459488 31.742327 31.215515 30.796127 31.176588
0:  32.37511  33.856834 35.349335 36.1818   36.58429  36.95068 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.320835 35.87814  36.427464 36.88929  37.142834 37.111645 37.563614
0:  37.889175 38.34282  38.735264 38.709896 38.623398 38.60793  38.89198
0:  39.551003 40.378777 41.281933 41.849236 38.887207 39.225647]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.544579 23.382942 23.356018 23.365704 23.281736 22.938858 23.045008
0:  22.985472 22.939613 22.802916 22.175674 21.460526 20.713484 20.432007
0:  20.699112 21.41258  22.418324 23.22759  22.464039 22.777302]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3385472  -1.6518312  -1.5849867  -1.1771007  -0.7123656  -0.4157672
0:  -0.47776318 -0.73859215 -1.579258   -2.4592328  -3.4148479  -4.5591464
0:  -5.417335   -6.1409545  -6.4452643  -6.303477   -5.8566794  -5.246548
0:  -5.410567   -5.5530896 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.180443 33.01694  32.600933 31.986938 31.385471 30.763966 31.180859
0:  31.665497 32.333973 32.8068   32.448196 32.227333 31.97541  32.420624
0:  33.537514 34.860332 36.097027 36.697227 38.676014 39.186077]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2903128 1.522183  2.101099  2.880136  3.5547438 4.0057163 4.3845997
0:  4.681412  4.7716    4.8592997 4.784626  4.461981  4.214621  4.0899763
0:  4.2660427 4.8366117 5.589902  6.293825  4.4164047 4.637697 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.9776495 3.0625107 3.5436165 4.1708374 4.6034985 4.694563  4.722746
0:  4.6341376 4.415325  4.305069  4.093191  3.6824195 3.3536968 3.2457948
0:  3.4340372 4.0238247 4.691575  5.2206798 3.3084455 3.3154664]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.947744  15.974894  16.216307  16.529482  16.655907  16.535177
0:  16.540495  16.444963  16.28593   16.191763  15.8564415 15.380646
0:  14.932201  14.69519   14.823776  15.2962265 15.870514  16.326214
0:  14.351268  14.332823 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.036165 23.937988 24.004917 24.115883 24.205133 24.143347 24.651619
0:  25.09725  25.642374 26.079205 25.986982 25.8361   25.686813 25.987114
0:  26.795885 27.918407 29.083172 29.948483 29.245306 29.422255]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.2471595  -2.0755801  -1.5223012  -0.7167754   0.06028605  0.6766281
0:   1.0744543   1.4481106   1.5518107   1.7422447   1.9147825   1.8915057
0:   1.9802094   2.1361613   2.5278792   3.2422915   4.0854826   4.851166
0:   3.2797575   3.519175  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6340628  -1.711299   -1.4495988  -0.8855157  -0.29948568  0.1159544
0:   0.20021152  0.08282137 -0.5563359  -1.1844544  -1.8788505  -2.718131
0:  -3.3680859  -3.8409643  -3.9611568  -3.6393547  -3.0860834  -2.4481354
0:  -4.331601   -4.58729   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.825126  -10.922637  -10.572125   -9.798865   -8.954301   -8.27957
0:   -7.7508535  -7.42089    -7.5918083  -7.8704414  -8.383878   -9.147087
0:   -9.645757   -9.919425   -9.739066   -9.133702   -8.457065   -7.81699
0:   -8.598684   -8.771874 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.288828  -5.356077  -4.96471   -4.157571  -3.2984457 -2.652195
0:  -2.3818302 -2.2961102 -2.6779127 -3.002781  -3.2986355 -3.7636442
0:  -4.004059  -4.1750803 -4.0787907 -3.59239   -2.8856907 -2.1311054
0:  -3.1273718 -3.304985 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.071465   9.15483    9.511414   9.886277  10.084507   9.949204
0:  10.121252  10.162781  10.312991  10.489686  10.2799635  9.929056
0:   9.4768715  9.376597   9.660329  10.359678  11.221632  11.958124
0:  10.415895  10.543916 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.734144 16.904388 17.311222 17.681171 17.862226 17.784733 17.861141
0:  17.919388 17.989025 18.064064 17.911945 17.605492 17.38589  17.36791
0:  17.802042 18.592873 19.548502 20.394596 18.27751  18.551514]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.904656  14.978417  15.335893  15.756455  16.038122  16.15084
0:  16.342575  16.512096  16.658113  16.788193  16.692625  16.438091
0:  16.256454  16.264353  16.66397   17.29023   17.918732  18.353012
0:  15.5121155 15.52443  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.987686  12.208273  12.624487  13.154806  13.574053  13.8048315
0:  13.767408  13.634197  13.129007  12.602029  12.020988  11.272312
0:  10.6422615 10.134234   9.97724   10.226292  10.789847  11.402471
0:   8.639808   8.621448 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.200585   -3.2921662  -3.0055118  -2.3180861  -1.5499253  -0.94981337
0:  -0.64650965 -0.4892726  -0.7868967  -1.0414739  -1.3077474  -1.762816
0:  -2.071261   -2.314712   -2.378366   -2.0711408  -1.5733256  -1.0240226
0:  -2.4213405  -2.784181  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.722975  5.691818  5.947667  6.358904  6.659364  6.776377  7.0882435
0:  7.318983  7.5247416 7.6869106 7.553728  7.249857  6.9546123 6.949145
0:  7.3075557 8.038334  8.833807  9.473785  7.740615  7.821336 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.90977955 0.59856653 0.6060915  0.9843116  1.4402418  1.8334508
0:  1.9628794  2.1271336  1.9230313  1.8453498  1.7826948  1.4751801
0:  1.2178335  0.88877964 0.74241495 0.9831443  1.5409636  2.2144597
0:  0.49281597 0.14674807]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.403486  19.954468  19.65741   19.320724  18.845402  18.01596
0:  17.500149  16.828146  16.22193   15.629465  14.70101   13.655102
0:  12.6601    12.1079235 12.035448  12.428967  13.068416  13.634855
0:  12.930109  12.80301  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.6983569  2.7338495  3.1322143  3.7104871  4.278409   4.6013136
0:   5.0431128  5.386401   5.6382656  5.936897   6.1045246  6.1246796
0:   6.283677   6.666398   7.3931837  8.387106   9.419519  10.222006
0:   7.8934236  8.073579 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.624896   7.8609266  8.127598   8.2210245  8.099015   7.837129
0:   8.156391   8.534924   9.00676    9.409353   9.278521   9.016185
0:   8.699287   8.972319   9.717419  10.783647  11.748951  12.27916
0:  14.9758    15.344507 ]
0: validation loss for strategy=forecast at epoch 39 : 0.3119180202484131
0: validation loss for velocity_u : 0.16086091101169586
0: validation loss for velocity_v : 0.3042217195034027
0: validation loss for specific_humidity : 0.16161559522151947
0: validation loss for velocity_z : 0.5346781611442566
0: validation loss for temperature : 0.13736672699451447
0: validation loss for total_precip : 0.572765052318573
0: 40 : 20:37:19 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 40, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1592, 1.1192, 1.0645, 1.0009, 0.9355, 0.8764, 0.8317, 0.8059, 0.7976, 0.8022, 0.8125, 0.8167, 0.8078, 0.7895,
0:         0.7675, 0.7473, 0.7348, 0.7213, 1.2363, 1.2130, 1.1717, 1.1137, 1.0470, 0.9832, 0.9278], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6939, -0.6624, -0.6407, -0.6312, -0.6266, -0.6179, -0.5984, -0.5653, -0.5245, -0.4934, -0.4903, -0.5294,
0:         -0.6167, -0.7329, -0.8339, -0.8824, -0.8745, -0.8317, -0.7733, -0.7420, -0.7107, -0.6860, -0.6686, -0.6573,
0:         -0.6475], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2951, -0.3279, -0.3552, -0.3875, -0.3824, -0.3603, -0.1780,  0.1117,  0.4523,  0.8071,  1.0533,  1.2435,
0:          1.2172,  1.0857,  0.8919,  0.6906,  0.5387,  0.4152, -0.2748, -0.3174, -0.3459, -0.3886, -0.4222, -0.4459,
0:         -0.3716], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8140, -0.6128, -0.3500, -0.1744, -0.1453, -0.1934, -0.2169, -0.1934, -0.1632, -0.1375, -0.1263, -0.1867,
0:         -0.2929, -0.3052, -0.3488, -0.7693, -1.3710, -1.5309, -0.7201, -0.5524, -0.3365, -0.2907, -0.5311, -0.7570,
0:         -0.6597], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.8013, 0.7962, 0.7950, 0.8024, 0.8159, 0.8305, 0.8439, 0.8545, 0.8617, 0.8666, 0.8663, 0.8554, 0.8352, 0.8151,
0:         0.8031, 0.7979, 0.8000, 0.8082, 0.8102, 0.7970, 0.7856, 0.7984, 0.8274, 0.8478, 0.8542], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2117, -0.2234, -0.2409, -0.2398, -0.2386, -0.2374, -0.2386, -0.2398, -0.2409, -0.0319, -0.1732, -0.2386,
0:         -0.2351, -0.2374, -0.2409, -0.2409, -0.2409, -0.2409,  0.2145, -0.1125, -0.2094, -0.2281, -0.2386, -0.2386,
0:         -0.2398], device='cuda:0')
0: [DEBUG] Epoch 40, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2115,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381, -0.2393,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan, -0.2393,     nan,     nan,
0:         -0.2393, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2324,     nan,
0:             nan,     nan,     nan, -0.2358,     nan,     nan,     nan,     nan, -0.2243,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,
0:         -0.2347,     nan, -0.2358,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,
0:             nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 40, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1052, 1.1321, 1.1516, 1.1461, 1.1286, 1.0889, 1.1061, 1.1247, 1.1525, 1.1693, 1.1210, 1.0629, 0.9938, 0.9676,
0:         0.9820, 1.0308, 1.0914, 1.1382, 1.1490, 1.2202, 1.2792, 1.2809, 1.2077, 1.1373, 1.0950], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0556, -0.0407, -0.0435, -0.0408, -0.0393, -0.0540, -0.0962, -0.1266, -0.1227, -0.1000, -0.0633, -0.0334,
0:         -0.0521, -0.1154, -0.1812, -0.1937, -0.1321, -0.0369, -0.1189, -0.0828, -0.0596, -0.0242, -0.0101, -0.0293,
0:         -0.0619], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.5943,  0.5851,  0.5532,  0.5122,  0.4752,  0.4397,  0.4005,  0.3682,  0.3322,  0.3092,  0.2784,  0.2485,
0:          0.2056,  0.1513,  0.0630, -0.0182, -0.1077, -0.1473,  0.6061,  0.5888,  0.5598,  0.5261,  0.4892,  0.4571,
0:          0.4400], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3184, -0.3184, -0.2279, -0.4325, -0.5700, -0.3156, -0.2516, -0.3110, -0.2773, -0.3634, -0.3890, -0.2624,
0:         -0.1080, -0.1456, -0.2675, -0.1555, -0.0790, -0.1730, -0.2847, -0.4791, -0.4594, -0.5789, -0.6719, -0.4446,
0:         -0.3913], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1715, -0.2091, -0.2267, -0.2350, -0.2335, -0.2325, -0.2287, -0.2255, -0.2202, -0.2163, -0.2181, -0.2275,
0:         -0.2398, -0.2492, -0.2634, -0.2809, -0.3025, -0.3156, -0.3162, -0.3067, -0.2946, -0.2917, -0.2983, -0.3143,
0:         -0.3386], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1993, -0.1975, -0.2086, -0.2056, -0.2084, -0.2057, -0.2109, -0.1997, -0.2003, -0.2033, -0.2092, -0.2044,
0:         -0.2113, -0.2137, -0.2073, -0.2095, -0.1950, -0.2076, -0.2114, -0.2107, -0.2155, -0.2133, -0.2089, -0.2109,
0:         -0.2094], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.22348922491073608; velocity_v: 0.33453047275543213; specific_humidity: 0.1882065236568451; velocity_z: 0.6265151500701904; temperature: 0.16517284512519836; total_precip: 0.6922332644462585; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24728858470916748; velocity_v: 0.4075305163860321; specific_humidity: 0.14800260961055756; velocity_z: 0.5871673226356506; temperature: 0.155170276761055; total_precip: 0.32819870114326477; 
0: epoch: 40 [1/5 (20%)]	Loss: 0.51022 : 0.30719 :: 0.21677 (2.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20462428033351898; velocity_v: 0.31852829456329346; specific_humidity: 0.19907017052173615; velocity_z: 0.5099956393241882; temperature: 0.1523745208978653; total_precip: 0.6726739406585693; 
0: epoch: 40 [2/5 (40%)]	Loss: 0.67267 : 0.30795 :: 0.21299 (15.79 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2674250602722168; velocity_v: 0.4018172025680542; specific_humidity: 0.16652660071849823; velocity_z: 0.6562520265579224; temperature: 0.14970199763774872; total_precip: 0.4740230441093445; 
0: epoch: 40 [3/5 (60%)]	Loss: 0.47402 : 0.31838 :: 0.21300 (15.78 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20813871920108795; velocity_v: 0.3430761396884918; specific_humidity: 0.1753426492214203; velocity_z: 0.4491342306137085; temperature: 0.13991937041282654; total_precip: 0.34191417694091797; 
0: epoch: 40 [4/5 (80%)]	Loss: 0.34191 : 0.24281 :: 0.21125 (16.03 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.67572021e-06
0:  7.62939453e-06 9.53674316e-07 9.53674316e-07 2.95639038e-05
0:  9.53674316e-05 6.29425049e-05 5.34057617e-05 4.76837158e-05
0:  7.24792480e-05 7.24792480e-05 3.62396240e-05 4.29153479e-05
0:  2.86102295e-05 2.67028809e-05 2.09808350e-05 9.53674316e-05
0:  1.00135803e-04 1.23977661e-05 1.03950500e-04 1.64985657e-04
0:  1.80244446e-04 1.95503235e-04 2.08854675e-04 1.98364258e-04
0:  2.03132629e-04 2.32696533e-04 2.75611877e-04 3.40461731e-04
0:  4.38690186e-04 4.95910645e-04 4.35829163e-04 3.57627869e-04
0:  1.58309937e-04 1.26838684e-04 1.93595886e-04 2.57492065e-04
0:  3.46183777e-04 2.95639038e-04 4.85420227e-04 7.97271729e-04
0:  4.94956970e-04 3.39508086e-04 1.17301941e-04 5.62667847e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.38690149e-05
0:  5.91278076e-05 6.67572021e-05 2.76565552e-05 1.23977661e-05
0:  1.33514404e-05 5.72204590e-06 8.58306885e-06 1.14440918e-05
0:  8.58306885e-06 1.04904175e-05 1.43051147e-05 3.05175781e-05
0:  9.82284546e-05 1.18255615e-04 2.15530396e-04 2.72750854e-04
0:  2.92778015e-04 3.29017668e-04 6.59942627e-04 7.46726990e-04
0:  7.50541687e-04 5.91278076e-04 3.46183777e-04 3.19480896e-04]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07
0:  4.7683716e-07 2.3841858e-06 2.3841858e-06 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [17.393852 17.610157 18.071901 18.5853   18.931564 19.071533 19.205793
0:  19.307085 19.368734 19.472527 19.476086 19.30956  19.225185 19.264282
0:  19.582872 20.111034 20.671638 21.021297 18.227154 18.412712]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.838, max = 2.050, mean = -0.060
0:          sample (first 20): tensor([0.9048, 0.9233, 0.9630, 1.0071, 1.0368, 1.0488, 1.0604, 1.0691, 1.0744, 1.0833, 1.0836, 1.0693, 1.0620, 1.0654,
0:         1.0928, 1.1381, 1.1863, 1.2163, 0.9811, 0.9901])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7486563 -6.0828295 -6.065506  -5.8626003 -5.7220945 -5.818198
0:  -5.862153  -6.031719  -6.3496275 -6.6275225 -7.106512  -7.740466
0:  -8.308379  -8.519788  -8.441608  -8.0798    -7.7478123 -7.6193113
0:  -8.36554   -8.39084  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.767403  10.090221  10.731327  11.412454  11.86231   12.04247
0:  12.201396  12.2599945 12.174537  12.135147  11.908005  11.527277
0:  11.267399  11.217894  11.607959  12.341017  13.146095  13.8222065
0:  11.037115  11.37878  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.004095  4.9384236 5.1626067 5.6034713 5.9038315 5.9223814 5.607337
0:  5.2318716 4.514146  3.9318807 3.4214547 2.7257507 2.2080793 1.7596664
0:  1.6171021 1.998384  2.749775  3.5952759 1.2364368 0.9238877]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.4581494 -4.5319543 -4.220062  -3.5168204 -2.7908435 -2.2288995
0:  -2.1042743 -2.1118846 -2.6063132 -3.037414  -3.4277053 -3.9801564
0:  -4.312759  -4.5714154 -4.536288  -4.048698  -3.2659316 -2.3691478
0:  -3.200027  -3.3995395]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.56464  19.636026 19.802006 19.853518 19.773369 19.43971  19.60687
0:  19.684538 19.825409 19.921314 19.517918 19.059486 18.534994 18.500376
0:  18.896452 19.659708 20.438    20.92474  19.900536 20.022192]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.534632 22.362598 22.42656  22.53187  22.492514 22.22563  22.1162
0:  21.904482 21.725859 21.536566 21.064049 20.49975  20.012947 19.759584
0:  19.989449 20.53019  21.206059 21.811142 18.622887 18.898094]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.7987185 4.788989  5.0250626 5.4616714 5.8391867 6.044989  6.3077903
0:  6.470675  6.4317846 6.41658   6.1793857 5.7325673 5.262496  4.9318647
0:  4.859258  5.1079464 5.4965367 5.841297  4.261737  4.411438 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.987877 24.636044 24.38958  24.22364  24.114735 23.697224 23.99204
0:  24.070831 24.299164 24.36133  23.875374 23.314297 22.781776 22.731394
0:  23.19815  24.132969 25.282484 26.18745  26.030577 25.923279]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.48806906 0.6846938  1.2209463  1.9717269  2.5876832  2.9418182
0:  3.1041594  3.1124475  2.7533884  2.4165082  1.9214563  1.199501
0:  0.6263361  0.19676256 0.15558338 0.58323526 1.2322922  1.9245191
0:  0.3095479  0.52945805]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.2278614   3.7971041   3.6580875   3.7547038   3.678603    3.325366
0:   2.6739378   2.0381083   1.2127051   0.5944247   0.04323292 -0.7266607
0:  -1.4312272  -1.9870634  -2.1867027  -1.7366376  -0.827899    0.2051673
0:  -0.6118946  -1.099637  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.415754  7.720013  8.074127  8.181217  8.03157   7.658774  7.669201
0:  7.654086  7.6388392 7.49711   6.883365  6.100402  5.3030815 5.0804734
0:  5.310276  5.9533854 6.550627  6.907886  6.197481  6.272949 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.20486498 -0.14822102  0.12350321  0.67807436  1.1965919   1.609726
0:   1.8059783   1.9821272   1.7829957   1.6302195   1.3699985   0.8760109
0:   0.4425106   0.06522369 -0.07709455  0.16143274  0.592226    1.0899749
0:  -0.50558996 -0.87225676]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5383224 -5.5575504 -5.101905  -4.2915573 -3.5057392 -2.9826374
0:  -2.6538506 -2.6380548 -3.0556736 -3.4945068 -4.1294146 -4.9295855
0:  -5.454217  -5.7233467 -5.575674  -5.0749607 -4.6077356 -4.2073383
0:  -5.7429113 -5.7686176]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.611507  23.93333   24.43729   24.864632  24.939575  24.502945
0:  24.308182  23.96032   23.717953  23.457829  22.805363  22.01353
0:  21.07787   20.37223   19.942268  19.783543  19.689219  19.406708
0:  15.847194  15.8632345]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.571535 20.827402 21.298948 21.70151  21.919724 21.879736 22.0394
0:  22.125118 22.21964  22.387423 22.31664  22.109518 22.000734 22.154003
0:  22.662985 23.484606 24.354286 25.008303 22.497055 22.760494]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.882332  -9.956394  -9.738817  -9.122831  -8.324568  -7.642554
0:  -7.0258737 -6.534018  -6.4815545 -6.4908795 -6.662213  -7.0384126
0:  -7.2097716 -7.17141   -6.7009225 -5.9008937 -4.95641   -4.169515
0:  -3.9687686 -3.8732028]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2560558 3.2487345 3.482446  3.743424  3.849975  3.698005  3.6193016
0:  3.4194014 3.174214  2.9418125 2.5276551 1.9605627 1.5099983 1.4613905
0:  1.8370175 2.6309361 3.532933  4.2910986 2.5307026 2.5564833]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.141884   9.441452  10.013461  10.712987  11.276739  11.620807
0:  11.73958   11.768406  11.473179  11.230602  10.965915  10.616549
0:  10.4172125 10.364659  10.650899  11.236172  11.950353  12.504835
0:  10.625963  10.8221   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.4497712   3.3018363   3.338298    3.3957572   3.3180075   2.9740264
0:   2.9002488   2.7376442   2.5416734   2.320778    1.7187891   0.9452739
0:   0.06173134 -0.35252905 -0.3936591  -0.0384922   0.34277678  0.4314704
0:  -0.74973726 -0.8190551 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.9912057 -8.203661  -7.90525   -7.261685  -6.6544967 -6.3099113
0:  -6.140781  -6.195872  -6.5496917 -6.922952  -7.4152446 -8.083374
0:  -8.557714  -8.811424  -8.785208  -8.3603    -7.832474  -7.353021
0:  -8.755587  -8.8715   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.3189034 7.244994  7.444261  7.875735  8.208608  8.315717  8.264115
0:  8.043916  7.536574  6.97618   6.2714653 5.3777456 4.5588875 3.9636538
0:  3.6970854 3.7667413 3.9889617 4.1695986 1.9954972 2.0175672]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.6551161  -0.7423997  -0.48855257  0.15584612  0.8262119   1.2722716
0:   1.3522449   1.1964645   0.49546623 -0.2374239  -1.035492   -1.9639802
0:  -2.6752977  -3.1504927  -3.2690535  -2.8962355  -2.296029   -1.6475716
0:  -2.9740906  -3.4111233 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.507767   -7.733092   -7.5353475  -7.04575    -6.5424933  -6.2203746
0:  -5.796826   -5.45824    -5.0533805  -4.608396   -4.29581    -4.069881
0:  -3.7491689  -3.2099967  -2.4090495  -1.3936143  -0.44035625  0.32609463
0:  -1.5396943  -1.5668931 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6680608 -3.5841699 -3.0923257 -2.3559613 -1.7954535 -1.5669155
0:  -1.6636405 -2.0006847 -2.777556  -3.5782456 -4.552366  -5.774569
0:  -6.823793  -7.686793  -8.108839  -8.007977  -7.625715  -7.188746
0:  -8.866838  -9.185253 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.416632 13.760464 14.271213 14.836106 15.217157 15.3326   15.587629
0:  15.737261 15.794928 15.844295 15.605428 15.225863 14.867808 14.7621
0:  14.998723 15.563561 16.228046 16.699387 15.090328 15.43472 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.940762 12.961772 13.183266 13.435013 13.574885 13.500631 13.800121
0:  14.031271 14.263752 14.421204 14.117701 13.692787 13.227852 13.175823
0:  13.618677 14.40263  15.299208 16.086342 15.697229 16.043486]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.81395  12.240341 11.818841 11.284507 10.486903  9.538559  9.02368
0:   9.128718  9.893802 11.265938 12.876635 14.278759 15.658303 16.983545
0:  18.339224 19.732203 20.837126 21.411743 15.408797 12.754117]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.352097 39.422245 39.35362  39.103203 38.6762   37.953445 37.870243
0:  37.70336  37.63404  37.47113  36.62089  35.672535 34.636406 34.013485
0:  33.893394 34.163754 34.497032 34.556587 31.319088 31.437496]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.413523  9.228214  9.257599  9.292051  8.908629  8.064154  7.0888886
0:  6.093558  5.0136924 4.2000656 3.3833232 2.4813027 1.6556268 1.1231637
0:  1.0421019 1.5229721 2.3558507 3.2333753 1.8551593 1.6527257]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.836552  31.974392  32.855244  33.4178    33.394337  32.653343
0:  31.429838  29.54765   27.018229  24.091839  20.757235  17.340656
0:  14.258469  11.791378  10.144852   9.200434   8.653991   8.341972
0:   7.0664263  6.8521204]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.20471  37.298893 37.30111  37.254997 37.123642 36.84477  37.40803
0:  37.947502 38.50961  38.576183 37.668484 36.44238  35.087772 34.28783
0:  34.219765 34.527397 34.902573 34.84455  31.14986  31.001518]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5426507  -1.4666381  -1.0393758  -0.34782887  0.30677176  0.7428136
0:   1.0195451   1.1287475   0.8582077   0.5432062   0.05101776 -0.6256919
0:  -1.133841   -1.4828362  -1.5195146  -1.2057195  -0.8298011  -0.4607587
0:  -1.8390393  -1.7813764 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.45483    6.9071794  6.7242293  6.7676034  6.8521028  6.698862
0:   6.9202266  7.0512204  7.3565545  7.663562   7.752406   7.759274
0:   7.9014177  8.38123    9.255672  10.485845  11.687336  12.700231
0:  11.4394    11.209468 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.606136  16.366581  16.20499   15.987366  15.760115  15.251516
0:  15.319889  15.235392  15.33333   15.317608  14.805029  14.274792
0:  13.69113   13.4878235 13.739128  14.286543  14.8464985 15.216719
0:  13.859776  13.595559 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.067446 16.855108 16.83273  16.912895 17.000847 16.902945 17.24962
0:  17.513721 17.781502 17.93425  17.622475 17.168383 16.65674  16.50565
0:  16.820158 17.522106 18.410555 19.117008 17.370436 17.170658]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.250572  11.596613  12.057476  12.572071  12.9160385 13.060541
0:  12.870665  12.679731  12.116888  11.654665  11.173375  10.538338
0:   9.988289   9.510963   9.358239   9.599899  10.132751  10.706331
0:   8.282661   8.494895 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.202797 28.237392 28.419464 28.5176   28.518257 28.243002 28.413387
0:  28.499853 28.672873 28.775755 28.4099   27.889198 27.331175 27.04987
0:  27.174393 27.65394  28.319052 28.841488 26.87084  26.719378]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.15124  16.136211 16.073372 15.950647 15.673744 15.199318 14.57278
0:  14.03211  13.31934  12.81723  12.35663  11.768381 11.223295 10.688834
0:  10.454573 10.628315 11.135412 11.733938 10.900042 10.614252]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.47722197 0.6395788  1.0939069  1.7487564  2.341679   2.745439
0:  3.0146413  3.1872456  3.0747595  3.0013146  2.818169   2.4462452
0:  2.1981845  2.1022248  2.3262563  2.8716512  3.5095932  4.082532
0:  2.970871   3.1841993 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.054255 17.972696 18.059637 18.192657 18.249956 18.01841  18.096104
0:  18.023613 18.034288 18.024012 17.727587 17.417088 17.151459 17.211897
0:  17.594168 18.260313 18.9768   19.479416 18.203146 18.24199 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.1272335  7.10951    7.4462566  8.203153   8.980388   9.662142
0:  10.069759  10.438759  10.438507  10.550856  10.671497  10.59853
0:  10.575478  10.522837  10.771814  11.436071  12.432074  13.482216
0:  12.545496  12.457253 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6709037  -3.6267667  -3.1965632  -2.4838014  -1.7513652  -1.1570449
0:  -0.5622697  -0.08524418  0.11929226  0.27510357  0.24296618  0.05775547
0:   0.04717112  0.2509489   0.786983    1.5889182   2.405497    3.1051307
0:   1.9924693   2.3387794 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.430185  16.233725  16.39337   16.651928  16.722385  16.39279
0:  16.192038  15.884439  15.588488  15.32266   14.836671  14.226978
0:  13.652039  13.331877  13.438633  14.0204935 14.88241   15.610031
0:  13.36611   13.073109 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.864475 17.553627 17.437653 17.387665 17.263329 16.921028 16.910038
0:  16.850943 16.855824 16.904366 16.668575 16.344334 15.971299 15.933812
0:  16.247036 16.850447 17.544243 18.025646 16.752064 16.647152]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2425909  -1.3050613  -0.9234319  -0.34053707  0.25118446  0.69709826
0:   1.2204881   1.6237869   1.8587465   2.0659747   2.0706878   1.8506942
0:   1.6834679   1.7274766   1.9910045   2.505692    2.9905667   3.2762575
0:   1.2352343   1.0255308 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.474485 12.205917 12.062506 11.960042 11.830477 11.464029 11.432598
0:  11.324176 11.21818  11.027847 10.476467  9.823679  9.210452  8.992456
0:   9.155349  9.766573 10.484337 11.079039  9.781257  9.632591]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.848323  -7.915447  -7.552167  -6.9263678 -6.305517  -5.904314
0:  -5.4832945 -5.167244  -4.95747   -4.8061867 -4.9257965 -5.2605305
0:  -5.627471  -5.695012  -5.4471874 -4.854374  -4.24306   -3.7667465
0:  -5.6528997 -5.685955 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.87001  26.92126  27.110214 27.27327  27.205547 26.927921 26.800869
0:  26.610947 26.398624 26.179943 25.72348  25.081863 24.56299  24.215834
0:  24.293844 24.692694 25.253525 25.74713  21.287264 21.373627]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.177732   9.230578   9.548895  10.013166  10.314539  10.3400135
0:  10.389288  10.363067  10.252768  10.169807   9.943501   9.566046
0:   9.251832   9.163906   9.462611  10.1468525 11.008604  11.779987
0:  10.178119   9.871207 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.681654 25.925978 26.24716  26.417871 26.390808 26.058964 25.98937
0:  25.8051   25.622356 25.376257 24.764866 24.059237 23.45331  23.227776
0:  23.46868  24.031256 24.673084 25.129547 24.84682  24.986609]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.139757  9.17867   9.471899  9.787166  9.968039  9.982342 10.217659
0:  10.465527 10.778774 11.137501 11.157127 10.873794 10.427212 10.158365
0:  10.181354 10.588348 11.12776  11.554619  9.930967 10.361992]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.323006 25.259579 25.136368 24.773445 24.330639 23.699783 23.808914
0:  23.87822  24.138416 24.221699 23.573505 22.914772 22.153786 22.04964
0:  22.554554 23.480923 24.513397 25.193386 26.482773 26.745087]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.078516 27.625004 28.473412 29.334047 29.98175  30.349207 30.838282
0:  31.243624 31.546066 31.719717 31.551353 31.059744 30.68935  30.447567
0:  30.607971 31.095806 31.712936 32.179615 27.750692 27.989994]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.49794  21.626965 21.943592 22.188072 22.251884 22.061958 22.04396
0:  21.9402   21.81989  21.673119 21.216227 20.61049  20.05964  19.790997
0:  19.963196 20.498936 21.210426 21.807243 19.451202 19.642746]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.935736  13.195178  12.694859  12.387551  12.083862  11.554781
0:  11.527703  11.323504  11.268307  11.117935  10.5572815  9.96874
0:   9.463665   9.458812   9.998069  10.981596  12.1060295 13.076233
0:  11.635435  11.441041 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.2143598 -7.383549  -7.2007813 -6.5338492 -5.7322707 -5.022842
0:  -4.7319064 -4.603111  -5.0363517 -5.4530616 -5.9061284 -6.505448
0:  -6.9398026 -7.2762876 -7.36023   -7.0315776 -6.437798  -5.719497
0:  -6.4682612 -6.730017 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.929575 30.737528 30.505903 30.315107 30.16927  29.686195 29.84789
0:  29.770473 29.85354  29.784857 29.239285 28.795881 28.48218  28.607855
0:  29.27106  30.228512 31.267475 32.030777 31.347223 31.42841 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.318565 23.520386 23.94011  24.43652  24.837486 24.970581 25.217045
0:  25.287457 25.254837 25.201511 24.80994  24.291138 23.833305 23.610348
0:  23.723415 24.052383 24.424347 24.62402  20.439648 19.717957]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.826838   6.9726095  7.497068   8.291835   9.002142   9.4392185
0:  10.036317  10.515129  10.932409  11.311338  11.413436  11.355457
0:  11.292794  11.502703  12.099031  13.030333  14.027302  14.803064
0:  13.920637  14.147432 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.409059   9.24893    9.464929   9.856487  10.011174   9.771275
0:   9.501876   9.061621   8.576405   8.122478   7.47701    6.6488404
0:   5.8976135  5.452262   5.496016   6.081943   6.934485   7.6941524
0:   6.2408323  6.320684 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.331961  -11.376532  -10.9842205 -10.126619   -9.235085   -8.59794
0:   -8.355293   -8.441612   -9.072603   -9.661413  -10.232452  -10.812916
0:  -11.036825  -11.052736  -10.721682  -10.088593   -9.422712   -8.778711
0:   -9.593296   -9.845844 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.753514 12.690655 13.611031 14.416668 15.105703 15.794874 17.351353
0:  19.016298 20.807598 22.442204 23.430443 24.214296 24.8736   25.978228
0:  27.381155 28.837193 29.979406 30.412724 31.5868   32.004543]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.39033  18.492483 18.835678 19.140656 19.301456 19.145979 19.417816
0:  19.526976 19.720505 19.895485 19.615463 19.199055 18.77398  18.778395
0:  19.276503 20.259052 21.410988 22.34284  20.88757  21.105854]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.017324 19.186914 19.47095  19.75443  19.877495 19.729668 19.939896
0:  20.007566 20.101122 20.140951 19.730005 19.254711 18.74972  18.661911
0:  18.926609 19.487732 20.007788 20.251936 18.73426  18.867098]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.328115  -7.7392926 -7.765575  -7.407555  -6.951507  -6.614883
0:  -6.4473433 -6.4483485 -6.8849387 -7.3434787 -7.904987  -8.60067
0:  -9.029306  -9.256344  -9.150244  -8.727959  -8.2842045 -7.9076667
0:  -9.24991   -9.607651 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.677681  1.8545156 2.443912  3.4207036 4.3817263 5.099328  5.4923496
0:  5.745336  5.6451793 5.635951  5.6106305 5.3265653 5.1239624 4.9402523
0:  5.012592  5.5507193 6.4484444 7.4235787 7.0338545 7.4269276]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.825287  11.800139  12.16589   12.73303   13.227172  13.522625
0:  13.902143  14.238007  14.479057  14.716459  14.741526  14.517263
0:  14.27236   14.083141  14.1202345 14.430452  14.852249  15.184109
0:  11.805489  11.751127 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.351381  6.4093776 6.7359667 7.5062985 8.238373  8.842281  9.060181
0:  9.329582  9.140717  9.032236  8.782601  8.002618  7.093599  6.0777607
0:  5.472126  5.545526  6.2568383 7.241602  4.8074036 4.400099 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.150074  8.922102  8.949735  9.218272  9.459333  9.577723  9.632534
0:  9.629789  9.294497  8.9042225 8.354632  7.6087666 6.983984  6.5306497
0:  6.464363  6.7034335 7.1383047 7.5382323 5.263568  5.12854  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7665677 -1.9392228 -1.8036933 -1.376266  -0.9408641 -0.6773486
0:  -0.7131629 -0.9272132 -1.6429162 -2.3681216 -3.1737752 -4.0975165
0:  -4.789313  -5.2989984 -5.4394317 -5.1938853 -4.7678103 -4.285396
0:  -3.9774528 -3.8822637]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.597914 20.531616 20.657316 20.794268 20.800428 20.44953  20.23124
0:  19.873528 19.498169 19.117514 18.465855 17.747988 17.04185  16.606358
0:  16.589365 16.934258 17.441025 17.804453 15.491333 15.381641]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4868045  -1.6096091  -1.3695006  -0.7537198  -0.07128286  0.42702246
0:   0.68280125  0.80099964  0.46229172  0.07823277 -0.4149189  -1.0853019
0:  -1.5739989  -1.9349122  -1.9795384  -1.6312137  -1.1546226  -0.6554551
0:  -2.0864854  -2.2504544 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.7977052   2.6738636   2.874364    3.2785134   3.53205     3.44154
0:   3.1335433   2.6937678   1.9361515   1.2653637   0.49161434 -0.44631577
0:  -1.2158389  -1.818346   -2.0903635  -1.9181194  -1.5859556  -1.286922
0:  -2.7386198  -2.8374329 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.920095  13.223515  13.787596  14.424978  14.889694  15.064819
0:  15.4524555 15.737392  16.121584  16.503813  16.647953  16.664719
0:  16.71229   16.980312  17.670874  18.715456  19.96189   21.10205
0:  19.765554  20.18784  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.211465  -6.3975677 -6.1637907 -5.519744  -4.8241835 -4.2995796
0:  -4.0823317 -4.0271473 -4.388706  -4.701831  -5.027426  -5.470626
0:  -5.6935883 -5.755128  -5.5198917 -4.9109583 -4.143943  -3.4394922
0:  -5.159381  -5.3403463]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.7917266 -3.9799132 -3.7848802 -3.108214  -2.299828  -1.5670905
0:  -1.2451663 -1.0047388 -1.2787395 -1.5015616 -1.6998773 -2.1111922
0:  -2.4036946 -2.667501  -2.6950412 -2.2157702 -1.37499   -0.3773756
0:  -0.8308778 -1.1873727]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.64290285 -0.6551051  -0.39424562  0.28528214  1.0879798   1.7334466
0:   2.0314026   2.2158995   1.8313074   1.4638133   0.99549294  0.430274
0:  -0.02757692 -0.3382349  -0.29713917  0.12362623  0.76132154  1.3662472
0:   0.3147483   0.1947093 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.500605 20.110636 20.077225 20.274073 20.426722 20.153034 19.981787
0:  19.573038 19.147772 18.699488 17.997581 17.242691 16.56755  16.157389
0:  16.182392 16.688183 17.449938 18.202637 17.022963 16.851892]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.5294428   1.4266706   1.4791946   1.5725389   1.5416794   1.3742833
0:   1.1965156   1.1011767   0.84152174  0.7435174   0.58211756  0.18028402
0:  -0.17511225 -0.4947033  -0.58493996 -0.38572025  0.00258827  0.44182968
0:  -1.2661467  -1.2844462 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.156213   6.1858325  6.640531   7.1914277  7.58706    7.7334065
0:   8.001639   8.130506   8.171737   8.284583   8.171184   7.8973813
0:   7.7319965  7.9118676  8.418098   9.219631   9.909422  10.340258
0:   9.381209   9.615516 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.5152936   0.33135414  0.5045004   1.0242043   1.4938416   1.7134395
0:   1.5087404   1.1984239   0.49686575 -0.04901981 -0.53613997 -1.1912093
0:  -1.7547021  -2.2721     -2.586784   -2.3959832  -1.8373399  -1.1081815
0:  -2.8880515  -3.1933923 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.6869545  -4.5329227  -4.022176   -3.1383028  -2.1541185  -1.2036867
0:  -0.4349513   0.35759354  0.7705178   1.2108445   1.5406528   1.6130543
0:   1.6679416   1.8625612   2.2704644   3.085023    4.1214733   5.0575867
0:   4.117456    4.349515  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.289497  5.337845  5.638997  6.028744  6.2680817 6.2055564 6.2833133
0:  6.204556  6.1335583 6.0220795 5.7103844 5.303859  5.0100822 5.085913
0:  5.5418296 6.3644094 7.250071  7.9340243 5.7381516 5.491564 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.784668  7.7071724 7.8563766 8.243445  8.641124  8.941539  9.034491
0:  9.061359  8.737608  8.430166  8.079235  7.602784  7.261601  7.08245
0:  7.248124  7.82173   8.65441   9.489042  8.707018  8.902395 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.39411402  0.34507465  0.6871605   1.3853669   2.0821943   2.5788088
0:   2.8000398   2.7706633   2.2493215   1.6673245   0.9194565  -0.06796741
0:  -0.91791296 -1.6703877  -2.1206918  -2.1135879  -1.816977   -1.4102035
0:  -2.5917726  -2.5893464 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5576711  -1.2259879  -0.44455862  0.580256    1.4401298   1.9765258
0:   2.3180714   2.4847603   2.3439608   2.2199688   1.9907508   1.5372572
0:   1.2578678   1.1202173   1.2987013   1.829886    2.4408212   2.943904
0:  -0.00422096  0.08106899]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6170464 -3.614726  -3.2529802 -2.6341133 -2.1206055 -1.8567448
0:  -1.7525558 -1.7775545 -2.0914392 -2.4239988 -2.877099  -3.5116787
0:  -3.9454927 -4.113022  -3.8341303 -3.1430616 -2.322523  -1.6210518
0:  -1.7862234 -1.6053782]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.355953  3.2448046 3.4811347 3.8778355 4.188593  4.2890906 4.3960714
0:  4.4144073 4.2935367 4.2264547 4.050717  3.7124383 3.57058   3.6490936
0:  4.0701246 4.8046017 5.593357  6.199168  4.921031  4.986155 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.28645   -12.345794  -11.875503  -10.926038   -9.883538   -9.057951
0:   -8.677222   -8.480873   -8.859267   -9.226502   -9.613084  -10.182307
0:  -10.502112  -10.726577  -10.704844  -10.2967615  -9.759866   -9.1771345
0:   -9.389542   -9.426605 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.050758 22.757072 22.428154 22.03194  21.697437 21.168816 21.379187
0:  21.405329 21.609236 21.607832 21.02528  20.485619 19.891418 19.80085
0:  20.216545 21.055103 22.095493 22.944042 23.134346 23.013924]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.712686  -6.6172385 -6.1099544 -5.361447  -4.672955  -4.2131143
0:  -3.9114022 -3.8980536 -4.304812  -4.7004476 -5.2480187 -5.9160314
0:  -6.370383  -6.540319  -6.3491073 -5.8770947 -5.408671  -5.0024514
0:  -5.777472  -5.5864377]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8579617 1.9966273 2.520267  3.232932  3.7640307 3.977979  3.9330344
0:  3.8170044 3.5231743 3.3647566 3.229178  2.862677  2.5833807 2.3748937
0:  2.4320955 2.9647417 3.7978654 4.7029185 2.9016895 3.0169678]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.608343 12.680331 12.949455 13.221111 13.28557  13.098735 13.046015
0:  12.924494 12.785126 12.705926 12.357917 11.90012  11.461006 11.31732
0:  11.577158 12.217279 13.009982 13.71221  12.567827 12.889912]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2409701 -3.2967048 -2.9162173 -2.2641807 -1.7262511 -1.4617095
0:  -1.4996438 -1.737947  -2.3014045 -2.6329942 -2.871694  -3.0780149
0:  -2.9715018 -2.5886292 -1.8017602 -0.7169676  0.4104042  1.3300095
0:   1.1685252  1.8123026]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.827288 17.975359 18.377323 18.874615 19.1974   19.203545 19.397272
0:  19.43669  19.489347 19.539358 19.277885 18.899368 18.486788 18.282282
0:  18.3824   18.755356 19.195498 19.494099 16.965868 16.882374]
0: validation loss for strategy=forecast at epoch 40 : 0.3464798033237457
0: validation loss for velocity_u : 0.15412218868732452
0: validation loss for velocity_v : 0.31707990169525146
0: validation loss for specific_humidity : 0.15275530517101288
0: validation loss for velocity_z : 0.5933557152748108
0: validation loss for temperature : 0.1064014658331871
0: validation loss for total_precip : 0.7551643252372742
0: 41 : 20:41:20 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 41, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8143, -0.8174, -0.8186, -0.8199, -0.8242, -0.8348, -0.8536, -0.8807, -0.9145, -0.9514, -0.9865, -1.0151,
0:         -1.0329, -1.0357, -1.0209, -0.9872, -0.9362, -0.8718, -0.7671, -0.7730, -0.7791, -0.7861, -0.7953, -0.8087,
0:         -0.8277], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5351, -0.5163, -0.5032, -0.5019, -0.5158, -0.5450, -0.5830, -0.6203, -0.6463, -0.6555, -0.6481, -0.6297,
0:         -0.6081, -0.5906, -0.5798, -0.5753, -0.5756, -0.5803, -0.5713, -0.5563, -0.5482, -0.5502, -0.5648, -0.5924,
0:         -0.6286], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6142, -0.6180, -0.6269, -0.6409, -0.6547, -0.6616, -0.6625, -0.6633, -0.6598, -0.6417, -0.6229, -0.6051,
0:         -0.5972, -0.5916, -0.5861, -0.6012, -0.6203, -0.6393, -0.6371, -0.6438, -0.6504, -0.6570, -0.6602, -0.6635,
0:         -0.6667], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4902, -0.4551, -0.2965, -0.0513,  0.1872,  0.3186,  0.2857,  0.1194, -0.0765, -0.1750, -0.0951,  0.1544,
0:          0.4739,  0.7158,  0.7475,  0.5144,  0.0537, -0.5186, -0.7353, -0.7254, -0.5700, -0.3096, -0.0415,  0.1292,
0:          0.1413], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.2889, 1.2892, 1.2889, 1.2885, 1.2874, 1.2825, 1.2706, 1.2489, 1.2166, 1.1753, 1.1315, 1.0923, 1.0651, 1.0523,
0:         1.0531, 1.0650, 1.0851, 1.1138, 1.1521, 1.2015, 1.2613, 1.3267, 1.3893, 1.4386, 1.4660], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431,
0:         -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431,
0:         -0.2431], device='cuda:0')
0: [DEBUG] Epoch 41, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2431,     nan,     nan,     nan,     nan,     nan, -0.2431,     nan,
0:             nan,     nan,     nan,     nan, -0.2431,     nan,     nan, -0.2431,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431, -0.2431,     nan,     nan, -0.2431,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2431,     nan,     nan,     nan, -0.2431,     nan,     nan,
0:         -0.2431,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2431,     nan,     nan, -0.2431,     nan, -0.2431,     nan,     nan,     nan,
0:         -0.2431,     nan,     nan,     nan, -0.2431,     nan,     nan, -0.2431,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431,     nan,     nan,     nan,
0:         -0.2431,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431,     nan,
0:             nan,     nan, -0.2431, -0.2431,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2431, -0.2431,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431,     nan, -0.2431,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2431,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431,     nan, -0.2431,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2431,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 41, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4620, -0.4326, -0.3669, -0.2873, -0.2213, -0.1892, -0.1785, -0.1691, -0.1716, -0.1608, -0.1477, -0.1549,
0:         -0.1587, -0.1654, -0.1544, -0.1154, -0.0616, -0.0093, -0.3270, -0.3465, -0.3357, -0.3021, -0.2721, -0.2534,
0:         -0.2403], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0750,  0.0461,  0.0114, -0.0204, -0.0587, -0.1070, -0.1828, -0.2391, -0.2548, -0.2390, -0.2088, -0.1804,
0:         -0.1975, -0.2464, -0.3001, -0.3064, -0.2653, -0.2163,  0.0587,  0.0509,  0.0401,  0.0251, -0.0080, -0.0750,
0:         -0.1482], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7715, -0.7722, -0.7716, -0.7674, -0.7587, -0.7500, -0.7500, -0.7469, -0.7435, -0.7400, -0.7402, -0.7391,
0:         -0.7319, -0.7258, -0.7245, -0.7211, -0.7251, -0.7250, -0.7635, -0.7569, -0.7538, -0.7436, -0.7391, -0.7417,
0:         -0.7403], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.4065,  0.5127,  0.5711,  0.4631,  0.2153,  0.0653, -0.0573, -0.1420, -0.0112,  0.1844,  0.3165,  0.4539,
0:          0.5383,  0.4481,  0.3133,  0.3905,  0.4359,  0.2157,  0.5033,  0.4795,  0.4889,  0.4161,  0.2465,  0.1809,
0:          0.0901], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4735, 0.4624, 0.4571, 0.4536, 0.4647, 0.4917, 0.5422, 0.5934, 0.6340, 0.6537, 0.6528, 0.6351, 0.6161, 0.6046,
0:         0.5854, 0.5547, 0.5209, 0.4983, 0.4999, 0.5249, 0.5565, 0.5761, 0.5739, 0.5475, 0.5061], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2396, -0.2430, -0.2589, -0.2467, -0.2454, -0.2399, -0.2423, -0.2421, -0.2356, -0.2444, -0.2455, -0.2436,
0:         -0.2492, -0.2468, -0.2439, -0.2437, -0.2371, -0.2503, -0.2469, -0.2455, -0.2487, -0.2474, -0.2402, -0.2459,
0:         -0.2456], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.20475910604000092; velocity_v: 0.3591294586658478; specific_humidity: 0.17742030322551727; velocity_z: 0.6258524060249329; temperature: 0.13601891696453094; total_precip: 0.7964394092559814; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19821226596832275; velocity_v: 0.31571125984191895; specific_humidity: 0.18877466022968292; velocity_z: 0.4930263161659241; temperature: 0.15597455203533173; total_precip: 0.5182931423187256; 
0: epoch: 41 [1/5 (20%)]	Loss: 0.65737 : 0.31356 :: 0.20526 (2.56 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21065160632133484; velocity_v: 0.36461907625198364; specific_humidity: 0.17243970930576324; velocity_z: 0.6294600963592529; temperature: 0.1525038778781891; total_precip: 0.570976734161377; 
0: epoch: 41 [2/5 (40%)]	Loss: 0.57098 : 0.31665 :: 0.21024 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20829541981220245; velocity_v: 0.35486504435539246; specific_humidity: 0.1697171926498413; velocity_z: 0.5450505614280701; temperature: 0.13373248279094696; total_precip: 0.3719116747379303; 
0: epoch: 41 [3/5 (60%)]	Loss: 0.37191 : 0.26397 :: 0.21165 (16.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21821211278438568; velocity_v: 0.35342520475387573; specific_humidity: 0.20755720138549805; velocity_z: 0.7119288444519043; temperature: 0.19099105894565582; total_precip: 0.7143071293830872; 
0: epoch: 41 [4/5 (80%)]	Loss: 0.71431 : 0.36383 :: 0.21021 (15.79 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [4.00543213e-05 4.10079956e-05 4.19616663e-05 4.38690186e-05
0:  4.48226929e-05 4.57763635e-05 4.57763635e-05 4.57763635e-05
0:  4.57763635e-05 4.48226929e-05 4.29153479e-05 4.19616663e-05
0:  4.10079956e-05 5.05447424e-05 6.10351562e-05 7.05718994e-05
0:  7.24792480e-05 5.91278076e-05 4.57763635e-05 3.14712524e-05
0:  2.76565552e-05 3.14712524e-05 2.67028809e-05 2.28881836e-05
0:  2.19345093e-05 2.00271606e-05 1.81198120e-05 1.81198120e-05
0:  1.90734863e-05 2.00271606e-05 2.09808350e-05 2.09808350e-05
0:  2.09808350e-05 2.09808350e-05 2.00271606e-05 1.90734863e-05
0:  1.90734863e-05 1.81198120e-05 1.71661377e-05 1.62124634e-05
0:  1.52587891e-05 1.43051147e-05 1.33514404e-05 1.33514404e-05
0:  1.23977661e-05 1.14440918e-05 1.04904175e-05 9.53674316e-06
0:  9.53674316e-06 1.23977661e-05 1.52587891e-05 1.81198120e-05
0:  2.00271606e-05 2.09808350e-05 2.28881836e-05 2.38418579e-05
0:  2.47955322e-05 2.57492065e-05 2.57492065e-05 2.47955322e-05
0:  2.28881836e-05 2.00271606e-05 1.81198120e-05 1.52587891e-05
0:  1.33514404e-05 1.33514404e-05 1.71661377e-05 2.09808350e-05
0:  2.57492065e-05 3.05175781e-05 2.86102295e-05 2.67028809e-05
0:  2.47955322e-05 2.28881836e-05 2.47955322e-05 2.47955322e-05
0:  2.38418579e-05 2.28881836e-05 2.28881836e-05 2.19345093e-05
0:  2.00271606e-05 1.71661377e-05 1.33514404e-05 1.04904175e-05
0:  1.14440918e-05 1.71661377e-05 1.43051147e-05 1.43051147e-05
0:  1.62124634e-05 1.81198120e-05 2.00271606e-05 1.90734863e-05
0:  1.71661377e-05 1.43051147e-05 1.14440918e-05 7.62939453e-06
0:  4.76837158e-06 3.81469727e-06 4.76837158e-06 4.76837158e-06
0:  4.76837158e-06 4.76837158e-06 1.04904175e-05 1.71661377e-05
0:  2.28881836e-05 2.76565552e-05 2.76565552e-05 3.05175781e-05
0:  2.28881836e-05 2.38418579e-05 2.47955322e-05 2.67028809e-05
0:  2.76565552e-05 2.86102295e-05 2.86102295e-05 2.67028809e-05
0:  2.47955322e-05 2.38418579e-05 2.19345093e-05 2.00271606e-05
0:  1.90734863e-05 1.71661377e-05 1.52587891e-05 1.33514404e-05
0:  1.33514404e-05 1.71661377e-05 2.09808350e-05 2.47955322e-05
0:  2.86102295e-05 2.95639038e-05 2.57492065e-05 2.09808350e-05
0:  1.62124634e-05 1.14440918e-05 8.58306885e-06 1.23977661e-05
0:  1.23977661e-05 1.23977661e-05 1.23977661e-05 1.33514404e-05
0:  7.62939453e-06 8.58306885e-06 8.58306885e-06 8.58306885e-06
0:  8.58306885e-06 8.58306885e-06 7.62939453e-06 7.62939453e-06
0:  6.67572021e-06 6.67572021e-06 5.72204590e-06 5.72204590e-06
0:  5.72204590e-06 5.72204590e-06 4.76837158e-06 4.76837158e-06
0:  4.76837158e-06 6.67572021e-06 7.62939453e-06 8.58306885e-06
0:  9.53674316e-06 1.33514404e-05 1.04904175e-05 1.23977661e-05
0:  1.43051147e-05 1.71661377e-05 1.90734863e-05 1.90734863e-05
0:  1.62124634e-05 1.33514404e-05 1.04904175e-05 7.62939453e-06
0:  6.67572021e-06 7.62939453e-06 7.62939453e-06 7.62939453e-06
0:  7.62939453e-06 7.62939453e-06 8.58306885e-06 1.04904175e-05
0:  1.14440918e-05 1.33514404e-05 1.43051147e-05 1.52587891e-05
0:  1.43051147e-05 1.43051147e-05 1.33514404e-05 1.33514404e-05
0:  1.14440918e-05 1.52587891e-05 2.09808350e-05 2.67028809e-05
0:  3.24249268e-05 4.00543213e-05 4.48226929e-05 4.29153479e-05
0:  4.00543213e-05 3.81469727e-05 3.52859497e-05 2.95639038e-05]
0: Target values (first 200):
0: [5.72204590e-06 6.19888306e-06 7.15255737e-06 7.62939453e-06
0:  8.58306885e-06 9.05990601e-06 9.53674316e-06 1.00135803e-05
0:  1.00135803e-05 1.04904175e-05 1.14440918e-05 1.00135803e-05
0:  7.62939453e-06 8.58306885e-06 1.00135803e-05 1.09672546e-05
0:  1.14440918e-05 9.53674316e-06 7.62939453e-06 5.24520874e-06
0:  4.29153442e-06 4.29153442e-06 4.29153442e-06 3.81469727e-06
0:  3.33786011e-06 2.86102295e-06 2.86102295e-06 2.38418579e-06
0:  2.38418579e-06 2.38418579e-06 2.38418579e-06 2.38418579e-06
0:  2.38418579e-06 2.38418579e-06 2.38418579e-06 2.38418579e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 2.38418579e-06 2.86102295e-06 3.33786011e-06
0:  3.33786011e-06 3.33786011e-06 3.81469727e-06 3.81469727e-06
0:  4.29153442e-06 4.29153442e-06 4.29153442e-06 4.29153442e-06
0:  4.29153442e-06 4.29153442e-06 4.29153442e-06 4.76837158e-06
0:  5.24520874e-06 5.72204590e-06 6.19888306e-06 5.72204590e-06
0:  5.72204590e-06 5.24520874e-06 5.72204590e-06 5.72204590e-06
0:  6.19888306e-06 6.19888306e-06 8.10623169e-06 7.15255737e-06
0:  5.24520874e-06 5.24520874e-06 5.24520874e-06 5.24520874e-06
0:  5.24520874e-06 5.72204590e-06 6.19888306e-06 6.67572021e-06
0:  7.15255737e-06 7.15255737e-06 7.62939453e-06 7.62939453e-06
0:  7.62939453e-06 7.62939453e-06 7.15255737e-06 7.62939453e-06
0:  8.10623169e-06 8.58306885e-06 9.05990601e-06 1.04904175e-05
0:  1.14440918e-05 1.28746033e-05 1.33514404e-05 1.33514404e-05
0:  1.33514404e-05 1.38282776e-05 1.33514404e-05 1.33514404e-05
0:  1.33514404e-05 1.28746033e-05 1.19209290e-05 2.57492065e-05
0:  1.09672546e-05 1.09672546e-05 1.19209290e-05 1.23977661e-05
0:  1.28746033e-05 1.33514404e-05 1.33514404e-05 1.43051147e-05
0:  1.52587891e-05 1.62124634e-05 1.71661377e-05 1.62124634e-05
0:  1.33514404e-05 1.00135803e-05 7.15255737e-06 4.29153442e-06
0:  3.33786011e-06 3.33786011e-06 3.33786011e-06 2.86102295e-06
0:  2.86102295e-06 2.86102295e-06 2.38418579e-06 2.38418579e-06
0:  2.38418579e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 2.38418579e-06 2.38418579e-06
0:  2.86102295e-06 2.86102295e-06 2.86102295e-06 3.33786011e-06
0:  3.33786011e-06 3.81469727e-06 3.81469727e-06 3.81469727e-06
0:  3.81469727e-06 4.29153442e-06 4.29153442e-06 4.29153442e-06
0:  4.76837158e-06 5.24520874e-06 6.19888306e-06 6.67572021e-06
0:  7.62939453e-06 8.58306885e-06 9.05990601e-06 1.04904175e-05
0:  1.23977661e-05 1.38282776e-05 1.57356262e-05 1.57356262e-05
0:  1.38282776e-05 1.14440918e-05 9.05990601e-06 6.67572021e-06
0:  5.72204590e-06 6.19888306e-06 6.19888306e-06 6.67572021e-06
0:  6.67572021e-06 7.15255737e-06 7.62939453e-06 8.10623169e-06
0:  8.58306885e-06 8.58306885e-06 9.05990601e-06 1.04904175e-05]
0: Prediction values (first 20):
0: [-5.0781293 -4.890537  -4.316398  -3.4637685 -2.649157  -2.0894675
0:  -1.6561131 -1.445117  -1.6069083 -1.7646708 -2.0979438 -2.5496278
0:  -2.7917438 -2.818151  -2.5327945 -1.9941831 -1.5671353 -1.3866696
0:  -2.628593  -2.441403 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.610, max = 1.194, mean = -0.089
0:          sample (first 20): tensor([-0.9865, -0.9705, -0.9213, -0.8483, -0.7786, -0.7307, -0.6936, -0.6755, -0.6894, -0.7029, -0.7314, -0.7701,
0:         -0.7908, -0.7931, -0.7686, -0.7225, -0.6860, -0.6705, -0.9685, -0.9896])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.475975 20.330374 20.219894 19.991383 19.729843 19.13226  19.23645
0:  19.150692 19.22486  19.13461  18.443287 17.794859 17.10141  17.060406
0:  17.582737 18.60175  19.697903 20.448147 20.484169 20.387653]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.8244839 0.992991  1.5587087 2.416974  3.2051437 3.782216  4.0870714
0:  4.2766275 4.027478  3.7791638 3.4013171 2.778685  2.2735891 1.8419886
0:  1.759687  2.1067944 2.7434683 3.424148  1.8927741 1.9158783]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7188463 -5.704213  -5.2846375 -4.483083  -3.6116028 -2.919899
0:  -2.4376907 -2.1136484 -2.299674  -2.5766964 -3.0470047 -3.7340326
0:  -4.18054   -4.486623  -4.384487  -3.9643955 -3.5468807 -3.2057333
0:  -4.3665495 -4.410139 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.321997  9.452055  9.856452 10.291267 10.541115 10.523827 10.638028
0:  10.649672 10.634014 10.703522 10.534534 10.210699  9.82518   9.738435
0:   9.969683 10.576624 11.304083 11.942925  9.912055 10.32837 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.347622 38.796314 39.146233 39.381657 39.60914  39.59962  40.406586
0:  41.038265 41.78464  42.33519  42.17807  41.997814 41.82539  42.12767
0:  42.93107  43.988045 45.031605 45.56763  44.80664  45.31216 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.533062 30.305626 30.123951 29.99408  29.931541 29.712967 30.225628
0:  30.58572  31.038311 31.26722  30.815498 30.405529 29.938023 30.022781
0:  30.649254 31.56026  32.469757 32.95614  31.469772 31.416489]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.276775 28.65254  29.18675  29.703396 29.97214  29.92723  30.055658
0:  30.076097 30.06692  30.026537 29.629362 29.0023   28.41903  28.034939
0:  28.054972 28.345663 28.681467 28.769623 24.63541  24.406437]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [52.597473 52.828773 52.75304  52.640522 52.273533 51.36132  51.12178
0:  50.47166  49.929127 48.967747 47.209995 45.53034  43.939407 42.88162
0:  42.576153 42.616463 42.89162  43.034092 48.37636  48.77337 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2670226 5.2144656 5.4802737 5.994569  6.5100446 6.8553576 7.2762446
0:  7.6055284 7.6153903 7.5896196 7.323051  6.8335824 6.4539604 6.211442
0:  6.299603  6.6849594 7.162984  7.5992336 5.0789576 5.070159 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.351368  12.359981  12.670948  13.059631  13.340542  13.361469
0:  13.592512  13.699497  13.8416    14.0049515 13.92456   13.737072
0:  13.6418    13.805886  14.359348  15.301459  16.42967   17.385427
0:  15.386389  15.366081 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.256839  4.153598  4.194524  4.4594064 4.774829  4.973834  5.272999
0:  5.4689736 5.408105  5.3196526 5.0327625 4.6779957 4.474929  4.5155964
0:  4.845623  5.400185  5.944906  6.387386  4.9915996 5.138592 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5844531 1.8844538 2.4490585 3.2006345 3.9187021 4.443653  4.800955
0:  4.9493484 4.6764646 4.368376  3.932869  3.3928206 3.0442796 2.8638377
0:  3.048163  3.5274882 4.1630692 4.7975864 4.431605  4.8874903]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2043123  -3.1588707  -2.683506   -1.8692684  -1.0525808  -0.48576832
0:  -0.14618444 -0.03413296 -0.31436348 -0.51080656 -0.76373434 -1.1011281
0:  -1.2456579  -1.3310289  -1.2070322  -0.9193435  -0.6494527  -0.49605513
0:  -1.6181583  -1.4919395 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0048127  -1.0060334  -0.62128973  0.20167255  0.9992094   1.6475263
0:   1.7504616   1.6838231   0.9645052   0.18007421 -0.71947813 -2.0016074
0:  -3.2364125  -4.466904   -5.3176866  -5.5670085  -5.2193036  -4.5489354
0:  -4.5356307  -4.795819  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.520593   6.7382627  7.331217   8.049443   8.55764    8.773715
0:   8.812966   8.882967   8.849413   8.98528    9.124577   8.959253
0:   8.791788   8.643639   8.858106   9.628979  10.87703   12.2786
0:  11.01708   11.511187 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.651919   9.585489   9.690599   9.805965   9.803016   9.620192
0:   9.560474   9.531551   9.467099   9.440529   9.307473   9.117907
0:   9.07291    9.363741  10.0881605 11.174031  12.374428  13.410402
0:  11.956254  12.151434 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.24092  25.451832 25.846743 26.310137 26.703707 26.814331 27.287792
0:  27.637157 28.023502 28.257086 28.004467 27.67032  27.249525 27.156553
0:  27.499544 28.023981 28.506386 28.695723 23.968565 23.875172]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.5061035   4.976769    4.8613014   5.085306    5.3108006   5.335978
0:   5.180119    4.837103    4.118845    3.4152613   2.5587268   1.4863725
0:   0.53028774 -0.29427624 -0.73275423 -0.8554282  -0.8043413  -0.7398062
0:  -4.509311   -4.88948   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.0282307 4.450257  5.093337  5.9087505 6.7198963 7.3587556 7.723315
0:  7.9435897 7.6960382 7.410269  7.038243  6.5162745 6.18351   6.024261
0:  6.182029  6.7271748 7.470864  8.190839  4.653982  4.597357 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.012522 24.235971 24.481655 24.573515 24.50983  24.195097 24.383175
0:  24.45639  24.585802 24.651796 24.214338 23.75943  23.321747 23.39843
0:  24.004635 24.998096 26.101728 26.92627  27.284761 27.522062]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.64884    8.722954   9.084545   9.635314  10.0301895 10.185178
0:  10.1134815 10.0991955  9.892263   9.822312   9.727577   9.331833
0:   8.908972   8.417297   8.1996     8.455674   9.159881   9.95334
0:   7.6595745  7.522808 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.71990347 -0.46370888  0.19187117  1.076016    1.7936049   2.2005763
0:   2.3250837   2.2729425   1.839776    1.3889318   0.8453932   0.1178956
0:  -0.4259057  -0.7258458  -0.6448636  -0.10255384  0.66460466  1.4015298
0:  -0.24186993 -0.13874197]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.845889    4.813578    5.0524373   5.555113    5.9794846   6.205549
0:   6.1178946   6.0086875   5.4907036   5.0950575   4.6874614   4.0362043
0:   3.5175407   2.9881449   2.6990669   2.8605666   3.3104      3.8116972
0:   0.5682268  -0.01762438]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.317498   11.315258   10.416979    9.744297    9.206991    8.754852
0:   8.359341    8.093749    7.4816246   6.8579397   6.040567    4.8577023
0:   3.6827536   2.4844654   1.5247073   1.072165    1.0483179   1.3173347
0:  -0.78365374 -1.2179027 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.160744   7.9794216  8.067679   8.322449   8.599622   8.660692
0:   9.369048   9.902009  10.607733  11.191649  11.272758  11.246051
0:  11.19559   11.714176  12.75498   14.300178  15.902076  17.122944
0:  15.867268  15.743879 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.197662 23.666367 24.284874 24.874767 25.208794 25.198889 25.364862
0:  25.423775 25.681414 25.980522 26.11201  26.073906 26.183712 26.435162
0:  27.024193 27.846601 28.731308 29.534765 25.467974 25.89449 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.086596  -11.202848  -10.835716   -9.951689   -8.885672   -8.002956
0:   -7.545689   -7.3447547  -7.756326   -8.172754   -8.638311   -9.171768
0:   -9.4679165  -9.643322   -9.623068   -9.358002   -8.998095   -8.676367
0:   -9.961939  -10.23411  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.71905   8.714296  8.964764  9.3094845 9.483267  9.39953   9.349266
0:  9.130182  8.743088  8.328863  7.679612  6.8853216 6.1537642 5.740075
0:  5.710497  6.02585   6.408787  6.653673  4.691099  4.811493 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.056562  15.466772  15.876518  16.233196  16.344028  16.212244
0:  16.040617  15.830439  15.440048  15.139917  14.735991  14.224279
0:  13.872967  13.67911   13.891668  14.495337  15.311747  16.05328
0:  14.34503   14.3157215]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.08477   14.888841  14.859592  14.878314  14.891382  14.558107
0:  14.675457  14.541557  14.4911175 14.321122  13.707189  13.062584
0:  12.409652  12.197187  12.4209175 13.0559025 13.8080845 14.493713
0:  14.555672  14.698686 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8033543 6.836151  6.937105  6.9257197 6.6314936 6.06701   5.688771
0:  5.324655  5.004705  4.6891117 4.041069  3.3203986 2.580536  2.2815666
0:  2.4727058 3.0982296 3.8559387 4.4510736 2.8841605 2.9447443]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0558224 -4.2964296 -4.212146  -3.77604   -3.31911   -3.0364447
0:  -2.8964176 -2.944003  -3.397378  -3.8933268 -4.4995384 -5.197339
0:  -5.6842804 -5.8952427 -5.763854  -5.291338  -4.770406  -4.4183044
0:  -7.325903  -7.7155895]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.901833 21.71841  21.607012 21.432238 21.095818 20.455133 20.173355
0:  19.77471  19.513613 19.254192 18.646038 18.051834 17.453365 17.316189
0:  17.590637 18.199154 18.86709  19.27433  17.326004 17.356754]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1005745  -2.1627455  -1.837821   -1.1378264  -0.35465002  0.30578518
0:   0.7195306   0.92902803  0.6507678   0.35388994 -0.04557467 -0.6210685
0:  -1.0320101  -1.353939   -1.3869181  -1.1020198  -0.6279292  -0.07611322
0:  -1.1920938  -1.3595686 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.373333  -4.423011  -4.125882  -3.438077  -2.7048578 -2.1691246
0:  -2.0231376 -2.106844  -2.6991081 -3.2854853 -3.9044447 -4.5841503
0:  -5.0369997 -5.3319025 -5.35009   -4.976304  -4.4437675 -3.8670073
0:  -4.3973627 -4.580079 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.010832   -9.549139   -9.642355   -9.170851   -8.518255   -7.988889
0:   -7.9812407  -8.129391   -8.891714   -9.619316  -10.317081  -11.193535
0:  -11.861326  -12.375868  -12.568899  -12.207569  -11.468014  -10.556736
0:  -11.119638  -11.791521 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.9186087   1.4656372   1.2266135   1.0057735   0.62857866 -0.00758314
0:  -0.27137423 -0.52864313 -0.67254543 -0.7578654  -1.2013636  -1.7853909
0:  -2.4948053  -2.6860952  -2.463745   -1.8454113  -1.2600055  -0.9800353
0:  -1.5371323  -1.6759443 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.297924  11.147338  11.249756  11.406375  11.484355  11.432325
0:  11.441502  11.405189  11.238119  11.017275  10.565048   9.991175
0:   9.556502   9.39633    9.682665  10.2916975 10.989326  11.560364
0:   8.8172245  8.949674 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.5742064  7.57156    7.8931036  8.369107   8.76914    8.89431
0:   9.190714   9.348983   9.410295   9.432504   9.127411   8.615221
0:   8.126038   7.9568963  8.217676   8.963836   9.870344  10.666121
0:  10.288456  10.449235 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.46119928  0.5564704   1.0359516   1.9433131   2.882466    3.6294832
0:   3.9768732   4.101699    3.495087    2.715126    1.6553588   0.24187088
0:  -0.98396254 -1.959681   -2.3969274  -2.1046882  -1.424274   -0.6456666
0:  -1.2427363  -1.4564195 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.429525 28.357828 28.276028 28.22203  28.080236 27.565128 27.710907
0:  27.603653 27.727465 27.685844 27.037455 26.413784 25.788467 25.571756
0:  25.847343 26.519667 27.310438 27.85918  26.64873  26.756285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.119715  12.827396  12.818265  12.9014225 12.894083  12.603732
0:  12.630222  12.585423  12.740557  12.979365  13.010719  12.983803
0:  12.992901  13.282612  13.95471   14.975075  16.095491  17.053188
0:  15.29464   15.478235 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.2745347  -0.64916277 -0.38220024  0.33485174  1.0448813   1.4914947
0:   1.9316001   2.227438    2.4655652   2.7375245   2.9167967   2.876432
0:   2.8983324   3.2334042   3.836753    4.7585583   5.6600347   6.2033153
0:   4.1230783   4.5279818 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.04378  21.487906 22.156042 22.810373 23.171799 23.21542  23.241901
0:  23.234884 23.195305 23.302694 23.317833 23.244175 23.23851  23.352444
0:  23.822836 24.541193 25.296326 25.869343 22.680178 22.990828]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.94301   -7.303345  -7.256001  -6.633549  -5.786041  -4.992637
0:  -4.643975  -4.435599  -4.857533  -5.3267236 -5.8501716 -6.631496
0:  -7.2677827 -7.7804503 -7.944576  -7.5551333 -6.6908116 -5.5873446
0:  -4.533115  -4.8574014]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.2032933 -4.2953315 -3.98027   -3.2844176 -2.5495973 -2.0156636
0:  -1.8886876 -2.0063815 -2.7012372 -3.3614898 -4.0587125 -4.820376
0:  -5.290151  -5.4935517 -5.3221173 -4.721338  -3.9454331 -3.189013
0:  -3.927362  -3.9901948]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5313239  -0.50303984 -0.12657022  0.64482546  1.4810891   2.2235022
0:   2.677814    3.0541987   3.0201092   3.0175257   2.975317    2.7421198
0:   2.596776    2.5387      2.7878523   3.5069537   4.5932055   5.7452736
0:   5.596185    5.823699  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.1056185 15.124349  15.505439  16.023262  16.303804  16.162825
0:  16.09402   15.9246645 15.807671  15.799276  15.642353  15.365572
0:  15.107319  15.036555  15.295022  15.844751  16.529106  17.0512
0:  14.711541  14.912162 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.110556 14.95505  15.136953 15.501783 15.816935 15.898073 16.343935
0:  16.632235 17.015335 17.373657 17.353127 17.165281 16.994265 17.121748
0:  17.690016 18.722033 19.926552 20.999563 19.794018 19.498991]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0327392  -3.0758252  -2.7177062  -2.1054454  -1.5790286  -1.2969241
0:  -1.0096564  -0.86572075 -0.8744912  -0.9023452  -1.1332564  -1.5630336
0:  -1.9205012  -2.0047421  -1.7667785  -1.1695466  -0.5275903  -0.11342573
0:  -1.5672431  -1.495997  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.095941  12.2116375 12.5484705 12.921078  13.084297  12.963772
0:  12.793983  12.505394  12.13802   11.851564  11.476728  10.9895935
0:  10.632292  10.488075  10.712358  11.293988  12.053217  12.724739
0:  11.333484  11.484768 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7791367 -5.5258856 -4.860471  -3.885601  -2.9725275 -2.338128
0:  -1.9994478 -1.9349661 -2.3355885 -2.7151327 -3.2030296 -3.7984648
0:  -4.1753387 -4.344309  -4.1326227 -3.558817  -2.8815336 -2.2741613
0:  -3.3093853 -2.6418133]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.72099  38.008896 38.04932  37.884907 37.637386 37.220955 37.6577
0:  37.97154  38.545174 38.865196 38.528553 38.181625 37.94302  38.171078
0:  38.954292 39.82086  40.63563  40.95584  40.741768 41.366127]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.730135  3.9052866 4.4058676 5.074357  5.6263294 5.9352846 6.1033072
0:  6.129101  5.916404  5.6909432 5.347866  4.81938   4.3943567 4.1051707
0:  4.0876    4.420883  4.911826  5.3665648 2.9290247 3.219332 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.018334  14.285147  14.694191  15.1922035 15.63964   15.870352
0:  16.539558  17.049156  17.494026  17.752611  17.460505  17.024094
0:  16.425413  16.155144  16.191666  16.487799  16.86555   17.060787
0:  15.123465  15.078085 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.80086565 -0.88586617 -0.6682792  -0.07864618  0.5757632   1.140676
0:   1.331181    1.4220338   1.0361962   0.7024884   0.40773964  0.01572704
0:  -0.19804668 -0.31415367 -0.17043114  0.31545448  1.0648036   1.8675117
0:   1.1257281   1.0192204 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.534767  -9.363888  -8.712034  -7.7031627 -6.729812  -6.077435
0:  -5.8155684 -5.929833  -6.5639234 -7.0840898 -7.6123567 -8.219969
0:  -8.522926  -8.646542  -8.445915  -7.9038777 -7.2666793 -6.599586
0:  -6.7068553 -6.338426 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0941153 -2.391941  -2.3808546 -2.0670357 -1.7378359 -1.5359693
0:  -1.672318  -1.8172007 -2.3405151 -2.717165  -3.0026922 -3.4312706
0:  -3.6495895 -3.7810326 -3.6182537 -3.0136023 -2.1074    -1.1168098
0:  -1.4689488 -1.5686116]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.5437012   1.5544882   1.9303517   2.5778322   3.0974097   3.3063006
0:   3.3004327   3.0028667   2.3344927   1.6611247   0.87804985  0.02019596
0:  -0.5909915  -0.83891153 -0.6157832   0.05296898  0.9132328   1.7152786
0:   1.9151053   2.3281827 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.2004   25.017496 24.707373 24.25107  23.736223 22.907795 22.826826
0:  22.586912 22.546251 22.386982 21.588615 20.902687 20.157377 20.017572
0:  20.418524 21.211203 22.095057 22.680397 23.4006   23.497683]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.901152 26.196566 26.522074 26.897194 27.252651 27.353474 28.157873
0:  28.725864 29.342665 29.763063 29.589344 29.408566 29.267984 29.661625
0:  30.462162 31.503551 32.46472  32.836372 32.091797 32.273354]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.36284828  0.2572651   0.43189383  0.75393105  0.998374    1.0711136
0:   1.2360725   1.2652683   1.1139903   0.95717525  0.55334425 -0.03908396
0:  -0.52382374 -0.71802044 -0.55061007 -0.18712282  0.07662439  0.10400629
0:  -1.1499186  -1.156951  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.539293 21.470833 21.539654 21.652058 21.73171  21.565445 21.895851
0:  22.077251 22.397984 22.579996 22.305618 21.951462 21.616999 21.708324
0:  22.24186  23.179604 24.272041 25.152462 24.363    24.436674]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.869928  -10.003828   -9.633398   -8.715889   -7.6257043  -6.662377
0:   -6.1831527  -5.9103065  -6.3146358  -6.7375207  -7.2101016  -7.8492503
0:   -8.234236   -8.502447   -8.467129   -8.000038   -7.288503   -6.497045
0:   -7.514297   -7.62855  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5030608 1.434289  1.7043872 2.12015   2.3998485 2.4038734 2.367354
0:  2.2717233 2.0923858 1.9890337 1.8024716 1.4587789 1.1666203 1.0720582
0:  1.3205056 1.9436846 2.708579  3.4261677 2.6353073 2.616814 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.437257  9.188459  9.175633  9.218107  9.050395  8.591293  8.096542
0:  7.569083  6.962844  6.46657   5.918746  5.2648993 4.7879257 4.56633
0:  4.715766  5.25819   6.0009193 6.656245  5.100408  5.004366 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.243765  14.023322  14.106861  14.330665  14.479485  14.381903
0:  14.531627  14.487958  14.381746  14.1614895 13.573476  12.816582
0:  12.116016  11.876118  12.191175  13.004133  14.033045  14.934816
0:  14.071108  14.088837 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6853013  -1.8847909  -1.6752067  -1.1578231  -0.67763805 -0.41613674
0:  -0.23318958 -0.18588781 -0.40098286 -0.61016464 -0.9688072  -1.5406165
0:  -1.9253564  -2.0759463  -1.9244676  -1.3815465  -0.7868848  -0.32544184
0:  -2.1955705  -2.1233096 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.510975  16.377111  16.528292  16.812277  16.936928  16.78377
0:  16.649525  16.446333  16.193436  16.017544  15.678593  15.179359
0:  14.718491  14.34795   14.2764435 14.507889  14.844763  15.097553
0:  12.149075  11.900129 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.220089  5.504326  6.037979  6.7106004 7.271475  7.568584  7.616003
0:  7.5965075 7.2263556 6.960255  6.659056  6.1646347 5.814931  5.5206594
0:  5.510832  5.858752  6.3655686 6.8614197 5.0973577 5.277787 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.690412   -1.6781659  -1.2862115  -0.6247802  -0.0526967   0.27301407
0:   0.3906622   0.38548517  0.07092381 -0.27041435 -0.7366977  -1.436461
0:  -2.0187297  -2.3903155  -2.4141011  -1.9432826  -1.2521129  -0.59722376
0:  -2.2755094  -2.3595538 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.0903354 6.8336625 6.7775917 6.734169  6.5397754 6.104739  5.7239666
0:  5.271959  4.743889  4.3005643 3.7133622 3.0391765 2.499323  2.3053794
0:  2.4482803 2.9376895 3.4650154 3.8118176 1.960506  2.1279368]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.300955 17.083328 17.146557 17.33287  17.458866 17.32255  17.39867
0:  17.299828 17.196487 17.113909 16.70504  16.237911 15.78013  15.675135
0:  16.000328 16.67577  17.480747 18.215132 16.963104 16.815376]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.7940044   5.422389    5.337059    5.299514    5.0129147   4.402794
0:   3.861294    3.2655354   2.6618724   2.1958208   1.5131907   0.64239883
0:  -0.28696156 -0.895864   -1.1645675  -1.1030602  -0.9577055  -0.9708786
0:  -1.8421569  -2.1967692 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.61285  21.772066 21.880558 21.862467 21.822653 21.494034 21.68288
0:  21.776272 22.052454 22.306711 22.195915 22.136105 22.053318 22.434519
0:  23.197334 24.193855 25.268356 26.112091 26.363316 26.481022]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.77435684 0.8563299  1.2752934  2.0478354  2.9311328  3.6937602
0:  4.2132688  4.40281    4.036623   3.665093   3.2059135  2.7260294
0:  2.6017141  2.712472   3.2649362  4.1650386  5.156928   6.0164423
0:  5.051044   5.148362  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.286539   8.403953   9.761537  11.017222  11.667882  11.641831
0:  11.2843    10.770212  10.207805   9.9531975  9.786575   9.551279
0:   9.3136425  8.974983   8.698886   8.6294365  8.791766   9.040869
0:   5.567728   5.425643 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.9681745 5.0283613 5.4583564 6.1064925 6.6414557 6.9314313 7.109328
0:  7.0568876 6.650372  6.127024  5.335018  4.387287  3.5687733 3.105988
0:  3.0757465 3.4937901 4.0613365 4.5233955 1.9765005 1.978468 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.646317 43.676758 43.631573 43.507988 43.301876 42.77331  42.812023
0:  42.596703 42.457615 42.132496 41.220127 40.289646 39.433014 38.981323
0:  39.04005  39.428745 39.9456   40.21179  38.571953 38.607174]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.330128  14.54216   14.9842415 15.450241  15.795309  15.907555
0:  16.170168  16.288973  16.41824   16.531258  16.428257  16.234514
0:  16.181112  16.416246  17.038982  18.010305  19.107977  20.147036
0:  18.627813  18.979334 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.09005833 -0.18434858  0.11931133  0.5693207   0.8294387   0.7709594
0:   0.7517023   0.5328412   0.12289429 -0.33257532 -1.1034098  -2.114215
0:  -3.0656757  -3.573205   -3.666234   -3.282978   -2.843049   -2.5392218
0:  -4.7228785  -4.5718536 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.822031   9.956669  10.37347   10.893996  11.300318  11.449754
0:  11.508154  11.4921875 11.278851  11.132086  10.872702  10.411839
0:  10.034769   9.732536   9.752895  10.17614   10.83995   11.51473
0:   9.609153   9.59946  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.045647 17.258635 17.76456  18.363718 18.853273 19.105228 19.456713
0:  19.63015  19.690601 19.674665 19.357796 18.833961 18.375154 18.1536
0:  18.316612 18.837526 19.515068 19.998549 16.477407 16.490631]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.30892   14.205885  14.324324  14.457588  14.421841  14.011532
0:  13.924097  13.690481  13.507624  13.341743  12.744265  12.026012
0:  11.147405  10.617168  10.463856  10.6888275 11.032532  11.259735
0:   9.392241   9.280918 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.157513 18.36333  18.625242 18.714958 18.571362 18.120884 17.946657
0:  17.692305 17.536303 17.432804 17.07609  16.670193 16.328753 16.302954
0:  16.67275  17.397514 18.257793 18.987015 18.347391 18.719946]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.9958782  1.3394794  2.3152347  3.7370756  5.1466904  6.349603
0:   7.4480085  8.408861   9.095166   9.732815  10.146805  10.255176
0:  10.349519  10.498359  10.896889  11.590149  12.323244  12.806174
0:   9.354589   9.416214 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.45235  17.440279 17.574905 17.655909 17.628963 17.377129 17.410202
0:  17.327255 17.220379 17.073162 16.597902 15.933086 15.278206 14.929484
0:  15.037724 15.607124 16.401852 17.15394  15.032087 15.272591]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.499468 22.381178 22.413393 22.443237 22.326796 21.980604 21.78811
0:  21.47383  21.054447 20.588123 19.869999 19.05736  18.435156 18.261896
0:  18.674557 19.53911  20.597761 21.485666 21.128136 21.156116]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.297966   -1.2862444  -0.82330036  0.01692152  0.87432957  1.4882731
0:   1.8299408   1.9072165   1.5252547   1.1315942   0.64116144 -0.02435637
0:  -0.4363122  -0.65385246 -0.51302624  0.10236788  0.8358364   1.4972963
0:   0.37978268  0.35619164]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.4109106 -5.686924  -5.5345116 -4.8925323 -4.1234474 -3.4838514
0:  -3.2851887 -3.2336788 -3.7489004 -4.2407312 -4.736788  -5.4431973
0:  -5.9552145 -6.356246  -6.455264  -6.0594244 -5.333376  -4.478801
0:  -4.599436  -4.916605 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.016342   -9.837751   -9.191553   -8.056721   -6.766187   -5.70873
0:   -4.780784   -4.17042    -4.052531   -4.030358   -4.2496114  -4.660402
0:   -4.8697515  -4.916248   -4.6228194  -4.105535   -3.62001    -3.2938552
0:   -5.7437987  -5.553366 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.436375 26.265278 26.062284 25.826437 25.622211 25.138979 25.232561
0:  25.074993 25.013214 24.779213 23.975277 23.3046   22.628843 22.463678
0:  22.751253 23.341908 24.017588 24.53321  24.458193 24.46718 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.335178  14.99745   14.806309  14.566254  14.108927  13.312074
0:  12.887453  12.379687  12.001595  11.696936  11.036614  10.317346
0:   9.557317   9.253368   9.465671  10.10718   10.93453   11.591505
0:  10.792306  10.5234995]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.911724 32.875206 32.962154 33.07302  33.1029   32.718945 32.685234
0:  32.421253 32.17747  31.848116 31.046871 30.188925 29.39002  28.856398
0:  28.801517 29.06842  29.4639   29.757063 27.45442  27.682959]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3138285  -3.437255   -3.1179447  -2.4129457  -1.6345234  -1.04532
0:  -0.6151285  -0.36065197 -0.513319   -0.74514675 -1.1661043  -1.7595844
0:  -2.1633282  -2.3747826  -2.2240963  -1.7249775  -1.1693516  -0.75437355
0:  -2.704811   -2.8667927 ]
0: validation loss for strategy=forecast at epoch 41 : 0.3250010907649994
0: validation loss for velocity_u : 0.16073618829250336
0: validation loss for velocity_v : 0.28097519278526306
0: validation loss for specific_humidity : 0.16906122863292694
0: validation loss for velocity_z : 0.5877237915992737
0: validation loss for temperature : 0.11370652914047241
0: validation loss for total_precip : 0.6378034949302673
0: 42 : 20:45:20 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 42, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4344, -0.4400, -0.4391, -0.4370, -0.4568, -0.4922, -0.5353, -0.5681, -0.5792, -0.5763, -0.5620, -0.5459,
0:         -0.5313, -0.5200, -0.5123, -0.5112, -0.5343, -0.5807, -0.4672, -0.5057, -0.5144, -0.5054, -0.5083, -0.5195,
0:         -0.5414], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0571, -0.0086,  0.0237,  0.0316,  0.0586,  0.1296,  0.2002,  0.2255,  0.2036,  0.1590,  0.1105,  0.0679,
0:          0.0401,  0.0267,  0.0130, -0.0169, -0.0484, -0.0659, -0.0346,  0.0196,  0.0654,  0.0849,  0.1054,  0.1447,
0:          0.1836], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([3.0807, 2.9019, 2.6815, 2.4394, 2.3604, 2.2728, 2.2701, 2.3776, 2.5074, 2.1631, 2.0628, 1.9573, 1.9101, 1.8058,
0:         1.5658, 1.3535, 0.9844, 0.4962, 2.7686, 2.5387, 2.2321, 2.0361, 1.9510, 2.0874, 2.1370], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6085,  0.3856,  0.0852, -0.0246, -0.0312, -0.0944, -0.2297, -0.4636, -0.6011, -0.5823, -0.4869, -0.4359,
0:         -0.3683, -0.3594, -0.2874, -0.1676, -0.1898, -0.2319,  0.5741,  0.0442, -0.3040, -0.1931, -0.0079, -0.1022,
0:         -0.3339], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1832, -0.3194, -0.3817, -0.2159,  0.1100,  0.3955,  0.5318,  0.4176,  0.3211,  0.4093,  0.5336,  0.7150,
0:          0.9236,  1.1333,  1.3412,  1.4088,  1.4089,  1.4617,  1.5220,  1.5508,  1.5464,  1.5154,  1.4951,  1.4756,
0:          1.5144], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2336, -0.0319, -0.1153, -0.0597, -0.0713, -0.1246,  0.0144,  0.1025, -0.0087, -0.2336, -0.2428, -0.2196,
0:         -0.2289, -0.2359, -0.1548,  0.0214,  0.1628,  0.0909, -0.2451, -0.2451, -0.2451, -0.2336, -0.2359, -0.1223,
0:          0.1326], device='cuda:0')
0: [DEBUG] Epoch 42, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0250,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0829,
0:         -0.1316,     nan,     nan,     nan,     nan,     nan,     nan, -0.0574,     nan,     nan, -0.1849,     nan,
0:         -0.2057,     nan,     nan,     nan,     nan,     nan, -0.0759,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2220,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2057,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.0457,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0933,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.3517,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2336,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1872,     nan,  0.2902,     nan,     nan,
0:             nan,     nan, -0.2440,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 42, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5957, -0.5874, -0.5386, -0.4643, -0.4182, -0.4089, -0.4346, -0.4661, -0.5093, -0.5307, -0.5465, -0.5886,
0:         -0.6318, -0.6816, -0.7100, -0.6991, -0.6641, -0.6204, -0.5006, -0.5394, -0.5254, -0.4810, -0.4457, -0.4353,
0:         -0.4478], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0039,  0.0108, -0.0088, -0.0288, -0.0533, -0.0862, -0.1285, -0.1716, -0.1857, -0.1796, -0.1538, -0.1161,
0:         -0.1012, -0.1147, -0.1415, -0.1322, -0.0870, -0.0185,  0.0183,  0.0356,  0.0280,  0.0069, -0.0215, -0.0694,
0:         -0.1127], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2719, 1.1580, 1.0048, 0.8321, 0.6495, 0.4715, 0.3253, 0.1660, 0.0710, 0.0284, 0.0937, 0.2365, 0.4799, 0.7543,
0:         0.9677, 1.1375, 1.2287, 1.2692, 0.9574, 0.8701, 0.7576, 0.6262, 0.5165, 0.4036, 0.2939], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2141,  0.1589,  0.2505,  0.3748,  0.2511,  0.4202,  0.6150,  0.6546,  0.8073,  0.7720,  0.5133,  0.1520,
0:         -0.1925, -0.4097, -0.5638, -0.4153, -0.2144, -0.4444,  0.1956, -0.1417, -0.2123,  0.0378,  0.1073,  0.3493,
0:          0.5060], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.0022, 1.0518, 1.1012, 1.1389, 1.1603, 1.1669, 1.1611, 1.1529, 1.1598, 1.1709, 1.1618, 1.1084, 1.0125, 0.9000,
0:         0.8004, 0.7345, 0.7007, 0.6828, 0.6715, 0.6624, 0.6660, 0.6753, 0.6837, 0.6827, 0.6662], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.2881, 0.3199, 0.3085, 0.3078, 0.2660, 0.2173, 0.1645, 0.1160, 0.0901, 0.3174, 0.3222, 0.3102, 0.3012, 0.2549,
0:         0.2199, 0.1566, 0.1160, 0.0736, 0.3185, 0.3189, 0.3203, 0.2992, 0.2640, 0.2203, 0.1467], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.2272958755493164; velocity_v: 0.3408430814743042; specific_humidity: 0.1784074455499649; velocity_z: 0.5970137715339661; temperature: 0.14748245477676392; total_precip: 0.621128261089325; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20361371338367462; velocity_v: 0.3339979648590088; specific_humidity: 0.21023263037204742; velocity_z: 0.6735374927520752; temperature: 0.18605615198612213; total_precip: 1.0316696166992188; 
0: epoch: 42 [1/5 (20%)]	Loss: 0.82640 : 0.36063 :: 0.21342 (2.53 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20582079887390137; velocity_v: 0.31527969241142273; specific_humidity: 0.18879835307598114; velocity_z: 0.5948350429534912; temperature: 0.1478928178548813; total_precip: 0.6947522759437561; 
0: epoch: 42 [2/5 (40%)]	Loss: 0.69475 : 0.32368 :: 0.21232 (15.84 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22592878341674805; velocity_v: 0.39328017830848694; specific_humidity: 0.18058830499649048; velocity_z: 0.5476922392845154; temperature: 0.1969149261713028; total_precip: 0.3633343577384949; 
0: epoch: 42 [3/5 (60%)]	Loss: 0.36333 : 0.28260 :: 0.21228 (15.85 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20936693251132965; velocity_v: 0.3368394076824188; specific_humidity: 0.20072902739048004; velocity_z: 0.6341713070869446; temperature: 0.18494471907615662; total_precip: 0.7457150220870972; 
0: epoch: 42 [4/5 (80%)]	Loss: 0.74572 : 0.34923 :: 0.21596 (16.21 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 5.24520874e-06 2.38418579e-05
0:  4.33921814e-05 3.48091125e-05 2.14576721e-05 1.33514404e-05
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 3.24249268e-05 7.53402710e-05
0:  2.49385834e-04 9.78469849e-04 1.42621994e-03 2.32219696e-03
0:  2.15578079e-03 2.12383270e-03 1.32369995e-03 7.46726990e-04
0:  4.35829163e-04 2.50816345e-04 9.67979431e-05 7.05718994e-05
0:  5.53131104e-05 4.67300451e-05 2.28881836e-05 1.52587891e-05
0:  2.19345093e-05 6.34193420e-05 1.44004822e-04 1.79767609e-04
0:  1.99794769e-04 4.18663025e-04 3.90052795e-04 6.20365143e-04
0:  6.23703003e-04 4.85420227e-04 3.84807587e-04 4.03404236e-04
0:  7.43389130e-04 1.80625904e-03 1.97124458e-03 1.81531918e-03
0:  1.88016891e-03 1.93071365e-03 1.71041489e-03 1.40142441e-03
0:  1.58405304e-03 2.14529037e-03 3.17192078e-03 4.24242020e-03
0:  3.93676758e-03 2.87914276e-03 1.49679184e-03 1.03855133e-03
0:  1.36327744e-03 1.78813934e-03 1.72138226e-03 1.14250183e-03
0:  1.40380859e-03 1.08098984e-03 8.05854797e-04 8.26835632e-04
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  9.53674316e-06 1.57356262e-05 2.14576721e-05 4.86373901e-05
0:  1.10149384e-04 7.29560852e-05 6.86645508e-05 2.52723694e-05
0:  1.14440918e-05 6.19888306e-06 2.86102295e-06 1.90734863e-06
0:  9.53674316e-07 4.76837158e-07 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.04904175e-05 3.38554382e-05
0:  1.25408173e-04 3.79085541e-04 7.13348389e-04 1.11484528e-03
0:  1.25885010e-03 1.26266479e-03 1.11532211e-03 6.62803650e-04
0:  4.34398651e-04 2.84194946e-04 1.06334686e-04 8.29696655e-05
0:  7.20024109e-05 5.86509705e-05 3.67164612e-05 4.14848291e-05
0:  1.81198120e-05 7.00950623e-05 2.35080719e-04 3.08513641e-04
0:  2.63690948e-04 4.11987305e-04 3.07083130e-04 2.45571136e-04
0:  2.18391418e-04 1.27315521e-04 5.05447388e-05 3.62396240e-05
0:  2.94208527e-04 3.22818756e-04 4.78744507e-04 5.66005707e-04
0:  6.76631927e-04 7.59601593e-04 7.75337219e-04 8.59260559e-04]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 4.7683716e-07 4.7683716e-07 5.8650970e-05
0:  8.3446503e-05 1.3160706e-04 4.9066544e-04 9.2935556e-04 6.5994263e-04
0:  4.2676926e-04 3.9625168e-04 2.5033951e-04 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4305115e-06
0:  2.6226044e-04 1.4972687e-04 0.0000000e+00 1.5258789e-05 4.6253201e-05
0:  1.4686584e-04 5.1307678e-04 5.7840353e-04 3.1614304e-04 4.2533875e-04
0:  1.4448166e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [11.695979 11.738588 11.927032 12.106749 12.11969  11.91979  11.995011
0:  11.957088 11.930208 11.858318 11.432318 10.933798 10.435391 10.345274
0:  10.663818 11.248234 11.859241 12.253274 10.971668 11.062402]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.891, max = 2.707, mean = 0.389
0:          sample (first 20): tensor([0.3660, 0.3697, 0.3863, 0.4022, 0.4033, 0.3857, 0.3923, 0.3890, 0.3866, 0.3803, 0.3428, 0.2989, 0.2550, 0.2470,
0:         0.2751, 0.3266, 0.3804, 0.4151, 0.4075, 0.4220])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.451202   -3.5972733  -3.2571273  -2.4500632  -1.5731711  -0.87225294
0:  -0.56175804 -0.4276681  -0.81863165 -1.2374091  -1.7157464  -2.4664264
0:  -3.0681996  -3.5868545  -3.8392587  -3.560515   -2.9487586  -2.1862388
0:  -4.952061   -5.69694   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7652984 -2.8612685 -2.6355376 -2.1038995 -1.6147456 -1.3466506
0:  -1.3895078 -1.5680194 -2.137731  -2.6227202 -3.0635438 -3.5837426
0:  -3.859735  -3.9798932 -3.8376021 -3.3605065 -2.7750735 -2.1941533
0:  -2.8300843 -2.792091 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.582982   4.4547606  3.9460886  3.9354923  4.0803957  3.942057
0:   4.3276033  4.480177   4.8479724  5.0787215  4.8110294  4.403104
0:   3.997122   4.0913415  4.8293524  6.1911163  7.899848   9.503642
0:  10.089886   9.756381 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.341227    0.78463984  0.61869526  0.84756756  1.1141319   1.1960225
0:   0.9957576   0.7868228   0.257751   -0.11520004 -0.47055864 -1.14041
0:  -1.7235398  -2.2492623  -2.5141988  -2.1688046  -1.413136   -0.4746642
0:  -1.7311482  -2.3880725 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.390429  12.603516  13.110104  13.583702  13.799947  13.7386055
0:  13.674307  13.662322  13.587302  13.472728  13.10741   12.593759
0:  12.067119  11.847213  12.197765  12.876366  13.6315155 14.238979
0:  10.082202  10.035364 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.481943 15.477814 15.621491 15.757829 15.812981 15.62734  15.802411
0:  15.890129 16.04115  16.204561 15.988294 15.679665 15.328112 15.301856
0:  15.66206  16.424461 17.374413 18.194052 17.501194 17.619026]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.89613247 -0.8611498  -0.4209528   0.37782478  1.1752229   1.7659359
0:   1.9982114   1.9909372   1.4625678   0.8482952   0.10426283 -0.83924055
0:  -1.6047707  -2.1861105  -2.3927813  -2.1243505  -1.600419   -1.0429997
0:  -2.7072153  -2.7893715 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.539473 16.40429  16.4938   16.702305 16.86741  16.832684 17.007856
0:  17.052462 16.97401  16.756775 16.201662 15.457844 14.77075  14.404577
0:  14.52433  15.085241 15.875271 16.564865 14.436262 14.493814]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.22188234  0.78601694  1.732687    2.8382401   3.6452706   4.0010977
0:   3.8967369   3.3703356   2.2393196   0.98173904 -0.4342413  -1.8704739
0:  -3.0468678  -3.8500094  -4.217031   -4.2405486  -4.1918263  -4.1863694
0:  -7.304663   -7.5100083 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.328116  3.7484403 4.635135  5.588027  6.1922646 6.3433743 6.3334737
0:  6.358703  6.3452673 6.5449595 6.713121  6.5831046 6.4568176 6.4058685
0:  6.651064  7.379332  8.401741  9.364599  6.2925835 6.5482006]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [44.182987 43.9452   43.372677 42.761166 42.057896 41.078228 40.9872
0:  40.772793 40.84898  40.734272 39.89852  39.28612  38.662777 38.606678
0:  39.19352  39.821156 40.330994 40.245804 41.579514 41.675797]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3597493  -5.622867   -5.4843106  -5.0425897  -4.560292   -4.1987596
0:  -3.710486   -3.3753257  -3.1726394  -3.0375743  -3.130219   -3.389092
0:  -3.515905   -3.3190074  -2.73027    -1.8685145  -1.0541348  -0.49131536
0:  -1.1532598  -1.1071439 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.494976  8.638855  8.944826  9.302288  9.538106  9.551842  9.74262
0:  9.812951  9.800713  9.804117  9.532828  9.164131  8.749859  8.622419
0:  8.7788105 9.141275  9.436801  9.453137  7.5658255 7.5120316]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.029596   9.998407  10.001369   9.839655   9.6228695  9.241766
0:   9.560421   9.8950405 10.33919   10.659541  10.30731    9.917942
0:   9.45277    9.679441  10.491694  11.750971  12.892586  13.6344795
0:  13.844778  13.858822 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.207412  4.011759  4.2456284 4.782634  5.330286  5.604069  5.969451
0:  6.073475  5.9631333 5.762941  5.304672  4.658964  4.0859656 3.6946526
0:  3.6810656 3.790834  3.8995216 3.7411427 2.2103596 2.1173692]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.89768  24.813358 25.034893 25.2517   25.19172  24.723549 24.20818
0:  23.443909 22.551924 21.62389  20.440256 19.150051 18.034155 17.210608
0:  16.943974 17.097683 17.50297  17.863531 15.545847 15.455614]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.022623  12.614229  12.554064  12.5922165 12.457649  11.936114
0:  11.54973   10.983263  10.404881   9.861413   9.03706    8.069268
0:   7.1886997  6.681612   6.730028   7.3165264  8.157402   8.882475
0:   6.1739607  5.5234623]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.877121 20.043766 20.51859  21.131445 21.582554 21.767052 22.10896
0:  22.384142 22.78443  23.186935 23.311794 23.307562 23.383537 23.623056
0:  24.28313  25.11791  26.018753 26.811993 26.43755  26.54145 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.4716249   1.5388274   1.8934722   2.469603    2.9077306   3.0999203
0:   3.0369377   2.8583732   2.2961478   1.8366961   1.2984486   0.61726475
0:   0.09456396 -0.23946953 -0.18688488  0.30461693  0.96891165  1.621079
0:   0.61875725  0.62858915]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1932328  3.0536504  3.230854   3.6710708  4.075561   4.264306
0:  4.147708   3.9443018  3.342125   2.7803288  2.2111373  1.454135
0:  0.852201   0.3536148  0.17664623 0.4689479  1.0518546  1.7550998
0:  0.53455925 0.41470432]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.45599    6.7289433  7.2331643  7.8268056  8.336153   8.664339
0:   9.084808   9.4181185  9.587706   9.73592    9.701161   9.499085
0:   9.453177   9.582251  10.040504  10.756075  11.5299    12.200869
0:  10.398752  10.281664 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.2139626 4.2163286 4.5345855 5.0363255 5.3506246 5.3925495 5.34754
0:  5.1672897 4.747948  4.384874  3.8725934 3.2101862 2.659916  2.3567162
0:  2.3902001 2.7810397 3.2105436 3.5142126 2.1263986 2.1209793]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.8040018  0.92066    1.3765779  2.1595795  2.896601   3.425336
0:  3.4562106  3.386102   2.8190775  2.3638837  1.9527202  1.3359938
0:  0.8336568  0.38537836 0.28489637 0.8143816  1.8891144  3.183427
0:  2.1986992  2.0903273 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.847332  3.7835526 4.09033   4.533755  4.8937654 5.0156264 5.399672
0:  5.6904974 6.0035315 6.293778  6.2012453 5.90932   5.528556  5.491143
0:  5.810106  6.4998636 7.2026997 7.658425  5.8129053 5.7845025]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.284841  -9.424691  -9.129191  -8.418148  -7.597536  -6.942976
0:  -6.4663477 -6.1468554 -6.282524  -6.422225  -6.7078257 -7.130807
0:  -7.338808  -7.354705  -7.0980473 -6.537958  -6.0066824 -5.6360674
0:  -8.189637  -8.463922 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.5008526   1.3342905   1.5811706   2.0103178   2.2714205   2.183383
0:   2.1118755   1.8458905   1.4209023   1.0660434   0.46966982 -0.30911922
0:  -1.0727181  -1.4701629  -1.5165591  -1.1574159  -0.75220346 -0.53648233
0:  -1.9638267  -1.9326749 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0814936 2.9117563 3.133742  3.5440385 3.8221169 3.825795  3.8355837
0:  3.7533703 3.5742676 3.456695  3.1926281 2.7354422 2.3337634 2.1113076
0:  2.1837711 2.6611786 3.2697663 3.7802737 2.0971231 1.9637637]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.060633   9.181263   9.636509  10.190981  10.551399  10.601313
0:  10.619011  10.547931  10.371235  10.247448   9.953138   9.50449
0:   9.110289   8.863422   8.979968   9.485706  10.172194  10.8202505
0:   8.901797   9.082231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.3336315 -9.760492  -9.689175  -9.0561905 -8.242189  -7.5261493
0:  -7.2435193 -7.1647553 -7.6491885 -8.151131  -8.622063  -9.2371235
0:  -9.606018  -9.77338   -9.6015415 -9.003658  -8.127628  -7.1786523
0:  -8.028021  -8.161512 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.331387  13.357029  13.700684  14.13892   14.390762  14.387629
0:  14.366107  14.3117695 14.243774  14.268283  14.182896  13.937471
0:  13.789876  13.78638   14.133756  14.786432  15.545633  16.21986
0:  14.118326  13.969732 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.0138   13.529444 14.308815 15.136654 15.785255 16.17591  16.630194
0:  16.982157 17.205713 17.37226  17.262955 16.951258 16.665009 16.569607
0:  16.826769 17.351177 17.976015 18.45688  16.523296 17.265545]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.836208  12.863049  12.925482  12.734386  12.421537  11.869279
0:  12.08773   12.322633  12.777931  13.117195  12.7984915 12.402491
0:  11.80533   11.80323   12.263025  13.102558  13.83692   14.146877
0:  15.482598  15.65483  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.5743165   1.4442039   1.5619578   2.0162883   2.494935    2.8508625
0:   2.825036    2.6460786   1.8790998   1.1089625   0.30022478 -0.6272588
0:  -1.2990546  -1.7951455  -1.9256406  -1.5765424  -0.90991163 -0.10579109
0:  -0.90723085 -1.1165261 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.8872414 4.7843475 4.936526  5.240085  5.476169  5.5365877 5.667746
0:  5.6693807 5.47935   5.292152  4.847291  4.20845   3.6510105 3.3368034
0:  3.3604696 3.6561973 3.9252129 4.0017643 2.4149256 2.4913874]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.6265936  1.1765838  1.2261105  1.6974449  2.390686   2.914322
0:   4.2031918  5.2871213  6.6563845  7.8836985  8.560625   9.195473
0:   9.724355  10.678957  12.112469  13.917625  15.767866  17.303665
0:  17.849632  18.025238 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.3665175  7.438952   7.93325    8.660747   9.303694   9.702683
0:  10.09937   10.369852  10.428243  10.461201  10.225668   9.730505
0:   9.260714   8.973295   8.991138   9.39517    9.903362  10.230459
0:   7.71186    7.5532513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.855992  -9.061517  -8.759141  -7.8917494 -6.8276596 -5.8690557
0:  -5.342702  -5.0289354 -5.325555  -5.672268  -6.0446033 -6.631978
0:  -6.9845448 -7.2493105 -7.191845  -6.690323  -5.897624  -4.9641504
0:  -4.9581766 -5.00422  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.869232 18.543785 18.51071  18.643103 18.708946 18.459736 18.529682
0:  18.349468 18.212786 17.945572 17.279789 16.499302 15.78021  15.521312
0:  15.821164 16.62943  17.697468 18.611252 16.774185 16.669712]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.7136626 -4.74194   -4.451875  -4.0199018 -3.6614013 -3.528319
0:  -3.2294946 -2.9983706 -2.8164172 -2.6086764 -2.7285485 -2.988913
0:  -3.3088822 -3.286077  -2.9585938 -2.4209266 -2.0105395 -1.8502669
0:  -2.5019994 -2.4092355]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.036961  10.999121  11.1407585 11.258743  11.219156  10.88426
0:  10.990009  11.007151  11.170714  11.310134  11.027332  10.571472
0:   9.995949   9.813911   9.981174  10.61359   11.339729  11.887091
0:  11.073969  11.197975 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2538385 2.5474796 3.1434984 3.9321012 4.6963873 5.274679  5.581325
0:  5.7754445 5.542908  5.333269  5.0287766 4.5169687 4.1295366 3.8063557
0:  3.793976  4.167887  4.769068  5.4404464 4.6864867 5.0057364]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.410986 -13.719576 -13.543034 -12.891672 -12.116949 -11.556156
0:  -11.431534 -11.534155 -12.20886  -12.847431 -13.440296 -14.131651
0:  -14.449894 -14.58489  -14.371483 -13.784216 -13.062828 -12.332888
0:  -13.193171 -13.49202 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5631576  1.5965128  1.9183578  2.5118222  3.0613346  3.4415512
0:  3.538813   3.564692   3.1774807  2.8307962  2.373931   1.733572
0:  1.1843452  0.80069923 0.7952776  1.238699   1.9076686  2.556069
0:  0.9537244  0.9794159 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.35403  25.170574 25.224915 25.402546 25.55224  25.439651 25.772148
0:  25.953161 26.244175 26.39945  26.149418 25.74438  25.352964 25.145248
0:  25.307674 25.742746 26.322659 26.779211 24.242363 23.963898]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.896831  4.8897734 5.12333   5.7023954 6.2831306 6.7202487 6.7730985
0:  6.779927  6.3602667 6.0469675 5.780819  5.341143  4.992201  4.6538916
0:  4.5471077 4.888702  5.5915623 6.4114294 4.8475137 4.447276 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2723227 5.347426  5.7187395 6.304032  6.840687  7.187335  7.190219
0:  7.1459346 6.778212  6.4861054 6.2190638 5.809993  5.398736  5.062832
0:  4.979741  5.3380833 6.018734  6.716188  5.2014146 5.2365427]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.968388 18.331068 18.93293  19.46026  19.69642  19.608526 19.562727
0:  19.393583 19.220554 19.158567 18.940865 18.62426  18.371952 18.351881
0:  18.670343 19.270126 19.94639  20.480614 18.012012 18.2995  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.68571  32.68651  32.664803 32.63209  32.438976 31.852627 31.91409
0:  31.649946 31.417753 30.903023 29.635336 28.359453 27.068167 26.37156
0:  26.32419  26.653658 27.081692 27.246195 26.670122 26.750553]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.700596 28.327835 27.822548 27.25002  26.804699 26.241474 26.53469
0:  26.63316  26.812881 26.669212 25.752972 24.930973 24.074917 23.903942
0:  24.277103 24.96059  25.667702 26.030533 27.237877 27.20095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.214962  5.4232073 5.9404387 6.6238856 7.129924  7.3555126 7.2977924
0:  7.1891847 6.813828  6.537238  6.2354207 5.704527  5.243553  4.8240213
0:  4.671522  4.9484444 5.525825  6.1597986 3.9191952 3.8563402]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0892124  -1.8311176  -1.2778187  -0.58560276  0.05373764  0.41983604
0:   0.5916915   0.5662222   0.27000093  0.02645206 -0.24083328 -0.5542631
0:  -0.5851965  -0.34540462  0.24193811  1.11971     2.0484648   2.8935208
0:   1.1427007   1.3132873 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.638217  12.110773  11.994505  12.119276  12.107783  11.731535
0:  11.539716  11.239906  11.035089  10.92207   10.633847  10.168651
0:   9.72028    9.519306   9.741593  10.459586  11.426863  12.25456
0:  10.012049   9.7962055]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.149868 18.998533 18.610302 17.927174 17.135912 16.229261 16.229778
0:  16.339924 16.711979 16.965076 16.508076 16.055311 15.40959  15.391155
0:  15.85603  16.556892 17.086967 17.073698 18.523918 18.642841]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.061102  10.970161  10.874656  10.594571  10.211063   9.635078
0:   9.796867   9.971441  10.3677635 10.6727    10.371061  10.017613
0:   9.446323   9.467467   9.973344  10.845898  11.706924  12.166674
0:  14.353285  14.539749 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.548057  10.436676  10.592923  10.702528  10.513344  10.063839
0:   9.703451   9.508585   9.323043   9.081713   8.490503   7.264642
0:   5.704732   4.096922   2.7087116  1.7459297  1.0901017  0.6481581
0:  -6.3343353 -6.590173 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.682184  -7.9993534 -7.8222303 -7.1999817 -6.469198  -5.9308834
0:  -5.71278   -5.7204657 -6.224283  -6.6873674 -7.2060795 -7.9109135
0:  -8.42215   -8.722604  -8.738883  -8.253299  -7.557705  -6.818428
0:  -7.839362  -7.831153 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9712873  3.928232   4.145188   4.4465623  4.5502644  4.4021235
0:  4.254573   3.9575512  3.4962015  3.084574   2.4883928  1.7526736
0:  1.120441   0.80774593 0.9025588  1.4017367  2.0180922  2.5452495
0:  1.5192299  1.8555865 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.972202 16.953764 16.851482 16.489979 15.97945  15.243132 15.31333
0:  15.398891 15.706865 15.925587 15.461283 14.981565 14.291089 14.203938
0:  14.55349  15.208738 15.742645 15.844934 17.130283 17.332493]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.693731  -8.025328  -7.9183755 -7.351951  -6.6721168 -6.1511407
0:  -6.007279  -6.0616193 -6.618542  -7.163032  -7.676823  -8.309296
0:  -8.660906  -8.883337  -8.868174  -8.4928665 -7.9738507 -7.3854957
0:  -8.564621  -8.826231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.37397718 -0.37963295  0.03794909  0.86174774  1.7174206   2.392953
0:   2.669661    2.83076     2.4992824   2.1909328   1.8288069   1.1831627
0:   0.60288334  0.02431583 -0.33542347 -0.23097467  0.22828245  0.8412442
0:  -1.2174153  -1.6195292 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.100777 27.99229  28.034847 28.208817 28.428093 28.419807 29.035252
0:  29.400852 29.851372 30.117401 29.684235 29.284256 28.830551 28.762737
0:  29.274433 30.022106 30.872242 31.554752 31.58134  31.630379]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.746124  3.7930317 4.163472  4.885437  5.592339  6.171525  6.451726
0:  6.6624784 6.417205  6.1682425 5.825329  5.242511  4.7451067 4.3254805
0:  4.229331  4.586114  5.248287  5.974577  4.047739  3.7932367]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.17428541 0.39671803 0.978385   1.7504439  2.4214852  2.8547847
0:  2.990402   3.0404906  2.7269328  2.4311962  2.071778   1.5389104
0:  1.1492395  0.94372797 1.1319242  1.7870512  2.73593    3.6865873
0:  2.8056896  2.8418665 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.6841674 6.7261505 7.074319  7.74601   8.300636  8.623393  8.506666
0:  8.357209  7.762536  7.2792935 6.783333  5.957158  5.125825  4.2612886
0:  3.7284145 3.816957  4.4708767 5.3704324 3.3301883 2.934835 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [42.768795 42.6787   42.225395 41.651245 41.051212 40.183884 40.237038
0:  40.07152  40.028614 39.68514  38.501194 37.408775 36.305286 35.844475
0:  36.06753  36.678978 37.42054  37.773262 38.76366  38.77761 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.980677   8.6263685  8.434574   8.306747   8.015231   7.4463687
0:  6.7432203  5.89418    4.896945   4.0883718  3.3788304  2.7486415
0:  2.3985548  2.3064919  2.4310598  2.7104735  2.9697092  3.1181111
0:  0.83190584 0.5262089 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.9584627 3.130591  3.5651221 4.1834855 4.703391  5.017532  5.3132925
0:  5.485923  5.4359865 5.429436  5.301506  5.036339  4.8829    4.90987
0:  5.263151  5.9215775 6.698412  7.409164  7.021467  7.4623966]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.6775     8.797601   9.100561   9.521429   9.819555   9.911162
0:   9.817862   9.7105875  9.3654785  9.142779   8.936695   8.601573
0:   8.376776   8.257845   8.442551   9.019022   9.876588  10.752459
0:   9.547346   9.8728075]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1510668   1.240521    1.5898046   2.0699234   2.3750682   2.3592305
0:   2.2969313   2.0716658   1.728056    1.3313599   0.80363226  0.19923878
0:  -0.24014711 -0.2775874   0.1735115   1.0873365   2.2002196   3.1609442
0:   1.4441423   1.3088789 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.638664 19.07537  19.700394 20.244705 20.554558 20.463558 20.34057
0:  20.06684  19.704395 19.34624  18.868916 18.288136 17.870731 17.684465
0:  17.865013 18.359228 19.001095 19.552513 17.652779 17.817669]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.75939  12.793022 13.13203  13.541491 13.779082 13.716111 13.663244
0:  13.465912 13.150114 12.83437  12.300214 11.649107 11.10338  10.837322
0:  11.007103 11.593375 12.327555 12.981698 10.82983  10.940077]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2904992   1.1450872   1.2777004   1.7141347   2.1414704   2.3898613
0:   2.3583636   2.1618905   1.5369449   0.91629696  0.20837402 -0.68048716
0:  -1.4618711  -2.0851946  -2.371604   -2.1566353  -1.5930305  -0.8919091
0:  -2.496951   -2.506136  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.3741212  1.3641529  1.7423787  2.4035711  2.9825168  3.296495
0:  3.4581964  3.5018153  3.2398071  3.0160055  2.6682982  2.122663
0:  1.6963062  1.4124737  1.4336114  1.8498421  2.38876    2.8339984
0:  1.0268416  0.98337173]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.324553 17.786669 18.265785 18.539179 18.540964 18.286123 17.940323
0:  17.62443  17.211979 16.940258 16.658173 16.298199 15.981573 15.778015
0:  15.835009 16.126596 16.514807 16.873325 15.106817 15.360077]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.034218 32.266006 32.437016 32.465984 32.447582 32.107533 32.341908
0:  32.354427 32.415802 32.350666 31.721977 31.100475 30.468267 30.284767
0:  30.559118 31.194324 31.957018 32.475067 31.679289 31.886393]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.039951 22.884563 22.797901 22.681969 22.463812 21.874382 21.609215
0:  21.182178 20.900238 20.523113 19.823208 19.17854  18.650684 18.441097
0:  18.6955   19.172615 19.688986 20.113491 19.92989  19.983643]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.9334135 8.845209  9.047619  9.300467  9.183981  8.585939  7.804435
0:  6.935008  5.954201  5.148427  4.3174353 3.3228414 2.386025  1.6683946
0:  1.399344  1.7503958 2.5529642 3.4823513 2.8692148 3.0237129]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.6660047 4.6095667 4.6945057 4.9453235 5.1198244 5.0864315 4.8633366
0:  4.5343676 3.8619423 3.2612038 2.655545  2.0241172 1.6525402 1.5056968
0:  1.7372835 2.268285  2.9215243 3.4912858 2.958305  3.3957202]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.117476  3.1895487 3.5734162 4.195013  4.750374  5.1310062 5.238578
0:  5.2768173 4.9579506 4.6532784 4.254138  3.6350067 3.1054351 2.68815
0:  2.6893497 3.2469459 4.1927376 5.158735  2.0481892 2.0159726]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.99833  35.90312  35.41773  34.53934  33.5482   32.38994  32.460316
0:  32.64699  33.135414 33.39767  32.66778  31.998068 31.139904 31.066113
0:  31.683243 32.607376 33.361797 33.369152 34.406498 34.593277]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.564053 15.324438 15.015175 14.566841 14.118277 13.497376 13.684767
0:  13.833359 14.197283 14.421375 14.009954 13.632351 13.18299  13.403713
0:  14.190811 15.400684 16.635769 17.451902 20.397217 20.598358]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.3457236 5.470319  5.965025  6.567329  6.891574  6.8562713 6.5125513
0:  6.087502  5.438079  4.899015  4.343558  3.5802066 2.9374342 2.4351919
0:  2.3363073 2.7559018 3.4755445 4.2597227 2.2483418 2.380586 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.347401   9.308029   9.611779  10.004548  10.185347  10.05012
0:  10.015036   9.906206   9.762106   9.638192   9.258663   8.67377
0:   8.078483   7.789559   7.8958645  8.46866    9.241667   9.893175
0:   8.2145405  8.43533  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3543973 -3.681089  -3.5788589 -3.1204085 -2.6294417 -2.2744117
0:  -1.9870515 -1.8475127 -1.9967208 -2.22502   -2.6402106 -3.2529607
0:  -3.6646228 -3.7722383 -3.4647393 -2.710462  -1.8547168 -1.1356101
0:  -1.7666631 -1.4889979]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.36095   -12.435335  -11.983807  -11.061111  -10.046059   -9.227552
0:   -8.691689   -8.410307   -8.608471   -8.864241   -9.209452   -9.677395
0:   -9.854279   -9.844181   -9.567329   -9.042627   -8.591522   -8.2491665
0:   -9.912037   -9.923639 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.717382 23.375982 24.229235 25.04014  25.599459 25.910831 26.150446
0:  26.410162 26.646109 27.035961 27.400932 27.59521  27.952173 28.387156
0:  29.14296  30.091051 31.01084  31.634363 28.48838  29.163282]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5293894 6.610148  6.759334  6.7208834 6.5431275 6.1899724 6.3730693
0:  6.574361  6.8753304 7.0852876 6.8395457 6.50725   6.0903544 6.263109
0:  6.816604  7.6361265 8.294185  8.444451  8.452412  8.601878 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.2249246 -1.6955252 -1.8645506 -1.6461372 -1.3367491 -1.1409168
0:  -1.2400937 -1.4995832 -2.2721171 -3.0033717 -3.74396   -4.5762897
0:  -5.09374   -5.418323  -5.3862815 -4.9654307 -4.317147  -3.626718
0:  -5.0192075 -5.350079 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.43141   19.967731  19.339321  18.549173  17.75069   16.776688
0:  16.654667  16.442007  16.39318   16.153776  15.211127  14.323223
0:  13.4334755 13.28504   13.839556  14.875797  16.02123   16.781685
0:  20.517757  20.358856 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.501778 17.598743 17.944042 18.142683 17.89613  17.114141 16.260807
0:  15.279135 14.345198 13.553001 12.742033 11.875701 11.253644 11.043829
0:  11.376766 12.165201 13.155552 14.027606 12.69186  12.691774]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.314826   0.18294668 0.36932707 0.9297104  1.5323796  2.0327687
0:  2.2369916  2.3474286  2.0102334  1.6950793  1.3436294  0.8092332
0:  0.4047923  0.10867786 0.15708351 0.67426014 1.569962   2.5868192
0:  2.113884   2.2467961 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.154827  -11.85475   -11.0855255  -9.965981   -8.894932   -8.098791
0:   -7.584945   -7.2801843  -7.391064   -7.4838624  -7.6970506  -8.128263
0:   -8.423832   -8.539017   -8.382126   -7.9163175  -7.484781   -7.138761
0:   -7.7617364  -7.626968 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.5544505  0.6193323  1.0534248  1.7924409  2.5164824  3.0292072
0:  3.3173747  3.438428   3.1305976  2.7779865  2.285539   1.604857
0:  1.1328244  0.8317294  0.8563647  1.2423563  1.6992493  2.1006453
0:  0.50347185 0.2713089 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.08416  32.930405 32.988266 33.16581  33.01958  32.222164 31.475313
0:  30.342775 29.263615 28.234653 27.089745 26.164833 25.667091 25.558847
0:  25.962692 26.631863 27.413984 28.023903 28.90106  28.850904]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.311188  7.3247323 7.711396  8.316915  8.765176  8.891634  8.884377
0:  8.754171  8.382196  8.07692   7.6240053 6.9504437 6.360803  5.9124527
0:  5.8276196 6.2125115 6.85275   7.4690595 5.1661253 5.067133 ]
0: validation loss for strategy=forecast at epoch 42 : 0.3034169375896454
0: validation loss for velocity_u : 0.17389459908008575
0: validation loss for velocity_v : 0.2817244231700897
0: validation loss for specific_humidity : 0.1593618541955948
0: validation loss for velocity_z : 0.46120044589042664
0: validation loss for temperature : 0.13865402340888977
0: validation loss for total_precip : 0.6056663393974304
0: 43 : 20:49:11 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 43, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3647,  0.3597,  0.3533,  0.3467,  0.3345,  0.3135,  0.2851,  0.2503,  0.2123,  0.1742,  0.1340,  0.0920,
0:          0.0492,  0.0050, -0.0353, -0.0612, -0.0702, -0.0649,  0.3921,  0.3861,  0.3781,  0.3700,  0.3575,  0.3353,
0:          0.3057], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4621, 0.6315, 0.7853, 0.9205, 1.0376, 1.1454, 1.2580, 1.3881, 1.5413, 1.7068, 1.8572, 1.9611, 2.0043, 2.0067,
0:         2.0006, 1.9875, 1.9413, 1.8507, 0.4036, 0.5730, 0.7286, 0.8678, 0.9897, 1.1026, 1.2191], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2042, -0.2311, -0.2514, -0.2639, -0.2705, -0.2710, -0.2663, -0.2621, -0.2607, -0.2620, -0.2546, -0.2436,
0:         -0.2362, -0.2566, -0.3476, -0.4688, -0.6020, -0.6468, -0.1911, -0.2192, -0.2424, -0.2570, -0.2641, -0.2650,
0:         -0.2598], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2087, 0.2374, 0.4094, 0.5506, 0.5814, 0.6211, 0.6719, 0.7049, 0.8020, 0.9189, 0.9630, 1.0005, 1.0512, 1.1262,
0:         1.3225, 1.5871, 1.7702, 1.8077, 0.1337, 0.2175, 0.4072, 0.5417, 0.5682, 0.6013, 0.6476], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4761, -0.4563, -0.4407, -0.4243, -0.4042, -0.3818, -0.3581, -0.3360, -0.3199, -0.3096, -0.2992, -0.2803,
0:         -0.2470, -0.2033, -0.1644, -0.1475, -0.1638, -0.2143, -0.2793, -0.3276, -0.3443, -0.3335, -0.3037, -0.2628,
0:         -0.2195], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501,
0:         -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501, -0.2501,
0:         -0.2501], device='cuda:0')
0: [DEBUG] Epoch 43, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([ 1.6259,     nan,     nan,     nan,     nan,     nan,     nan, -0.1693,     nan,     nan,  0.1376,     nan,
0:         -0.0908,     nan,     nan,     nan,     nan,     nan,  0.1745,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0678, -0.0055,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1301,     nan,     nan,     nan,     nan,  0.5875,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1191,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2270,     nan,     nan, -0.0378,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1716,     nan, -0.0932,     nan,     nan,     nan, -0.1670,     nan, -0.1831,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2074,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.0661,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1301,     nan,  0.0361,     nan,     nan, -0.2501, -0.2501,     nan,     nan,     nan, -0.0793,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2501,     nan,     nan, -0.1658, -0.1197,     nan, -0.2501,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 43, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1471, 1.1839, 1.2423, 1.3134, 1.3674, 1.4004, 1.4359, 1.4631, 1.4692, 1.4727, 1.4604, 1.4440, 1.4479, 1.4690,
0:         1.5323, 1.6148, 1.7003, 1.7482, 1.1043, 1.1546, 1.2272, 1.2770, 1.3027, 1.3132, 1.3137], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.7072,  0.6715,  0.6051,  0.5201,  0.4338,  0.3313,  0.2116,  0.1055,  0.0184, -0.0629, -0.1433, -0.2365,
0:         -0.3777, -0.5521, -0.7318, -0.8446, -0.8720, -0.8200,  0.7077,  0.6858,  0.6236,  0.5483,  0.4605,  0.3653,
0:          0.2712], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7249, -0.7210, -0.7203, -0.7139, -0.7045, -0.7014, -0.7036, -0.7106, -0.7203, -0.7314, -0.7417, -0.7498,
0:         -0.7532, -0.7515, -0.7534, -0.7489, -0.7461, -0.7333, -0.7351, -0.7404, -0.7424, -0.7351, -0.7346, -0.7322,
0:         -0.7284], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2141,  0.7714,  1.2804,  1.0559,  0.4702,  0.1090, -0.1300, -0.3171, -0.4768, -0.6561, -0.7596, -0.8600,
0:         -0.9205, -1.0070, -1.2978, -1.4321, -1.2338, -0.9902,  0.5583,  0.8223,  0.9705,  0.6539,  0.2404,  0.0381,
0:         -0.1335], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-2.7170, -2.7806, -2.7840, -2.7362, -2.6572, -2.5934, -2.5322, -2.4659, -2.3829, -2.2959, -2.2222, -2.1610,
0:         -2.0911, -1.9763, -1.8238, -1.6578, -1.5189, -1.4232, -1.3639, -1.3134, -1.2593, -1.1940, -1.1189, -1.0396,
0:         -0.9519], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.2447,  0.2258,  0.1616,  0.1073,  0.0596,  0.0153, -0.0232, -0.0378, -0.0510,  0.2208,  0.1877,  0.1291,
0:          0.0809,  0.0345, -0.0112, -0.0410, -0.0433, -0.0565,  0.1673,  0.1301,  0.0944,  0.0517,  0.0175, -0.0168,
0:         -0.0486], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2112332284450531; velocity_v: 0.3345017731189728; specific_humidity: 0.19601523876190186; velocity_z: 0.6447238922119141; temperature: 0.1699889749288559; total_precip: 0.49623411893844604; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2502915859222412; velocity_v: 0.3643209934234619; specific_humidity: 0.17779965698719025; velocity_z: 0.5515171885490417; temperature: 0.6706970930099487; total_precip: 0.5194851756095886; 
0: epoch: 43 [1/5 (20%)]	Loss: 0.50786 : 0.34685 :: 0.20759 (2.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20976558327674866; velocity_v: 0.3075147569179535; specific_humidity: 0.18640018999576569; velocity_z: 0.634407639503479; temperature: 0.13802996277809143; total_precip: 0.785211443901062; 
0: epoch: 43 [2/5 (40%)]	Loss: 0.78521 : 0.34291 :: 0.21208 (15.67 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26402848958969116; velocity_v: 0.3752923011779785; specific_humidity: 0.18430352210998535; velocity_z: 0.6600804924964905; temperature: 0.15601055324077606; total_precip: 0.9204527139663696; 
0: epoch: 43 [3/5 (60%)]	Loss: 0.92045 : 0.39136 :: 0.21667 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21933408081531525; velocity_v: 0.3823116421699524; specific_humidity: 0.16527357697486877; velocity_z: 0.5983384847640991; temperature: 0.1448776125907898; total_precip: 0.8136594295501709; 
0: epoch: 43 [4/5 (80%)]	Loss: 0.81366 : 0.35276 :: 0.21665 (16.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [3.81469727e-06 6.67572021e-06 6.67572021e-06 1.85966492e-05
0:  2.19345093e-05 2.38418579e-05 1.33514404e-05 4.76837158e-06
0:  2.38418579e-06 4.76844434e-07 9.53674316e-07 4.76844434e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.90734863e-06 1.90734863e-06 2.86102295e-06
0:  2.09808350e-05 2.09808350e-05 2.71797180e-05 1.04904175e-05
0:  1.23977661e-05 5.24520874e-06 1.90734863e-06 4.76844434e-07
0:  4.76844434e-07 4.76844434e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76844434e-07 1.43050420e-06 4.76844434e-07 0.00000000e+00
0:  9.53674316e-07 4.29153442e-06 5.24520874e-06 8.10623169e-06
0:  9.53674316e-07 1.43050420e-06 3.33786011e-06 1.90734863e-06
0:  4.76844434e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76844434e-07 5.72204590e-06 6.19888306e-06 1.71661377e-05
0:  9.53674316e-06 1.00135803e-05 6.19888306e-06 3.33786011e-06
0:  1.04904175e-05 1.28746033e-05 1.38282776e-05 1.52587891e-05
0:  1.57356262e-05 1.71661377e-05 1.09672546e-05 3.33786011e-06
0:  1.43050420e-06 4.76844434e-07 9.53674316e-07 4.76844434e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 1.43050420e-06 3.33786011e-06 5.24520874e-06
0:  2.09808350e-05 2.47955322e-05 3.19480896e-05 1.52587891e-05
0:  9.53674316e-06 2.38418579e-06 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76844434e-07 0.00000000e+00 4.76844434e-07
0:  1.90734863e-06 4.76837158e-06 4.29153442e-06 5.72204590e-06]
0: Target values (first 200):
0: [2.38418579e-06 9.53674316e-07 9.53674316e-07 1.90734863e-06
0:  1.90734863e-06 2.38418579e-06 1.43050420e-06 9.53674316e-07
0:  4.76844434e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.50134277e-04 1.96933746e-04 2.66551971e-04 4.18186188e-04
0:  7.72476196e-04 7.51972198e-04 1.43527985e-04 1.16443634e-03
0:  1.24359131e-03 2.44617462e-04 1.35421753e-04 9.53674316e-06
0:  8.10623169e-06 6.53266907e-05 5.55038452e-04 5.95092773e-04
0:  4.94480133e-04 2.86102295e-06 4.76844434e-07 4.76844434e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76844434e-07 9.53674316e-07 3.33786011e-06
0:  7.62939453e-06 7.15255737e-06 2.38418579e-05 4.81605530e-05
0:  4.81605530e-05 3.62396240e-05 3.57627869e-05 3.57627869e-05
0:  4.33921814e-05 3.52859497e-05 3.48091125e-05 2.14576721e-05
0:  4.29153442e-06 3.81469727e-06 1.90734863e-06 4.76844434e-07
0:  4.76844434e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76844434e-07 4.76844434e-07 4.76844434e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76844434e-07
0:  4.76844434e-07 4.76844434e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  5.24520874e-06 4.76837158e-06 3.33786011e-06 9.53674316e-07
0:  4.76844434e-07 9.53674316e-07 4.76844434e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  6.62803650e-05 9.67979431e-05 1.79767609e-04 2.86102295e-04
0:  6.41345978e-04 5.02109528e-04 7.32898712e-04 8.77857208e-04
0:  1.20973587e-03 8.22067261e-04 9.58442688e-05 9.75131989e-04
0:  9.62734222e-04 8.20159912e-05 2.05039978e-05 1.71661377e-05
0:  4.76837158e-06 4.76844434e-07 4.76844434e-07 4.76844434e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76844434e-07
0:  2.86102295e-06 3.81469727e-06 3.33786011e-06 4.76837158e-06
0:  5.72204590e-06 1.23977661e-05 8.86917114e-05 1.20162964e-04
0:  1.72615051e-04 4.13894653e-04 4.90188599e-04 5.60283661e-04
0:  6.09874725e-04 5.73158264e-04 5.56468964e-04 5.43117581e-04
0:  5.59329987e-04 6.17504120e-04 4.73022461e-04 3.29971313e-04
0:  2.71797180e-04 1.30176544e-04 1.06811523e-04 8.44001770e-05
0:  2.00271606e-05 1.09672546e-05 8.10623169e-06 4.76844434e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76844434e-07 4.76844434e-07 4.76844434e-07
0:  4.76844434e-07 4.76844434e-07 0.00000000e+00 0.00000000e+00]
0: Prediction values (first 20):
0: [ 4.018503   3.4557314  3.3035293  3.5173144  3.793711   3.8575966
0:   3.581345   3.002642   1.8687234  0.5641217 -0.9688058 -2.726501
0:  -4.3425364 -5.643356  -6.439512  -6.5622773 -6.2886157 -5.795873
0:  -8.560642  -8.679058 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.885, max = 2.730, mean = -0.173
0:          sample (first 20): tensor([-0.1767, -0.2241, -0.2369, -0.2189, -0.1956, -0.1902, -0.2135, -0.2622, -0.3577, -0.4675, -0.5965, -0.7445,
0:         -0.8806, -0.9901, -1.0571, -1.0674, -1.0444, -1.0029, -0.1635, -0.2526])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.18131208  0.25501823  0.6507931   1.2519002   1.7593632   2.0130343
0:   2.0643296   1.8980751   1.3071432   0.6088953  -0.27938318 -1.3200154
0:  -2.130526   -2.6046233  -2.54009    -1.9994407  -1.2733846  -0.6116204
0:  -2.4196134  -2.3703427 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.95462  31.035118 31.035866 30.9772   30.991714 30.844261 31.541836
0:  32.09626  32.833096 33.306507 33.083168 32.811916 32.42683  32.505283
0:  33.081646 33.944134 34.92513  35.53161  36.47302  37.08133 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.029141   9.66261    9.39935    9.076863   8.727159   8.137985
0:   8.224136   8.251728   8.542962   8.820679   8.559585   8.264809
0:   7.8398     7.9281816  8.465416   9.44467   10.446789  11.143452
0:  12.197026  12.19713  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.51281786 0.6049452  1.0659695  1.6156921  1.9779825  2.0270853
0:  2.206892   2.2789907  2.422954   2.6059632  2.5957165  2.3962831
0:  2.2144747  2.336626   2.7542093  3.551866   4.4105225  5.105229
0:  3.239681   3.3367546 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.544834  11.37371   11.568806  11.96965   12.257412  12.231579
0:  12.285511  12.221315  12.073814  11.927159  11.560174  10.997379
0:  10.487968  10.271774  10.493404  11.16165   11.99458   12.630653
0:   9.9872675  9.845154 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.506264 24.851192 25.349375 25.787083 26.098175 26.171936 26.685678
0:  27.053593 27.517473 27.862677 27.770624 27.549    27.419048 27.763819
0:  28.631115 29.827288 31.076395 31.937382 30.429691 30.807295]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.377804 -18.474897 -18.071531 -17.066986 -15.844631 -14.812012
0:  -14.334923 -14.312344 -15.137167 -16.073442 -17.194305 -18.424688
0:  -19.345022 -19.951532 -20.135525 -19.842693 -19.292355 -18.68472
0:  -19.842653 -20.41334 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.9499984 5.8846326 6.246787  6.8721094 7.3568416 7.5969114 7.8090434
0:  8.005508  8.110127  8.261398  8.283669  7.986082  7.717455  7.603442
0:  7.821823  8.446672  9.261561  9.928272  7.3599463 7.392593 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.0323944 -7.05877   -6.6101465 -5.7927914 -4.9269323 -4.295622
0:  -4.0548887 -4.0442734 -4.53131   -4.987501  -5.434333  -6.014448
0:  -6.397132  -6.644538  -6.664007  -6.3024187 -5.733759  -5.0786886
0:  -6.423319  -6.4276047]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5279517 -3.4680314 -3.1047378 -2.4909272 -2.101873  -2.1466117
0:  -2.736176  -3.509336  -4.7205014 -5.684549  -6.502724  -7.402283
0:  -8.033865  -8.580147  -8.785175  -8.532312  -7.978678  -7.2237525
0:  -7.0876727 -6.7735496]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.901928 30.270891 30.612478 30.801434 30.758774 30.350546 30.530989
0:  30.610252 30.914026 31.1797   30.93254  30.631    30.288189 30.367502
0:  30.915537 31.858282 32.861427 33.562054 32.90341  33.33706 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.000996 18.143887 18.388494 18.615294 18.698492 18.486105 18.520102
0:  18.432583 18.381058 18.339266 18.012358 17.606253 17.18025  17.03059
0:  17.187115 17.60157  18.10862  18.388512 16.554375 16.617012]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.243762  2.215549  2.366889  2.4695525 2.5223718 2.495718  2.9605088
0:  3.480672  4.005041  4.4701366 4.526569  4.380607  4.243415  4.5813494
0:  5.2815843 6.239252  7.021554  7.406567  7.141107  7.465326 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.357988  6.5432262 7.0210314 7.679442  8.170929  8.447208  8.3965225
0:  8.278966  7.834998  7.49501   7.1693788 6.7200985 6.4232497 6.220283
0:  6.386466  6.957227  7.8360925 8.71176   6.2856035 6.4502263]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.7853465 1.8329725 2.2293062 2.8263602 3.318874  3.5570192 3.7366443
0:  3.758353  3.494844  3.299015  2.9502168 2.4502149 2.0718827 1.9649143
0:  2.1596527 2.7535799 3.4128506 3.9788597 2.1457672 2.2391224]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.903527  4.9042044 5.2867446 5.9239144 6.409821  6.5758333 6.5735464
0:  6.365741  5.8635297 5.376632  4.774927  3.9974833 3.3458793 2.929308
0:  2.8882596 3.3021765 3.882381  4.3630295 3.362791  3.2835264]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.49163342  0.4438634   0.80480385  1.4594464   2.0153294   2.296729
0:   2.3525248   2.207901    1.6826344   1.1575918   0.5024805  -0.3673544
0:  -1.0373564  -1.528461   -1.6180243  -1.2092385  -0.57467747  0.08139086
0:  -1.9115252  -1.81745   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.234972  -10.087135   -9.417179   -8.353788   -7.2372975  -6.369861
0:   -5.8005013  -5.4195094  -5.4498954  -5.455048   -5.4842362  -5.6407294
0:   -5.55553    -5.332351   -4.7969556  -4.0157275  -3.1930385  -2.507195
0:   -4.2499814  -3.999457 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.069042  9.507552 10.391905 11.38962  12.045205 12.280193 12.422848
0:  12.562479 12.729664 13.044762 13.299927 13.281851 13.312132 13.44035
0:  13.894642 14.777447 15.858015 16.820435 13.201647 13.556347]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [50.725735 50.90924  50.374355 49.547604 48.534267 47.410526 47.582966
0:  47.668285 48.092747 48.15074  47.310265 46.55496  45.829887 45.85153
0:  46.663704 47.5843   48.289494 48.081184 47.91193  48.43596 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1577785  2.955751   3.0316238  3.4426806  3.8826854  4.1997128
0:  4.123644   4.0034795  3.404721   2.8728428  2.3523955  1.6255622
0:  0.99759245 0.4646778  0.2261014  0.55537367 1.3262949  2.2601209
0:  1.2580323  0.8324356 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.0766    4.0274725 4.2970705 4.802274  5.250749  5.477877  5.5421968
0:  5.5711694 5.28349   5.016296  4.606575  3.9765682 3.3937    2.9511647
0:  2.848636  3.2092006 3.818527  4.413878  2.9096355 2.7994676]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.300528 17.539524 17.957817 18.37797  18.546629 18.439993 18.268156
0:  18.008533 17.607845 17.261398 16.75344  16.055016 15.381485 14.828848
0:  14.68831  14.964581 15.538875 16.151482 13.571804 13.6771  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.894063  3.9361107 4.378578  5.2028217 6.0574474 6.7792954 7.1092954
0:  7.3598223 7.0961237 6.8918805 6.6799216 6.2816267 6.0357943 5.8275166
0:  5.9724784 6.5446653 7.406663  8.2434    6.536834  6.5129747]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.39471  33.67481  32.912666 32.173157 31.24417  29.985235 29.425467
0:  28.793629 28.50699  28.14585  27.259722 26.502985 25.76932  25.54568
0:  25.852545 26.363495 26.87433  27.092016 28.680939 28.42606 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.8663907 -4.892408  -4.522859  -3.777021  -2.9654279 -2.3142476
0:  -1.9882283 -1.8189411 -2.1028762 -2.3346906 -2.5565848 -2.903216
0:  -3.017345  -3.0176587 -2.7659364 -2.2167382 -1.5506511 -0.8818288
0:  -2.730803  -2.8731542]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.294144  -7.179894  -6.526462  -5.381745  -4.216046  -3.322309
0:  -2.8400102 -2.7667546 -3.332026  -4.054356  -5.0248246 -6.2807765
0:  -7.3547044 -8.268568  -8.715172  -8.63752   -8.243864  -7.6291785
0:  -8.361515  -7.647616 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.762753  -8.900141  -8.482819  -7.556375  -6.474855  -5.5830817
0:  -5.1506877 -5.0114427 -5.495054  -5.974005  -6.425587  -6.9655776
0:  -7.199407  -7.3081183 -7.2116942 -6.870647  -6.4790015 -6.0868535
0:  -7.4493785 -7.721683 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.00569248 -0.18558311  0.02049971  0.6416626   1.3500156   1.9443703
0:   2.0753193   2.098621    1.5672164   1.1021008   0.67207     0.04951429
0:  -0.40251017 -0.83077574 -1.0059147  -0.6962619   0.00606537  0.8967509
0:  -0.5826092  -0.8048949 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5147154 3.4544704 3.7883778 4.463891  5.0540614 5.380572  5.3483787
0:  5.2344728 4.704482  4.253377  3.7685583 3.0301516 2.3718276 1.7599626
0:  1.4531913 1.7246962 2.423007  3.2473114 1.6872501 1.3298788]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.647359   5.252176   6.1062336  6.8955684  7.4238744  7.595986
0:   7.718554   7.7690754  7.858412   8.067578   8.234164   8.253557
0:   8.321174   8.552461   9.047598   9.90938   10.935556  11.946675
0:   9.321636   9.891042 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.666317   8.769577   9.165227   9.698189  10.132756  10.390216
0:  10.645874  10.798491  10.775257  10.634207  10.248344   9.657179
0:   9.151411   8.8902645  9.001502   9.396836   9.853367  10.133086
0:   6.888383   6.5435925]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.160906 20.584131 21.380114 22.261662 22.903719 23.164928 23.251934
0:  23.151066 22.840517 22.491947 21.955294 21.1693   20.445143 19.870333
0:  19.720728 20.009573 20.574518 21.102087 17.156427 17.28803 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-23.43156  -24.077808 -24.204296 -23.651848 -22.724903 -21.758759
0:  -20.557945 -19.600681 -19.100227 -18.841118 -19.103191 -19.702568
0:  -20.03006  -20.033806 -19.515343 -18.74961  -18.262455 -18.175259
0:  -20.767605 -21.390148]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6295156  -2.468464   -1.8048773  -0.79303646  0.2651329   1.1504264
0:   1.9362373   2.4691105   2.5871184   2.4963489   2.05089     1.2492337
0:   0.43679    -0.22406387 -0.6114836  -0.6018181  -0.41716957 -0.20976639
0:  -2.5592546  -2.6674776 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.291245 19.368465 19.5253   19.569794 19.558544 19.421413 19.81543
0:  20.097599 20.419455 20.641651 20.480335 20.272907 20.130505 20.466263
0:  21.16717  22.09428  22.965221 23.498737 22.767208 22.647957]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.51685  25.333817 25.492912 25.800266 26.02813  26.012337 26.426472
0:  26.70026  26.968826 27.128433 26.767544 26.045197 25.294403 24.702324
0:  24.525822 24.7938   25.211975 25.541054 21.258951 21.190315]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.167355 15.125124 15.391684 15.794762 16.072771 16.071192 16.257416
0:  16.284912 16.274593 16.215807 15.862602 15.358473 14.904154 14.733786
0:  15.035213 15.781593 16.750593 17.637785 16.203869 16.175777]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.41728   -7.414195  -6.94855   -6.049225  -5.0393314 -4.274296
0:  -3.8706417 -3.7203312 -4.185976  -4.6522865 -5.2189407 -5.9671507
0:  -6.4052105 -6.702299  -6.6022577 -6.1273355 -5.572049  -5.0104504
0:  -6.672921  -6.846176 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.2791257 7.0509386 7.2205563 7.6149673 7.801716  7.6245685 7.4331765
0:  7.1643677 6.7818522 6.407409  5.790037  4.862398  3.8996782 3.1858053
0:  2.947751  3.2430341 3.8649054 4.4361353 1.6422558 1.414998 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.931005 17.113317 17.596428 18.245544 18.82763  19.083748 19.594696
0:  19.917854 20.156511 20.293247 20.008324 19.604988 19.108799 18.912292
0:  18.946434 19.101242 19.127604 18.889528 13.858685 13.319761]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.338006  8.083291  8.09435   8.244035  8.348635  8.20787   8.351793
0:  8.368951  8.411034  8.37763   8.012308  7.506894  6.9843273 6.80317
0:  7.0130377 7.5927043 8.224342  8.645014  6.182041  5.84171  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.444038  -16.457672  -16.20026   -15.657876  -15.250417  -14.925511
0:  -14.527978  -14.019477  -13.756     -13.547202  -13.711997  -14.331208
0:  -15.06899   -15.532983  -15.58566   -15.225757  -14.576783  -13.983857
0:  -12.9495325 -12.715734 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.2901835 -4.4092183 -4.092116  -3.3285403 -2.484733  -1.817184
0:  -1.4900899 -1.4169374 -1.9210896 -2.4811606 -3.117703  -3.855824
0:  -4.307795  -4.522243  -4.4194436 -3.9652934 -3.4274259 -2.9603772
0:  -3.2321734 -3.2955337]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.78574  -19.188576 -19.059181 -18.297913 -17.297684 -16.365685
0:  -15.85662  -15.58623  -15.982358 -16.423515 -16.89695  -17.439568
0:  -17.620445 -17.596981 -17.174908 -16.417377 -15.566579 -14.784993
0:  -17.7473   -18.38245 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.430651  -10.706379  -10.403618   -9.5808525  -8.646479   -7.8788095
0:   -7.316877   -7.0370126  -7.177469   -7.443626   -7.87732    -8.492964
0:   -8.880999   -9.003035   -8.704233   -7.964995   -7.141992   -6.3611197
0:   -7.660093   -7.935805 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.598639  -5.6851296 -5.2324824 -4.2643304 -3.1772084 -2.240456
0:  -1.5363984 -1.0963731 -1.2394996 -1.5169263 -2.0259128 -2.8117576
0:  -3.4003968 -3.8183303 -3.8624778 -3.4354396 -2.8159022 -2.171575
0:  -4.3437886 -4.2973475]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.8185337 4.028824  4.5315585 5.1353865 5.5393696 5.579588  5.4706473
0:  5.0751014 4.4427404 3.8155012 3.0505512 2.216319  1.5828981 1.3129778
0:  1.4668546 1.9742827 2.5896983 3.0523388 2.00007   2.3589513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.4484377   0.22632027  0.38907766  0.83951235  1.2001424   1.3418179
0:   1.2643175   1.1631479   0.7205796   0.3823619  -0.04711342 -0.7473731
0:  -1.3882322  -1.949296   -2.2116728  -1.9239864  -1.2883587  -0.536293
0:  -2.6872263  -2.9244184 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.578589  8.550305  8.85968   9.3345375 9.679583  9.78038   9.813609
0:  9.71628   9.421511  9.132468  8.693262  8.077588  7.552388  7.2121377
0:  7.2270765 7.6001344 8.115896  8.53687   6.1779227 6.113487 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4276314 4.147733  4.184396  4.474567  4.770458  4.9432144 5.1290197
0:  5.282887  5.2704024 5.295274  5.182576  4.8436937 4.551982  4.4096174
0:  4.53628   5.052904  5.746483  6.388287  4.2092576 4.2070775]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.2774594   3.4047647   3.8635058   4.470417    4.841567    4.8585763
0:   4.636347    4.1217585   3.2970788   2.503957    1.5890236   0.56080866
0:  -0.31071138 -0.9164095  -1.1661153  -1.0298824  -0.7788825  -0.5854635
0:  -2.6330476  -2.5312543 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.480997  2.6010003 3.1290965 3.9212968 4.6233854 5.0624285 5.4027753
0:  5.5874825 5.5629807 5.516776  5.2887096 4.8388023 4.472856  4.3148
0:  4.4787827 4.9557343 5.5045657 5.8371854 2.8197918 2.525126 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.821812  -8.897954  -8.562851  -7.866246  -7.1510754 -6.702976
0:  -6.5251865 -6.578063  -7.0993795 -7.6174583 -8.247284  -9.041957
0:  -9.572717  -9.844504  -9.715205  -9.223386  -8.714498  -8.265968
0:  -9.519622  -9.6083555]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.37177324 0.34747458 0.639472   1.0380487  1.3477659  1.457591
0:  1.8779554  2.2296321  2.5700665  2.8766813  2.795799   2.4928665
0:  2.1053314  2.0931478  2.414269   3.073199   3.6385453  3.9524462
0:  2.9408464  2.9948983 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.621746  -4.6791267 -4.292161  -3.530923  -2.8166227 -2.4382887
0:  -2.4729443 -2.7984304 -3.7057228 -4.5836277 -5.509528  -6.500229
0:  -7.19643   -7.5685825 -7.527233  -7.0218935 -6.3597574 -5.7151375
0:  -7.0666714 -7.3181295]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0477922 2.8151002 2.9882054 3.405631  3.7515953 3.8790574 4.0828476
0:  4.0816402 3.8809314 3.6937997 3.25346   2.6670961 2.2354603 2.049302
0:  2.2444234 2.7189445 3.1697047 3.4764693 2.6553006 2.4914613]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.8196025 0.8097739 1.1874909 1.8732405 2.5330806 3.023088  3.2844272
0:  3.493017  3.324864  3.186128  2.960352  2.452671  2.0531774 1.6749349
0:  1.5787754 1.9169044 2.5308366 3.2074678 1.4890285 1.5915818]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.329794 30.427784 30.751791 31.019564 30.870056 30.293104 29.67432
0:  28.84082  27.858574 26.72994  25.209469 23.532415 22.089437 21.12757
0:  21.004732 21.331905 21.870455 22.085434 18.080347 17.641352]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.802943 14.7041   14.914387 15.151222 15.213932 14.959555 14.869062
0:  14.671289 14.55658  14.487421 14.203456 13.761936 13.345577 13.192463
0:  13.4611   14.158016 15.095232 15.986458 14.363576 14.374659]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.501816 32.609528 32.7827   33.00364  33.22599  33.322598 34.01587
0:  34.599804 35.253487 35.83841  35.984047 36.057186 36.226646 36.73988
0:  37.704037 38.861496 40.05945  40.901005 37.04865  37.131905]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.535715 19.06791  18.812328 18.685171 18.64232  18.444517 18.659435
0:  18.75127  18.849813 18.861708 18.463018 18.021673 17.573706 17.521376
0:  17.95543  18.742985 19.684504 20.478745 19.230923 19.074104]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1163006   0.99669886  1.2778277   1.9834142   2.757092    3.3723643
0:   3.6159365   3.593964    2.9944787   2.3298726   1.577096    0.70907545
0:   0.09614754 -0.29759264 -0.26168537  0.26720572  1.0644927   1.9076877
0:  -0.01264286 -0.6196294 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8275156 6.5596986 6.467388  6.565011  6.630294  6.498599  6.666748
0:  6.642551  6.6130285 6.477701  5.9865828 5.426464  4.9053183 4.8599195
0:  5.257861  6.080803  6.982047  7.6507635 6.3650475 6.371202 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.2005754   1.0246267   1.2300181   1.7261872   2.206483    2.4627993
0:   2.5557103   2.4705167   1.9323511   1.3536682   0.5576043  -0.44397926
0:  -1.2426124  -1.8653278  -2.1380615  -2.0647984  -1.9636021  -1.9460545
0:  -4.6649313  -4.8061295 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.1444697 -5.5622888 -5.535167  -4.9784446 -4.23579   -3.5434756
0:  -3.3202462 -3.2105527 -3.7088828 -4.1804967 -4.6278815 -5.2880073
0:  -5.710366  -6.041903  -6.046021  -5.5287414 -4.6045957 -3.4678712
0:  -3.7429929 -4.1561036]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.841458  -5.801928  -5.3717895 -4.7029138 -4.1384463 -3.8587785
0:  -3.751347  -3.9699187 -4.5980124 -5.167461  -5.870233  -6.673863
0:  -7.2441306 -7.476624  -7.297397  -6.7267194 -6.0854173 -5.462276
0:  -4.9638124 -4.6510997]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.611433 24.445385 24.531704 24.784178 25.157166 25.471676 26.463997
0:  27.454885 28.675013 29.888144 30.55283  31.136198 31.579287 32.319656
0:  33.457535 34.862354 36.27076  37.30583  35.40415  35.5953  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0862584  -1.9960437  -1.4472566  -0.6230879   0.08391809  0.46313286
0:   0.51076317  0.3671384  -0.17824888 -0.64265823 -1.117065   -1.6666436
0:  -1.8987927  -1.9414306  -1.6074414  -0.95711946 -0.26801538  0.30314398
0:  -1.3049493  -1.2552581 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.256025 19.037708 18.895044 18.74788  18.594177 18.248358 18.627003
0:  18.807999 19.117147 19.153515 18.517273 17.872772 17.18518  17.139383
0:  17.713226 18.67725  19.790316 20.580555 21.139534 21.279814]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7352972  -2.3792543  -1.6878843  -0.87643003 -0.21124029  0.16565895
0:   0.53782606  0.70123196  0.7236471   0.81185865  0.7467599   0.5801921
0:   0.4942279   0.70656204  1.2371125   2.0920095   2.960865    3.6636193
0:   3.9270253   4.5650783 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.931637 12.171347 12.605879 13.071852 13.382286 13.499168 13.806486
0:  14.057159 14.354567 14.704737 14.813702 14.759972 14.684658 14.837097
0:  15.271593 15.97584  16.758934 17.39961  16.388773 16.61379 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.40891314  0.41169167  0.748374    1.4411902   2.1167455   2.64085
0:   2.8905263   3.0482666   2.7147985   2.3037786   1.7257743   0.88561535
0:   0.14871311 -0.44078827 -0.59431934 -0.19327927  0.5740957   1.4037752
0:  -0.2968979  -0.34353304]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0010266  -2.0865083  -1.7195172  -1.0010676  -0.32354498  0.08026886
0:   0.15862131  0.06348658 -0.40394926 -0.7873349  -1.1423745  -1.649282
0:  -1.9639716  -2.1453433  -2.058947   -1.5498157  -0.8348789  -0.08824396
0:  -1.9335523  -1.9504762 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.797893  2.0577183 2.6453087 3.432394  4.1595697 4.601119  4.8568864
0:  4.9711604 4.708514  4.5032716 4.233436  3.8606071 3.7374747 3.7429993
0:  4.102426  4.73642   5.4759674 6.1084437 3.9792051 4.142258 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.479362 34.63647  34.68051  34.591557 34.45672  34.167095 34.673862
0:  35.05868  35.610577 35.988564 35.731834 35.46868  35.19212  35.413006
0:  36.1407   37.14368  38.195858 38.782246 37.295334 37.62378 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0084848 2.935748  3.1698427 3.6388557 4.0945377 4.3731027 4.5771713
0:  4.704723  4.4914007 4.3061695 3.9427998 3.4089518 2.9858165 2.7107801
0:  2.732314  3.0616221 3.463076  3.7971995 2.1771655 2.1942582]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.447317 11.490475 11.727929 12.051964 12.274443 12.282431 12.200808
0:  12.055602 11.663084 11.290037 10.801474 10.184149  9.679099  9.340848
0:   9.374272  9.789519 10.380888 10.894753  8.387216  8.412179]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.109196 33.220207 33.70237  34.140636 34.35688  34.221436 34.286186
0:  34.419327 34.652935 35.010475 35.124348 34.95244  34.862225 34.815918
0:  35.161797 35.805054 36.595303 37.244247 33.224033 33.536438]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.02475  22.943932 22.731339 22.291939 21.638039 20.659122 20.325008
0:  19.94893  19.76623  19.583399 18.85565  18.142513 17.268883 16.888195
0:  16.901573 17.214771 17.508327 17.493576 16.20514  16.108501]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.175415  13.09486   13.292231  13.576632  13.73791   13.5521755
0:  13.55813   13.399118  13.251159  13.107775  12.684719  12.142643
0:  11.644964  11.415213  11.583605  12.156948  12.978863  13.726458
0:  12.369022  12.273011 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.2187014 -6.332649  -6.006809  -5.223489  -4.2866116 -3.4770937
0:  -2.9866266 -2.6565113 -2.890236  -3.1525555 -3.4778585 -3.9908876
0:  -4.2390957 -4.3694944 -4.2136536 -3.6921568 -3.0652351 -2.3925362
0:  -3.5823193 -3.745966 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.505688 36.33326  36.230843 36.202087 36.130913 35.77507  35.9174
0:  35.795547 35.689915 35.298874 34.31102  33.237335 32.28036  31.770945
0:  31.856585 32.268314 32.831604 33.206497 31.598034 31.391172]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.3803   35.841644 36.25165  36.458706 36.334877 35.781345 35.50825
0:  35.094822 34.7906   34.529274 33.93449  33.305603 32.726112 32.472473
0:  32.617733 33.05082  33.544792 33.81715  30.602043 30.76968 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -2.9263625  -3.4093575  -3.5008006  -3.1381469  -2.672009   -2.3361201
0:   -2.4379153  -2.6714044  -3.479032   -4.2820897  -5.116652   -6.1957955
0:   -7.097135   -7.920343   -8.473413   -8.444767   -8.0329075  -7.3648944
0:   -9.439699  -10.331875 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.104115   8.660023   8.71299    9.026145   9.200268   9.105595
0:   9.08819    9.0581665  8.974949   8.93137    8.668276   8.197439
0:   7.7952814  7.761556   8.329277   9.427953  10.696745  11.675285
0:   9.803963   9.825172 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.061369  -7.9083667 -7.403135  -6.5651746 -5.67504   -4.956763
0:  -4.63106   -4.5106883 -4.988169  -5.511185  -6.0854917 -6.768987
0:  -7.157161  -7.3708005 -7.262362  -6.7830224 -6.1568656 -5.528271
0:  -7.028682  -7.353191 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.544219  2.9258919 3.636078  4.553402  5.3389573 5.845538  6.2400246
0:  6.3818026 6.210617  5.9170423 5.3599586 4.608265  3.9778328 3.5855029
0:  3.5423584 3.8769302 4.265683  4.526596  3.5019464 3.6675558]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.617701  10.701194  11.1352215 11.762636  12.294703  12.615044
0:  12.935706  13.143089  13.282318  13.55447   13.760789  13.881477
0:  14.146818  14.60433   15.363323  16.328823  17.258781  17.919447
0:  15.753246  15.858953 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.531176 20.971718 21.56462  22.142092 22.561495 22.616074 22.932089
0:  23.01321  23.01273  22.853851 22.18073  21.355587 20.440851 19.900114
0:  19.794538 20.053905 20.458384 20.692791 18.9014   18.783829]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5596442  -3.7058587  -3.4478145  -2.9542413  -2.426721   -2.211204
0:  -1.6083026  -1.2665095  -0.82434607 -0.50885916 -0.6495819  -0.9273586
0:  -1.2796426  -1.1714573  -0.6524558   0.3495679   1.4633436   2.410202
0:   1.3588414   1.1954703 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.710295 16.52576  16.742945 17.022188 17.114946 16.787138 16.462446
0:  15.955465 15.399002 14.817402 13.985608 13.020173 12.070043 11.368484
0:  11.164736 11.443991 12.076958 12.864033 11.866646 12.092146]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.955951  6.02418   6.411845  6.9965887 7.410524  7.5417194 7.5039706
0:  7.456158  7.151317  6.9304557 6.6029873 6.0630803 5.570166  5.1430087
0:  5.0352407 5.3574405 5.90368   6.430368  4.3649397 4.3082795]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.7403383  6.8233166  7.1495433  7.5021477  7.825442   8.1240635
0:   9.176773  10.302234  11.617459  12.797121  13.376173  13.647739
0:  13.636246  13.963779  14.493631  15.1749115 15.758106  15.813259
0:  10.447509  10.480687 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.945984 25.811205 25.791697 25.695477 25.555902 25.233868 25.567312
0:  25.785658 26.143902 26.364582 25.956083 25.474474 24.94354  24.988096
0:  25.574886 26.522902 27.550037 28.178608 28.004547 28.259092]
0: validation loss for strategy=forecast at epoch 43 : 0.3160873353481293
0: validation loss for velocity_u : 0.1706501990556717
0: validation loss for velocity_v : 0.29999807476997375
0: validation loss for specific_humidity : 0.15709909796714783
0: validation loss for velocity_z : 0.5590490102767944
0: validation loss for temperature : 0.12609589099884033
0: validation loss for total_precip : 0.5836315155029297
0: 44 : 20:53:13 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 44, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1874, 0.2958, 0.4175, 0.5381, 0.6452, 0.7308, 0.7893, 0.8234, 0.8429, 0.8533, 0.8559, 0.8537, 0.8494, 0.8448,
0:         0.8425, 0.8429, 0.8392, 0.8243, 0.0933, 0.1731, 0.2760, 0.3984, 0.5323, 0.6638, 0.7766], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.0019,  0.8864,  0.7562,  0.6204,  0.4901,  0.3759,  0.2795,  0.1947,  0.1148,  0.0332, -0.0553, -0.1482,
0:         -0.2398, -0.3312, -0.4257, -0.5264, -0.6367, -0.7538,  1.2116,  1.1299,  1.0162,  0.8710,  0.7040,  0.5353,
0:          0.3829], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6218, -0.6129, -0.6081, -0.6214, -0.6258, -0.6366, -0.6417, -0.6471, -0.6403, -0.6145, -0.5933, -0.5648,
0:         -0.5438, -0.5193, -0.5087, -0.5245, -0.5345, -0.5606, -0.6613, -0.6518, -0.6391, -0.6340, -0.6287, -0.6383,
0:         -0.6322], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.0605,  1.2050,  1.4561,  1.7250,  1.9273,  2.0161,  1.9184,  1.6095,  1.2272,  0.8828,  0.5672,  0.2783,
0:          0.0427, -0.1217, -0.1773, -0.1040, -0.0240, -0.1106,  1.1094,  1.2272,  1.4428,  1.6695,  1.8606,  2.0295,
0:          2.0850], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.0732, -1.0686, -1.0535, -1.0265, -0.9889, -0.9413, -0.8896, -0.8410, -0.7968, -0.7558, -0.7173, -0.6786,
0:         -0.6353, -0.5821, -0.5099, -0.4127, -0.2964, -0.1788, -0.0780, -0.0038,  0.0468,  0.0866,  0.1245,  0.1607,
0:          0.1971], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2450, -0.2260, -0.1949, -0.1592, -0.1258, -0.1687, -0.0948, -0.0399,  0.0054, -0.2426, -0.2093, -0.1592,
0:         -0.0709, -0.0638, -0.1329, -0.2188, -0.2236, -0.0614, -0.2450, -0.2403, -0.0996, -0.0042,  0.0197, -0.0757,
0:         -0.1663], device='cuda:0')
0: [DEBUG] Epoch 44, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  2.1458,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.7864,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.0173,     nan,  0.1187,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2450,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2474,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  7.9554,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  5.8269,     nan,  4.5355,  4.4091,
0:             nan,  2.7707,     nan,     nan,     nan,     nan,     nan,     nan,  3.1713,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0411,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0626,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2462,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  5.9521,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          6.6950,     nan,     nan,     nan,     nan,  6.7606,     nan,     nan,  4.2505,     nan,  4.8598,     nan,
0:             nan,     nan,  6.5805,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.3667,     nan, -0.2081,     nan,     nan,     nan, -0.2438,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 44, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.4256, -1.3993, -1.3442, -1.2722, -1.1936, -1.1170, -1.0031, -0.8892, -0.7721, -0.6637, -0.5849, -0.5216,
0:         -0.4586, -0.3704, -0.2679, -0.1422, -0.0195,  0.0793, -1.3867, -1.3561, -1.2878, -1.2189, -1.1534, -1.0631,
0:         -0.9488], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3311, -0.3336, -0.3921, -0.4931, -0.6142, -0.7312, -0.8322, -0.8891, -0.8896, -0.8597, -0.7989, -0.7331,
0:         -0.6964, -0.6895, -0.6864, -0.6761, -0.6371, -0.5749, -0.2736, -0.2563, -0.2908, -0.3759, -0.4936, -0.6174,
0:         -0.7184], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4658, 0.4625, 0.4461, 0.4282, 0.4111, 0.3927, 0.3639, 0.3410, 0.3162, 0.2997, 0.2854, 0.2706, 0.2615, 0.2483,
0:         0.2226, 0.2052, 0.1794, 0.1714, 0.4030, 0.3919, 0.3647, 0.3485, 0.3294, 0.3035, 0.2901], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-4.4542, -4.4699, -4.3158, -4.5146, -4.4124, -4.0091, -3.6866, -3.2405, -2.7580, -2.3563, -2.0850, -1.9485,
0:         -1.7755, -1.7898, -1.9750, -1.8445, -1.6011, -1.4872, -3.7941, -3.9526, -3.8508, -3.9100, -3.6069, -3.1771,
0:         -3.0080], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3528, -0.3902, -0.4079, -0.4069, -0.3962, -0.3921, -0.4096, -0.4459, -0.4801, -0.4985, -0.4993, -0.4766,
0:         -0.4512, -0.4286, -0.4193, -0.4223, -0.4376, -0.4549, -0.4663, -0.4725, -0.4743, -0.4790, -0.4797, -0.4674,
0:         -0.4349], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([5.5246, 5.8901, 6.1217, 6.0790, 5.9195, 5.6120, 5.2893, 4.9324, 4.4256, 4.7891, 5.0694, 5.1921, 5.2111, 5.0109,
0:         4.7938, 4.5632, 4.2962, 3.9548, 3.4684, 3.6418, 3.6444, 3.6156, 3.5812, 3.4441, 3.3027], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.2177424430847168; velocity_v: 0.3732117712497711; specific_humidity: 0.15523117780685425; velocity_z: 0.6047188639640808; temperature: 0.14018625020980835; total_precip: 0.3773289620876312; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24015650153160095; velocity_v: 0.38735005259513855; specific_humidity: 0.20562440156936646; velocity_z: 0.6938940286636353; temperature: 0.15653593838214874; total_precip: 0.8033539056777954; 
0: epoch: 44 [1/5 (20%)]	Loss: 0.59034 : 0.32815 :: 0.21736 (2.65 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19448840618133545; velocity_v: 0.34221792221069336; specific_humidity: 0.22457590699195862; velocity_z: 0.7648980021476746; temperature: 0.1785513311624527; total_precip: 1.1276932954788208; 
0: epoch: 44 [2/5 (40%)]	Loss: 1.12769 : 0.43610 :: 0.20778 (16.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21039262413978577; velocity_v: 0.4127318263053894; specific_humidity: 0.1809885948896408; velocity_z: 0.5523301362991333; temperature: 0.17439468204975128; total_precip: 0.827391505241394; 
0: epoch: 44 [3/5 (60%)]	Loss: 0.82739 : 0.35732 :: 0.21379 (15.83 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2083585262298584; velocity_v: 0.2948454022407532; specific_humidity: 0.18714997172355652; velocity_z: 0.5755935311317444; temperature: 0.13685446977615356; total_precip: 0.8416873812675476; 
0: epoch: 44 [4/5 (80%)]	Loss: 0.84169 : 0.34011 :: 0.21086 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [3.33786011e-06 4.29153442e-06 7.15255737e-06 1.47819519e-05
0:  1.81198120e-05 2.57492065e-05 3.57627869e-05 3.05175781e-05
0:  2.33650208e-05 1.81198120e-05 2.28881836e-05 2.71797180e-05
0:  2.76565552e-05 3.52859497e-05 4.43458557e-05 5.38825989e-05
0:  6.05583191e-05 5.48362732e-05 6.19888306e-05 3.52859497e-05
0:  2.19345093e-05 3.86238098e-05 3.91006470e-05 2.05039978e-05
0:  8.05854797e-05 1.24931335e-04 8.82148743e-05 8.10623169e-06
0:  9.05990601e-06 7.96318054e-05 7.77244568e-05 5.05447388e-05
0:  9.58442688e-05 1.54018402e-04 1.86920166e-04 2.89440155e-04
0:  5.64575195e-04 6.49929047e-04 6.95228577e-04 7.60555267e-04
0:  7.90119171e-04 8.05377960e-04 1.76715851e-03 1.91020977e-03
0:  5.14507294e-04 3.71932983e-04 8.20159912e-05 9.82284546e-05
0:  8.24928284e-05 9.82284546e-05 7.72476196e-05 5.05447388e-05
0:  8.39233398e-05 9.15527344e-05 9.10758972e-05 3.71932983e-05
0:  4.76837195e-05 6.29425049e-05 1.17301941e-04 5.02109528e-04
0:  9.71317233e-04 8.17775726e-04 3.57151031e-04 3.61442566e-04
0:  2.23636627e-04 1.25408173e-04 6.72340393e-05 3.19480896e-05
0:  2.95639038e-05 5.72204590e-05 1.17301941e-04 2.00748444e-04
0:  2.17914581e-04 2.62260437e-04 3.13282042e-04 4.18186188e-04
0:  5.55515289e-04 7.17163086e-04 6.47544861e-04 7.37667084e-04
0:  6.85214996e-04 7.04288483e-04 6.99043274e-04 5.84125519e-04
0:  4.83512878e-04 3.95298004e-04 3.01361084e-04 2.64644623e-04
0:  1.68800354e-04 1.24454498e-04 1.30176544e-04 2.37464905e-04
0:  2.25543976e-04 1.57833099e-04 2.44617462e-04 4.03404236e-04
0:  6.88552856e-04 1.68037415e-03 2.61783600e-03 3.04889679e-03
0:  3.11470032e-03 2.70414352e-03 2.22444534e-03 2.14624405e-03
0:  1.64842606e-03 1.10197067e-03 1.00421906e-03 1.00421906e-03
0:  2.86102295e-06 6.19888306e-06 7.62939453e-06 1.76429749e-05
0:  1.71661377e-05 2.52723694e-05 3.52859497e-05 3.62396240e-05
0:  2.62260437e-05 1.90734863e-05 1.81198120e-05 2.33650208e-05
0:  3.14712524e-05 3.67164612e-05 3.86238098e-05 4.81605530e-05
0:  4.14848328e-05 3.52859497e-05 9.58442688e-05 2.28881836e-05
0:  1.04904175e-05 8.58306885e-06 1.04904175e-05 6.62803650e-05
0:  1.26361847e-04 9.96589661e-05 1.85966492e-05 1.66893005e-05
0:  3.81469727e-05 6.48498535e-05 6.86645508e-05 3.09944153e-05
0:  3.14712524e-05 9.01222229e-05 2.49862671e-04 4.05788422e-04
0:  6.57081604e-04 6.93321228e-04 5.56468964e-04 5.20706235e-04
0:  5.99384308e-04 7.52449036e-04 1.71566010e-03 2.23207474e-03
0:  7.56263733e-04 4.71591949e-04 2.38418579e-04 1.56879425e-04
0:  6.58035278e-05 7.20024109e-05 4.95910681e-05 2.28881836e-05
0:  5.62667847e-05 7.77244568e-05 7.05718994e-05 3.24249268e-05
0:  6.10351562e-05 1.31130219e-04 1.91688538e-04 2.19345093e-04
0:  6.89029694e-04 8.48293304e-04 1.78813934e-04 2.43186951e-04
0:  1.36375427e-04 1.49250031e-04 1.15394592e-04 6.58035278e-05
0:  2.81333923e-05 5.62667847e-05 1.07765198e-04 1.99794769e-04
0:  1.91688538e-04 2.02178955e-04 2.05516815e-04 2.68936157e-04
0:  4.24861908e-04 4.27246094e-04 3.57627869e-04 3.74794006e-04
0:  4.17709351e-04 4.53472137e-04 4.35352325e-04 3.24726105e-04
0:  2.41756439e-04 1.68800354e-04 1.38282776e-04 1.01566315e-04
0:  8.29696655e-05 5.14984167e-05 3.71932983e-05 3.33786011e-05]
0: Target values (first 200):
0: [1.43051147e-05 1.23977661e-05 1.81198120e-05 1.33514404e-05
0:  1.23977661e-05 6.67572021e-06 2.86102295e-06 5.72204590e-06
0:  5.72204590e-06 6.67572021e-06 8.58306885e-06 1.62124634e-05
0:  2.09808350e-05 2.57492065e-05 1.52587891e-05 1.81198120e-05
0:  2.00271606e-05 1.23977661e-05 1.90734863e-05 2.47955322e-05
0:  2.67028809e-05 2.28881836e-05 1.90734863e-05 1.71661377e-05
0:  2.76565552e-05 4.38690149e-05 4.95910681e-05 7.82012939e-05
0:  9.82284546e-05 1.04904175e-04 8.48770142e-05 7.43865967e-05
0:  9.05990601e-05 9.53674316e-05 4.29153442e-05 4.67300415e-05
0:  7.72476196e-05 7.91549683e-05 1.10626221e-04 9.15527344e-05
0:  9.15527344e-05 2.98500061e-04 3.81469727e-04 5.17845154e-04
0:  5.02586365e-04 3.81469727e-05 7.62939453e-06 9.53674316e-07
0:  8.58306885e-06 3.33786011e-05 2.57492065e-05 5.91278076e-05
0:  8.29696655e-05 2.95639038e-05 3.05175781e-05 4.76837195e-05
0:  2.09808350e-05 1.43051147e-05 1.52587891e-05 1.52587891e-05
0:  5.14984167e-05 5.53131104e-05 1.09672546e-04 1.57356262e-04
0:  4.29153442e-05 1.43051147e-05 6.67572021e-06 0.00000000e+00
0:  0.00000000e+00 1.90734863e-06 1.90734863e-06 9.53674316e-07
0:  5.72204590e-06 1.62124634e-05 1.04904175e-05 1.04904175e-05
0:  2.57492065e-05 6.96182251e-05 6.96182251e-05 2.95639038e-05
0:  1.62124634e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 9.53674316e-07 9.53674316e-07
0:  4.76837158e-06 2.09808350e-05 4.00543213e-05 4.48226929e-05
0:  4.19616699e-05 4.10079956e-05 8.58306885e-05 1.20162964e-04
0:  1.21116638e-04 1.21116638e-04 1.02043152e-04 1.36375427e-04
0:  2.15530396e-04 7.19070435e-04 1.21307373e-03 1.42383575e-03
0:  1.20162964e-03 1.29222870e-03 1.62696838e-03 1.13868713e-03
0:  1.33514404e-05 1.43051147e-05 1.71661377e-05 1.90734863e-05
0:  1.04904175e-05 1.43051147e-05 9.53674316e-06 1.33514404e-05
0:  1.33514404e-05 8.58306885e-06 7.62939453e-06 1.23977661e-05
0:  2.47955322e-05 3.14712524e-05 2.67028809e-05 2.95639038e-05
0:  3.14712524e-05 2.00271606e-05 2.57492065e-05 3.52859497e-05
0:  4.57763635e-05 2.76565552e-05 2.00271606e-05 2.95639038e-05
0:  4.10079956e-05 6.19888306e-05 5.72204590e-05 7.24792480e-05
0:  9.34600830e-05 9.91821289e-05 6.10351562e-05 5.14984167e-05
0:  4.76837195e-05 5.24520874e-05 5.72204590e-05 6.10351562e-05
0:  1.33514404e-04 1.39236450e-04 1.71661377e-04 8.67843628e-05
0:  8.20159912e-05 1.69754028e-04 2.53677368e-04 4.89234924e-04
0:  5.78880310e-04 5.14984167e-05 2.86102295e-06 9.53674316e-07
0:  2.86102295e-06 2.95639038e-05 3.43322754e-05 6.86645508e-05
0:  8.58306885e-05 1.23977661e-05 6.10351562e-05 8.01086426e-05
0:  7.91549683e-05 5.81741333e-05 6.19888306e-05 3.52859497e-05
0:  7.15255737e-05 1.00135803e-04 1.57356262e-04 3.56674194e-04
0:  2.07901001e-04 1.22070312e-04 5.05447388e-05 4.29153442e-05
0:  6.48498535e-05 6.67572021e-05 5.72204590e-06 3.81469727e-06
0:  3.81469727e-06 1.14440918e-05 5.72204590e-06 1.23977661e-05
0:  2.47955322e-05 2.67028809e-05 2.67028809e-05 1.62124634e-05
0:  0.00000000e+00 9.53674316e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  1.62124634e-05 3.62396240e-05 6.10351562e-05 1.02043152e-04]
0: Prediction values (first 20):
0: [-1.4673505  -1.5895     -1.2684994  -0.76019096 -0.26779175  0.05966473
0:   0.63904333  1.1591039   1.7195067   2.4149714   2.9135528   3.2740948
0:   3.5792332   4.149955    4.861572    5.772864    6.4845824   6.8573556
0:   5.2614713   5.5948095 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -0.767, max = 2.358, mean = 0.373
0:          sample (first 20): tensor([-0.6385, -0.6488, -0.6218, -0.5790, -0.5375, -0.5100, -0.4612, -0.4174, -0.3702, -0.3117, -0.2697, -0.2393,
0:         -0.2137, -0.1656, -0.1057, -0.0290,  0.0309,  0.0623, -0.5413, -0.5711])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5677273  3.4882603  3.7450206  4.223817   4.6362376  4.833276
0:  5.027581   5.1337175  5.0442524  4.9102983  4.556746   3.9915833
0:  3.4601135  3.117553   3.0587142  3.3279858  3.6928487  3.9215062
0:  0.96596766 0.86848736]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.328268  4.555538  5.133335  5.902878  6.5719743 7.0680923 7.285772
0:  7.4751225 7.3958406 7.382622  7.35675   7.1013184 6.8915577 6.6714196
0:  6.7623916 7.3313756 8.268365  9.348249  7.0836573 7.3159513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.577498  -3.7983708 -3.5677524 -2.8773828 -2.080771  -1.4140315
0:  -1.0779605 -0.8286934 -1.0308642 -1.2017713 -1.4132857 -1.8765063
0:  -2.2400231 -2.5772824 -2.7126918 -2.4323068 -1.8829026 -1.2514634
0:  -2.4551988 -2.7427   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.0401974 4.08509   4.4588284 5.019975  5.4010124 5.4840827 5.264922
0:  5.071825  4.619251  4.3363786 4.0626774 3.5378642 3.0441713 2.5758266
0:  2.3878856 2.735899  3.4681144 4.2791796 2.6472025 2.5318036]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.892561    4.51224     4.4211574   4.4235077   4.320444    3.9820766
0:   3.8436196   3.6352363   3.3676674   3.0860305   2.519472    1.7541819
0:   0.9727955   0.55765676  0.4391613   0.66589737  0.9026923   0.93185043
0:  -2.3614297  -2.6468463 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8602333 -6.625869  -5.975622  -5.1500587 -4.5023518 -4.150092
0:  -3.9207726 -3.8997765 -4.1842136 -4.415142  -4.831312  -5.4942045
0:  -6.0366373 -6.362628  -6.3238344 -5.8107815 -5.173809  -4.5269117
0:  -5.47332   -5.1430674]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2774453 5.1966343 5.4140887 5.781238  6.0233355 6.0097866 6.04653
0:  5.945019  5.760356  5.5579395 5.136094  4.5520005 3.9748127 3.7095373
0:  3.768262  4.2014613 4.7341037 5.1188917 3.5142279 3.5867903]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5996094 -1.9493303 -1.9302425 -1.8249111 -1.8386755 -2.1128101
0:  -2.4523544 -2.9975595 -3.7843742 -4.502531  -5.2975836 -6.086466
0:  -6.5326166 -6.562543  -6.234763  -5.6293    -5.172387  -4.9258404
0:  -5.7598677 -5.6561413]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.607326  -11.906579  -11.737874  -11.143771  -10.528644  -10.163711
0:  -10.110734  -10.2989855 -10.972215  -11.662446  -12.440156  -13.4143095
0:  -14.0907755 -14.487134  -14.511205  -14.054116  -13.508111  -12.9258585
0:  -14.339406  -14.756991 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.122864  -5.2149878 -4.8233776 -4.035964  -3.16675   -2.5476236
0:  -2.216629  -2.1293287 -2.5331106 -2.9046297 -3.3569832 -4.008418
0:  -4.398261  -4.7284765 -4.730756  -4.425565  -4.0060472 -3.639738
0:  -4.818762  -4.9746265]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1047366 3.047807  3.4191551 4.017767  4.4661245 4.532971  4.5925584
0:  4.4712887 4.2613945 4.098296  3.7601666 3.2145324 2.7603495 2.6518235
0:  2.991349  3.8730717 4.996793  6.0573673 4.730629  4.622636 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.235731 22.423006 22.828066 23.188437 23.32228  23.171854 23.05514
0:  22.90234  22.74845  22.675505 22.498915 22.204735 22.074457 22.131433
0:  22.616049 23.393478 24.26005  24.970911 22.144531 22.380693]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.31112  21.169416 21.25211  21.449295 21.555819 21.379967 21.586235
0:  21.582788 21.599653 21.582993 21.134342 20.584558 20.11032  20.013952
0:  20.463522 21.446611 22.663094 23.744175 22.066519 22.368029]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.199817  -14.538377  -14.328876  -13.620572  -12.7440195 -12.025891
0:  -11.807969  -11.864863  -12.56821   -13.30586   -14.037421  -14.860414
0:  -15.28704   -15.443765  -15.175609  -14.402003  -13.428073  -12.413939
0:  -14.06947   -14.176303 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.45153  30.11142  29.693134 29.350914 29.142994 28.668808 29.105316
0:  29.224197 29.357292 29.10051  28.03038  26.967438 25.875732 25.378345
0:  25.502686 25.98481  26.605759 26.908283 27.711079 28.071617]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.5464945  4.7653503  4.1639504  3.6008303  2.9859285  2.1874065
0:  1.8154383  1.4655943  1.3081799  1.2951527  0.9861994  0.6820612
0:  0.35420752 0.5330682  1.1582031  2.085765   2.9874437  3.6651204
0:  4.6807184  4.8546276 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3492699  -1.7874622  -1.8319778  -1.4232388  -0.9205904  -0.521297
0:  -0.508049   -0.52491236 -0.99292374 -1.3836126  -1.7460375  -2.3865395
0:  -2.95654    -3.5119562  -3.8222494  -3.5387154  -2.781065   -1.7806721
0:  -3.267539   -3.7251606 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.093843   0.99816275 1.2716527  1.8124161  2.3119593  2.5708885
0:  2.8058429  2.8574457  2.660192   2.4630194  2.0793104  1.5659833
0:  1.1822996  1.0296483  1.1753006  1.6086631  2.0385132  2.2938223
0:  0.97408915 1.1717944 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8578205  -3.196083   -3.019599   -2.28819    -1.4731855  -0.9783764
0:  -0.40475178 -0.15252352 -0.19309568 -0.29285812 -0.74128294 -1.4475689
0:  -2.0885048  -2.4754696  -2.44063    -2.2025814  -2.0117035  -2.2052717
0:  -4.096571   -5.0606203 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.3255687  3.542693   4.0841703  4.749415   5.159032   5.240392
0:  5.247927   5.2023215  5.075179   4.933841   4.6061835  4.066484
0:  3.5272992  3.2162461  3.2796228  3.7332575  4.3662205  4.921224
0:  1.3614354  0.92392445]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.876532  -13.259359  -13.1979065 -12.598047  -11.829063  -11.1700115
0:  -10.95142   -10.925318  -11.504486  -12.1594715 -12.8743305 -13.742529
0:  -14.286037  -14.581823  -14.477499  -13.956699  -13.327833  -12.620863
0:  -13.261982  -13.8161335]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.8560557 4.933892  5.2063313 5.48695   5.5508156 5.3402176 5.016118
0:  4.588318  4.039218  3.5732014 3.0747252 2.5126696 2.1372657 2.1084576
0:  2.4933639 3.2093608 3.985121  4.5983963 3.2869878 3.5880773]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.550522  13.599808  13.695609  13.65621   13.584591  13.380714
0:  13.738054  14.075766  14.5154915 14.874992  14.715181  14.543409
0:  14.302619  14.598234  15.403183  16.564037  17.751345  18.622377
0:  20.982954  21.309576 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.010645  13.606079  13.303154  13.057083  12.820932  12.377337
0:  12.522873  12.560926  12.747473  12.860774  12.516684  12.139979
0:  11.6744175 11.624127  11.964608  12.628229  13.318618  13.728243
0:  13.932127  13.725878 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.2359095 4.2272167 4.5366683 4.987153  5.3243    5.401066  5.631691
0:  5.780281  5.8480453 5.931488  5.7314734 5.324366  4.9158306 4.806723
0:  5.036976  5.659496  6.37307   6.91352   5.5769515 5.7147665]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.249762   -3.033194   -2.4632583  -1.672833   -0.97911215 -0.5227251
0:  -0.22865534 -0.03191042 -0.15432024 -0.23972702 -0.45579052 -0.84737587
0:  -1.0952468  -1.1580615  -0.7468438   0.1257205   1.2538462   2.305512
0:   3.490509    4.2963285 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.6351476  -7.489306   -6.863197   -5.8120656  -4.7046046  -3.8974519
0:   -3.6322398  -3.7304025  -4.5266213  -5.41544    -6.418014   -7.621762
0:   -8.585505   -9.387331   -9.878342   -9.833517   -9.487509   -8.896862
0:  -10.122229  -10.449163 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.764713  11.907418  11.896553  11.577742  11.125956  10.570927
0:  10.904396  11.365866  12.094814  12.702978  12.609943  12.491682
0:  12.151831  12.444872  13.218074  14.247846  15.1177225 15.461095
0:  19.527088  19.845745 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.04417181 -0.24239445 -0.06500435  0.4618311   0.94375277  1.211617
0:   1.1246357   1.0123725   0.5414038   0.21907759 -0.10484171 -0.68552446
0:  -1.1939983  -1.6434536  -1.8139658  -1.4072657  -0.6074257   0.3224039
0:  -1.7981405  -2.2479715 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-17.955153 -18.6941   -19.078348 -19.035667 -18.805286 -18.61232
0:  -18.099503 -17.727982 -17.545681 -17.458284 -17.812004 -18.362677
0:  -18.768604 -18.662485 -18.043446 -17.063293 -16.268312 -15.744572
0:  -17.396387 -18.075146]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.711824  10.626436  10.7626    10.858959  10.837668  10.626215
0:  10.98503   11.3372345 11.734846  12.013388  11.649963  10.984268
0:  10.128923   9.737356   9.778877  10.263533  10.73414   11.030327
0:  10.297553  10.434035 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.0875525  4.0640163  4.4860206  5.1111465  5.7985716  6.349745
0:   7.156712   7.8624687  8.527993   9.065631   9.261284   9.174873
0:   9.106611   9.311494   9.923805  10.9148655 12.095842  13.12656
0:  11.378202  11.4518175]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.440756 23.67716  24.048601 24.334885 24.410046 24.221312 24.305162
0:  24.324131 24.406979 24.487064 24.219105 23.841372 23.501295 23.448452
0:  23.825672 24.57942  25.496695 26.340956 24.61771  24.919703]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4141755 2.2386298 2.4305706 3.0392418 3.6855214 4.190631  4.262037
0:  4.2536144 3.7360265 3.2777011 2.839538  2.17562   1.6282182 1.1377764
0:  0.9647999 1.2907181 2.015571  2.9085765 1.499445  1.0702138]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.089762    -9.959442    -9.790848    -9.537827    -9.233681
0:   -8.959108    -8.066212    -7.174639    -6.354634    -5.6540875
0:   -5.476113    -5.4672303   -5.4659677   -4.840072    -3.749205
0:   -2.3278203   -0.8775468    0.10598469   1.2929363    1.4817343 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-17.818008 -18.223701 -18.153824 -17.540241 -16.76205  -16.091274
0:  -15.621188 -15.433282 -15.690639 -16.01051  -16.408249 -16.898369
0:  -17.041    -16.786118 -16.039707 -15.013972 -13.955054 -13.138989
0:  -16.464779 -17.39914 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.382015 39.313366 39.17995  39.08927  38.965218 38.49384  38.627773
0:  38.520744 38.631367 38.61309  38.028126 37.521774 36.9624   36.7729
0:  36.981968 37.404434 37.91662  38.145752 36.827274 36.93593 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6719866  -3.773817   -3.442916   -2.6043272  -1.608696   -0.69050646
0:  -0.16914701  0.22413921  0.04996872 -0.12313414 -0.30512953 -0.696568
0:  -0.9243355  -1.1020069  -1.0118561  -0.485384    0.35431004  1.3057051
0:   0.17431831  0.10004616]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.061394   -9.784609   -9.08729    -8.1405525  -7.3113523  -6.8454556
0:   -6.627276   -6.7291913  -7.2224517  -7.705951   -8.322401   -9.036619
0:   -9.427292   -9.617998   -9.429453   -9.012953   -8.7260475  -8.610943
0:  -10.257044  -10.067297 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.437017 30.122711 29.607574 29.03432  28.5567   27.869751 28.136524
0:  28.281576 28.687168 28.887455 28.38464  28.008541 27.57335  27.706053
0:  28.41225  29.378363 30.424574 30.997066 32.736988 32.825512]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.131681   8.056766   8.360282   8.844977   9.171931   9.240071
0:   9.327769   9.338528   9.275064   9.249047   9.032053   8.611155
0:   8.2210045  8.0090885  8.1442175  8.703547   9.411369  10.03126
0:   8.502647   8.471141 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.979748  13.225485  13.77582   14.3970375 14.731268  14.734131
0:  14.521664  14.368894  14.096942  14.023363  13.953527  13.668793
0:  13.390005  13.112438  13.119627  13.482805  14.090406  14.652606
0:  11.77592   11.775002 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.35079  17.355427 17.48441  17.505943 17.433687 17.128046 17.560368
0:  17.906214 18.398634 18.690788 18.23919  17.593966 16.689068 16.342506
0:  16.43146  16.941292 17.526852 17.782764 16.30349  16.457445]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.058856  9.403487  9.891358 10.332109 10.564707 10.553954 10.86599
0:  11.087442 11.297171 11.529832 11.367516 11.068066 10.709113 10.76422
0:  11.131805 11.785279 12.310782 12.522709 11.45621  11.729765]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.9539   33.4668   32.98693  32.505604 32.077915 31.335155 31.267288
0:  30.973696 30.858086 30.546316 29.63847  28.843647 28.052563 27.72497
0:  27.924057 28.420702 29.027014 29.350843 29.031273 28.853573]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.439206  -6.2451487 -5.6102705 -4.701641  -3.8093758 -3.2016258
0:  -2.8461475 -2.6993542 -2.9373064 -3.1236172 -3.3767185 -3.80657
0:  -4.033391  -4.1000547 -3.9459352 -3.421112  -2.8980036 -2.450163
0:  -3.1674585 -2.9563217]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.794641  -5.7703776 -5.726765  -5.5735774 -5.581782  -5.7300954
0:  -5.045957  -4.6146655 -4.1641316 -4.1748834 -4.9403663 -6.068916
0:  -7.187673  -7.535727  -6.8412824 -5.431564  -3.6837392 -2.0966773
0:   3.9513032  5.340991 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.426144 18.296019 18.46251  18.729551 18.846687 18.72879  18.761452
0:  18.721851 18.62351  18.42352  17.861116 17.062057 16.277266 15.84676
0:  15.911955 16.3252   16.876333 17.254267 14.364632 13.947884]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.3607965  -4.4379287  -4.1154656  -3.3880801  -2.490868   -1.6516633
0:  -1.0944982  -0.66513014 -0.7578969  -0.91310215 -1.1210275  -1.5536885
0:  -1.7892327  -1.955606   -1.8490644  -1.3745751  -0.6658659   0.04335833
0:  -1.32266    -1.4089513 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5771542e-01 -5.5217028e-01 -2.0805788e-01  5.4261875e-01
0:   1.4331346e+00  2.1728187e+00  2.5646648e+00  2.7491851e+00
0:   2.2935300e+00  1.8250637e+00  1.2484303e+00  4.9891043e-01
0:   9.3746185e-04 -4.1906452e-01 -4.8075867e-01 -1.5351772e-01
0:   3.8664770e-01  1.0297809e+00  6.8146420e-01  5.5180693e-01]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.489232 14.499526 14.897145 15.582621 16.180895 16.608437 16.841427
0:  17.120613 17.134907 17.321735 17.470146 17.388494 17.351816 17.295784
0:  17.555542 18.10569  18.753841 19.236574 17.442986 17.39687 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.970585  11.751787  11.817284  12.0811825 12.283775  12.286677
0:  12.431515  12.563835  12.676039  12.906611  13.007843  13.024691
0:  13.130173  13.427633  13.980007  14.702365  15.335603  15.63646
0:  11.802455  11.43557  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.334778 -13.528208 -13.201567 -12.309837 -11.239283 -10.328707
0:   -9.900127  -9.765606 -10.293325 -10.875328 -11.48881  -12.178452
0:  -12.546818 -12.675537 -12.451681 -11.822699 -11.015968 -10.149872
0:  -11.087786 -11.409111]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.81198   6.732423  7.040557  7.538826  7.9385743 8.077741  8.130602
0:  7.995729  7.5620375 7.0676455 6.355761  5.4283423 4.617625  4.054883
0:  3.8507514 4.0943027 4.4845896 4.8022337 2.1127286 2.1228194]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.0972476   5.6246443   5.2912645   5.0646133   4.7279077   4.1878576
0:   3.389266    2.5355744   1.372724    0.4037528  -0.42539978 -1.2273507
0:  -1.7204795  -1.9924378  -1.9145465  -1.4144363  -0.6816449   0.05877161
0:  -1.4513326  -1.7387767 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.635475 11.407885 11.557983 11.894985 12.055762 11.858067 11.770592
0:  11.576893 11.423397 11.352622 11.139319 10.757551 10.476804 10.487465
0:  10.889914 11.718973 12.719311 13.540572 11.161401 11.030681]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.763926 15.961287 16.470016 17.11644  17.511084 17.49668  17.325233
0:  16.965267 16.366549 15.76479  14.918747 13.795042 12.62359  11.513799
0:  10.835822 10.698689 11.026488 11.50634   8.572643  8.039989]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.080957  -4.14965   -3.772286  -3.1104693 -2.510098  -2.1575847
0:  -1.8975224 -1.8209558 -2.0183187 -2.2795482 -2.7289844 -3.3926196
0:  -3.9165702 -4.180907  -4.1033244 -3.642551  -3.1504836 -2.8130364
0:  -5.5616097 -5.6185746]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.1453323 4.423277  5.064964  6.0054474 6.920085  7.6552753 8.133165
0:  8.506606  8.492922  8.450008  8.295574  7.885235  7.5408278 7.2879558
0:  7.315664  7.733235  8.346695  8.914064  6.8255453 6.7753296]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.076788 23.025116 23.157503 23.233849 23.028833 22.435873 21.919239
0:  21.18262  20.367107 19.480999 18.298756 16.961666 15.763027 15.012182
0:  14.816099 15.192818 15.931988 16.675499 14.20598  14.218615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.56485  16.701677 17.099482 17.527819 17.789612 17.829014 17.820198
0:  17.878942 17.818443 17.81962  17.708586 17.385372 17.11515  16.91151
0:  17.042276 17.436817 17.930851 18.27945  14.707392 14.577499]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2245302 1.0319743 1.2809525 1.5520473 1.8025308 1.7937193 2.2124615
0:  2.5547562 3.0363657 3.5417826 3.6364512 3.4850407 3.277479  3.4612472
0:  4.1272955 5.3237047 6.7142634 7.971942  7.3607564 7.388716 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.29296   7.9457865 7.7918897 7.760582  7.616688  7.360094  7.0567446
0:  6.9171996 6.641279  6.5903044 6.568993  6.3187857 6.0834217 5.831166
0:  5.6951246 5.866357  6.242792  6.565943  3.1959803 1.3737998]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8011346  -0.71593714 -0.26594973  0.42699242  1.1004434   1.4788718
0:   1.6603408   1.7276573   1.4647322   1.3870955   1.2598157   0.9337721
0:   0.7955146   0.73467445  0.92741203  1.398108    1.8202066   2.0606308
0:   1.9493012   2.3805647 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.7869034 5.84277   6.2680774 7.05768   7.934028  8.721231  9.322414
0:  9.628295  9.414422  9.030243  8.433965  7.686113  7.096462  6.665199
0:  6.540458  6.6852    6.9258714 7.1205134 3.9687965 3.782283 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.039805   7.027094   7.4837976  8.179683   8.862857   9.377433
0:   9.91241   10.363523  10.6516695 11.043908  11.319891  11.458368
0:  11.676464  11.967175  12.457535  13.061348  13.486031  13.590542
0:  10.726067  10.359866 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.8579087 5.045028  5.547642  6.1558676 6.598165  6.7506437 6.876487
0:  6.8617916 6.7306    6.6970124 6.49408   6.1268277 5.7485056 5.5117927
0:  5.5285144 5.8514333 6.2864904 6.626837  4.763184  5.0711374]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7725978  -1.6996522  -1.2415395  -0.46857166  0.25974703  0.7181883
0:   0.84700155  0.84687805  0.45701408  0.15769291 -0.1064043  -0.49748135
0:  -0.68463516 -0.75633144 -0.5919113  -0.06815052  0.5507841   1.1260686
0:  -1.4085383  -1.6743865 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9547873  -1.4564638  -0.36989832  1.0395975   2.5298357   3.907629
0:   5.3690457   6.6579895   7.6739664   8.416222    8.77717     8.807216
0:   8.885434    9.210061    9.886796   10.906382   11.962025   12.788451
0:  10.099834   10.16544   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8241405 1.8786223 2.2470722 2.956267  3.6485317 4.209453  4.4334383
0:  4.6578655 4.511478  4.468864  4.4630804 4.257222  4.102013  3.9888804
0:  4.1213813 4.729495  5.708737  6.7735653 6.5921426 6.7423544]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.61193705 0.697135   1.2000346  2.0508833  2.9291766  3.621682
0:  4.0548463  4.367352   4.247248   4.098469   3.835931   3.3733928
0:  2.9796352  2.7177486  2.7388442  3.156385   3.7758877  4.3780475
0:  3.8576946  4.0463953 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.904987  -8.542042  -7.5728717 -6.222367  -4.9559245 -4.0270143
0:  -3.32717   -2.995625  -3.1020093 -3.2213197 -3.575358  -4.130591
0:  -4.53091   -4.7074485 -4.6141744 -4.217377  -3.9082608 -3.7367473
0:  -6.6097436 -6.548228 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.100106   7.3734703  8.04367    9.007994   9.934108  10.652176
0:  11.149679  11.478985  11.450279  11.388235  11.18862   10.806965
0:  10.465488  10.228729  10.300907  10.712189  11.287859  11.790788
0:  10.357813  10.648176 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.662277  -9.892616  -9.607336  -8.756571  -7.7313895 -6.863793
0:  -6.467356  -6.280563  -6.693976  -7.1413236 -7.5804143 -8.151676
0:  -8.47624   -8.632057  -8.526701  -8.011454  -7.3345914 -6.586242
0:  -7.535504  -8.052639 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.9310713  -0.8103514  -0.4294138   0.05330896  0.3792138   0.360981
0:   0.06675816 -0.38864565 -1.2450862  -2.0726595  -3.0169787  -4.0917487
0:  -4.8884006  -5.429026   -5.5378594  -5.141792   -4.5303245  -3.906424
0:  -5.8362746  -5.928492  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.0710325  1.8966274  2.109489   2.5566053  2.919807   2.984602
0:  3.0312235  2.8626904  2.558776   2.2556376  1.8002672  1.2274752
0:  0.8019767  0.75785494 1.0926967  1.791439   2.5041723  2.9798014
0:  1.6859922  1.7442813 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.180624  10.093727  10.094566  10.026221   9.841024   9.451087
0:   9.429416   9.297164   9.224268   9.129502   8.7224865  8.2964735
0:   7.930969   8.152846   8.80892    9.812063  10.800655  11.392944
0:  11.418319  11.5323925]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.156462  -7.4172378 -7.273626  -6.8218255 -6.4045663 -6.217543
0:  -6.163984  -6.32659   -6.812016  -7.358196  -8.0472145 -8.8692
0:  -9.45517   -9.760269  -9.693367  -9.379889  -9.139683  -8.974003
0:  -9.163351  -9.310797 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.745153 21.328278 21.10526  20.993937 20.916998 20.568272 20.787516
0:  20.818666 20.991941 21.051172 20.629889 20.13657  19.585917 19.3706
0:  19.59412  20.20863  21.020794 21.659409 21.111776 20.874966]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [49.082222 49.14609  48.600258 47.88617  47.06116  46.09862  46.397423
0:  46.63521  47.279778 47.46965  46.859505 46.205547 45.469578 45.46313
0:  46.224724 47.154175 47.94704  47.80096  47.40433  47.891663]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.093443  9.97446  10.099615 10.328339 10.541853 10.659712 10.835602
0:  11.003407 11.029376 11.114798 11.098903 11.008609 11.068207 11.308017
0:  11.860738 12.610258 13.295202 13.766455 11.840609 12.028136]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.167446  11.473534  12.065575  12.729044  13.170189  13.294357
0:  13.293907  13.235854  13.072075  13.006252  12.8897915 12.62075
0:  12.435808  12.404257  12.78112   13.513359  14.467691  15.316853
0:  13.0821    13.255853 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.9329844  7.865812   7.9126186  7.9692435  7.9847407  7.9065323
0:   8.517822   9.161577  10.01835   10.828344  11.135499  11.3402
0:  11.402294  11.992733  13.03844   14.425249  15.769957  16.692007
0:  18.888556  18.800987 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.679768  8.454669  8.638817  9.085854  9.489792  9.649624  9.595644
0:  9.382887  8.8575945 8.3527155 7.742833  6.969509  6.3240047 5.836332
0:  5.696328  5.9534593 6.3219743 6.5779157 4.012762  3.8386192]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.0235205  -6.044013   -5.7880173  -5.4124413  -5.1538925  -5.1690893
0:  -5.1058226  -5.057374   -4.8649454  -4.617646   -4.4554825  -4.460906
0:  -4.3316107  -3.8693018  -3.022169   -2.0094013  -1.2429323  -0.77074623
0:  -0.00681448  0.1725955 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.68195   16.567638  16.848015  17.343533  17.679646  17.652313
0:  17.824493  17.801208  17.728546  17.641216  17.176762  16.475477
0:  15.712704  15.199445  15.153546  15.604608  16.36141   17.028957
0:  13.8011265 13.511228 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8277316 -3.0411458 -2.7700944 -2.0536485 -1.3844151 -1.0040574
0:  -1.0221138 -1.3010106 -2.1035972 -2.8988109 -3.7602277 -4.7894673
0:  -5.5432982 -6.063165  -6.161347  -5.726835  -5.053452  -4.363621
0:  -6.258716  -6.47378  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.42244768  0.13526392  0.2382617   0.72036314  1.1614656   1.3824296
0:   1.2483373   0.9316373   0.16901445 -0.5725398  -1.3480587  -2.326025
0:  -3.1024237  -3.7115154  -3.9483929  -3.6388416  -2.983801   -2.1991825
0:  -3.5299568  -3.7233171 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6668258  -2.7548738  -2.48737    -1.701798   -0.6755738   0.33907843
0:   0.99812746  1.5538821   1.5204501   1.4540372   1.4200268   1.3212199
0:   1.4874439   1.7231245   2.2536125   3.074739    4.0945754   5.0937624
0:   5.6599407   5.7868495 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3115234 -1.850544  -1.9547873 -1.5808697 -1.1525993 -0.8915324
0:  -1.0961909 -1.4143677 -2.288046  -3.1021624 -3.877366  -4.8762693
0:  -5.632457  -6.2338295 -6.453944  -6.0678835 -5.300295  -4.3817205
0:  -5.265294  -5.715581 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.930934  13.164869  13.687771  14.27363   14.743994  14.925281
0:  15.182163  15.228226  15.00144   14.688623  14.025404  13.252209
0:  12.547312  12.150658  12.081818  12.312131  12.619205  12.696715
0:  10.0320215 10.084415 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.991858   -1.2022243  -0.94538736 -0.2642727   0.41876554  0.861958
0:   0.9110155   0.74255466  0.11014652 -0.4748435  -1.0223031  -1.6704869
0:  -2.067348   -2.2625384  -2.1217256  -1.5204768  -0.68341637  0.18534613
0:  -1.3156075  -1.6288733 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.09299  34.424072 34.945866 35.351727 35.375797 35.009697 34.530243
0:  33.951874 33.161236 32.399673 31.32895  30.135872 28.977758 28.003017
0:  27.30734  26.781494 26.192497 25.33995  21.71765  21.346392]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.608516   4.8366585  5.4886975  6.25718    6.820165   7.129863
0:   7.346758   7.4867697  7.5093365  7.60392    7.643209   7.5261064
0:   7.55238    7.7085814  8.202804   9.017697   9.929995  10.727409
0:   8.638197   8.970772 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.11436   6.096148  6.473162  7.0399766 7.457573  7.5574617 7.623066
0:  7.590582  7.4278665 7.281212  7.0000763 6.5418334 6.1455464 5.9715667
0:  6.143173  6.697427  7.3705063 7.8985066 5.2121015 5.1482005]
0: validation loss for strategy=forecast at epoch 44 : 0.3487064242362976
0: validation loss for velocity_u : 0.19036614894866943
0: validation loss for velocity_v : 0.283584326505661
0: validation loss for specific_humidity : 0.1699668914079666
0: validation loss for velocity_z : 0.6133601069450378
0: validation loss for temperature : 0.11451222747564316
0: validation loss for total_precip : 0.7204490303993225
0: 45 : 20:57:13 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 45, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1553, -0.1416, -0.1290, -0.1177, -0.1084, -0.1018, -0.0989, -0.1003, -0.1060, -0.1160, -0.1293, -0.1453,
0:         -0.1631, -0.1816, -0.2002, -0.2186, -0.2371, -0.2565, -0.2074, -0.2030, -0.1992, -0.1961, -0.1943, -0.1943,
0:         -0.1966], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1827, 0.1971, 0.2156, 0.2378, 0.2623, 0.2881, 0.3141, 0.3401, 0.3651, 0.3885, 0.4091, 0.4263, 0.4395, 0.4485,
0:         0.4531, 0.4533, 0.4489, 0.4395, 0.2013, 0.2152, 0.2315, 0.2502, 0.2703, 0.2910, 0.3120], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7979, -0.7964, -0.7960, -0.7959, -0.7964, -0.7972, -0.7986, -0.7999, -0.8020, -0.8034, -0.8053, -0.8068,
0:         -0.8080, -0.8095, -0.8101, -0.8111, -0.8119, -0.8131, -0.8050, -0.8045, -0.8044, -0.8041, -0.8047, -0.8056,
0:         -0.8066], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1662, 0.1839, 0.2304, 0.2945, 0.3698, 0.4516, 0.5313, 0.5999, 0.6397, 0.6397, 0.6021, 0.5468, 0.4915, 0.4428,
0:         0.4074, 0.3897, 0.3985, 0.4207, 0.2149, 0.2127, 0.2414, 0.2923, 0.3521, 0.4118, 0.4693], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.6216, -1.6272, -1.6305, -1.6313, -1.6299, -1.6260, -1.6196, -1.6109, -1.6009, -1.5903, -1.5795, -1.5689,
0:         -1.5590, -1.5500, -1.5414, -1.5328, -1.5231, -1.5123, -1.5003, -1.4880, -1.4762, -1.4656, -1.4563, -1.4488,
0:         -1.4425], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2537, -0.2537, -0.2514, -0.2514, -0.2468, -0.2422, -0.2330, -0.2191, -0.2052, -0.2537, -0.2537, -0.2514,
0:         -0.2514, -0.2468, -0.2422, -0.2330, -0.2168, -0.2006, -0.2537, -0.2537, -0.2537, -0.2514, -0.2468, -0.2422,
0:         -0.2330], device='cuda:0')
0: [DEBUG] Epoch 45, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,  0.0072,     nan,     nan,     nan,     nan, -0.0182,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.0072,     nan, -0.0228,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0505,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1567,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1498,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0713,
0:             nan,     nan,     nan,     nan,     nan, -0.1036, -0.0297,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1337,     nan,
0:         -0.0390,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0967,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1036,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2006,     nan,
0:             nan,     nan, -0.1752,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2168,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0932,     nan,     nan,     nan, -0.1533,     nan, -0.1683,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0909,     nan,     nan,
0:             nan,     nan,     nan, -0.1729,     nan, -0.1937,     nan, -0.1140,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1579,     nan,     nan,     nan,     nan, -0.2041,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2133,     nan,     nan, -0.2145,     nan,     nan,
0:             nan,     nan,     nan, -0.2295, -0.2307, -0.2341,     nan,     nan,     nan, -0.2341,     nan,     nan,
0:         -0.2387,     nan,     nan])
0: [DEBUG] Epoch 45, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8428, -0.8743, -0.8730, -0.8430, -0.8207, -0.8197, -0.8593, -0.9016, -0.9778, -1.0372, -1.0935, -1.1701,
0:         -1.2459, -1.3106, -1.3606, -1.3653, -1.3487, -1.3240, -0.8869, -0.9856, -1.0337, -1.0346, -1.0066, -0.9864,
0:         -0.9792], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0885, -0.0476, -0.0524, -0.0830, -0.1240, -0.1570, -0.1732, -0.1687, -0.1295, -0.0866, -0.0433, -0.0117,
0:         -0.0162, -0.0488, -0.0926, -0.1157, -0.1127, -0.0881, -0.1511, -0.0839, -0.0565, -0.0623, -0.0954, -0.1359,
0:         -0.1547], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8330, -0.8288, -0.8244, -0.8177, -0.8059, -0.7984, -0.7989, -0.7989, -0.8006, -0.8044, -0.8113, -0.8121,
0:         -0.8071, -0.7989, -0.7961, -0.7922, -0.7970, -0.7918, -0.8170, -0.8055, -0.7987, -0.7871, -0.7838, -0.7849,
0:         -0.7840], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2281, -0.1364, -0.0060,  0.0013,  0.0280,  0.0763,  0.0913,  0.1044,  0.0845,  0.0275, -0.0793, -0.1282,
0:         -0.0433, -0.1039, -0.3138, -0.3497, -0.3619, -0.5237, -0.2059, -0.3210, -0.2515, -0.1975, -0.0839,  0.0163,
0:          0.0437], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.8137, -0.8200, -0.7931, -0.7419, -0.6826, -0.6355, -0.6066, -0.5892, -0.5738, -0.5526, -0.5353, -0.5213,
0:         -0.5107, -0.4889, -0.4587, -0.4159, -0.3733, -0.3251, -0.2724, -0.2109, -0.1491, -0.0942, -0.0484, -0.0091,
0:          0.0396], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0950,  0.1046,  0.0770,  0.0826,  0.0709,  0.0710,  0.0557,  0.0496,  0.0438,  0.0933,  0.0893,  0.0651,
0:          0.0615,  0.0447,  0.0310,  0.0267,  0.0268,  0.0077,  0.0583,  0.0547,  0.0547,  0.0229,  0.0130, -0.0061,
0:         -0.0201], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.20951597392559052; velocity_v: 0.3509083390235901; specific_humidity: 0.18871232867240906; velocity_z: 0.46290940046310425; temperature: 0.20613163709640503; total_precip: 0.5485968589782715; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21854862570762634; velocity_v: 0.39818957448005676; specific_humidity: 0.17199821770191193; velocity_z: 0.5740755200386047; temperature: 0.1445416510105133; total_precip: 0.6216325759887695; 
0: epoch: 45 [1/5 (20%)]	Loss: 0.58511 : 0.30651 :: 0.21014 (2.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2384408712387085; velocity_v: 0.34410184621810913; specific_humidity: 0.2003379762172699; velocity_z: 0.5544315576553345; temperature: 0.14759182929992676; total_precip: 0.5271943807601929; 
0: epoch: 45 [2/5 (40%)]	Loss: 0.52719 : 0.30090 :: 0.21554 (15.72 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22582721710205078; velocity_v: 0.3715127110481262; specific_humidity: 0.16227278113365173; velocity_z: 0.5316323637962341; temperature: 0.12325628101825714; total_precip: 0.5651769638061523; 
0: epoch: 45 [3/5 (60%)]	Loss: 0.56518 : 0.29639 :: 0.21484 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22858701646327972; velocity_v: 0.4001978039741516; specific_humidity: 0.1796104609966278; velocity_z: 0.6817751526832581; temperature: 0.1608816236257553; total_precip: 0.4964083433151245; 
0: epoch: 45 [4/5 (80%)]	Loss: 0.49641 : 0.32300 :: 0.21178 (16.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07
0:  4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 9.5367432e-07 2.3841858e-06
0:  2.3841858e-06 9.5367432e-07 4.7683716e-07 4.7683716e-07 4.7683716e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 4.7683716e-07 2.3841858e-06 3.3378601e-06
0:  4.7683716e-07 4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 4.7683716e-07 1.4305115e-06 2.8610229e-06
0:  2.3841858e-05 5.2452087e-05 1.3732910e-04 9.5844269e-05 2.8133392e-05
0:  5.1975250e-05 2.2554398e-04 5.6838989e-04 2.6702881e-04 6.5803528e-05
0:  2.3365021e-05 3.4809113e-05 4.3869022e-05 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 1.9073486e-06 1.9073486e-06 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 4.7683716e-07 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 2.3841858e-06 6.1988831e-06 1.1444092e-05 6.1988831e-06
0:  3.3378601e-06 2.8610229e-06 2.3841858e-06 1.9073486e-06 9.5367432e-07
0:  9.5367432e-07 1.4305115e-06 1.4305115e-06 1.4305115e-06 1.9073486e-06
0:  2.3841858e-06 1.7166138e-05 1.7166138e-05 2.8610229e-06 1.9073486e-06
0:  1.4305115e-06 1.4305115e-06 1.4305115e-06 1.9073486e-06 1.9073486e-06
0:  1.9073486e-06 1.4305115e-06 1.4305115e-06 9.5367432e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  4.7683716e-07 4.7683716e-07 4.7683716e-07 9.5367432e-07 9.5367432e-07
0:  9.5367432e-07 4.7683716e-07 4.7683716e-07 4.7683716e-07 9.5367432e-07
0:  9.5367432e-07 9.5367432e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  4.7683716e-07 4.7683716e-07 4.7683716e-07 0.0000000e+00 0.0000000e+00]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 1.4305115e-06 2.1457672e-05 7.7724457e-05
0:  1.0108948e-04 1.0681152e-04 8.1062317e-05 5.5313110e-05 3.2901764e-05
0:  5.7697296e-05 9.4413757e-05 1.3828278e-04 1.5258789e-04 2.0456314e-04
0:  3.7622452e-04 3.7097931e-04 3.3950806e-04 2.7513504e-04 2.6226044e-04
0:  2.6130676e-04 2.6178360e-04 3.0994415e-04 2.3412704e-04 1.9454956e-04
0:  2.0313263e-04 1.8501282e-04 1.3732910e-04 6.4849854e-05 3.1471252e-05
0:  1.7642975e-05 1.1444092e-05 4.7683716e-06 1.9073486e-06 9.5367432e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 4.7683716e-07 4.7683716e-07 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 2.3841858e-06 5.2452087e-06
0:  1.0013580e-05 9.0599060e-06 5.7220459e-06 3.8146973e-06 2.8610229e-06
0:  4.2915344e-06 7.6293945e-06 1.1444092e-05 1.8596649e-05 2.4318695e-05
0:  3.0040741e-05 4.0054321e-05 4.6730042e-05 4.5299530e-05 3.3855438e-05
0:  2.0027161e-05 1.4305115e-05 1.0967255e-05 9.5367432e-06 1.2874603e-05
0:  3.9100647e-05 1.2493134e-04 1.6832352e-04 2.1219254e-04 2.3031235e-04
0:  2.0790100e-04 1.5640259e-04 2.2029877e-04 1.9073486e-04 9.0599060e-06
0:  2.8610229e-06 1.4305115e-06 9.5367432e-07 9.5367432e-07 1.4305115e-06
0:  5.2452087e-06 5.2452087e-06 1.3828278e-05 1.4305115e-05 1.2397766e-05
0:  1.2493134e-04 1.5020370e-04 1.2779236e-04 5.1975250e-05 5.2452087e-06
0:  4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 1.8596649e-05 4.2915344e-05 7.5817108e-05 6.2465668e-05
0:  4.4822693e-05 2.0503998e-05 1.7166138e-05 3.8146969e-05 8.0108643e-05
0:  9.9658966e-05 1.4352798e-04 2.1266937e-04 2.2840500e-04 2.8514862e-04
0:  3.9100647e-04 3.6907196e-04 4.2915344e-04 5.7601929e-04 3.8766861e-04
0:  2.9182434e-04 2.7656555e-04 2.0790100e-04 2.0885468e-04 2.7751923e-04
0:  2.2792816e-04 1.7786026e-04 9.9182129e-05 7.6770782e-05 4.4345856e-05
0:  2.5272369e-05 1.3828278e-05 4.7683716e-06 1.4305115e-06 9.5367432e-07
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07 1.4305115e-06
0:  2.8610229e-06 9.5367432e-07 4.7683716e-07 4.7683716e-07 4.7683716e-07
0:  4.7683716e-07 9.5367432e-07 2.8610229e-06 3.3378601e-06 1.9073486e-06
0:  1.4305115e-06 9.5367432e-07 9.5367432e-07 2.8610229e-06 3.8146973e-06
0:  3.8146973e-06 4.2915344e-06 5.2452087e-06 6.6757202e-06 6.6757202e-06
0:  6.6757202e-06 6.6757202e-06 5.7220459e-06 5.2452087e-06 4.7683716e-06
0:  4.2915344e-06 5.7220459e-06 8.1062317e-06 1.4305115e-05 4.2915344e-05
0:  8.4400177e-05 1.5878677e-04 1.7309189e-04 1.4877319e-04 5.1021576e-05
0:  5.7220459e-06 1.9073486e-06 4.7683716e-07 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [14.995372 14.79233  14.740392 14.794634 14.749224 14.346178 14.275832
0:  13.944306 13.586053 13.107315 12.222061 11.311043 10.348679  9.900689
0:   9.82302  10.102356 10.476581 10.656182  7.540263  6.87403 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.408, max = 3.192, mean = 0.353
0:          sample (first 20): tensor([0.6513, 0.6346, 0.6303, 0.6348, 0.6310, 0.5977, 0.5919, 0.5645, 0.5349, 0.4953, 0.4222, 0.3469, 0.2674, 0.2304,
0:         0.2239, 0.2470, 0.2779, 0.2928, 0.6721, 0.6701])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.5331087 -7.721423  -7.34243   -6.4495916 -5.4959297 -4.7877903
0:  -4.3490934 -4.1915607 -4.5504694 -5.0413313 -5.744211  -6.6265707
0:  -7.2592497 -7.5335407 -7.3479395 -6.691662  -5.9686604 -5.438946
0:  -7.8777924 -8.174584 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.843395 19.04084  19.381107 19.745907 20.04906  20.36669  21.1017
0:  21.858578 22.654892 23.370947 23.66843  23.905142 24.20309  24.907753
0:  26.08379  27.404081 28.62372  29.46853  26.837557 27.230742]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.653868 10.928399 11.524056 12.212505 12.693209 12.881427 12.875663
0:  12.87291  12.645774 12.520195 12.358444 11.969218 11.65831  11.4095
0:  11.463348 11.901274 12.546797 13.156872 10.157826 10.391048]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.115204 38.737816 38.2006   37.73321  37.324    36.706745 36.93097
0:  36.99327  37.38191  37.576473 37.09142  36.6545   36.24204  36.198177
0:  36.702454 37.374695 38.180557 38.675724 39.82327  39.771137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.4111176 5.4453545 5.8010902 6.5092616 7.139806  7.569271  7.5977993
0:  7.573695  7.043374  6.5115137 5.852369  4.8307633 3.8574023 2.9293106
0:  2.399935  2.5339875 3.1932054 4.06126   0.7535181 0.2139883]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9204025  -1.9431553  -1.60116    -1.0625691  -0.6080203  -0.4251256
0:  -0.20519829 -0.13573456 -0.16881037 -0.19891214 -0.44132757 -0.86868715
0:  -1.2205691  -1.3336515  -1.1726713  -0.6420841  -0.09970856  0.33596563
0:  -0.94057083 -0.9221482 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.794442  4.998077  5.518821  6.224392  6.8144345 7.129738  7.7084966
0:  8.039128  8.213917  8.176538  7.673762  6.9180713 6.201539  6.0302734
0:  6.353738  7.070291  7.80445   8.262152  6.8250947 6.7121997]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [44.237976 44.092247 43.6552   43.035538 42.291718 41.262966 41.147926
0:  40.955524 41.03786  40.924393 39.96102  39.018383 37.925655 37.43305
0:  37.524685 37.847984 38.161934 37.964005 37.206486 37.31871 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.0945067 6.3569393 7.00306   7.786285  8.287085  8.428068  8.3373995
0:  8.288204  8.06982   8.030973  7.979128  7.6698103 7.418308  7.211211
0:  7.3364334 7.9548826 8.865742  9.770393  7.7891498 7.8931303]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.07303619 -0.17719316  0.11569643  0.8157444   1.6136708   2.3159447
0:   2.6583495   2.879694    2.55341     2.2070003   1.8011966   1.2455735
0:   0.8377361   0.5613723   0.5615878   0.94741106  1.522037    2.095366
0:  -0.32061672 -0.90578794]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.054203   0.8162627  0.75669384 0.81117535 0.8396435  0.7543783
0:  0.8544073  0.92907    0.9834018  1.0821524  1.0369587  0.89419746
0:  0.89034843 1.2455325  2.0977983  3.373542   4.9342594  6.5288134
0:  7.863756   8.430536  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.2990575   3.6740363   3.5173628   3.6018462   3.6503394   3.522065
0:   3.2332544   2.9140384   2.3419158   1.800621    1.1419868   0.26215887
0:  -0.5131531  -1.0715303  -1.2396107  -1.0139742  -0.62420225 -0.2590108
0:  -2.3622813  -2.9449    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.0226245 -5.7856812 -5.1276994 -4.1570907 -3.2048984 -2.5018153
0:  -2.1404061 -2.0560317 -2.4489942 -2.891255  -3.3830123 -3.937055
0:  -4.1804967 -4.2067857 -3.9427085 -3.42487   -2.9020658 -2.468038
0:  -3.8907151 -3.9041123]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4079347  -3.2899003  -2.7847428  -2.0206475  -1.2892022  -0.7881594
0:  -0.51382494 -0.37809992 -0.64143896 -0.8796835  -1.2152638  -1.7334867
0:  -2.0411754  -2.2370296  -2.1408124  -1.6946645  -1.1639514  -0.61639357
0:  -1.8296409  -1.7295642 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.8058705 -7.1218753 -6.985064  -6.429156  -5.873132  -5.5273356
0:  -5.4740596 -5.5535655 -5.971476  -6.371302  -6.748383  -7.2601647
0:  -7.5678573 -7.584801  -7.2034516 -6.411884  -5.438564  -4.5046735
0:  -4.0270925 -3.8253465]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.70944405 -0.9009142  -0.79413223 -0.3552041   0.18317413  0.6385064
0:   0.7718687   0.8370881   0.3312359  -0.22366571 -0.8469491  -1.5890908
0:  -2.141055   -2.480011   -2.496172   -2.1153965  -1.5921946  -1.1190495
0:  -2.30972    -2.5576658 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.200323   9.12797    9.36421    9.768505  10.068443  10.145012
0:  10.262398  10.270977  10.101305   9.8950615  9.399168   8.726654
0:   8.100307   7.7818556  7.835766   8.210295   8.623976   8.791051
0:   6.025509   5.7810154]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4687824 3.471541  3.8407342 4.45023   4.9988174 5.306927  5.379987
0:  5.3435984 4.9364133 4.546494  4.092758  3.4685702 3.0109074 2.6624126
0:  2.6023011 2.9160297 3.4121966 3.8885145 1.8908238 1.8578463]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.328224  -11.432236  -11.001965   -9.98263    -8.740341   -7.623881
0:   -6.98167    -6.556127   -6.7966313  -7.0971103  -7.4352107  -7.898356
0:   -8.132193   -8.192066   -7.998465   -7.4266534  -6.7102456  -5.975188
0:   -6.578632   -6.772796 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.653614 27.127354 27.476036 27.629684 27.579468 27.291634 27.757969
0:  28.189575 28.806889 29.282436 29.109493 28.83302  28.454605 28.587746
0:  29.162914 30.024414 30.886219 31.319088 30.69939  31.030998]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.578505  -8.70257   -8.353706  -7.5532374 -6.678826  -6.031937
0:  -5.8095813 -5.833013  -6.413112  -6.997394  -7.6122518 -8.292019
0:  -8.686512  -8.85302   -8.722317  -8.21685   -7.6131654 -7.0172305
0:  -7.7612977 -7.950565 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.7499948  7.7128253  7.924017   8.195265   8.449621   8.525364
0:   8.9092     9.240823   9.618676   9.950066   9.987368   9.908995
0:   9.836024  10.077264  10.709938  11.686891  12.7405205 13.6093235
0:  12.606518  12.847149 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.812119  -10.022787   -9.697855   -8.871433   -7.9313016  -7.181339
0:   -6.8520513  -6.787413   -7.360947   -7.942165   -8.566504   -9.323631
0:   -9.693371   -9.86977    -9.6449585  -9.025974   -8.31122    -7.5872407
0:   -8.423126   -8.605938 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.572191   -4.6131916  -4.237588   -3.5554276  -2.7736616  -2.1480255
0:  -1.4410477  -0.93343353 -0.78926086 -0.7154312  -0.9719162  -1.4310541
0:  -1.6572876  -1.6377172  -1.2180176  -0.53919363  0.03863049  0.41360617
0:  -3.298521   -3.3797703 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.762728  -6.2965236 -6.3450923 -5.9829135 -5.577269  -5.363338
0:  -5.276644  -5.2599463 -5.5059114 -5.731053  -6.042382  -6.5381694
0:  -6.8633466 -6.9627404 -6.7355094 -6.0743194 -5.3236613 -4.6395288
0:  -6.265631  -6.7157702]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.208696 28.525757 28.817741 28.93212  28.875671 28.59687  28.760412
0:  28.85566  29.019016 29.066477 28.593235 28.094757 27.558105 27.516216
0:  27.938334 28.594786 29.281387 29.64395  28.419107 28.621569]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.6966612 2.743687  3.1696045 3.814144  4.380809  4.701814  4.705564
0:  4.5917673 4.143442  3.755454  3.3418734 2.7677958 2.338296  2.0466626
0:  2.0680199 2.5273416 3.2167833 3.8916988 1.7097282 1.6876922]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.831756  -7.937613  -7.6146655 -6.771683  -5.69239   -4.7513833
0:  -4.1364036 -3.791142  -4.122045  -4.5462604 -5.1123757 -5.792852
0:  -6.196943  -6.4067607 -6.1981797 -5.711145  -5.110172  -4.6782117
0:  -4.9473643 -4.965834 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.170994 25.023945 25.044147 25.004215 24.752647 24.160957 23.714188
0:  23.150902 22.67163  22.311634 21.841366 21.37379  21.075489 21.009224
0:  21.31137  21.844654 22.468948 22.928322 20.969883 20.853706]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1109028  -2.1401062  -1.8068771  -1.1612387  -0.4307356   0.15027523
0:   0.49564457  0.68332005  0.33368397 -0.00243759 -0.4818554  -1.1314483
0:  -1.5959377  -1.9031     -1.882268   -1.4438543  -0.81741047 -0.07774115
0:  -0.24641228 -0.38538074]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.7165389   1.8323107   2.2896438   3.0352309   3.7325156   4.1811647
0:   4.251889    4.1545706   3.505243    2.8728306   2.167737    1.304729
0:   0.68302965  0.2574849   0.2188096   0.6790619   1.3922596   2.15379
0:  -0.11593246 -0.4112811 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.49052     0.71335125  1.2834439   2.032432    2.6192021   2.8425884
0:   2.731952    2.3808067   1.655292    1.0581245   0.45834398 -0.17651463
0:  -0.55807924 -0.74870825 -0.6464324  -0.2857771   0.08534813  0.34531164
0:  -2.2535748  -2.2327323 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.08524   6.1333213 6.5068135 7.1215734 7.65088   7.9682117 8.019431
0:  8.011021  7.626574  7.281404  6.846512  6.142587  5.500297  4.9081883
0:  4.628415  4.8340626 5.3677034 5.968693  4.9767227 4.9898796]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.83536530e+00 -4.68864918e+00 -4.13550282e+00 -3.18261623e+00
0:  -2.10833168e+00 -1.17013454e+00 -4.72748280e-01  1.23310089e-03
0:  -1.19623184e-01 -3.65761280e-01 -8.63613605e-01 -1.60370064e+00
0:  -2.19263744e+00 -2.66015625e+00 -2.85491848e+00 -2.65736675e+00
0:  -2.34319019e+00 -2.03678131e+00 -4.74451256e+00 -5.00604820e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.8130264  6.958477   7.1349382  7.0521727  6.7287803  6.219179
0:   6.3358297  6.562776   7.032976   7.5186143  7.471408   7.2990603
0:   6.898659   6.9711785  7.406293   8.109437   8.65444    8.811863
0:  10.299653  10.587328 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3499427 -5.1222205 -4.4530234 -3.524259  -2.7285962 -2.2641912
0:  -2.061358  -2.1021342 -2.5657449 -3.066481  -3.7194839 -4.5758805
0:  -5.205477  -5.522368  -5.4339175 -4.8849874 -4.2589383 -3.7382255
0:  -5.112472  -5.00063  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.6403341   0.48538303  0.6149454   0.85133076  0.8699856   0.54845905
0:   0.37952614  0.05032682 -0.29628277 -0.6941519  -1.4605408  -2.4602485
0:  -3.5454926  -4.19533    -4.4441466  -4.2796903  -4.035052   -3.9468093
0:  -5.7750974  -6.020822  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.556124  8.42573   8.308283  8.006166  7.510978  6.7219615 6.3626785
0:  5.921707  5.4748373 5.0160136 4.107894  3.1609974 2.2303538 2.0040865
0:  2.3133092 3.1570096 4.085541  4.7499537 6.2974496 6.494207 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.482137  4.2596045 4.2924166 4.3373723 4.2639713 3.9917176 4.3316336
0:  4.720366  5.287516  5.8651543 5.8910117 5.728136  5.3423033 5.5051126
0:  6.1190605 7.17529   8.191684  8.816603  8.362261  8.610385 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.04367  24.873207 24.520426 24.142998 23.81839  23.362673 23.88601
0:  24.277494 24.954762 25.397278 25.132862 24.886196 24.5811   24.917093
0:  25.872013 27.129436 28.410131 29.102274 30.002214 30.392752]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.033127   -8.932753   -8.340162   -7.2954946  -6.1677833  -5.275743
0:   -4.825601   -4.7445226  -5.368216   -6.1275764  -7.054321   -8.153
0:   -8.997559   -9.668844  -10.024952   -9.991787   -9.760487   -9.411146
0:  -11.143183  -11.35788  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.02709   19.770823  19.587078  19.400509  18.957722  18.265358
0:  17.405521  16.622005  15.7423725 15.067086  14.470196  13.758556
0:  13.194561  12.775848  12.77245   13.206804  13.974643  14.786184
0:  12.057228  11.834171 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.31499   -7.8141403 -7.8571672 -7.3419466 -6.664316  -6.0484266
0:  -5.8616514 -5.762685  -6.158453  -6.513425  -6.797987  -7.2848206
0:  -7.601021  -7.818159  -7.8020353 -7.2977495 -6.470393  -5.4740133
0:  -6.180583  -6.6201553]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7074919 2.9249861 3.3712764 3.8461525 4.171941  4.2170196 4.5514097
0:  4.7393513 4.8794913 4.9593887 4.6013684 4.124058  3.6052654 3.6154525
0:  4.0581427 4.924567  5.7529974 6.323674  5.6345124 5.7571244]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.832822  -14.879565  -14.360414  -13.338986  -12.1871805 -11.277808
0:  -10.880387  -10.783095  -11.341176  -11.929957  -12.548915  -13.251819
0:  -13.5571165 -13.657886  -13.396916  -12.831363  -12.236583  -11.697055
0:  -13.072942  -13.260678 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-20.770538 -20.669285 -20.021957 -18.841125 -17.469059 -16.233315
0:  -15.40368  -14.800937 -14.793866 -14.841966 -14.968841 -15.248025
0:  -15.279534 -15.115207 -14.55782  -13.58014  -12.451924 -11.354268
0:  -11.950543 -11.877948]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.848144  -5.12888   -4.9697995 -4.321378  -3.5591693 -2.9426084
0:  -2.7866006 -2.7800946 -3.327639  -3.8129277 -4.2310224 -4.784653
0:  -5.0882783 -5.3193097 -5.244555  -4.712993  -3.885056  -2.9101481
0:  -3.7406468 -3.7949538]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.67042   -15.95948   -15.64266   -14.74828   -13.700037  -12.855883
0:  -12.479485  -12.41387   -13.026367  -13.691189  -14.431377  -15.349705
0:  -15.865112  -16.141434  -16.050108  -15.443556  -14.715773  -13.921108
0:  -15.8616085 -16.302164 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.47902  30.330967 30.246483 30.017975 29.794605 29.482327 30.033085
0:  30.630695 31.518955 32.303173 32.37707  32.394104 32.317333 32.655678
0:  33.480225 34.62695  35.791973 36.66932  37.012997 37.304527]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.0431175 4.2420816 4.8525896 5.697447  6.4049993 6.803625  7.003204
0:  7.054295  6.752325  6.4858017 6.0755777 5.4713793 5.0064936 4.653456
0:  4.621331  4.985709  5.480089  5.9493704 4.119028  4.3150578]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.1964726 -5.767202  -6.1283746 -6.1523223 -5.977568  -5.719645
0:  -5.528848  -5.35508   -5.6825385 -6.1822143 -6.792578  -7.520205
0:  -7.838348  -7.7631946 -7.2149367 -6.287191  -5.395741  -4.7136636
0:  -5.4303927 -5.466824 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.9855585 1.978581  2.304151  2.862263  3.278794  3.4311583 3.5300024
0:  3.5231652 3.3341932 3.2418196 3.036408  2.663403  2.3913288 2.3522696
0:  2.6587455 3.386942  4.266987  5.046252  4.40831   4.5455513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.836239 -14.956516 -14.561617 -13.619345 -12.519575 -11.622875
0:  -11.193285 -11.075432 -11.650988 -12.293673 -13.017445 -13.789101
0:  -14.201582 -14.360088 -14.17687  -13.675211 -13.112257 -12.567476
0:  -12.987949 -13.361288]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.087609  7.796764  7.879711  8.07383   8.001467  7.5203485 7.0400333
0:  6.559476  6.1139283 5.8588133 5.5937543 5.1444798 4.809205  4.706188
0:  4.9628115 5.730618  6.737532  7.686076  5.9725437 6.0795517]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2790751  1.2241702  1.5792623  2.192286   2.67112    2.8643565
0:  2.9350371  2.939664   2.751084   2.6216009  2.3936095  1.9237118
0:  1.4961596  1.1860294  1.1460972  1.5041175  2.0660877  2.5820854
0:  0.92077255 0.93916464]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.90184  23.900146 23.945663 23.899883 23.849308 23.543392 23.791225
0:  23.890556 24.04166  24.110527 23.679611 23.194492 22.654148 22.522831
0:  22.80618  23.487843 24.352291 25.0708   23.790897 23.826302]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0548344  -1.4191194  -1.4652858  -1.174644   -0.89794636 -0.77716875
0:  -0.97039366 -1.1091156  -1.5490193  -1.8016114  -1.9400668  -2.2712636
0:  -2.4753814  -2.5918756  -2.4125743  -1.7002397  -0.6434145   0.48155022
0:  -0.49729443 -0.5701771 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.9649949 2.0577202 2.6709886 3.581421  4.4069357 4.9663725 5.4717913
0:  5.834793  5.989032  6.1858816 6.2172575 5.97354   5.8288136 5.8528314
0:  6.252906  7.1362534 8.220212  9.22661   7.7845926 7.8161154]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.294002  -5.569185  -5.3679967 -4.6339564 -3.621139  -2.6489778
0:  -2.0391755 -1.609549  -1.869823  -2.2003474 -2.6520438 -3.3087993
0:  -3.7188706 -4.023457  -3.9813085 -3.5154738 -2.7691436 -1.9527249
0:  -3.4027314 -3.773087 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.239555  15.522655  16.094002  16.684696  16.877272  16.661041
0:  16.159443  15.819458  15.401191  15.281653  15.226839  14.8563175
0:  14.416439  13.848166  13.551804  13.76273   14.486547  15.402105
0:  12.333048  12.489409 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.53396606 0.57838106 0.9998226  1.7198524  2.4699125  2.9793224
0:  3.2338314  3.3275743  2.947358   2.537541   2.0256405  1.3514123
0:  0.9295397  0.677093   0.7575841  1.1517029  1.5931072  1.9668684
0:  0.20713043 0.11228895]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.423021  8.954744  8.719694  8.663481  8.620325  8.365055  8.378685
0:  8.309192  8.133481  7.8564286 7.0934505 6.0977507 4.9219913 4.095146
0:  3.6698787 3.5714054 3.55706   3.4066489 2.9929788 1.4455447]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.748877  12.582927  12.732128  12.963676  12.997145  12.719714
0:  12.346598  11.975996  11.43223   10.905416  10.153637   9.040429
0:   7.917486   7.0151243  6.684407   7.0916495  7.9514527  8.893319
0:   7.037813   6.691879 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.150054   -7.792054   -7.991291   -7.8311214  -7.6920633  -7.7656584
0:   -7.8442497  -8.052446   -8.49965    -8.831643   -9.324935   -9.978112
0:  -10.356238  -10.340888   -9.797812   -8.876743   -8.03315    -7.4916162
0:   -8.728912   -8.469829 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.10869  18.404173 19.175697 19.999376 20.635918 20.983606 21.52827
0:  22.079021 22.638865 23.254484 23.558943 23.48663  23.363564 23.252018
0:  23.469934 24.097523 24.916561 25.65992  22.102844 22.329441]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.533501 -13.856895 -13.642742 -12.806814 -11.647446 -10.588768
0:   -9.868383  -9.330711  -9.492176  -9.839666 -10.383232 -11.177021
0:  -11.762753 -12.151718 -12.218885 -11.877752 -11.369955 -10.851814
0:  -11.691336 -12.191149]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.122072  10.1630535 10.543798  11.265291  11.961202  12.405584
0:  12.621818  12.683329  12.321795  12.06441   11.713442  11.258638
0:  10.913143  10.624878  10.747676  11.151983  11.642138  11.978007
0:  10.066889  10.070091 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.2544947 4.1776476 4.4765906 4.895124  5.196832  5.2225146 5.43242
0:  5.4846997 5.544603  5.552     5.245467  4.8062973 4.3883834 4.3701963
0:  4.7540083 5.526356  6.270228  6.7757583 5.438713  5.4847274]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.784262 20.480694 20.275229 19.962017 19.535126 18.77226  18.532148
0:  18.171545 17.980738 17.757843 17.050196 16.306221 15.486877 15.151203
0:  15.321627 15.967497 16.697472 17.221449 16.64494  16.555866]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.402546 18.595406 19.055084 19.532238 19.757116 19.560463 19.311747
0:  18.976078 18.548502 18.205788 17.768726 17.175468 16.666431 16.281214
0:  16.264297 16.634872 17.281029 17.884117 15.406093 15.556987]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.724717 11.870518 12.273865 12.854533 13.429869 13.782356 14.616873
0:  15.257277 15.903288 16.405834 16.363527 16.149078 15.833038 15.95692
0:  16.518417 17.461567 18.43195  19.065685 18.490826 18.453293]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.9951811 1.1169138 1.5680056 2.3740435 3.2011578 3.8112082 4.1429143
0:  4.3015003 3.9638882 3.6565933 3.2905746 2.8362064 2.6238236 2.5287628
0:  2.7410011 3.1916838 3.6708412 4.078596  2.5393643 2.6426587]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.086285   9.023041   9.183758   9.458154   9.7031765  9.807611
0:  10.15481   10.435206  10.641769  10.745265  10.568383  10.188734
0:   9.897207   9.95231   10.388805  11.150455  11.971137  12.55785
0:   9.579356   9.685352 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2320702 3.2871845 3.5725405 3.8412793 3.9331799 3.7835574 3.8998156
0:  3.9418006 3.885678  3.7651868 3.202382  2.3500853 1.4884787 1.0737624
0:  1.1505694 1.7479148 2.4023666 2.8582592 2.935596  3.1059623]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.138409 15.18668  15.530703 15.994068 16.291847 16.176924 16.24886
0:  16.13767  16.068178 16.03397  15.720242 15.315065 14.901001 14.709433
0:  14.885319 15.470022 16.30574  17.084835 15.29076  15.312086]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.90347  25.47332  25.010052 24.603584 24.230118 23.561998 23.596601
0:  23.429098 23.542397 23.515795 22.895489 22.332237 21.671238 21.453993
0:  21.694107 22.25972  22.932981 23.318445 22.104576 21.814074]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.99062  22.79175  22.545618 22.144081 21.746313 21.224695 21.485249
0:  21.666323 21.959606 22.12512  21.66221  21.196234 20.675404 20.698612
0:  21.18416  21.917269 22.645445 22.976337 22.924223 22.844858]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.630986  5.6239786 5.8790164 6.30166   6.569189  6.558997  6.559155
0:  6.389121  6.0803213 5.815413  5.3981867 4.8978605 4.533656  4.5133
0:  4.9250593 5.6317177 6.366542  6.8693633 5.521842  5.5672245]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7629128 -2.0169196 -1.9078937 -1.5383487 -1.2310896 -1.1090803
0:  -1.1766553 -1.1859455 -1.4426737 -1.587295  -1.7999067 -2.3347635
0:  -2.8714914 -3.3771448 -3.6266136 -3.3106995 -2.602025  -1.7324805
0:  -3.2779922 -3.2638426]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.64442  22.41309  22.249031 22.02847  21.766518 21.249361 21.401802
0:  21.476757 21.72779  21.893936 21.454334 20.944027 20.235277 19.952768
0:  20.136114 20.685528 21.356358 21.785694 21.432129 21.48238 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.746895 25.949383 26.050285 25.866344 25.513882 24.94553  25.10278
0:  25.287685 25.69231  26.026852 25.763466 25.446098 25.048897 25.214817
0:  25.949726 27.046593 28.14151  28.730259 29.728271 30.137096]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.687666  12.66368   12.751173  12.751864  12.602598  12.1774435
0:  12.152224  12.089764  12.14768   12.130578  11.694879  11.193945
0:  10.630018  10.481764  10.763866  11.437565  12.25455   12.932961
0:  11.086319  11.139589 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.924689  -8.202282  -8.007378  -7.2901216 -6.418204  -5.6856494
0:  -5.4820714 -5.477533  -6.1144633 -6.754126  -7.3577476 -8.112791
0:  -8.572813  -8.915058  -8.956438  -8.514982  -7.797997  -6.9229236
0:  -8.030163  -8.330942 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.20637512 -0.13946724  0.33090878  1.0204573   1.6619177   2.0725098
0:   2.4157243   2.5623894   2.4520793   2.3329387   2.02736     1.5681305
0:   1.2491798   1.1610289   1.413332    1.9423914   2.4901772   2.8875756
0:   1.360117    1.5584164 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.699425 34.402912 34.1097   33.918777 33.77817  33.29406  33.40536
0:  33.23023  33.258385 33.20747  32.62605  32.108723 31.559074 31.349297
0:  31.560303 32.10682  32.838036 33.30603  32.769913 32.698013]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.7656908   1.9033213   2.4535036   3.2087874   3.7490797   3.8748019
0:   3.7372482   3.3947973   2.7641187   2.238927    1.6823902   0.990293
0:   0.52018976  0.24058723  0.27654076  0.6638646   1.1110344   1.4047885
0:  -1.7081141  -1.8303285 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.7124934   3.614915    3.8270693   4.188694    4.44611     4.4033794
0:   4.184085    3.8625078   3.2591588   2.7911472   2.2982678   1.6625619
0:   1.1507864   0.7416191   0.56679964  0.8473606   1.3512263   1.8598852
0:  -0.5719609  -0.917099  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.8197303 -5.712958  -5.160806  -4.1926665 -3.1821609 -2.4003663
0:  -2.027904  -1.9580312 -2.448151  -2.9598022 -3.5421977 -4.213824
0:  -4.6232924 -4.8504815 -4.734101  -4.260675  -3.6483006 -3.0480294
0:  -4.0732455 -3.9236102]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.35392  33.050568 32.254948 31.199924 30.080238 28.831379 28.905888
0:  29.113605 29.65768  29.930733 29.125225 28.412529 27.46072  27.344654
0:  28.009453 28.868603 29.611    29.482971 33.417526 33.43247 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.3219855 3.3507683 3.6676738 4.261954  4.875954  5.338482  5.4304333
0:  5.442754  4.9925632 4.5386553 4.052494  3.3708766 2.783422  2.324821
0:  2.225658  2.585587  3.2309964 3.911013  2.5412946 2.4823112]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.272735   7.202113   7.511168   7.968944   8.287314   8.351903
0:   8.399255   8.357728   8.243615   8.202574   8.079975   7.8030734
0:   7.620487   7.686877   8.112359   8.899551   9.846539  10.631803
0:   9.15071    9.624149 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.267065 26.175734 26.264877 26.288746 26.08111  25.488626 25.187592
0:  24.920507 24.895164 25.064995 25.14147  25.144335 25.264326 25.49727
0:  26.0703   26.899427 27.88282  28.790617 27.033815 27.283905]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.410101  -8.744135  -8.610962  -7.90957   -6.971042  -6.0615554
0:  -5.5903983 -5.2511687 -5.540935  -5.866037  -6.2110934 -6.7128577
0:  -7.029799  -7.160608  -6.9932923 -6.3615375 -5.4454694 -4.441385
0:  -4.8424315 -5.235043 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.1400132  1.6745641  1.7240627  2.080497   2.4737966  2.702396
0:   3.222214   3.6499188  4.1255636  4.616111   4.8123107  4.7941575
0:   4.8693643  5.3153415  6.213303   7.5843115  8.992756  10.116293
0:   9.299203   9.55786  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.192478  14.335178  14.805499  15.448729  15.816606  15.823556
0:  15.481506  15.113848  14.450974  13.945214  13.390375  12.5384655
0:  11.673225  10.798847  10.298404  10.336174  10.83199   11.450322
0:   8.199922   8.007499 ]
0: validation loss for strategy=forecast at epoch 45 : 0.33292341232299805
0: validation loss for velocity_u : 0.18949204683303833
0: validation loss for velocity_v : 0.29953187704086304
0: validation loss for specific_humidity : 0.1591101884841919
0: validation loss for velocity_z : 0.5548230409622192
0: validation loss for temperature : 0.11989116668701172
0: validation loss for total_precip : 0.6746931076049805
0: 46 : 21:01:09 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 46, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2072, 1.2366, 1.2624, 1.2846, 1.3100, 1.3396, 1.3667, 1.3898, 1.4133, 1.4395, 1.4645, 1.4880, 1.5153, 1.5484,
0:         1.5811, 1.6086, 1.6343, 1.6624, 1.0785, 1.1081, 1.1308, 1.1471, 1.1682, 1.1976, 1.2266], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3952, 0.3686, 0.3430, 0.3195, 0.2968, 0.2758, 0.2573, 0.2411, 0.2246, 0.2072, 0.1882, 0.1668, 0.1429, 0.1175,
0:         0.0909, 0.0643, 0.0383, 0.0119, 0.3746, 0.3409, 0.3155, 0.2949, 0.2723, 0.2486, 0.2288], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1835, -0.2071, -0.2228, -0.2355, -0.2424, -0.2480, -0.2444, -0.2382, -0.2339, -0.2266, -0.2178, -0.2135,
0:         -0.2146, -0.2241, -0.2250, -0.2270, -0.2304, -0.2312, -0.2772, -0.3010, -0.3165, -0.3211, -0.3274, -0.3366,
0:         -0.3401], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2023, -0.2078, -0.1365, -0.0653, -0.0905, -0.0861,  0.0641,  0.2110,  0.2143,  0.2033,  0.3119,  0.4281,
0:          0.4281,  0.3777,  0.3711,  0.3744,  0.3185,  0.2099,  0.0466,  0.0597,  0.1036,  0.2395,  0.3524,  0.4292,
0:          0.5838], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.1113, 1.1308, 1.1511, 1.1726, 1.1929, 1.2118, 1.2316, 1.2523, 1.2709, 1.2854, 1.2971, 1.3083, 1.3193, 1.3304,
0:         1.3406, 1.3491, 1.3553, 1.3592, 1.3608, 1.3601, 1.3576, 1.3537, 1.3473, 1.3380, 1.3273], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399,
0:         -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399,
0:         -0.2399], device='cuda:0')
0: [DEBUG] Epoch 46, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([        nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:          1.8907e+00,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan, -1.9798e-02,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:         -2.0001e-01,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan, -2.3994e-01,         nan,         nan,
0:         -1.5667e-01, -2.2055e-01,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan, -1.8861e-01,         nan,  4.6358e-02,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan, -5.1735e-02,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan, -2.3195e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan, -1.5895e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan, -1.5478e-03,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  1.5358e-01,
0:                 nan,         nan,  8.5139e-02,  1.2164e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan, -9.7359e-02,         nan,         nan,
0:                 nan,         nan,  6.6229e-01,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan])
0: [DEBUG] Epoch 46, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0426, 1.0164, 1.0048, 1.0020, 1.0024, 1.0020, 1.0261, 1.0501, 1.0669, 1.0691, 1.0314, 0.9616, 0.8919, 0.8340,
0:         0.8165, 0.8329, 0.8594, 0.8745, 1.0721, 1.0428, 1.0225, 0.9974, 0.9580, 0.9341, 0.9246], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.3555, 1.3251, 1.2507, 1.1654, 1.0687, 0.9751, 0.8772, 0.7907, 0.7212, 0.6582, 0.6074, 0.5634, 0.5118, 0.4512,
0:         0.4078, 0.3979, 0.4111, 0.4451, 1.3527, 1.3190, 1.2317, 1.1231, 1.0080, 0.8981, 0.8064], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2904,  0.2972,  0.2806,  0.2495,  0.2194,  0.1912,  0.1626,  0.1416,  0.1197,  0.0968,  0.0596,  0.0135,
0:         -0.0399, -0.1126, -0.1851, -0.2441, -0.2959, -0.3094,  0.2050,  0.1954,  0.1740,  0.1516,  0.1192,  0.0918,
0:          0.0856], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9760, -0.9315, -1.0359, -1.4586, -1.9427, -2.3270, -2.6498, -2.5072, -1.8964, -1.3127, -0.8483, -0.5559,
0:         -0.2879, -0.0925, -0.1140,  0.0494,  0.2679,  0.2498, -0.6731, -0.8561, -1.0606, -1.3986, -1.8777, -2.4068,
0:         -2.8839], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.3929, 1.3562, 1.3123, 1.2723, 1.2580, 1.2825, 1.3403, 1.4027, 1.4482, 1.4636, 1.4536, 1.4226, 1.3918, 1.3784,
0:         1.3648, 1.3486, 1.3338, 1.3296, 1.3460, 1.3843, 1.4328, 1.4711, 1.4830, 1.4534, 1.3809], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0748, -0.0502, -0.0533, -0.0414, -0.0277, -0.0116, -0.0083, -0.0157, -0.0108, -0.0596, -0.0540, -0.0494,
0:         -0.0329, -0.0167, -0.0166, -0.0089, -0.0030, -0.0134, -0.0670, -0.0563, -0.0339, -0.0342, -0.0164, -0.0092,
0:         -0.0159], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23860898613929749; velocity_v: 0.3800979256629944; specific_humidity: 0.17299795150756836; velocity_z: 0.636584460735321; temperature: 0.13849475979804993; total_precip: 0.7339432239532471; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20301097631454468; velocity_v: 0.3555815815925598; specific_humidity: 0.20747946202754974; velocity_z: 0.6453561782836914; temperature: 0.14111293852329254; total_precip: 0.49335169792175293; 
0: epoch: 46 [1/5 (20%)]	Loss: 0.61365 : 0.32745 :: 0.20899 (2.53 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24233946204185486; velocity_v: 0.33650273084640503; specific_humidity: 0.18625855445861816; velocity_z: 0.6010542511940002; temperature: 0.15964528918266296; total_precip: 0.6745843291282654; 
0: epoch: 46 [2/5 (40%)]	Loss: 0.67458 : 0.33182 :: 0.21646 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2310045212507248; velocity_v: 0.29565688967704773; specific_humidity: 0.2188110202550888; velocity_z: 0.4889572560787201; temperature: 0.219147190451622; total_precip: 0.5790101289749146; 
0: epoch: 46 [3/5 (60%)]	Loss: 0.57901 : 0.30202 :: 0.21332 (16.39 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21043893694877625; velocity_v: 0.3210962116718292; specific_humidity: 0.17976197600364685; velocity_z: 0.5412420034408569; temperature: 0.15007475018501282; total_precip: 0.6132891774177551; 
0: epoch: 46 [4/5 (80%)]	Loss: 0.61329 : 0.30154 :: 0.21166 (16.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 2.86102295e-06
0:  2.86102295e-06 2.86102295e-06 3.81469727e-06 4.76837158e-06
0:  6.67572021e-06 7.62939453e-06 9.53674316e-06 1.14440918e-05
0:  1.23977661e-05 1.33514404e-05 1.23977661e-05 1.23977661e-05
0:  1.14440918e-05 1.14440918e-05 1.04904175e-05 1.04904175e-05
0:  1.04904175e-05 9.53674316e-06 8.58306885e-06 8.58306885e-06
0:  7.62939453e-06 6.67572021e-06 5.72204590e-06 4.76837158e-06
0:  4.76837158e-06 4.76837158e-06 3.81469727e-06 3.81469727e-06
0:  3.81469727e-06 2.86102295e-06 2.86102295e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.53674316e-07
0:  9.53674316e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  2.86102295e-06 2.86102295e-06 2.86102295e-06 2.86102295e-06
0:  2.86102295e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 2.86102295e-06 2.86102295e-06
0:  3.81469727e-06 4.76837158e-06 4.76837158e-06 5.72204590e-06
0:  5.72204590e-06 6.67572021e-06 6.67572021e-06 7.62939453e-06
0:  7.62939453e-06 8.58306885e-06 8.58306885e-06 9.53674316e-06
0:  9.53674316e-06 1.04904175e-05 1.14440918e-05 1.23977661e-05
0:  1.33514404e-05 1.33514404e-05 1.33514404e-05 1.43051147e-05
0:  1.43051147e-05 1.52587891e-05 1.52587891e-05 1.62124634e-05
0:  1.43051147e-05 1.23977661e-05 1.14440918e-05 9.53674316e-06
0:  8.58306885e-06 6.67572021e-06 6.67572021e-06 5.72204590e-06
0:  4.76837158e-06 4.76837158e-06 3.81469727e-06 2.86102295e-06
0:  2.86102295e-06 2.86102295e-06 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  9.53674316e-07 9.53674316e-07 1.90734863e-06 1.90734863e-06
0:  1.90734863e-06 1.90734863e-06 2.86102295e-06 2.86102295e-06
0:  2.86102295e-06 2.86102295e-06 2.86102295e-06 2.86102295e-06
0:  2.86102295e-06 2.86102295e-06 2.86102295e-06 2.86102295e-06
0:  2.86102295e-06 2.86102295e-06 2.86102295e-06 2.86102295e-06
0:  2.86102295e-06 2.86102295e-06 2.86102295e-06 1.90734863e-06]
0: Target values (first 200):
0: [2.40325928e-04 2.41279602e-04 2.41279602e-04 2.41279602e-04
0:  2.41279602e-04 2.41279602e-04 2.22206116e-04 2.05993652e-04
0:  2.14576721e-04 2.23159790e-04 2.31742859e-04 2.40325928e-04
0:  2.48908997e-04 2.47001648e-04 2.41279602e-04 2.33650208e-04
0:  2.26974487e-04 2.19345093e-04 2.12669373e-04 2.09808350e-04
0:  2.14576721e-04 2.14576721e-04 2.13623047e-04 2.12669373e-04
0:  2.11715698e-04 2.10762024e-04 2.06947327e-04 2.19345093e-04
0:  2.47001648e-04 2.73704529e-04 3.00407410e-04 3.28063965e-04
0:  3.54766846e-04 3.74794006e-04 3.81469727e-04 3.88145447e-04
0:  3.94821167e-04 4.01496887e-04 4.08172607e-04 4.13894653e-04
0:  4.07218933e-04 3.78608704e-04 3.50952148e-04 3.23295622e-04
0:  2.95639038e-04 2.67028809e-04 2.48908997e-04 2.28881836e-04
0:  2.06947327e-04 1.85012817e-04 1.63078308e-04 1.41143799e-04
0:  1.23977661e-04 1.31607056e-04 1.27792358e-04 1.23977661e-04
0:  1.20162964e-04 1.16348267e-04 1.12533569e-04 1.04904175e-04
0:  9.72747803e-05 9.44137573e-05 9.15527344e-05 8.77380371e-05
0:  8.48770142e-05 8.20159912e-05 6.48498535e-05 6.10351562e-05
0:  5.91278076e-05 5.72204590e-05 5.53131104e-05 5.34057617e-05
0:  5.43594360e-05 6.77108765e-05 1.02043152e-04 1.36375427e-04
0:  1.70707703e-04 2.05039978e-04 2.39372253e-04 2.57492065e-04
0:  2.47955322e-04 2.25067139e-04 2.02178955e-04 1.79290771e-04
0:  1.56402588e-04 1.38282776e-04 1.98364258e-04 2.30789185e-04
0:  2.61306763e-04 2.91824341e-04 3.22341948e-04 3.52859497e-04
0:  3.60488892e-04 3.52859497e-04 3.70979309e-04 3.89099121e-04
0:  4.06265259e-04 4.24385071e-04 4.42504883e-04 4.60624695e-04
0:  4.72068787e-04 4.80651855e-04 4.89234924e-04 4.97817993e-04
0:  5.05447388e-04 5.14984189e-04 5.26428223e-04 5.35011292e-04
0:  4.97817993e-04 4.79698181e-04 4.62532043e-04 4.44412231e-04
0:  4.27246094e-04 4.09126282e-04 3.72886658e-04 3.35693359e-04
0:  2.98500061e-04 2.61306763e-04 2.24113464e-04 1.94549561e-04
0:  1.85012817e-04 1.82151794e-04 1.79290771e-04 1.75476074e-04
0:  1.72615051e-04 1.69754028e-04 1.71661377e-04 1.78337097e-04
0:  1.85012817e-04 1.92642212e-04 1.99317932e-04 2.06947327e-04
0:  2.13623047e-04 2.08854675e-04 2.03132629e-04 1.97410583e-04
0:  1.91688538e-04 1.86920166e-04 1.81198120e-04 1.88827515e-04
0:  2.08854675e-04 2.28881836e-04 2.48908997e-04 2.68936157e-04
0:  2.89916992e-04 3.06129456e-04 3.09944153e-04 3.12805176e-04
0:  3.16619873e-04 3.20434570e-04 3.23295622e-04 3.28063965e-04
0:  3.18527222e-04 3.02314758e-04 2.86102295e-04 2.69889832e-04
0:  2.53677368e-04 2.37464905e-04 2.30789185e-04 2.39372253e-04
0:  2.47001648e-04 2.54631042e-04 2.63214111e-04 2.71797180e-04
0:  2.74658203e-04 2.63214111e-04 2.50816345e-04 2.39372253e-04
0:  2.26974487e-04 2.15530396e-04 2.03132629e-04 1.96456909e-04
0:  1.77383423e-04 1.59263611e-04 1.40190125e-04 1.22070312e-04
0:  1.02996826e-04 9.72747803e-05 1.04904175e-04 1.11579895e-04
0:  1.19209290e-04 1.26838684e-04 1.34468079e-04 1.42097473e-04
0:  1.53541565e-04 1.64031982e-04 1.74522400e-04 1.85012817e-04
0:  1.95503235e-04 2.05993652e-04 2.39372253e-04 3.01361084e-04
0:  3.63349915e-04 4.25338745e-04 4.87327576e-04 5.48362732e-04
0:  5.77926694e-04 5.55038452e-04 5.33103943e-04 5.11169434e-04]
0: Prediction values (first 20):
0: [ 9.069114   9.327634   9.900875  10.475796  10.858365  10.9458065
0:  11.105631  11.180855  11.220892  11.315856  11.137183  10.78591
0:  10.428241  10.313084  10.590648  11.234331  11.997772  12.660656
0:  10.262402  10.614127 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.389, max = 2.613, mean = 0.226
0:          sample (first 20): tensor([0.1636, 0.1837, 0.2282, 0.2728, 0.3025, 0.3093, 0.3217, 0.3276, 0.3307, 0.3380, 0.3242, 0.2969, 0.2691, 0.2602,
0:         0.2817, 0.3317, 0.3910, 0.4425, 0.2106, 0.2286])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.360137   -1.1924324  -0.61351824  0.21954966  1.0629392   1.7232699
0:   2.3410687   2.8569775   3.1320138   3.361098    3.4185827   3.330345
0:   3.3030536   3.462369    3.8924031   4.559164    5.2564354   5.7640066
0:   4.11848     4.2045684 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.0403    -3.057221  -2.7287993 -2.2196765 -1.7967343 -1.5750556
0:  -1.2678237 -1.0570979 -1.0220923 -1.0369554 -1.3792624 -1.9957547
0:  -2.5537887 -2.8296628 -2.7557092 -2.336372  -1.912189  -1.6678548
0:  -3.3526216 -3.228147 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4691086 3.644938  4.205502  5.226221  6.310035  7.2764854 7.767827
0:  8.172796  8.025257  7.9222455 7.7762947 7.326847  6.8381968 6.264005
0:  5.9730554 6.299413  7.153412  8.203108  5.46614   5.114665 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1009843 3.0470393 3.4087727 3.9527411 4.3900557 4.5679026 4.644025
0:  4.5783825 4.328117  4.082794  3.7351954 3.255528  2.9554322 2.9069278
0:  3.2135866 3.8857243 4.6643715 5.2990694 3.853555  4.0048122]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.749387    2.4150815   2.373529    2.4968698   2.5811872   2.5367095
0:   2.5926983   2.5492482   2.2411857   1.9872422   1.5160875   0.84759855
0:   0.3208356  -0.01985025 -0.07552528  0.14907932  0.35410023  0.50148344
0:  -0.872911   -1.239305  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.8150625 -11.7291975 -11.147677  -10.222468   -9.324697   -8.686862
0:   -8.194849   -7.859995   -7.907752   -7.9310865  -8.128609   -8.5446205
0:   -8.771862   -8.772878   -8.381057   -7.5982437  -6.7510967  -5.9444246
0:   -7.156028   -7.144555 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.508263 24.81844  25.164757 25.309978 25.205997 24.76368  24.69381
0:  24.53061  24.566185 24.484802 24.001945 23.437561 22.98396  22.884583
0:  23.254822 23.9538   24.698013 25.459026 22.724037 23.242718]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.486878   -2.5407128  -2.1650472  -1.4132829  -0.67446136 -0.11879063
0:  -0.00878859  0.04167032 -0.34254885 -0.61208487 -0.81625795 -1.2205148
0:  -1.5158467  -1.7555318  -1.7195888  -1.1320348  -0.11611509  1.0862675
0:   0.42278624  0.4069786 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7987103 2.8688703 3.2551289 3.9022732 4.41685   4.690324  4.8430014
0:  4.9404693 4.799021  4.7494025 4.581818  4.249891  3.9724612 3.8661935
0:  4.1386175 4.7956133 5.655273  6.397065  5.6703377 5.854763 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.225376   -7.395689   -7.2711697  -6.989274   -6.7531977  -6.868824
0:  -6.576009   -6.5492086  -6.180752   -5.7455177  -5.57627    -5.5153456
0:  -5.4284     -4.864917   -3.9713268  -2.64042    -1.442666   -0.51743126
0:  -2.8002071  -2.7108355 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9506042 3.7861657 3.950465  4.336502  4.6359625 4.704558  4.5194383
0:  4.3156586 3.8087368 3.3961723 2.9935746 2.3939834 1.9326677 1.5936551
0:  1.5928855 2.1277666 2.9756489 3.8954585 1.5979357 1.3433404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.776201 17.478264 17.30049  17.128786 16.92842  16.509954 16.653536
0:  16.702312 16.853786 16.981575 16.61485  16.163488 15.542683 15.332209
0:  15.438694 15.783125 16.10399  16.150955 13.249814 13.028224]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.173809  10.140923  10.524925  11.199649  11.883602  12.4335785
0:  12.952534  13.458756  13.734907  14.088444  14.331673  14.330317
0:  14.425051  14.546301  14.885357  15.396484  15.821604  15.964355
0:  12.796104  12.6951885]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.541775 20.94882  21.39059  21.574213 21.682146 21.711187 22.262651
0:  22.844744 23.519619 24.108921 24.306225 24.39141  24.602676 25.220806
0:  26.285845 27.609451 28.991943 30.059845 29.446445 30.201967]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.302429   10.304983    9.626515    9.259956    8.855424    8.3060875
0:   7.576466    6.8878922   5.90592     5.0183353   4.017724    2.7447863
0:   1.5214624   0.47798634 -0.09661818 -0.06499529  0.3937359   0.9590459
0:  -0.6516018  -0.8241029 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-27.024769 -27.003578 -26.522556 -25.466595 -24.380844 -23.514805
0:  -23.201542 -23.294483 -24.228912 -25.267632 -26.513916 -27.830204
0:  -28.700138 -29.14286  -28.92023  -28.236084 -27.419128 -26.63379
0:  -26.009678 -26.187359]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7368817  -2.8434768  -2.529715   -1.7883911  -0.98675966 -0.30579185
0:  -0.04299402  0.18361044 -0.08165789 -0.2646222  -0.45270157 -0.881927
0:  -1.2602043  -1.6022196  -1.6722322  -1.1887035  -0.2535925   0.8507676
0:   0.12493134 -0.05657625]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.6705003 -6.8095107 -6.326821  -5.3114    -4.249453  -3.4537978
0:  -3.143455  -3.0342708 -3.379497  -3.694738  -4.019832  -4.549026
0:  -4.9165273 -5.223266  -5.2820992 -4.95628   -4.4961104 -3.9819865
0:  -5.885304  -6.208114 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.038637  -12.891586  -12.313923  -11.358639  -10.368351   -9.7104435
0:   -9.455166   -9.625399  -10.477129  -11.440205  -12.565214  -13.741674
0:  -14.496077  -14.851913  -14.673971  -14.135294  -13.67749   -13.37047
0:  -15.439051  -15.707096 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.702084 17.548092 17.618101 17.696228 17.587732 17.14542  16.762709
0:  16.307278 15.862341 15.44906  14.838139 14.094318 13.369441 12.844709
0:  12.688753 12.924082 13.400074 13.861327 11.703766 11.382571]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.254705   9.946362  10.025713  10.362841  10.747061  10.977522
0:  11.3811245 11.671793  11.848324  12.000013  11.854254  11.538155
0:  11.261808  11.222033  11.517192  12.1280775 12.787915  13.275433
0:  10.607123  10.400888 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.478247  13.637745  14.053041  14.514614  14.786972  14.768098
0:  14.560754  14.229464  13.662874  13.1370535 12.549944  11.820635
0:  11.261327  10.920492  11.015003  11.511639  12.183992  12.736614
0:   9.851321  10.115255 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.8715010e-01 -3.2453585e-01  4.0458918e-01  1.3667593e+00
0:   2.1641140e+00  2.5883670e+00  2.7305365e+00  2.6837733e+00
0:   2.3061125e+00  1.9301543e+00  1.4399099e+00  7.2503328e-01
0:   1.7847347e-01 -1.8300438e-01 -1.6523170e-01  3.1964445e-01
0:   9.8129129e-01  1.5862927e+00 -2.2856236e-01 -1.8043518e-03]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.433615  21.432022  21.538662  21.568136  21.392365  20.899256
0:  20.729364  20.469406  20.194923  19.830128  18.956837  17.897459
0:  16.678076  15.813631  15.4342    15.5       15.810707  16.081877
0:  15.058018  15.0535755]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7563562 3.0697966 3.6639645 4.3813906 4.8717318 5.069539  5.07145
0:  4.996259  4.6986427 4.433895  4.1339884 3.741441  3.571318  3.7202935
0:  4.399171  5.5705895 6.9896817 8.341657  7.4770823 7.668081 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.122716   -3.0421743  -2.2557793  -0.8501935   0.5674076   1.6584611
0:   2.3664877   2.5781927   2.155179    1.3505797   0.11839294 -1.6251454
0:  -3.3298464  -4.7497416  -5.586462   -5.6023073  -5.1005507  -4.4032335
0:  -5.013954   -5.200777  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3834004  -5.4477153  -5.0032697  -4.0775676  -3.007833   -2.1186633
0:  -1.6348147  -1.332603   -1.601069   -1.8551297  -2.1307807  -2.5844846
0:  -2.7937417  -2.9347682  -2.801951   -2.2665124  -1.5271373  -0.72899914
0:  -1.814508   -1.620709  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.723143   -2.8966317  -2.6172261  -2.0332756  -1.4701705  -1.1169472
0:  -0.9122515  -0.88277483 -1.1861739  -1.465622   -1.8380737  -2.3502297
0:  -2.6242976  -2.6556792  -2.3479705  -1.6302385  -0.81232595 -0.05252314
0:  -1.9934993  -1.8604813 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.9563823  4.7644157  4.857496   5.0476294  5.0124154  4.587047
0:  4.2597055  3.806985   3.3582838  2.95518    2.3541489  1.613162
0:  0.90247726 0.56901646 0.6739507  1.2404156  1.9744849  2.6191683
0:  0.88636255 0.8094616 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.462203    1.3946986   1.7552824   2.3194425   2.8535      3.1900573
0:   3.4738455   3.6279752   3.5000865   3.356418    3.047722    2.5748808
0:   2.1913106   1.9122648   1.857738    1.9966812   2.147343    2.2993202
0:   0.15845442 -0.15147305]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.034222   -6.9160943  -6.336738   -5.4242816  -4.533664   -3.8739886
0:  -3.4459357  -3.1618648  -3.1857963  -3.1874785  -3.2040439  -3.3006377
0:  -3.185165   -2.8505483  -2.168961   -1.1804171  -0.10292578  0.81010294
0:   0.5550046   0.6250534 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.3887725 -7.256964  -6.7561765 -6.056064  -5.389704  -4.9205155
0:  -4.453419  -4.074738  -3.9234586 -3.7752347 -3.8561502 -4.1738906
0:  -4.344765  -4.2648587 -3.7754931 -2.984614  -2.204615  -1.638236
0:  -2.631991  -2.4293952]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2572331 1.5414991 2.2837617 3.2103868 3.9394789 4.359087  4.559843
0:  4.7571754 4.736449  4.8022933 4.798001  4.5156136 4.318549  4.1861095
0:  4.356311  4.9497757 5.7458043 6.463806  4.567774  4.81497  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.58646   7.6296315 7.972786  8.566022  9.077064  9.401495  9.451873
0:  9.424276  9.070096  8.771078  8.457199  8.022784  7.722931  7.52407
0:  7.629331  8.0307455 8.5825205 9.030808  6.7223396 6.231036 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.459419  3.4502172 3.8724618 4.5362945 5.0757766 5.333108  5.300216
0:  5.144904  4.6282353 4.13478   3.5682666 2.772924  2.14602   1.6814861
0:  1.6146264 2.0550823 2.7780843 3.5105107 1.294426  1.4301648]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.704395   -10.956253   -10.680637    -9.85277     -8.764452
0:   -7.7188725   -6.63572     -5.662109    -5.0499167   -4.529122
0:   -4.0734887   -3.775292    -3.1822553   -2.3258414   -1.1532564
0:    0.23305464   1.5327787    2.5002613    0.54291725   0.60828304]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.367088 17.243286 17.276663 17.291325 17.217472 16.851732 17.018867
0:  17.0267   17.135723 17.179829 16.633547 15.991809 15.239981 15.017986
0:  15.318748 16.118378 17.108444 17.916874 18.381702 18.608711]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5521884  -0.4617815   0.01011086  0.70850563  1.3615913   1.8034687
0:   2.0744634   2.1095915   1.7368388   1.3623848   0.89443254  0.31669378
0:   0.0853281   0.06238222  0.40144968  1.0582128   1.7634768   2.4521518
0:   1.1364903   1.2060046 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5906417 3.5790908 4.004626  4.6964436 5.3075056 5.7309294 6.0598454
0:  6.3542523 6.4839993 6.6204357 6.581494  6.2097774 5.78054   5.396122
0:  5.2941904 5.554598  6.045223  6.487117  2.6028092 2.0973635]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.346724  -10.677631  -10.496342   -9.747948   -8.814669   -8.026552
0:   -7.7250047  -7.666678   -8.3234825  -9.012688   -9.742729  -10.600262
0:  -11.124769  -11.428783  -11.343296  -10.785173  -10.010887   -9.150404
0:   -9.86334   -10.274683 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.481688   6.6404247  7.2126784  7.97227    8.660294   9.124786
0:   9.697713  10.103759  10.382958  10.647215  10.659646  10.483659
0:  10.424995  10.640039  11.207119  12.096817  12.965433  13.620451
0:  11.962738  11.839754 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.756714 22.829407 23.282528 23.708565 23.877773 23.714195 23.525951
0:  23.268944 22.916824 22.549461 21.91735  21.140175 20.445673 20.00737
0:  20.105412 20.511862 21.054693 21.48246  18.33198  18.195927]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.570724 38.25689  37.760025 37.23634  36.737667 36.02475  36.24665
0:  36.31109  36.56459  36.602642 35.900246 35.296864 34.68262  34.664623
0:  35.275875 36.136578 36.99691  37.364536 37.4515   37.41311 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.1409793  0.7904196  0.926517   1.4429264  1.989366   2.3209658
0:  2.6267815  2.7036777  2.4523563  2.244565   1.8466935  1.2986221
0:  0.95989895 0.8513088  1.1289363  1.6658025  2.0908756  2.2307043
0:  0.6292491  0.35240936]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.08899  20.364212 20.704388 21.061583 21.317883 21.298552 21.815138
0:  22.201298 22.680119 23.00938  22.779785 22.478233 22.05661  22.099155
0:  22.635815 23.50528  24.399616 24.992645 23.82542  24.078037]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.962389  5.8240886 6.1233306 6.7235847 7.321819  7.724188  8.007235
0:  8.162131  8.010062  7.8806057 7.634169  7.2050896 6.921684  6.796146
0:  6.962904  7.4472094 7.9938335 8.400385  4.950041  4.62057  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [44.7253   44.401356 43.88348  43.544243 43.2228   42.749134 43.098892
0:  43.30976  43.802288 44.05454  43.591755 43.194813 42.812366 42.87058
0:  43.62888  44.48071  45.364906 45.817448 46.05025  46.060265]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.132502 22.996592 23.965914 24.830896 25.606733 25.976807 26.911594
0:  27.52193  28.108774 28.354128 27.734411 26.942904 25.924065 25.33956
0:  25.19811  25.33944  25.501282 25.299402 23.729399 23.608686]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.7654357  0.62951565 0.79760647 1.1116381  1.3054037  1.2135673
0:  1.271729   1.2116861  1.1853704  1.1991649  1.0064998  0.70039034
0:  0.4212017  0.5214739  0.9846134  1.8242698  2.6632614  3.2754807
0:  1.2379746  1.0714664 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.095646 26.081627 26.17366  26.25652  26.281643 26.022583 26.255249
0:  26.333935 26.526367 26.654144 26.287762 25.860859 25.361317 25.201084
0:  25.440117 25.985435 26.632248 27.036545 25.468967 25.469551]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.377121 18.505003 18.020367 17.69227  17.473127 17.20201  17.283825
0:  17.267338 17.198359 16.958162 16.2265   15.185768 14.102144 13.386744
0:  13.194323 13.544926 14.253483 15.012444 14.344347 14.201437]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.1947155 7.210508  7.523931  7.9443274 8.080838  7.810586  7.488681
0:  7.034273  6.457638  5.8783655 5.0844336 4.129236  3.1927094 2.5978584
0:  2.5116725 2.951158  3.6537886 4.2706394 2.2079425 2.3262181]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.969288   9.805399  10.041599  10.476023  10.757526  10.747909
0:  10.604204  10.383648   9.978716   9.635137   9.173872   8.531399
0:   7.9218564  7.4389772  7.252765   7.4425125  7.758445   8.00644
0:   5.904522   5.646355 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.16431  38.02102  37.91525  37.980286 38.073826 37.95235  38.603924
0:  39.09146  39.715538 40.084415 39.66947  39.182    38.568882 38.426483
0:  38.750446 39.289806 39.746475 39.654694 35.976467 35.796547]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.0128894  -5.779817   -5.053453   -3.8572755  -2.523213   -1.3088994
0:  -0.3966422   0.2958417   0.32805777  0.199821   -0.17390442 -0.8391762
0:  -1.3842244  -1.8143115  -1.9634461  -1.7307563  -1.326611   -0.8876982
0:  -2.7813153  -2.7947812 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.993334 18.506008 18.150177 17.784115 17.340944 16.603369 16.432526
0:  16.177794 16.15384  16.135178 15.611389 14.96363  14.202274 13.900106
0:  14.097654 14.790951 15.679575 16.393185 16.365276 16.327875]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.812895   4.8605127  5.3021545  5.961173   6.4862742  6.7554064
0:   7.0207753  7.2399817  7.39015    7.5796776  7.6309066  7.461485
0:   7.3166785  7.3368     7.698737   8.436061   9.311572  10.078697
0:   8.22185    8.172493 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.253125   7.774543   7.679731   7.897107   8.173425   8.212119
0:   8.534307   8.682048   8.942886   9.1802025  9.225882   9.267666
0:   9.390472   9.880774  10.798985  12.152704  13.739058  15.166975
0:  15.091864  15.068701 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.244932  8.066433  8.207514  8.581455  8.940989  9.162     9.207308
0:  9.146764  8.74468   8.399803  7.9587646 7.392623  6.9380374 6.606306
0:  6.629525  7.0183735 7.662236  8.301779  5.7055893 5.767035 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.135074  -8.54184   -8.355682  -7.611739  -6.717203  -5.993698
0:  -5.689865  -5.591322  -6.061022  -6.581732  -7.1503687 -7.974715
0:  -8.548422  -8.968371  -9.042761  -8.508338  -7.6386414 -6.6697617
0:  -8.405094  -8.785427 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.7456932 1.9759164 2.646278  3.5947728 4.4939356 5.169917  5.745532
0:  6.106159  6.115591  6.042371  5.709964  5.125861  4.625039  4.2914076
0:  4.2976265 4.649167  5.1190977 5.5255446 3.9322126 4.1773586]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.74668  22.64904  22.964981 23.502836 23.976845 24.242819 24.72167
0:  25.216732 25.753666 26.172419 26.26993  26.1414   26.026844 26.044703
0:  26.594135 27.454762 28.363253 29.135643 24.852903 24.360474]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-20.511086 -21.076603 -21.046576 -20.287733 -19.172132 -18.043602
0:  -17.352139 -16.950363 -17.294275 -17.712736 -18.179525 -18.650522
0:  -18.737179 -18.579218 -18.00616  -17.126404 -16.099472 -15.160404
0:  -17.957836 -18.261047]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.5565   24.950527 25.31641  25.485039 25.442738 25.06127  25.179928
0:  25.205458 25.36721  25.493248 25.13097  24.614685 23.972084 23.647207
0:  23.687103 24.097416 24.741323 25.257092 24.663795 24.967812]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.709691   -2.5100899  -1.990324   -1.3544297  -0.9049964  -0.72540283
0:  -0.6706972  -0.7526746  -1.0670376  -1.3198996  -1.6801329  -2.1881332
0:  -2.5318737  -2.6925259  -2.5038548  -1.9452868  -1.2569427  -0.58822155
0:  -1.481494   -1.1907139 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.583754  13.455738  13.285919  13.055805  12.863164  12.42552
0:  12.713339  12.793452  13.091532  13.245011  12.768744  12.285643
0:  11.668271  11.5746155 11.9754    12.912027  14.054554  15.003981
0:  17.657217  17.947659 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.8588796  4.3051834  5.2805862  6.6614733  7.9813366  9.002813
0:   9.849265  10.447445  10.65846   10.7634325 10.61738   10.261373
0:   9.87323    9.674612   9.864378  10.411297  11.10634   11.626135
0:  10.361471  10.479242 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.5675766   3.5044003   3.8074274   4.2446146   4.5934052   4.640934
0:   4.588323    4.281094    3.5919597   2.798212    1.7550621   0.57003975
0:  -0.3368225  -0.87812996 -0.9188018  -0.5733228  -0.13209581  0.15384245
0:  -4.477751   -4.903222  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.148077  5.005878  5.1883707 5.7246757 6.2811317 6.6864686 6.685965
0:  6.594631  5.991906  5.4081316 4.801982  4.0099688 3.345846  2.7774532
0:  2.5805464 2.8552537 3.5033212 4.247397  1.960865  1.4869509]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.3400435  3.3833342  3.7480307  4.2856007  4.681123   4.7839384
0:   4.8761926  4.825992   4.594541   4.357954   3.8742917  3.1772296
0:   2.5437338  2.1443062  2.0241904  2.2298398  2.4473145  2.5632205
0:  -0.9856944 -1.3326807]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.4030094  -0.25367355  0.20141935  0.8081732   1.334621    1.5945292
0:   1.7674294   1.7973642   1.5583682   1.3467126   0.99518347  0.50258875
0:   0.2130208   0.15501785  0.41767073  0.9553876   1.3980017   1.6516571
0:  -1.7701592  -1.8623147 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.09045  12.378658 12.832233 13.134112 13.2871   13.246733 13.592745
0:  13.935852 14.311756 14.648405 14.58424  14.301146 13.955698 13.927792
0:  14.2018   14.781767 15.325949 15.642761 13.977569 14.131705]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.519478  4.396258  4.439     4.508688  4.5544815 4.433159  4.539056
0:  4.5560994 4.519245  4.414893  4.0548    3.61849   3.2854514 3.3742876
0:  3.8399668 4.6403646 5.460168  6.069313  4.863651  4.9949675]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.559505   5.485845   5.785103   6.336765   6.8066816  7.0562215
0:  7.1281347  7.1597495  6.8656063  6.595172   6.1809835  5.509529
0:  4.887638   4.3063726  4.0212517  4.1343617  4.504539   4.9052453
0:  1.16258    0.72435856]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.585432 25.809765 26.01319  26.026108 25.97791  25.63479  25.834066
0:  25.895393 26.080544 26.168486 25.793745 25.409142 25.051569 25.178928
0:  25.818027 26.82158  28.00236  28.883461 29.465008 30.108639]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6417356  -1.5554471  -1.055512   -0.22376204  0.6003709   1.2129035
0:   1.666204    1.9738007   1.9159007   1.7632446   1.4179544   0.8111844
0:   0.30409288 -0.0625782  -0.14863825  0.14274883  0.59059954  0.9893646
0:  -1.9279284  -2.0770917 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9038134 -5.0506487 -4.7530527 -4.0292187 -3.2244482 -2.5727596
0:  -2.3551931 -2.26793   -2.7238045 -3.1045465 -3.4438457 -3.9555607
0:  -4.208894  -4.353137  -4.1637287 -3.4714794 -2.470749  -1.3832412
0:  -1.9054904 -2.0953703]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.5769615  -9.665352   -9.33671    -8.597389   -7.7569814  -7.125032
0:   -7.0029054  -7.1501193  -7.912021   -8.664873   -9.384485  -10.043287
0:  -10.341091  -10.331387   -9.950624   -9.199728   -8.315165   -7.468024
0:   -7.3421683  -7.42246  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.670464 29.693932 29.806664 29.87099  29.906536 29.680754 29.983814
0:  30.0994   30.33078  30.572365 30.371246 30.156048 29.954748 30.039259
0:  30.497808 31.258123 32.148823 32.998787 31.20119  31.595413]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.456616   8.406972   8.690651   9.058323   9.229345   9.103748
0:   8.868878   8.594323   8.246648   8.049759   7.88225    7.6029644
0:   7.5339403  7.647796   8.0601635  8.758456   9.482615  10.0174265
0:   7.1577377  7.076    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.491043 22.636381 22.934372 23.241833 23.432545 23.332539 23.519611
0:  23.585987 23.76648  23.961956 23.764397 23.49783  23.169277 23.086948
0:  23.380463 23.933868 24.53616  25.005077 22.522167 22.83118 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.237312 26.721989 26.168083 25.490242 24.76117  23.956013 23.48841
0:  23.126179 22.81808  22.607264 22.097622 21.417189 20.62439  20.006668
0:  19.662607 19.547894 19.458061 19.28759  15.525472 15.274497]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0847831  -1.0364213  -0.5297837   0.26777458  1.0020313   1.4350371
0:   1.6327128   1.6068864   1.2513185   0.890707    0.4270649  -0.22036219
0:  -0.7155423  -0.9713063  -0.92372656 -0.42282438  0.20818615  0.77656174
0:  -0.55273294 -0.5110545 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.278027 22.502865 22.987751 23.506367 23.889778 24.035065 24.441635
0:  24.73994  25.06411  25.351395 25.299366 25.125942 24.989029 25.145622
0:  25.749187 26.632935 27.614773 28.378204 26.198528 26.557735]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.187201   -8.293923   -7.838332   -6.9017925  -5.9676037  -5.4253902
0:   -5.200712   -5.3839846  -6.141856   -7.0196314  -8.1446905  -9.459071
0:  -10.4107065 -11.095813  -11.22099   -11.03119   -10.871671  -10.871247
0:  -10.780543  -11.015576 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.0213838 1.03335   1.2641354 1.4473319 1.5334935 1.4763203 1.7418861
0:  2.004631  2.2708373 2.492206  2.35358   2.0814128 1.790453  1.9329648
0:  2.3837557 3.1339803 3.7255619 4.023909  2.198349  2.391612 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.59421  13.082158 13.894344 14.853615 15.530027 15.917103 16.00772
0:  16.187508 16.130806 16.203861 16.217812 15.866804 15.405521 14.845909
0:  14.598503 14.867132 15.634937 16.584385 13.123322 13.457401]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.029182 34.86308  34.634007 34.462425 34.31407  33.874443 34.00727
0:  33.941517 34.085545 34.1092   33.59148  33.068047 32.561203 32.313755
0:  32.46522  32.813023 33.264526 33.586483 33.812084 33.80953 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.397961  13.405912  13.725828  14.151508  14.471745  14.5228615
0:  14.667835  14.694342  14.690571  14.704565  14.523333  14.184746
0:  13.9163265 13.830101  14.102205  14.722609  15.531937  16.302244
0:  14.277653  14.259457 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.099823  -4.131675  -3.7320948 -2.9320707 -2.0807333 -1.4260435
0:  -1.1943278 -1.11162   -1.5935555 -2.0569534 -2.540904  -3.234352
0:  -3.6856794 -4.0482016 -4.153987  -3.7937565 -3.2143955 -2.540184
0:  -5.0339375 -5.327468 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.983647  -12.037778  -11.558324  -10.564277   -9.370583   -8.336159
0:   -7.724121   -7.329507   -7.587002   -7.913641   -8.33683    -8.883783
0:   -9.173874   -9.239663   -8.821126   -8.017639   -6.98524    -6.0515127
0:   -7.3496475  -7.4669847]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.797097  13.337675  12.953138  12.540052  12.056391  11.306163
0:  10.999924  10.431873   9.786494   8.863058   7.290963   5.607239
0:   3.7989535  2.6467237  2.0955052  2.080748   2.3577733  2.4455323
0:   3.8197231  3.5125897]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.422873  8.324932  8.68967   9.235072  9.676767  9.775314 10.021374
0:  10.172199 10.347756 10.554374 10.508579 10.264824 10.002308  9.944556
0:  10.206002 10.858811 11.623115 12.261826  9.214429  9.030295]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9181228 -3.9466825 -3.4906316 -2.7470841 -2.157205  -1.9499159
0:  -2.1998796 -2.763246  -3.781156  -4.739557  -5.644774  -6.5955606
0:  -7.17724   -7.549377  -7.555448  -7.1674323 -6.660251  -6.1202445
0:  -6.3420157 -6.2716947]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.492216 16.673885 17.12949  17.624567 17.893595 17.843985 17.801935
0:  17.676311 17.50559  17.404114 17.177988 16.82816  16.59761  16.598183
0:  16.964226 17.634588 18.383656 18.95481  17.143072 17.203817]
0: validation loss for strategy=forecast at epoch 46 : 0.3176610767841339
0: validation loss for velocity_u : 0.1752353459596634
0: validation loss for velocity_v : 0.31398481130599976
0: validation loss for specific_humidity : 0.16599205136299133
0: validation loss for velocity_z : 0.5321345329284668
0: validation loss for temperature : 0.10547512769699097
0: validation loss for total_precip : 0.6131443381309509
0: 47 : 21:05:17 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 47, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0866, -1.0737, -1.0840, -1.1154, -1.1601, -1.2115, -1.2555, -1.2813, -1.2921, -1.2961, -1.3004, -1.3093,
0:         -1.3226, -1.3349, -1.3419, -1.3427, -1.3383, -1.3289, -1.0699, -1.0674, -1.0704, -1.0858, -1.1156, -1.1566,
0:         -1.1975], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1549, -1.2903, -1.3628, -1.3829, -1.3996, -1.4479, -1.5229, -1.6036, -1.6791, -1.7445, -1.7982, -1.8421,
0:         -1.8761, -1.8970, -1.9067, -1.9071, -1.8966, -1.8753, -1.2080, -1.3267, -1.3994, -1.4283, -1.4429, -1.4786,
0:         -1.5403], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2161, 1.1568, 1.0780, 1.0076, 0.9609, 0.9203, 0.8929, 0.8697, 0.8736, 0.8657, 0.8684, 0.8597, 0.8397, 0.8156,
0:         0.7682, 0.7313, 0.6815, 0.5848, 1.1710, 1.1076, 1.0695, 1.0161, 0.9614, 0.9158, 0.9013], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0633,  0.4902,  0.5163,  0.4945,  0.5729,  0.5860,  0.5512,  0.5424,  0.4924,  0.4052,  0.3138,  0.1897,
0:          0.0481, -0.0935, -0.2503, -0.3766, -0.4310, -0.4441,  0.2789,  0.3639,  0.2593,  0.2833,  0.3835,  0.4074,
0:          0.4140], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.0870, 1.0330, 0.9907, 0.9449, 0.8859, 0.8379, 0.8118, 0.7967, 0.7935, 0.8038, 0.8194, 0.8428, 0.8775, 0.9207,
0:         0.9709, 1.0265, 1.0796, 1.1272, 1.1689, 1.2029, 1.2324, 1.2615, 1.2886, 1.3115, 1.3285], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0439, -0.1491, -0.1709, -0.1994, -0.2235, -0.2292, -0.2292, -0.2292, -0.2315, -0.1709, -0.2155, -0.2177,
0:         -0.2177, -0.2269, -0.2257, -0.2246, -0.2246, -0.2315, -0.2326, -0.2326, -0.2280, -0.2212, -0.2246, -0.2257,
0:         -0.2212], device='cuda:0')
0: [DEBUG] Epoch 47, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2280,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2406,     nan,     nan,     nan,     nan,
0:             nan,  0.1619,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2360,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.5874,     nan,     nan, -0.2143,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2155,     nan,     nan,  0.1654,     nan,  0.6045,     nan, -0.2372,     nan,
0:             nan, -0.2097,     nan,     nan,     nan,     nan, -0.2418,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2326,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2349, -0.2326,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2303,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2418,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2418,     nan,     nan, -0.2395,     nan,     nan,     nan,
0:             nan,     nan, -0.2418,     nan,     nan, -0.2418,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2395,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2418,     nan,     nan,
0:             nan, -0.2257,     nan])
0: [DEBUG] Epoch 47, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2661, 0.2685, 0.3079, 0.3607, 0.3971, 0.4014, 0.4182, 0.4157, 0.4045, 0.3928, 0.3548, 0.3000, 0.2431, 0.2145,
0:         0.2132, 0.2535, 0.2950, 0.3232, 0.2955, 0.2931, 0.3201, 0.3484, 0.3542, 0.3541, 0.3550], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9577, -0.9584, -0.9747, -1.0033, -1.0380, -1.1202, -1.2411, -1.3463, -1.3507, -1.2644, -1.1122, -0.9623,
0:         -0.8955, -0.9430, -1.0336, -1.0923, -1.0782, -1.0239, -0.9520, -0.9566, -0.9722, -0.9886, -1.0425, -1.1436,
0:         -1.2633], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5536, -0.5407, -0.5285, -0.5179, -0.5173, -0.5230, -0.5337, -0.5387, -0.5406, -0.5436, -0.5434, -0.5379,
0:         -0.5260, -0.5081, -0.5033, -0.4904, -0.4820, -0.4586, -0.6089, -0.5953, -0.5785, -0.5605, -0.5600, -0.5647,
0:         -0.5614], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6784, -0.7261, -0.5856, -0.6777, -0.7955, -0.6923, -0.7724, -0.7868, -0.5138, -0.2682, -0.2478, -0.3569,
0:         -0.3527, -0.4675, -0.6495, -0.4922, -0.2190, -0.1336, -0.4380, -0.6634, -0.7051, -0.8539, -0.8988, -0.6551,
0:         -0.6384], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4367, 0.4543, 0.4609, 0.4511, 0.4395, 0.4397, 0.4584, 0.4819, 0.5031, 0.5140, 0.5154, 0.5135, 0.5173, 0.5348,
0:         0.5464, 0.5503, 0.5491, 0.5537, 0.5698, 0.5993, 0.6331, 0.6608, 0.6800, 0.6850, 0.6786], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0647, -0.0312, -0.0313,  0.0010,  0.0313,  0.0752,  0.1225,  0.1622,  0.1879, -0.0745, -0.0628, -0.0543,
0:         -0.0157,  0.0275,  0.0681,  0.1181,  0.1672,  0.1962, -0.1053, -0.0851, -0.0569, -0.0282,  0.0077,  0.0593,
0:          0.1034], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.22953076660633087; velocity_v: 0.37528687715530396; specific_humidity: 0.15844698250293732; velocity_z: 0.6207330226898193; temperature: 0.1302981972694397; total_precip: 0.59195476770401; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20974265038967133; velocity_v: 0.34440234303474426; specific_humidity: 0.18938466906547546; velocity_z: 0.8329450488090515; temperature: 0.16642877459526062; total_precip: 0.6449902653694153; 
0: epoch: 47 [1/5 (20%)]	Loss: 0.61847 : 0.33991 :: 0.21145 (2.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18942847847938538; velocity_v: 0.36766737699508667; specific_humidity: 0.17065943777561188; velocity_z: 0.6286187171936035; temperature: 0.1388837695121765; total_precip: 0.7190613746643066; 
0: epoch: 47 [2/5 (40%)]	Loss: 0.71906 : 0.33505 :: 0.20932 (16.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22730621695518494; velocity_v: 0.3852080702781677; specific_humidity: 0.15943565964698792; velocity_z: 0.6756366491317749; temperature: 0.15060344338417053; total_precip: 0.770714282989502; 
0: epoch: 47 [3/5 (60%)]	Loss: 0.77071 : 0.35955 :: 0.21618 (16.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20749124884605408; velocity_v: 0.2973707914352417; specific_humidity: 0.21256369352340698; velocity_z: 0.5540133714675903; temperature: 0.16105437278747559; total_precip: 0.6336811780929565; 
0: epoch: 47 [4/5 (80%)]	Loss: 0.63368 : 0.30934 :: 0.21181 (16.02 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [1.14440918e-05 1.23977661e-05 1.28746033e-05 1.23977661e-05
0:  1.23977661e-05 1.19209290e-05 1.04904175e-05 1.00135803e-05
0:  8.10623169e-06 3.81469727e-06 7.62939453e-06 1.52587891e-05
0:  2.62260437e-05 3.14712524e-05 3.62396240e-05 5.72204590e-05
0:  8.24928284e-05 1.33037567e-04 2.64167786e-04 3.94821167e-04
0:  6.40392303e-04 8.45432281e-04 1.02376938e-03 1.15156174e-03
0:  1.21736526e-03 1.20401382e-03 1.24454498e-03 1.22976303e-03
0:  1.14440918e-03 1.00994110e-03 8.10623169e-04 6.81877136e-04
0:  4.76837158e-04 1.85489655e-04 8.82148743e-05 3.95774841e-05
0:  2.86102295e-05 2.24113464e-05 1.95503235e-05 1.95503235e-05
0:  5.00679016e-05 1.25885010e-04 1.34944916e-04 1.33037567e-04
0:  1.09672546e-04 1.01089478e-04 8.39233398e-05 1.10626221e-04
0:  1.22070312e-04 9.77516174e-05 7.96318054e-05 5.86509705e-05
0:  8.01086426e-05 1.02519989e-04 1.22070312e-04 8.58306885e-05
0:  5.91278076e-05 5.43594360e-05 6.91413879e-05 7.34329224e-05
0:  4.95910645e-05 3.86238098e-05 3.71932983e-05 1.62124634e-05
0:  1.90734863e-06 1.43051147e-06 9.53681592e-07 7.27595761e-12
0:  2.86102295e-06 6.19888306e-06 9.53674316e-06 9.53674316e-06
0:  7.15255737e-06 4.76837158e-07 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 4.76837158e-07 4.76837158e-07
0:  4.76837158e-07 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 1.43051147e-06
0:  2.14576721e-05 4.48226929e-05 7.10487366e-05 3.57627869e-05
0:  1.00135803e-05 1.38282776e-05 2.00271606e-05 3.00407410e-05
0:  3.95774841e-05 4.38690186e-05 4.57763635e-05 4.48226929e-05
0:  3.67164612e-05 3.00407410e-05 2.24113464e-05 2.05039978e-05
0:  1.71661377e-05 1.14440918e-05 1.28746033e-05 1.52587891e-05
0:  7.15255737e-06 7.15255737e-06 1.33514404e-05 1.43051147e-05
0:  1.90734863e-05 3.14712524e-05 8.39233398e-05 1.32083893e-04
0:  1.56402588e-04 2.83241272e-04 4.58717346e-04 6.46114349e-04
0:  8.42094421e-04 1.09434128e-03 1.19209290e-03 1.22213364e-03
0:  1.15156174e-03 1.08861923e-03 1.02281559e-03 5.92231750e-04
0:  3.00407410e-04 2.41756439e-04 1.90734863e-04 1.46865845e-04
0:  5.96046448e-05 2.43186951e-05 1.66893005e-05 1.47819519e-05
0:  1.81198120e-05 2.05039978e-05 2.52723694e-05 3.24249268e-05
0:  6.15119934e-05 8.63075256e-05 1.06334686e-04 1.33991241e-04
0:  1.55448914e-04 1.66416168e-04 9.58442688e-05 2.05039978e-05
0:  2.24113464e-05 2.14576721e-05 1.43051147e-05 1.47819519e-05
0:  2.00271606e-05 4.00543213e-05 7.86781311e-05 1.03950500e-04
0:  5.67436218e-05 4.91142273e-05 6.72340393e-05 3.95774841e-05
0:  1.90734863e-05 9.53674316e-06 4.29153442e-06 9.53681592e-07
0:  4.76837158e-07 9.53681592e-07 1.90734863e-06 9.53681592e-07
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 2.38418579e-06
0:  5.72204590e-06 1.14440918e-05 8.58306885e-06 4.76837158e-06]
0: Target values (first 200):
0: [7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  9.53681592e-07 3.33786011e-06 1.33514404e-05 2.43186951e-05
0:  2.86102295e-05 2.52723694e-05 1.85966492e-05 2.33650208e-05
0:  3.48091125e-05 6.00814819e-05 4.19616699e-05 2.14576721e-05
0:  2.14576721e-05 6.24656677e-05 1.23500824e-04 9.77516174e-05
0:  6.38961792e-05 2.33650208e-05 5.14984094e-05 8.91685486e-05
0:  8.82148743e-05 8.77380371e-05 9.25064087e-05 3.57627869e-05
0:  5.14984094e-05 1.64031982e-04 2.13623047e-04 2.26020813e-04
0:  1.21593475e-04 1.03950500e-04 1.39236450e-04 1.02519989e-04
0:  5.96046448e-05 2.33650208e-05 1.23977661e-05 1.43051147e-05
0:  1.04904175e-05 1.47819519e-05 2.95639038e-05 2.38418579e-05
0:  4.57763635e-05 1.34944916e-04 1.35898590e-04 1.34944916e-04
0:  1.25408173e-04 8.34465027e-05 1.81198120e-05 3.24249268e-05
0:  3.48091125e-05 2.71797180e-05 4.48226929e-05 7.00950623e-05
0:  1.20639801e-04 1.50203705e-04 1.84059143e-04 2.91824341e-04
0:  4.30107117e-04 5.58376254e-04 6.84738159e-04 6.95705414e-04
0:  4.18186188e-04 2.24590302e-04 1.05857849e-04 1.16825104e-04
0:  1.62124634e-04 2.88486481e-04 2.93254852e-04 2.68936157e-04
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 7.27595761e-12
0:  7.27595761e-12 7.27595761e-12 7.27595761e-12 1.43051147e-06
0:  2.38418579e-06 1.90734863e-06 1.19209290e-05 1.95503235e-05
0:  6.19888306e-06 2.86102295e-06 5.24520874e-06 2.47955322e-05
0:  3.67164612e-05 3.57627869e-05 2.47955322e-05 1.43051147e-05
0:  2.05039978e-05 3.43322754e-05 4.86373865e-05 5.10215796e-05
0:  4.91142273e-05 5.10215796e-05 4.38690186e-05 4.10079956e-05
0:  5.34057617e-05 3.76701355e-05 5.24520874e-06 6.34193420e-05
0:  1.12533569e-04 1.38759613e-04 1.17778778e-04 8.72612000e-05
0:  1.08718872e-04 1.10626221e-04 1.05381012e-04 1.45435333e-04
0:  1.36852264e-04 6.19888306e-05 3.71932983e-05 2.81333923e-05
0:  2.62260437e-05 1.81198120e-05 7.15255737e-06 4.76837158e-06
0:  4.29153442e-06 5.72204590e-06 1.06334686e-04 2.06470490e-04
0:  1.91688538e-04 1.39713287e-04 6.15119934e-05 1.16348267e-04
0:  1.44958496e-04 1.41143799e-04 1.82628632e-04 2.15053558e-04]
0: Prediction values (first 20):
0: [14.523446  14.430979  14.367736  14.156106  13.880707  13.409649
0:  13.593827  13.725746  14.0160475 14.193479  13.805214  13.348299
0:  12.776636  12.743005  13.188593  14.007978  14.874705  15.417848
0:  16.147835  15.999238 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.585, max = 2.969, mean = 1.117
0:          sample (first 20): tensor([0.7259, 0.7180, 0.7127, 0.6947, 0.6714, 0.6315, 0.6471, 0.6583, 0.6829, 0.6979, 0.6650, 0.6263, 0.5779, 0.5750,
0:         0.6128, 0.6822, 0.7556, 0.8016, 0.7038, 0.7442])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.470764 33.689957 33.97178  34.147263 34.056267 33.62054  33.43371
0:  33.110325 32.89982  32.772873 32.319656 31.797153 31.34177  31.16173
0:  31.440062 32.064117 32.821045 33.43137  31.05448  31.209612]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.105127  15.167904  15.454851  15.709047  15.813594  15.560187
0:  15.63031   15.5657425 15.554519  15.5407715 15.088685  14.392521
0:  13.567319  13.022237  12.858641  13.117624  13.52465   13.760588
0:  10.583422  10.300925 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2806587 2.5577765 3.107139  3.6497822 3.9006455 3.820921  3.7350273
0:  3.5734022 3.339508  3.156703  2.7602935 2.1519828 1.5875807 1.2963352
0:  1.4336243 1.9133782 2.405805  2.8070593 0.8486643 1.1546278]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -5.7443714  -6.1245203  -6.224222   -6.01295    -5.7707186  -5.7317514
0:   -6.08619    -6.5725536  -7.5280714  -8.381078   -9.140042   -9.858602
0:  -10.2371435 -10.341048  -10.140674   -9.63738    -9.074816   -8.539434
0:   -8.906029   -9.150556 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.474555  -4.578168  -4.2688875 -3.5703392 -2.8448896 -2.2876234
0:  -2.1193352 -2.1725302 -2.8381357 -3.6271534 -4.5579424 -5.699443
0:  -6.6091795 -7.3222923 -7.637641  -7.4599705 -7.014284  -6.4401836
0:  -7.401662  -7.4531765]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7427275 2.836748  3.3056414 3.9860654 4.5139713 4.7704144 4.916919
0:  5.04314   4.9581532 4.92126   4.7790046 4.4119425 4.132127  4.0029144
0:  4.2163925 4.8594723 5.6915483 6.4755893 4.31104   4.474358 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.20089    -4.3071303  -3.969419   -3.1752973  -2.2461734  -1.4000683
0:  -0.9559212  -0.6235852  -0.8409138  -1.0343633  -1.2292824  -1.624887
0:  -1.8382382  -1.9609361  -1.800415   -1.1149068  -0.08268023  1.0507383
0:   1.100141    1.0335846 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.976143 10.692165 10.554377 10.49244  10.381218 10.110321 10.271706
0:  10.366959 10.516395 10.638831 10.36907  10.006751  9.654108  9.873312
0:  10.613764 11.795248 13.087866 14.109046 14.593199 14.710718]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.33247995  0.39629316  0.8221688   1.4555364   1.9435358   2.141004
0:   2.3224258   2.2672553   1.9415021   1.5728388   0.9404106   0.12941122
0:  -0.5192418  -0.8334198  -0.729157   -0.17861366  0.476871    1.0278664
0:  -0.4651518  -0.3028431 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4480917 3.3809407 3.6176841 4.0913754 4.627391  5.0257163 5.566799
0:  6.0280037 6.362848  6.6185255 6.604343  6.3682404 6.140275  6.1234384
0:  6.375369  6.888427  7.353617  7.564202  4.971882  4.7399635]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.13047  15.58312  16.286663 16.99916  17.31085  17.189571 16.729422
0:  16.431602 16.03649  15.939798 15.83569  15.337347 14.679842 13.898008
0:  13.442371 13.610615 14.388965 15.344614 12.139439 12.218624]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0054073 -3.9834833 -3.508545  -2.8024912 -2.170226  -1.7820158
0:  -1.4676156 -1.270791  -1.3721709 -1.4660926 -1.7508836 -2.3061419
0:  -2.7647953 -3.047328  -2.995843  -2.5693512 -2.0153847 -1.5549483
0:  -4.1586056 -4.0067043]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.678247  9.156739  9.791087 10.385575 10.785849 11.00313  11.492676
0:  11.932686 12.335522 12.697908 12.676584 12.515148 12.387468 12.680808
0:  13.363674 14.338083 15.246607 15.859129 15.746663 16.2161  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.545593 32.455147 32.61092  32.7547   32.68848  32.176765 31.727482
0:  31.1301   30.573074 30.073019 29.333263 28.39635  27.487852 26.661995
0:  26.251165 26.191374 26.41403  26.592373 23.385544 22.965954]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.092484 24.64959  24.699547 24.943428 25.219383 25.114918 25.363655
0:  25.435547 25.580956 25.693008 25.40268  24.824093 24.281887 23.915703
0:  24.005608 24.463913 25.148228 25.683708 23.03254  22.295715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.575485  -12.411699  -11.666616  -10.462206   -9.141863   -8.0794525
0:   -7.301353   -6.926289   -7.164148   -7.4664054  -7.9131436  -8.466429
0:   -8.590214   -8.449102   -7.9152493  -7.096145   -6.401507   -5.8795304
0:   -7.4277825  -7.081531 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.3459706 1.4303727 1.9253979 2.6079488 3.1695323 3.4438517 3.6499383
0:  3.6953566 3.5686693 3.462559  3.199977  2.7849984 2.4939356 2.410139
0:  2.6312833 3.1989715 3.8083463 4.2457833 2.2486382 2.394351 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.756783   -2.91579    -2.6865487  -2.103857   -1.5151215  -1.123693
0:  -0.96375084 -0.998785   -1.4075303  -1.7379646  -2.0935578  -2.5193458
0:  -2.6983018  -2.671945   -2.3563619  -1.7554712  -1.1296186  -0.62855196
0:  -1.8851385  -2.0002537 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.093855 13.014496 13.220151 13.555688 13.721062 13.6243   13.625979
0:  13.590498 13.555458 13.622818 13.558458 13.370907 13.268785 13.374528
0:  13.85181  14.624271 15.467545 16.09209  13.970874 13.96891 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.4944744 6.6738815 7.202081  7.8810043 8.374654  8.605906  8.730534
0:  8.724495  8.483366  8.3132515 7.9919634 7.526334  7.164268  6.9392953
0:  7.0734267 7.543799  8.121659  8.651929  6.918762  7.133216 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.428972 13.186679 13.181065 13.17059  13.0851   12.663451 12.771524
0:  12.691613 12.874277 13.021105 12.62977  12.196703 11.738702 11.739996
0:  12.253084 13.251623 14.406458 15.427593 15.076151 15.168114]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.180281  10.973791  11.048443  11.212208  11.350041  11.236678
0:  11.227803  11.073509  10.855879  10.618477  10.13521    9.494441
0:   8.896857   8.541986   8.5680485  8.980277   9.591237  10.103138
0:   7.9186807  7.7161484]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.991564  -9.047842  -8.64019   -7.769586  -6.848924  -6.1852393
0:  -5.8504987 -5.8639693 -6.405104  -6.968799  -7.6361203 -8.362495
0:  -8.728357  -8.853764  -8.514645  -7.8400035 -7.142648  -6.5113816
0:  -6.621323  -6.692963 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.118031    2.940898    3.0779727   3.3031135   3.244512    2.762323
0:   2.0434465   1.1979737   0.1275959  -0.68784523 -1.4378581  -2.2482944
0:  -2.8405948  -3.2484612  -3.2991443  -2.951385   -2.402659   -1.8484979
0:  -3.81947    -3.6139045 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3336625 6.4054065 6.751549  7.194866  7.530179  7.623911  7.5956798
0:  7.3326826 6.8264804 6.1900635 5.432152  4.621743  4.134459  4.1060734
0:  4.6986923 5.7651963 7.0270095 8.227294  8.076906  8.548797 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.653629  -14.657921  -14.226074  -13.432119  -12.616251  -12.037411
0:  -11.7714615 -11.67429   -12.01074   -12.353777  -12.772341  -13.381603
0:  -13.6704445 -13.747303  -13.509056  -12.924484  -12.3956375 -11.939457
0:  -13.650667  -13.849642 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.48400974 -0.20924234  0.48182297  1.5195522   2.584869    3.4234726
0:   3.901758    4.051999    3.530876    2.9492488   2.2510695   1.4763718
0:   1.0228662   0.7755046   0.91920424  1.362597    1.8385954   2.171424
0:  -0.13150072 -0.26461077]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.309992 19.952532 19.697786 19.534906 19.403448 18.889973 18.957298
0:  18.821608 18.876915 18.832745 18.203234 17.492104 16.542933 15.814266
0:  15.354373 15.164864 15.018913 14.755125 14.401744 14.031565]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.035587 23.136457 23.38677  23.607061 23.667883 23.432335 23.478626
0:  23.395287 23.400324 23.422564 23.151073 22.80082  22.515194 22.58299
0:  23.05262  23.801151 24.576256 25.088972 22.377274 22.480158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.09919  27.856922 27.57679  27.08633  26.62452  25.93177  26.097164
0:  26.129433 26.31461  26.320045 25.615646 24.909374 24.135227 23.983131
0:  24.406925 25.253635 26.169971 26.6768   26.529861 26.585579]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.004103  7.81948   7.9936094 8.429421  8.847545  9.016557  9.031344
0:  8.99228   8.708386  8.517432  8.2829075 7.8500986 7.4754457 7.2068634
0:  7.186942  7.566815  8.125093  8.687004  6.469368  6.429725 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1602249  -1.0183592  -0.48687935  0.3444085   1.1767387   1.7373719
0:   1.9931653   2.0156357   1.5702281   1.1412449   0.67959166  0.14412498
0:  -0.10756493 -0.2628827  -0.13580084  0.23501444  0.6560931   1.0372176
0:  -1.0805273  -1.166934  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.437819  11.615216  12.111347  12.674843  13.045101  13.131741
0:  13.144095  13.09719   12.8989105 12.741453  12.498842  12.098583
0:  11.84379   11.761488  12.025577  12.626896  13.326808  13.94939
0:  10.493727  10.672569 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.043798  -6.4051833 -6.3420434 -5.9063516 -5.4540143 -5.1666923
0:  -5.0312476 -5.0127654 -5.3546834 -5.6977496 -6.2117333 -6.940993
0:  -7.5010705 -7.857754  -7.83767   -7.451546  -7.0023904 -6.611921
0:  -7.8051333 -7.909885 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.32886314 -0.12599564  0.48357868  1.3473978   2.1684191   2.6695945
0:   3.0237534   3.1316042   2.8617616   2.6033504   2.1926668   1.6651845
0:   1.3377762   1.2261834   1.5077443   2.0562992   2.6431453   3.090589
0:   2.2963092   2.5245726 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8343496 6.905245  7.2817726 7.71975   7.9831944 7.951819  7.7869062
0:  7.4159102 6.78327   6.2140837 5.5180135 4.742935  4.1214952 3.7498975
0:  3.7521143 4.1124077 4.598012  5.0040393 3.2422051 3.3902164]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.30936    3.6047745  4.199711   4.9752207  5.598406   5.8957486
0:  5.977695   5.8625627  5.402447   4.8855133  4.205659   3.3240333
0:  2.57903    2.036859   1.8572998  2.1000485  2.515564   2.912723
0:  0.7356944  0.57963324]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.3244705   4.175464    4.343342    4.78171     5.1955357   5.390775
0:   5.216695    4.891801    4.077714    3.2982068   2.5048113   1.6199365
0:   0.95580006  0.44103098  0.2849903   0.46485996  0.8385954   1.2332397
0:  -0.43583107 -0.49965477]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.032272 29.593967 29.032082 28.414154 27.895206 27.179817 27.303993
0:  27.30212  27.506908 27.512222 26.796253 26.07093  25.26503  24.92627
0:  25.073994 25.524075 26.022976 26.20124  25.935677 25.868225]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.507309  4.512955  4.869446  5.6080165 6.4418845 7.0706563 7.302731
0:  7.350263  6.8383517 6.3221216 5.781545  5.17618   4.750839  4.4446764
0:  4.4321523 4.622034  4.961851  5.224203  3.5362551 3.3929386]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.376282   -4.3071465  -3.7413526  -2.7432065  -1.6257205  -0.71607065
0:  -0.07750368  0.2658658   0.07809401 -0.11520958 -0.3971734  -0.74654436
0:  -0.7794566  -0.57481146  0.07926464  1.0767875   2.1798315   3.1770484
0:   2.362831    3.1048532 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.048388  19.161205  18.956688  18.374928  17.470047  16.279352
0:  15.757889  15.216954  14.87004   14.343631  13.132034  11.843761
0:  10.334909   9.505534   9.3206215  9.51006    9.775701   9.662039
0:  14.054068  14.113    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.734802 12.929569 13.426521 14.063608 14.456149 14.471789 14.199438
0:  13.826263 13.192135 12.720316 12.242819 11.601304 11.029771 10.541169
0:  10.373995 10.611267 11.065035 11.476512  9.647505  9.669729]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.475193 18.355669 18.424355 18.493168 18.460144 18.36168  18.33953
0:  18.376339 18.300243 18.156176 17.809563 17.303337 16.970245 16.926973
0:  17.386944 18.202023 19.184212 20.104603 18.704912 18.901224]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.053097 16.763262 16.819752 17.000916 17.145767 17.042955 17.192408
0:  17.25131  17.241625 17.174911 16.798344 16.243525 15.773279 15.661465
0:  16.031992 16.869081 17.952478 18.920538 17.43875  17.445913]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.327989  4.402903  4.7432837 5.1274834 5.3838887 5.381054  5.2270823
0:  4.9374495 4.377686  3.8394246 3.242551  2.5914066 2.129545  1.8868804
0:  2.010248  2.4407141 3.0204408 3.5677814 1.6842997 2.1316137]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.542065  7.3399744 7.276963  7.163333  6.9371634 6.459673  6.582149
0:  6.7092857 7.0262685 7.334498  7.121233  6.7743073 6.205622  6.1396904
0:  6.5273275 7.3109827 8.127605  8.603563  8.874745  8.964973 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.5094447 2.5108304 2.9093294 3.6247597 4.296097  4.7379    4.8618684
0:  4.883012  4.5079794 4.2327385 3.9787462 3.5748692 3.375565  3.3013458
0:  3.497349  4.07875   4.82572   5.4874334 3.6736422 3.5917182]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.428129  -8.4298525 -8.02      -7.257328  -6.4697027 -5.932433
0:  -5.734412  -5.7204514 -6.1765194 -6.6437225 -7.1935024 -7.8840227
0:  -8.368499  -8.6188965 -8.547221  -8.049521  -7.4692307 -6.8702908
0:  -7.3101263 -7.3907423]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.07899   5.1432514 5.5158525 5.9927382 6.2563915 6.200343  6.1386724
0:  5.9998646 5.743431  5.5679507 5.2406883 4.7683353 4.365773  4.2121744
0:  4.473566  5.211641  6.163814  7.0268154 5.915551  5.997238 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -3.99058    -4.1421742  -3.8587098  -3.1659942  -2.512444   -2.1559424
0:   -2.0546823  -2.1845803  -2.715611   -3.2540822  -3.9287667  -4.792866
0:   -5.515219   -6.1390758  -6.520764   -6.6832356  -6.826713   -7.0371766
0:  -10.206551  -10.968191 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -7.9288745  -7.9565997  -7.580664   -6.949636   -6.4256806  -6.2411704
0:   -6.4293313  -6.8919024  -7.749904   -8.535593   -9.279057  -10.062277
0:  -10.499107  -10.6158695 -10.362423   -9.660715   -8.839273   -8.064323
0:   -8.841562   -8.778879 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.2068105 7.265257  7.6653776 8.143635  8.417057  8.330839  8.359214
0:  8.254112  8.145544  8.108199  7.788243  7.2911863 6.7443557 6.471898
0:  6.5423584 7.066173  7.753604  8.366737  6.4245744 6.525249 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.207459   -1.4669232  -1.3657193  -0.8844676  -0.27184868  0.1711874
0:   0.22123098  0.13278675 -0.5447583  -1.2634664  -2.057156   -3.0258775
0:  -3.8117871  -4.450413   -4.8118844  -4.728601   -4.4145675  -4.011393
0:  -6.40762    -7.147001  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.916718 25.567753 24.948017 24.032314 23.067413 21.93351  21.966068
0:  22.12505  22.635208 22.989468 22.403378 21.782742 20.85854  20.591505
0:  20.924692 21.544744 22.063026 21.928925 24.611164 24.62526 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9415097 -5.120382  -4.834068  -4.0904145 -3.2148376 -2.4556127
0:  -2.0865493 -1.8259425 -2.160931  -2.51341   -2.8999782 -3.5127978
0:  -3.8983846 -4.2177224 -4.266069  -3.8869681 -3.2993007 -2.660707
0:  -4.63158   -4.8595634]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.33454  18.083347 18.061789 18.1928   18.307163 18.240229 18.461216
0:  18.544785 18.6195   18.69425  18.479004 18.18622  18.005646 18.196138
0:  18.83358  19.860294 21.067427 22.14157  21.16729  21.550776]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.8872547   0.45026636  0.36513853  0.4204278   0.45344448  0.336895
0:   0.03506279 -0.4010043  -1.1550903  -1.9308991  -2.8416247  -3.8663163
0:  -4.6124654  -4.9401164  -4.735809   -4.0331483  -3.1141458  -2.2032337
0:  -2.725202   -1.94663   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.26351738 -0.37494564 -0.09725475  0.5054021   1.1749768   1.6953335
0:   1.9427457   2.1163654   1.8988442   1.6861048   1.439599    1.0483923
0:   0.8135705   0.6921854   0.85317755  1.3591375   2.018084    2.6676111
0:   0.57352304  0.51994514]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.3343825 6.485713  7.0725193 7.7867756 8.2394705 8.304525  8.205901
0:  8.076826  7.8263702 7.7269545 7.535656  7.087304  6.667966  6.313809
0:  6.32029   6.8106465 7.5783377 8.391233  6.321055  6.299552 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9257479  -2.0087547  -1.8976874  -1.7549362  -1.7713223  -2.11793
0:  -2.1117492  -2.1087728  -1.9144225  -1.4755192  -1.1358328  -0.7355795
0:  -0.22673607  0.7258477   1.9424193   3.3655155   4.6124763   5.4750476
0:   5.267011    5.3024826 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.206522  2.3611226 2.8417962 3.5949073 4.330332  4.876134  5.092799
0:  5.202005  4.830322  4.48248   4.10061   3.5573468 3.1710935 2.8779404
0:  2.9031703 3.2786944 3.8636425 4.432008  2.826881  2.878404 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3746743 4.451376  4.9309464 5.4715786 5.8337345 5.8931923 6.1155624
0:  6.2256207 6.305882  6.394916  6.155293  5.6976786 5.1941824 4.9844375
0:  5.117019  5.6570354 6.300025  6.792539  5.481048  5.6687403]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.707878 18.830212 19.30677  19.862732 20.258928 20.398754 20.733835
0:  21.040613 21.405682 21.789581 21.928867 21.953064 22.006523 22.264297
0:  22.916529 23.826565 24.794945 25.590305 22.812984 23.02788 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.0555835 -6.159409  -5.9031205 -5.2825646 -4.5310903 -3.882996
0:  -3.551333  -3.4394526 -3.8251872 -4.2870317 -4.8104224 -5.4002967
0:  -5.714043  -5.7604136 -5.4721675 -4.902674  -4.3119445 -3.805427
0:  -6.7476144 -6.8752837]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.4111266 4.253587  4.451524  5.0276327 5.668172  6.2676816 6.504453
0:  6.74547   6.5573945 6.489157  6.4448366 6.23809   6.1088696 6.040241
0:  6.25979   6.9515524 7.975057  9.005033  7.0082994 7.017701 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.7537374 -5.8236866 -5.456744  -4.8095484 -4.210655  -3.8710413
0:  -3.69135   -3.763174  -4.2428684 -4.6849623 -5.280366  -5.9827967
0:  -6.420985  -6.544775  -6.2887096 -5.645899  -5.014904  -4.471116
0:  -5.8019333 -5.845175 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.7605815 -8.245695  -8.290375  -7.7700152 -7.040199  -6.3465304
0:  -6.191114  -6.1652646 -6.799934  -7.408936  -7.9363036 -8.553219
0:  -8.88048   -9.033735  -8.861294  -8.201355  -7.2138314 -6.1147017
0:  -6.673121  -7.275994 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.018148  10.981446  11.232128  11.569554  11.683739  11.447329
0:  11.02269   10.597126  10.029736   9.612248   9.1334095  8.417209
0:   7.7326984  7.109811   6.8378177  7.0429144  7.596484   8.206734
0:   5.6252933  5.3156204]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.32377   8.012211  9.489664 11.436298 13.369508 14.964075 16.395878
0:  17.482313 18.168106 18.525118 18.333664 17.647436 16.873335 16.201048
0:  15.900107 16.009972 16.145473 16.195942 12.870834 12.622682]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.28814  27.055895 26.939165 26.885794 26.858849 26.6496   26.823156
0:  26.856413 26.94253  26.97957  26.639215 26.28122  25.946728 25.98206
0:  26.46537  27.27547  28.294724 29.212488 27.944017 28.019709]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.205657  -9.620477  -9.56527   -8.904692  -8.048534  -7.290491
0:  -6.962141  -6.845784  -7.2925396 -7.7488937 -8.084764  -8.502305
0:  -8.622964  -8.459826  -7.9364853 -6.9138966 -5.5700417 -4.123679
0:  -2.3387032 -2.0632792]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.0880661   0.97696066  1.1391611   1.5081167   1.725544    1.5946956
0:   1.4682665   1.107141    0.51423836 -0.13805199 -1.1603642  -2.4572906
0:  -3.6316977  -4.4111795  -4.5709248  -4.341199   -4.1150265  -4.0628786
0:  -5.1236897  -5.1069145 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.797562  -6.8305707 -6.4146304 -5.5868106 -4.7060475 -4.0479903
0:  -3.653047  -3.467321  -3.7691703 -4.0641494 -4.5296226 -5.1852937
0:  -5.665014  -5.9893746 -5.98684   -5.6333556 -5.137976  -4.6915765
0:  -7.2593865 -7.2573876]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.828314    4.3969536   4.30062     4.482355    4.7353816   4.8469934
0:   4.7100754   4.4163795   3.6623116   2.9096267   2.061695    1.1702795
0:   0.5224638   0.13332844  0.15086603  0.49006653  0.9327688   1.2653141
0:  -0.85306215 -1.6624274 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.048588  -9.1850815 -8.805399  -7.8935647 -6.824219  -5.9036756
0:  -5.454677  -5.209274  -5.5873456 -5.9817367 -6.408774  -6.9613423
0:  -7.2908483 -7.4598594 -7.3279395 -6.818259  -6.120381  -5.428801
0:  -7.3384614 -7.690145 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.27673   5.3653626 5.7566524 6.375176  6.9671164 7.385495  7.65017
0:  7.800319  7.6134157 7.3908844 7.050285  6.555439  6.228214  6.045463
0:  6.212673  6.7143555 7.32481   7.863551  6.2798915 6.3468623]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.216003   8.316502   8.70421    9.201172   9.607347   9.792128
0:  10.005277  10.1258545 10.061386   9.99099    9.686322   9.202713
0:   8.7677555  8.501402   8.545253   8.902475   9.313949   9.576796
0:   7.9623213  8.064611 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.44680738  0.3108387   0.5520911   1.1493721   1.7615652   2.1715603
0:   2.1771302   2.044313    1.4389224   0.88609123  0.36471605 -0.2986679
0:  -0.76970625 -1.0902119  -1.0789957  -0.56395197  0.2869072   1.2512031
0:   0.11566401 -0.07255268]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.452203  -4.6302366 -4.3464375 -3.6785192 -2.9899654 -2.468813
0:  -2.18826   -2.0573626 -2.3615093 -2.7022247 -3.1509438 -3.7906551
0:  -4.1946087 -4.410445  -4.289566  -3.7778502 -3.2004166 -2.6751223
0:  -4.7941723 -4.9752665]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.862409 15.647356 15.705624 15.889768 15.915653 15.683901 15.794743
0:  15.795893 15.786042 15.754653 15.267581 14.494205 13.609638 13.024187
0:  12.905006 13.32895  14.010429 14.62417  13.038134 12.691593]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.996481   7.1184855  7.5922294  8.29324    9.007338   9.434572
0:   9.722028   9.834315   9.561834   9.359153   9.091658   8.7164955
0:   8.584937   8.621944   8.933679   9.469167   9.926729  10.158197
0:   7.883854   7.8619494]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.159224 19.933352 19.957397 20.074492 20.153484 19.956564 20.093304
0:  20.11613  20.240482 20.33704  20.136627 19.888504 19.768288 19.921404
0:  20.57542  21.61648  22.836796 23.969673 23.45608  23.586151]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.219797  10.247427  10.513031  10.750854  10.871058  10.773047
0:  10.967118  11.120525  11.365448  11.5804    11.450629  11.179118
0:  10.867311  10.891956  11.254138  11.941934  12.692997  13.226893
0:  12.692175  12.7554245]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.232437 26.29552  26.405312 26.431347 26.456394 26.185303 26.468113
0:  26.566082 26.790195 26.92928  26.553318 26.174488 25.848114 25.930202
0:  26.54491  27.551624 28.791939 29.891396 30.075253 30.18607 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.713281  -9.062742  -8.889482  -8.1607485 -7.2458057 -6.4494314
0:  -6.0136414 -5.7959805 -6.108778  -6.466304  -6.8321114 -7.296975
0:  -7.5043955 -7.5138454 -7.207141  -6.5704956 -5.809745  -5.0772266
0:  -5.0211754 -4.830803 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.520894    1.3089828   1.4256625   1.9583943   2.6315398   3.195383
0:   3.5128284   3.675697    3.258578    2.7309957   2.0697808   1.2186246
0:   0.5771141   0.11257219  0.05638361  0.30006552  0.7278633   1.1172509
0:  -0.9768772  -1.2266397 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.926224 32.001553 32.068043 32.029686 32.01501  31.830608 32.424374
0:  32.94894  33.62204  34.167843 34.021885 33.863934 33.59223  33.78147
0:  34.433846 35.265213 36.006645 36.305004 35.585102 35.74238 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.93452   -13.128293  -12.949     -12.42826   -11.817172  -11.312009
0:  -10.734843  -10.291001  -10.223815  -10.298055  -10.731265  -11.462237
0:  -11.989234  -12.199792  -11.931783  -11.32285   -10.7479315 -10.375411
0:  -11.776277  -11.804517 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.978564    3.876613    4.121785    4.5481277   4.852536    4.875579
0:   4.724172    4.461701    3.9091568   3.4172428   2.8269193   2.0217853
0:   1.3408718   0.81113195  0.58981323  0.81061745  1.2390947   1.6922345
0:  -0.4637499  -0.66013956]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.066849  -12.060421  -11.639992  -10.827801   -9.986586   -9.40753
0:   -9.091505   -9.02326    -9.419302   -9.799078  -10.294813  -10.864323
0:  -11.144405  -11.170317  -10.886181  -10.324743   -9.905663   -9.634308
0:  -11.0648155 -11.225321 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.020523 26.337902 26.846159 27.237299 27.42911  27.235338 27.159666
0:  27.012388 26.929047 26.752748 26.30715  25.695593 25.05518  24.578297
0:  24.483221 24.638458 24.924618 25.239763 22.907806 23.254404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.269093 36.902153 36.42156  35.92648  35.42076  34.611465 34.558784
0:  34.350147 34.287918 34.062904 33.115414 32.241203 31.342842 30.979038
0:  31.213974 31.75793  32.46658  32.892998 33.35763  33.432674]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1946082  -0.84082747 -0.11391783  0.87978506  1.7869883   2.3933787
0:   2.6540203   2.6750402   2.2160501   1.733809    1.164177    0.43281937
0:  -0.15775776 -0.61054564 -0.70452833 -0.37300062  0.23032713  0.90539885
0:  -0.51768875 -0.58225346]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.469456 14.800592 15.339304 15.797541 16.05005  16.052374 16.280003
0:  16.436886 16.666449 16.886547 16.766394 16.478565 16.202646 16.188507
0:  16.571829 17.258951 18.043089 18.686846 15.777962 16.075306]
0: validation loss for strategy=forecast at epoch 47 : 0.26051029562950134
0: validation loss for velocity_u : 0.1700427085161209
0: validation loss for velocity_v : 0.2770625650882721
0: validation loss for specific_humidity : 0.1359424740076065
0: validation loss for velocity_z : 0.42374786734580994
0: validation loss for temperature : 0.09914033859968185
0: validation loss for total_precip : 0.45712578296661377
0: 48 : 21:09:21 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 48, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.3132, -1.3187, -1.3236, -1.3277, -1.3314, -1.3344, -1.3369, -1.3391, -1.3407, -1.3422, -1.3441, -1.3464,
0:         -1.3494, -1.3534, -1.3582, -1.3639, -1.3702, -1.3765, -1.3992, -1.3995, -1.3995, -1.3994, -1.3994, -1.3995,
0:         -1.3997], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3579, -0.3625, -0.3667, -0.3704, -0.3736, -0.3763, -0.3788, -0.3815, -0.3844, -0.3879, -0.3923, -0.3978,
0:         -0.4044, -0.4124, -0.4215, -0.4318, -0.4426, -0.4539, -0.3606, -0.3621, -0.3631, -0.3642, -0.3652, -0.3669,
0:         -0.3692], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7444, -0.7433, -0.7423, -0.7414, -0.7405, -0.7396, -0.7388, -0.7381, -0.7374, -0.7366, -0.7359, -0.7357,
0:         -0.7353, -0.7350, -0.7348, -0.7350, -0.7351, -0.7353, -0.7425, -0.7420, -0.7413, -0.7407, -0.7400, -0.7395,
0:         -0.7389], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2030,  0.2074,  0.2052,  0.1965,  0.1813,  0.1617,  0.1378,  0.1117,  0.0835,  0.0574,  0.0379,  0.0227,
0:          0.0183,  0.0205,  0.0313,  0.0422,  0.0509,  0.0531, -0.0186, -0.0230, -0.0295, -0.0382, -0.0534, -0.0751,
0:         -0.0990], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.3155, -1.3152, -1.3134, -1.3103, -1.3058, -1.3005, -1.2940, -1.2868, -1.2791, -1.2708, -1.2623, -1.2541,
0:         -1.2461, -1.2387, -1.2317, -1.2253, -1.2189, -1.2123, -1.2058, -1.1990, -1.1919, -1.1852, -1.1787, -1.1730,
0:         -1.1679], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2231, -0.2254, -0.2277, -0.2254, -0.2254, -0.2231, -0.2231, -0.2231, -0.2231, -0.2185, -0.2162, -0.2162,
0:         -0.2162, -0.2162, -0.2162, -0.2162, -0.2139, -0.2116, -0.2139, -0.2116, -0.2093, -0.2093, -0.2093, -0.2093,
0:         -0.2093], device='cuda:0')
0: [DEBUG] Epoch 48, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2185,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2139,     nan,     nan, -0.2093,     nan,     nan,     nan,
0:         -0.2024,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2139,     nan, -0.2300,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2116, -0.2116,     nan,     nan,     nan,     nan,     nan,     nan, -0.2070,     nan,     nan, -0.2116,
0:             nan, -0.2116,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2024,
0:             nan, -0.2047,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2254,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2369,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2288, -0.2277,     nan,     nan,     nan,     nan, -0.2277, -0.2265,     nan,
0:             nan,     nan, -0.2162,     nan, -0.2185, -0.2208,     nan,     nan,     nan,     nan,     nan, -0.2185,
0:             nan,     nan,     nan,     nan, -0.2219,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2380,     nan,     nan,     nan,     nan,
0:         -0.2392, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392, -0.2392,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 48, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7179, -0.7362, -0.7267, -0.6904, -0.6563, -0.6303, -0.6330, -0.6438, -0.6936, -0.7469, -0.8129, -0.8853,
0:         -0.9351, -0.9514, -0.9114, -0.8310, -0.7391, -0.6675, -0.5632, -0.6014, -0.6222, -0.6100, -0.5929, -0.5711,
0:         -0.5593], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9201, 0.9593, 0.9609, 0.9566, 0.9431, 0.9197, 0.8833, 0.8514, 0.8131, 0.7869, 0.7548, 0.7322, 0.6903, 0.6395,
0:         0.6063, 0.5936, 0.6237, 0.6466, 0.8996, 0.9287, 0.9240, 0.9191, 0.8943, 0.8704, 0.8365], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7912, -0.7932, -0.7924, -0.7884, -0.7757, -0.7633, -0.7595, -0.7540, -0.7547, -0.7588, -0.7646, -0.7675,
0:         -0.7570, -0.7436, -0.7336, -0.7247, -0.7292, -0.7328, -0.7926, -0.7886, -0.7826, -0.7713, -0.7616, -0.7562,
0:         -0.7451], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4765, 0.6712, 0.7254, 0.6088, 0.5439, 0.5945, 0.5609, 0.5147, 0.5334, 0.4683, 0.4412, 0.5379, 0.6155, 0.5474,
0:         0.4426, 0.5032, 0.5496, 0.4558, 0.4313, 0.3575, 0.3474, 0.3281, 0.3659, 0.4727, 0.4721], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.2940, -0.3171, -0.3320, -0.3430, -0.3525, -0.3640, -0.3735, -0.3799, -0.3830, -0.3845, -0.3940, -0.4043,
0:         -0.4133, -0.4161, -0.4203, -0.4298, -0.4495, -0.4734, -0.4937, -0.5049, -0.5123, -0.5196, -0.5301, -0.5447,
0:         -0.5572], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2326, -0.2322, -0.2493, -0.2418, -0.2393, -0.2371, -0.2402, -0.2403, -0.2349, -0.2341, -0.2342, -0.2430,
0:         -0.2396, -0.2362, -0.2375, -0.2391, -0.2363, -0.2425, -0.2283, -0.2335, -0.2311, -0.2367, -0.2311, -0.2368,
0:         -0.2386], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.21543008089065552; velocity_v: 0.2998497486114502; specific_humidity: 0.2139529138803482; velocity_z: 0.6598905920982361; temperature: 0.1589604914188385; total_precip: 0.8346150517463684; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2087935209274292; velocity_v: 0.3453344702720642; specific_humidity: 0.19775405526161194; velocity_z: 0.7542515993118286; temperature: 0.1886088252067566; total_precip: 0.9334211349487305; 
0: epoch: 48 [1/5 (20%)]	Loss: 0.88402 : 0.38180 :: 0.21708 (2.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22284409403800964; velocity_v: 0.36396360397338867; specific_humidity: 0.16589699685573578; velocity_z: 0.6618823409080505; temperature: 0.1599806696176529; total_precip: 0.5366908311843872; 
0: epoch: 48 [2/5 (40%)]	Loss: 0.53669 : 0.31699 :: 0.21980 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.203060582280159; velocity_v: 0.3441786468029022; specific_humidity: 0.17208656668663025; velocity_z: 0.6864909529685974; temperature: 0.15437854826450348; total_precip: 1.2285656929016113; 
0: epoch: 48 [3/5 (60%)]	Loss: 1.22857 : 0.43041 :: 0.21460 (16.06 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2169930636882782; velocity_v: 0.3027549684047699; specific_humidity: 0.20763416588306427; velocity_z: 0.5459583401679993; temperature: 0.1371346414089203; total_precip: 0.5695062279701233; 
0: epoch: 48 [4/5 (80%)]	Loss: 0.56951 : 0.29531 :: 0.21272 (16.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [5.2452087e-05 4.9114227e-05 4.5299534e-05 4.3392178e-05 4.2915348e-05
0:  4.1961670e-05 4.1484833e-05 4.1961670e-05 4.3392178e-05 4.5299534e-05
0:  4.6730042e-05 4.7683716e-05 4.9114227e-05 5.0067902e-05 5.1021580e-05
0:  5.1021580e-05 5.1498409e-05 5.1975250e-05 5.1498409e-05 4.9591064e-05
0:  4.7683716e-05 4.6253204e-05 4.4345856e-05 4.2438507e-05 4.0531158e-05
0:  3.8146973e-05 3.7670135e-05 3.7670135e-05 3.7670135e-05 3.7670135e-05
0:  4.1961670e-05 4.7206879e-05 5.2452087e-05 5.7697296e-05 7.0095062e-05
0:  8.2015991e-05 9.4413757e-05 1.0728836e-04 1.2493134e-04 1.4257431e-04
0:  1.6021729e-04 1.7976761e-04 2.0265579e-04 2.2506714e-04 2.4700165e-04
0:  2.7704239e-04 3.0899048e-04 3.3998489e-04 3.6907196e-04 3.9672852e-04
0:  4.2486191e-04 4.5204163e-04 4.8160553e-04 5.1879883e-04 5.5551529e-04
0:  5.9223175e-04 6.2847137e-04 6.6804886e-04 7.0714951e-04 7.4577332e-04
0:  7.8153610e-04 8.1968307e-04 8.5783005e-04 8.8930130e-04 8.8500977e-04
0:  8.8071823e-04 8.7547302e-04 8.7499619e-04 8.7833405e-04 8.8119507e-04
0:  8.8357925e-04 8.7642670e-04 8.6641312e-04 8.5592270e-04 8.4495544e-04
0:  8.3446503e-04 8.2302094e-04 8.1110001e-04 7.9774857e-04 7.7772141e-04
0:  7.5721741e-04 7.3623657e-04 7.1525574e-04 6.9475174e-04 6.7329407e-04
0:  6.5183640e-04 6.2179565e-04 5.8889389e-04 5.5551529e-04 5.2261353e-04
0:  4.7636032e-04 4.2915344e-04 3.8194656e-04 3.3760071e-04 2.8610229e-04
0:  2.3460388e-04 1.8310547e-04 1.4972687e-04 1.2683868e-04 1.0347366e-04
0:  8.0585480e-05 6.6280365e-05 5.3405762e-05 4.0531158e-05 2.8610229e-05
0:  2.4318695e-05 2.0027161e-05 1.5735626e-05 2.0027161e-05 1.9550323e-05
0:  1.9073486e-05 1.8596649e-05 1.9073486e-05 1.9550323e-05 2.0027161e-05
0:  2.0503998e-05 2.2411346e-05 2.4795532e-05 2.6702881e-05 2.7179718e-05
0:  2.7656555e-05 2.8133392e-05 2.8610229e-05 2.9087067e-05 2.9563904e-05
0:  3.0040741e-05 3.0994415e-05 3.3855438e-05 3.6239624e-05 3.8623810e-05
0:  3.6716461e-05 4.1961670e-05 4.8160557e-05 5.3882599e-05 6.8187714e-05
0:  8.5353851e-05 1.0251999e-04 1.1062622e-04 1.2540817e-04 1.5115738e-04
0:  1.7690659e-04 2.0408630e-04 2.3651123e-04 2.6845932e-04 2.9945374e-04
0:  3.1423569e-04 3.5190582e-04 3.9052963e-04 4.2819977e-04 4.5681000e-04
0:  4.8208237e-04 5.0735474e-04 5.3071976e-04 5.6648254e-04 6.0606003e-04
0:  6.4516068e-04 6.7996979e-04 7.0381165e-04 7.2669983e-04 7.4958801e-04
0:  7.8248978e-04 7.7438354e-04 7.6389313e-04 7.5340271e-04 7.4720383e-04
0:  7.4195862e-04 7.3671341e-04 7.5626373e-04 7.7819824e-04 7.7962875e-04
0:  7.8105927e-04 7.7915192e-04 7.6580048e-04 7.5292587e-04 7.4052811e-04
0:  7.3957443e-04 7.4529648e-04 7.5101852e-04 7.5721741e-04 7.5769424e-04
0:  7.5674057e-04 7.5578690e-04 7.4958801e-04 7.3480606e-04 7.1716309e-04
0:  6.9952011e-04 7.1334839e-04 8.2302094e-04 9.3269354e-04 1.0366440e-03
0:  1.0151863e-03 1.0676384e-03 1.1224747e-03 1.1768341e-03 1.0385513e-03
0:  8.3541870e-04 6.3180923e-04 5.2022934e-04 4.8017502e-04 4.4918060e-04]
0: Target values (first 200):
0: [5.72204590e-06 6.67572021e-06 7.62939453e-06 7.62939453e-06
0:  9.53674316e-06 1.14440918e-05 1.23977661e-05 1.62124634e-05
0:  1.81198120e-05 1.90734863e-05 2.09808350e-05 2.57492065e-05
0:  3.33786011e-05 4.10079956e-05 4.76837158e-05 4.38690186e-05
0:  4.00543213e-05 3.62396240e-05 3.33786011e-05 2.95639038e-05
0:  2.67028809e-05 2.38418579e-05 2.28881836e-05 2.28881836e-05
0:  2.38418579e-05 2.38418579e-05 2.28881836e-05 2.09808350e-05
0:  1.90734863e-05 1.71661377e-05 2.00271606e-05 2.28881836e-05
0:  2.57492065e-05 2.67028809e-05 2.47955322e-05 2.28881836e-05
0:  2.00271606e-05 2.00271606e-05 2.19345093e-05 2.28881836e-05
0:  2.47955322e-05 3.14712524e-05 4.00543213e-05 4.95910645e-05
0:  5.81741333e-05 7.62939453e-05 9.63211060e-05 1.16348267e-04
0:  1.37329102e-04 1.70707703e-04 2.05039978e-04 2.38418579e-04
0:  2.67028809e-04 2.91824341e-04 3.16619873e-04 3.41415405e-04
0:  3.06129456e-04 2.46047974e-04 1.85966492e-04 1.26838684e-04
0:  1.15394592e-04 1.03950500e-04 9.34600830e-05 8.10623169e-05
0:  7.53402710e-05 6.86645508e-05 6.19888306e-05 5.34057617e-05
0:  4.48226929e-05 3.52859497e-05 2.67028809e-05 2.09808350e-05
0:  1.71661377e-05 1.33514404e-05 9.53674316e-06 8.58306885e-06
0:  7.62939453e-06 6.67572021e-06 7.62939453e-06 9.53674316e-06
0:  1.14440918e-05 1.33514404e-05 3.62396240e-05 4.95910645e-05
0:  6.38961792e-05 7.82012939e-05 8.86917114e-05 9.82284546e-05
0:  1.08718872e-04 1.14440918e-04 1.05857849e-04 9.72747803e-05
0:  8.96453857e-05 8.58306885e-05 1.08718872e-04 1.31607056e-04
0:  1.54495239e-04 1.42097473e-04 1.01089478e-04 6.00814819e-05
0:  2.00271606e-05 8.58306885e-06 1.23977661e-05 1.52587891e-05
0:  1.90734863e-05 1.81198120e-05 1.81198120e-05 1.71661377e-05
0:  9.53674316e-06 1.14440918e-05 1.33514404e-05 1.43051147e-05
0:  1.43051147e-05 1.52587891e-05 1.62124634e-05 1.71661377e-05
0:  1.71661377e-05 1.62124634e-05 1.62124634e-05 2.00271606e-05
0:  2.67028809e-05 3.24249268e-05 3.91006470e-05 3.62396240e-05
0:  3.43322754e-05 3.24249268e-05 3.14712524e-05 3.33786011e-05
0:  3.52859497e-05 3.71932983e-05 3.62396240e-05 3.62396240e-05
0:  3.62396240e-05 3.62396240e-05 3.43322754e-05 3.14712524e-05
0:  2.95639038e-05 2.67028809e-05 3.05175781e-05 3.62396240e-05
0:  4.19616699e-05 4.48226929e-05 3.81469727e-05 3.14712524e-05
0:  2.47955322e-05 2.47955322e-05 3.05175781e-05 3.52859497e-05
0:  4.10079956e-05 4.38690186e-05 4.57763635e-05 4.67300415e-05
0:  5.24520874e-05 7.15255737e-05 9.05990601e-05 1.09672546e-04
0:  1.21116638e-04 1.09672546e-04 9.91821289e-05 9.05990601e-05
0:  1.20162964e-04 1.51634216e-04 1.83105469e-04 2.14576721e-04
0:  2.01225281e-04 1.73568726e-04 1.45912170e-04 1.00135803e-04
0:  9.15527344e-05 8.58306885e-05 8.01086426e-05 7.24792480e-05
0:  6.10351562e-05 4.86373865e-05 3.62396240e-05 2.86102295e-05
0:  2.47955322e-05 2.09808350e-05 1.71661377e-05 1.62124634e-05
0:  1.62124634e-05 1.62124634e-05 1.43051147e-05 1.43051147e-05
0:  1.62124634e-05 1.71661377e-05 1.81198120e-05 1.52587891e-05
0:  1.33514404e-05 1.04904175e-05 1.90734863e-05 2.38418579e-05
0:  2.86102295e-05 3.24249268e-05 1.29699707e-04 2.56538391e-04
0:  3.84330750e-04 4.47273254e-04 4.56809998e-04 4.86373901e-04]
0: Prediction values (first 20):
0: [0.86130714 0.8857646  1.3750191  2.0820436  2.6953287  3.0767164
0:  3.366278   3.5048006  3.4427814  3.459048   3.367645   3.1007993
0:  2.9506714  3.0092852  3.3333664  4.0214405  4.741647   5.408692
0:  4.262777   4.2502995 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -0.943, max = 1.933, mean = 0.227
0:          sample (first 20): tensor([-0.4315, -0.4294, -0.3879, -0.3280, -0.2761, -0.2438, -0.2193, -0.2075, -0.2128, -0.2114, -0.2191, -0.2417,
0:         -0.2545, -0.2495, -0.2220, -0.1638, -0.1028, -0.0462, -0.3378, -0.3627])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.4730525  -0.7430272  -0.55498075  0.08787394  0.74868155  1.1848397
0:   1.1627402   1.000884    0.28933144 -0.3743739  -1.0288477  -1.8834791
0:  -2.5456257  -3.0921822  -3.3452387  -3.081151   -2.5145454  -1.8249674
0:  -3.2646189  -3.773613  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.685995  15.247007  15.039343  14.969225  14.852024  14.43767
0:  14.4927845 14.343021  14.336969  14.239998  13.732102  13.096451
0:  12.371496  11.966133  11.939298  12.319275  12.980335  13.591947
0:  12.133953  12.058081 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4943604  -4.0994925  -4.2299886  -3.9468608  -3.5362668  -3.2137485
0:  -2.134575   -1.2330165   0.01191282  1.013032    1.3388262   1.3533192
0:   1.1888924   1.4780693   2.2185843   3.27911     4.270383    4.7666183
0:   3.722124    3.6639104 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.259106 30.245255 30.144815 29.859554 29.505268 28.977062 29.24624
0:  29.477528 29.919134 30.164642 29.634123 29.043312 28.287842 28.063805
0:  28.370949 28.948284 29.557806 29.78876  29.96635  30.24552 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.3982725  -4.144635   -3.3728619  -2.2115126  -1.1552525  -0.4884143
0:  -0.42466354 -0.62457037 -1.3655872  -2.0281239  -2.6195774  -3.2990522
0:  -3.6670341  -3.837679   -3.6985826  -3.0727901  -2.2772093  -1.4971619
0:  -5.8146353  -6.898471  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.521144 10.349853 10.82519  11.424745 11.515531 10.878331  9.995836
0:   9.125347  8.470689  8.329512  8.480755  8.531838  8.760686  9.065928
0:   9.755569 10.985279 12.593815 14.185051 12.081713 11.83329 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.4407797  7.529443   7.992792   8.611005   9.077934   9.23425
0:   9.363131   9.504188   9.628472   9.925166  10.163696  10.238151
0:  10.3018875 10.420443  10.735309  11.286588  11.8639965 12.253586
0:   9.868137   9.962477 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.81479  23.860874 24.185507 24.550905 24.80235  24.699041 24.754913
0:  24.633606 24.562021 24.57492  24.333439 23.987516 23.618309 23.374891
0:  23.42049  23.700459 24.053173 24.22812  20.695408 20.659685]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.487837  12.6562195 13.093561  13.646629  13.953508  13.947508
0:  13.6587515 13.380228  12.81605   12.400925  11.95562   11.294659
0:  10.690205  10.153663  10.0181265 10.39779   11.19178   12.054425
0:   9.932449   9.989495 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.394583  -8.641771  -8.429396  -7.798494  -7.0750194 -6.5644546
0:  -6.4344096 -6.5813775 -7.2989173 -8.019102  -8.726927  -9.450775
0:  -9.763908  -9.835184  -9.572908  -9.023336  -8.477936  -7.9564395
0:  -8.060305  -8.260653 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.064358  11.978638  12.15759   12.433172  12.418623  12.125048
0:  11.640438  11.219231  10.621059  10.17195    9.688724   9.007289
0:   8.378012   7.810223   7.622522   7.7886896  8.200304   8.597818
0:   6.3081093  6.2230964]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.236286 26.388107 26.682533 26.879482 26.787003 26.320599 26.096031
0:  25.769154 25.480343 25.35587  24.992756 24.520756 24.141777 24.039635
0:  24.347084 24.961105 25.533676 25.763222 21.968613 21.841187]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.879581 26.918718 26.99776  27.074022 27.165281 27.001486 27.509405
0:  27.796167 28.295553 28.618732 28.425941 28.263523 28.122011 28.398996
0:  29.231348 30.37499  31.702394 32.786922 33.745815 34.277042]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [41.31349  41.247253 40.655575 39.92383  39.127644 38.180855 38.312336
0:  38.496964 38.941284 39.10039  38.342125 37.674988 36.83492  36.618263
0:  37.07618  37.595062 38.06577  37.798435 38.51646  38.695183]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9104114 -5.1887403 -4.960819  -4.3021045 -3.5815473 -3.0563922
0:  -2.8690782 -2.886973  -3.3447652 -3.829373  -4.3817744 -5.0982327
0:  -5.6371493 -5.963104  -5.972199  -5.624624  -5.100697  -4.603472
0:  -7.584418  -7.8580985]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.685032  11.323751  11.112146  10.890271  10.567508  10.06482
0:   9.81157    9.487745   9.19286    8.968487   8.55444    8.115365
0:   7.8170996  7.9314623  8.444752   9.308794  10.253317  11.011383
0:   9.258201   9.049014 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.852458  7.6673307 7.697153  7.788026  7.755877  7.482332  7.5815983
0:  7.5254326 7.4157014 7.166749  6.3742156 5.3862987 4.326483  3.7651727
0:  3.6676674 4.0474076 4.527998  4.888482  4.85282   4.6770706]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.2384815 -6.0577145 -5.367614  -4.297905  -3.1486216 -2.2029886
0:  -1.5972762 -1.2541776 -1.4282022 -1.6538587 -1.9551086 -2.376274
0:  -2.5832796 -2.638465  -2.3519726 -1.8183694 -1.192812  -0.6558261
0:  -2.047556  -1.9297595]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.582773  -14.671839  -14.196657  -13.166674  -11.948673  -10.9005165
0:  -10.296387   -9.977449  -10.259701  -10.641411  -11.075813  -11.6186905
0:  -11.923965  -12.052696  -11.831295  -11.235149  -10.43169    -9.580963
0:  -10.170275  -10.179998 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.595509  -4.497227  -3.9641852 -3.0863023 -2.1984048 -1.5380993
0:  -1.1051459 -0.9079881 -1.1711097 -1.4840617 -1.9302392 -2.5029607
0:  -2.8542466 -2.9891877 -2.794911  -2.2774491 -1.7102609 -1.2545147
0:  -2.9005256 -2.9414592]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.5681033  -0.63255167 -0.23545504  0.5784497   1.4635782   2.204751
0:   2.5030494   2.6861348   2.3040032   1.9259939   1.4897299   0.8357277
0:   0.31504774 -0.14935303 -0.31836033 -0.02024364  0.6032429   1.3277907
0:  -1.1645703  -1.5709138 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6344037 -3.5027623 -2.926393  -2.1009512 -1.3997154 -1.0307608
0:  -0.9441676 -0.9977741 -1.3894358 -1.7022953 -2.054377  -2.5703664
0:  -2.9223437 -3.1013856 -2.9785342 -2.4396043 -1.7689791 -1.1775532
0:  -2.6778474 -2.6201673]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1125994 -4.5890837 -4.613064  -4.250986  -3.860775  -3.689355
0:  -3.7051039 -3.9219847 -4.5384235 -5.1642437 -5.929675  -6.8760223
0:  -7.5538006 -7.974739  -7.9994297 -7.539523  -6.9561596 -6.4089527
0:  -7.9254103 -8.139084 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.031078  -9.324177  -9.0975    -8.473316  -7.8766093 -7.540548
0:  -7.573076  -7.721935  -8.164129  -8.509268  -8.805538  -9.273981
0:  -9.630339  -9.839264  -9.819103  -9.313555  -8.569696  -7.7393384
0:  -8.631109  -9.036753 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.354279  25.551584  24.902927  24.320251  23.626577  22.542585
0:  21.942842  21.12233   20.426678  19.60936   18.264296  16.774994
0:  15.248501  14.117926  13.565623  13.500736  13.766922  13.9993515
0:  11.922022  10.800991 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.3899426  5.6959114  6.198888   6.681123   7.034366   7.192663
0:   7.6524653  8.079702   8.487535   8.935236   9.012327   8.9326105
0:   8.789573   9.022842   9.541731  10.320585  10.960651  11.213615
0:  11.202496  11.645285 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5360637 3.3698437 3.4236479 3.6853228 3.9379435 4.0409636 4.189521
0:  4.226515  4.0457263 3.875944  3.50499   3.0586581 2.8016443 2.8326054
0:  3.3022377 3.895527  4.312818  4.447919  4.3319545 4.2975903]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.944942  -9.336285  -9.222298  -8.536625  -7.627748  -6.7763457
0:  -6.49075   -6.3604035 -6.9120183 -7.4451327 -7.9527974 -8.641237
0:  -9.090229  -9.436722  -9.482683  -8.981817  -8.0918045 -6.992495
0:  -7.907129  -8.460539 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.640555   9.651186   9.863642  10.056074  10.1103325  9.896674
0:  10.114361  10.279762  10.547939  10.832902  10.672827  10.399776
0:   9.971153   9.959796  10.289097  10.977753  11.6778555 12.139324
0:  11.755788  12.014086 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.269075 20.311884 20.602667 20.965315 21.295755 21.42703  21.860222
0:  22.146471 22.445831 22.72835  22.682999 22.581696 22.553661 22.903585
0:  23.687355 24.791464 25.980501 26.933434 24.793472 25.07336 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.719784 18.72874  18.820877 18.915833 18.932655 18.737194 19.105751
0:  19.516441 20.260273 21.092941 21.595356 22.08762  22.501873 23.238659
0:  24.367561 25.679787 26.987465 27.889942 25.828678 25.879307]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.6326084 4.763585  5.4395795 6.527168  7.521688  8.183288  8.412821
0:  8.553532  8.359928  8.238296  8.045073  7.4758034 6.7648473 6.0139227
0:  5.528479  5.597155  6.102862  6.787809  5.04373   4.8054214]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.169209 22.019907 21.82874  21.56432  21.292313 20.847519 21.228878
0:  21.516531 21.983255 22.285666 21.883352 21.496542 20.971548 21.064075
0:  21.6854   22.645931 23.548685 23.992937 24.763388 24.77894 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.38653     5.5717583   6.1158457   6.7821493   7.0898275   6.8740916
0:   6.140169    5.3249707   4.233965    3.4292498   2.7869115   2.028459
0:   1.4214764   0.88512564  0.65986156  0.9722333   1.670577    2.4884892
0:   0.04393864 -0.35217476]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.043405 14.287809 14.824244 15.433327 15.913614 16.077927 16.180641
0:  16.211926 16.134762 16.153313 16.063854 15.769985 15.581469 15.501906
0:  15.853323 16.669874 17.84014  19.000122 16.996609 17.209879]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.85834026 1.0863299  1.7164979  2.611669   3.3955517  3.925421
0:  4.20251    4.422035   4.352236   4.3450336  4.265481   3.9831183
0:  3.8186226  3.8129966  4.159996   4.904529   5.792433   6.540969
0:  4.6926775  4.8752027 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.196917 19.198776 19.208073 18.98762  18.629665 17.910187 17.64557
0:  17.24992  17.074453 16.909565 16.27102  15.548277 14.688726 14.019897
0:  13.744522 13.840435 14.34476  15.043394 14.282843 14.681702]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5354056  -3.821753   -3.6433825  -2.9468179  -2.0709033  -1.3154855
0:  -0.95355177 -0.78063726 -1.259933   -1.8178067  -2.4381032  -3.2281976
0:  -3.746677   -4.0736165  -4.0333257  -3.6025157  -2.9801564  -2.333653
0:  -2.51442    -2.6558843 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1932163  -3.3621407  -3.0390596  -2.2869883  -1.5585637  -1.0178919
0:  -0.64882755 -0.3861332  -0.43663025 -0.46313095 -0.6170263  -1.022397
0:  -1.3996382  -1.6480546  -1.5772872  -1.096777   -0.41521788  0.20907164
0:  -1.045145   -1.3677635 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.798871  -11.730248  -11.175334  -10.204158   -9.153702   -8.350372
0:   -7.9978848  -7.9823217  -8.632653   -9.339628  -10.10789   -10.927363
0:  -11.375649  -11.586526  -11.493531  -11.101638  -10.6957445 -10.282475
0:  -11.499362  -11.70453  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.581106 29.663748 29.762707 29.645775 29.392244 28.754663 28.489017
0:  28.034101 27.585344 27.02548  25.961813 24.805029 23.596472 22.762531
0:  22.382158 22.400429 22.589918 22.694729 20.545502 20.264683]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.573479   -7.572164   -7.194301   -6.700932   -6.2292695  -5.951882
0:  -5.320702   -4.740071   -4.1092353  -3.4801526  -3.2204432  -3.0992122
0:  -3.0567193  -2.5801482  -1.7485614  -0.611825    0.40710688  1.1133752
0:   1.7456126   1.9653225 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5290647  -1.7751417  -1.651463   -1.1204104  -0.53201246 -0.1068964
0:  -0.09832335 -0.23887968 -0.9127717  -1.5496082  -2.186161   -2.9130878
0:  -3.4555492  -3.8483553  -3.9536533  -3.6574965  -3.1162643  -2.4887009
0:  -2.9820013  -3.1445093 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7202315 2.7385592 3.084268  3.715025  4.258635  4.5803127 4.554471
0:  4.5115614 4.1148696 3.8377771 3.5915916 3.143038  2.7710428 2.4680295
0:  2.4682589 2.9417667 3.7750292 4.7095814 3.0756006 2.865283 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6043429  -1.8957019  -1.7834649  -1.2481923  -0.559       0.08077955
0:   0.40077734  0.658164    0.421638    0.2423749   0.0512042  -0.35748672
0:  -0.6162076  -0.83143616 -0.78781176 -0.23804712  0.6559186   1.7275667
0:   0.5760603   0.37005615]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.8654876  7.1727343  7.857123   8.611044   9.289846   9.746891
0:  10.254666  10.734914  11.081312  11.386875  11.519602  11.454287
0:  11.497005  11.757152  12.449591  13.468281  14.635557  15.721441
0:  13.919043  14.217146 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.414646 22.309649 22.31604  22.323595 22.269497 21.954208 21.979313
0:  21.851173 21.813713 21.75526  21.393753 21.002853 20.69755  20.702866
0:  21.124996 21.832253 22.715073 23.436354 21.919384 21.899242]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.281784 35.883663 35.332222 34.741127 34.221622 33.525444 33.716743
0:  33.81103  34.13933  34.36075  33.9206   33.634663 33.392662 33.697132
0:  34.56756  35.683178 36.82337  37.503696 38.23588  38.258568]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.190572  8.952969  9.000924  9.390242  9.757712  9.971922  9.81191
0:  9.634169  9.022349  8.534209  8.063458  7.381078  6.735453  6.0782204
0:  5.6939125 5.823323  6.4326453 7.266692  4.6065464 4.106976 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.062493  4.964798  5.2951126 5.722626  5.9319506 5.7453394 5.5961556
0:  5.407355  5.286233  5.302987  5.17502   4.858473  4.5353575 4.4224734
0:  4.689044  5.4389353 6.403447  7.2760606 5.8354316 5.859333 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.6702914  7.8000455  8.271482   8.839968   9.283818   9.453819
0:   9.7037115  9.799892   9.80689    9.805866   9.556549   9.172548
0:   8.830383   8.732902   8.911177   9.388326   9.9383545 10.332948
0:   8.352898   8.532401 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [42.423622 42.24046  41.944923 41.466312 40.857487 39.99192  39.591736
0:  39.067986 38.64997  38.12096  37.09928  36.07529  35.13434  34.534187
0:  34.442715 34.547802 34.764683 34.81245  33.618477 33.57869 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.5185018  -2.6175113  -2.3893132  -1.802444   -1.177135   -0.79072905
0:  -0.8799195  -1.2446637  -2.205316   -3.1261535  -4.0149083  -4.854198
0:  -5.3275275  -5.469647   -5.2013907  -4.511147   -3.6598468  -2.818027
0:  -2.3308806  -2.0097833 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.120743 29.974163 30.094063 30.199713 30.195135 29.898006 29.813215
0:  29.59621  29.33145  28.992435 28.338135 27.589075 26.971918 26.687502
0:  26.992632 27.663563 28.418303 29.023228 28.560427 28.490782]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.084528   7.4678173  7.8506794  7.9141617  7.687436   7.267191
0:   7.3027816  7.420265   7.68606    7.946267   7.799728   7.5170817
0:   7.1685867  7.282079   7.7692156  8.618868   9.403877   9.931726
0:  10.799868  11.189264 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.518003 15.267252 15.26519  15.499838 15.866447 16.066273 16.842184
0:  17.415417 18.142538 18.824743 19.21513  19.559002 20.018896 20.813652
0:  21.98352  23.46703  25.049303 26.290373 23.43012  23.554733]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.811104   8.396229   8.453385   8.7155075  8.8809805  8.870386
0:   9.003975   9.073661   9.133794   9.249467   9.171835   8.975851
0:   8.8806095  9.031118   9.658811  10.614289  11.663145  12.586057
0:  10.455465  10.216717 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.497604 24.469864 24.498852 24.436306 24.250324 23.826096 23.891804
0:  23.842243 23.880001 23.877735 23.405827 22.815002 22.244823 22.092545
0:  22.431091 23.221527 24.2439   25.12685  24.131195 24.358765]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [47.985126 47.61298  46.848656 45.92414  44.976315 43.883743 43.85246
0:  43.890903 44.26146  44.247475 43.584385 42.791798 41.76965  41.441814
0:  41.69751  42.142467 42.559956 42.13013  42.99779  43.1692  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.46739   10.608458  10.999123  11.44262   11.717747  11.7500725
0:  11.804585  11.839054  11.841203  11.956234  11.943308  11.7331295
0:  11.536798  11.489832  11.769973  12.405697  13.279633  14.089821
0:  11.402771  11.547916 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.204423  3.1196663 3.4175518 3.8773134 4.1922755 4.248172  4.1127696
0:  4.0620117 3.9049551 3.9876633 4.098316  4.0079126 3.9387653 3.9872334
0:  4.337496  5.130759  6.185341  7.182496  4.878228  5.089858 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.869657  -13.656661  -14.01674   -13.867461  -13.485608  -13.08047
0:  -12.529269  -12.097363  -11.903247  -11.928397  -12.154973  -12.443099
0:  -12.388137  -11.769166  -10.532989   -8.826953   -7.141902   -5.7302876
0:   -3.0121918  -3.087946 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.708214  -11.896309  -11.535925  -10.557028   -9.359983   -8.336317
0:   -7.7300754  -7.4421515  -7.7801633  -8.153915   -8.510023   -8.90514
0:   -8.965227   -8.774475   -8.232964   -7.3497796  -6.3192677  -5.3189883
0:   -5.258884   -5.3625703]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.133159 14.988312 15.201771 15.677225 16.085691 16.405617 16.659422
0:  17.074848 17.468006 18.104263 18.695421 18.925116 18.918823 18.600845
0:  18.27466  18.110073 18.145292 18.204569 15.461248 15.823347]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.6672435   0.67641306  0.94992256  1.5504651   2.2217054   2.7573977
0:   2.8834176   2.817275    2.0722156   1.3051176   0.4471488  -0.45858908
0:  -1.098939   -1.4518719  -1.4076204  -0.8608551  -0.05690718  0.80532503
0:  -0.18658829 -0.29220438]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.902381  7.9446282 8.339759  8.914966  9.351571  9.503979  9.567477
0:  9.57038   9.368523  9.192635  8.855604  8.236998  7.6509523 7.2194867
0:  7.1805196 7.6143327 8.29769   8.97256   6.4549794 6.4109774]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [41.531055 41.272392 40.84022  40.094128 38.90113  37.010284 35.396744
0:  33.443207 31.491879 29.497276 27.0512   24.811249 22.855457 21.705637
0:  21.46999  21.850685 22.46687  22.913391 23.396465 22.99524 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.515768  -10.009691  -10.068626   -9.79887    -9.476828   -9.33387
0:   -9.275841   -9.351652   -9.720945  -10.052399  -10.536697  -11.07349
0:  -11.343882  -11.26582   -10.72653    -9.788628   -8.693556   -7.6082644
0:  -11.360031  -11.080452 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0027394  -1.7838387  -1.1080518  -0.16993809  0.64847136  1.1773992
0:   1.5452771   1.7790189   1.7506809   1.7959247   1.7380834   1.4694295
0:   1.3363504   1.3380728   1.6454115   2.2833667   2.9969585   3.5896544
0:   2.41577     2.5928223 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.252546  -6.5011163 -6.2331185 -5.4661727 -4.6656423 -4.140228
0:  -4.0957923 -4.4133716 -5.386802  -6.362779  -7.3496594 -8.418335
0:  -9.054449  -9.394514  -9.2782955 -8.653044  -7.8448443 -6.9686694
0:  -7.582848  -7.8400054]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.9881506  8.118133   8.524614   8.938106   9.158621   9.0696945
0:   9.243403   9.306868   9.437321   9.601004   9.434959   9.107543
0:   8.728925   8.738896   9.146343   9.998649  10.993806  11.845697
0:  10.283626  10.47016  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.58307  20.621584 19.826588 19.07579  18.337246 17.37311  16.8962
0:  16.3539   15.949846 15.575007 14.833451 13.994911 13.132387 12.599052
0:  12.519896 12.844526 13.346144 13.768209 11.375766 10.425158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8931637  -3.551818   -2.728548   -1.6929541  -0.77613163 -0.15147209
0:   0.40252924  0.8102789   0.98928833  1.0907917   1.0069895   0.6967411
0:   0.549284    0.6650152   1.1560044   1.9879894   2.8242135   3.4807777
0:   2.4859571   2.779048  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7754946 4.0133386 4.560286  5.1465135 5.622347  5.857171  5.9831133
0:  6.0444117 5.8852015 5.7262897 5.4485474 5.0608363 4.7647095 4.6696506
0:  4.9220376 5.5157566 6.257551  6.9324894 4.5302415 4.926121 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.698451   -8.337661   -7.4029284  -6.2158947  -5.211501   -4.692233
0:   -4.6700974  -5.158491   -6.252056   -7.322466   -8.490624   -9.630364
0:  -10.388368  -10.785176  -10.810164  -10.527146  -10.334348  -10.1832
0:   -9.116577   -8.967103 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.2828274 6.2252793 6.533697  6.9809093 7.2834454 7.28655   7.4004397
0:  7.403821  7.4004593 7.4104457 7.1696672 6.7272825 6.2465224 6.032608
0:  6.150574  6.713646  7.44912   8.110188  6.605715  6.5784197]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.700356 31.57338  31.239988 30.707287 30.190413 29.556755 29.964767
0:  30.350313 30.971785 31.410866 31.068901 30.765602 30.445213 30.779646
0:  31.811987 33.132187 34.436497 35.156414 36.832832 37.285645]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.294277  12.58972   13.1550255 13.853052  14.438116  14.902335
0:  15.194702  15.623818  15.924302  16.363243  16.827267  17.11227
0:  17.434303  17.79903   18.423668  19.196947  19.967663  20.484657
0:  18.489555  18.611095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.748786  5.599517  5.786029  6.103863  6.2857533 6.171127  6.225422
0:  6.1246486 5.9517183 5.783697  5.2896237 4.5369267 3.7106416 3.252469
0:  3.1838806 3.6601076 4.251388  4.753599  2.8573933 2.9072351]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.321129  -9.003582  -8.137232  -6.9757514 -5.9456058 -5.2519784
0:  -4.749766  -4.5243335 -4.651486  -4.7571073 -5.0286193 -5.5127454
0:  -5.8064666 -5.8748155 -5.651649  -5.13084   -4.655023  -4.302054
0:  -6.7566657 -6.5075603]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4944701 1.5309949 2.0008068 2.6908448 3.2772245 3.5772395 3.7355504
0:  3.7078018 3.3789928 3.0855017 2.6282806 1.9891658 1.546135  1.3271141
0:  1.521173  2.0933337 2.7722826 3.340916  1.3510256 1.4365644]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.967637   7.9985957  8.404641   8.957619   9.347033   9.468541
0:   9.5614605  9.578875   9.502969   9.441032   9.202154   8.778971
0:   8.423833   8.301765   8.538044   9.127343   9.815227  10.353012
0:   9.100457   9.319473 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.038664  -12.540916  -12.707716  -12.649216  -12.500179  -12.435633
0:  -11.905256  -11.405573  -10.869129  -10.457338  -10.554895  -10.832527
0:  -11.20487   -11.037474  -10.470339   -9.587719   -8.859419   -8.522909
0:   -9.101757   -9.3782425]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.905884 16.347158 17.129847 17.948626 18.521782 18.740358 18.94403
0:  19.114048 19.306828 19.572594 19.758072 19.934875 20.272417 20.893312
0:  22.042871 23.517223 24.995882 26.274178 24.147734 24.94593 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.414923  8.784799  9.546345 10.430822 11.212904 11.731192 12.34681
0:  12.834879 13.275429 13.660307 13.805363 13.789675 13.783907 13.987587
0:  14.443121 15.09045  15.715107 16.16248  13.047041 13.03116 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.562094 28.928354 29.501123 30.070593 30.451262 30.662153 31.017248
0:  31.426693 31.86776  32.43234  32.81208  33.08906  33.433712 33.899075
0:  34.76086  35.7057   36.56389  37.070602 32.906284 33.230812]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.49612713 0.4804535  0.7768307  1.4926963  2.379475   3.258609
0:  3.7693465  4.233883   4.1778784  4.159981   4.1609516  4.0503273
0:  4.0597553  4.1926923  4.56028    5.303955   6.30353    7.342884
0:  6.6770434  6.809667  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.32969    -3.2171688  -2.6539755  -1.694829   -0.7442193  -0.01180553
0:   0.11619329  0.03186083 -0.75210476 -1.5479708  -2.4072952  -3.4491415
0:  -4.2559996  -4.8808403  -5.0950613  -4.7249584  -3.9774513  -3.077817
0:  -4.9387474  -5.006527  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.53187084  0.42194223  0.63532686  1.1130009   1.4938259   1.5920982
0:   1.3310142   0.8828459  -0.00652266 -0.8964639  -1.8082433  -2.7942529
0:  -3.4625888  -3.7984538  -3.7233396  -3.1893086  -2.5508828  -1.9207406
0:  -3.9981465  -4.270782  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.957333 14.119057 14.479656 14.940804 15.112335 14.996349 14.595694
0:  14.340837 13.941282 13.832794 13.830147 13.619055 13.431977 13.166539
0:  13.176762 13.569929 14.316505 15.084164 12.354889 12.285796]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.2301583 7.263816  7.5982413 8.056207  8.3665    8.446061  8.660646
0:  8.801184  8.867804  8.878798  8.560961  7.99239   7.3793616 7.042491
0:  7.0266385 7.309246  7.6184864 7.7688622 5.023843  4.726301 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.51214   -7.635078  -7.2597766 -6.4235663 -5.4668984 -4.6797757
0:  -4.3223376 -4.14249   -4.472013  -4.761474  -5.019898  -5.4273534
0:  -5.596436  -5.6464515 -5.4149547 -4.732942  -3.7881122 -2.7639642
0:  -3.7578874 -3.823813 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.91833115 0.92620087 1.392232   2.2609072  3.2706072  4.14409
0:  4.6482015  5.0403733  4.8415136  4.633212   4.396038   3.9105377
0:  3.6459794  3.3877368  3.4220858  3.862624   4.599928   5.4097056
0:  3.1273146  3.0590057 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1682444  -2.8572683  -2.2717953  -1.5988913  -1.021523   -0.6841874
0:  -0.29421568 -0.0703969  -0.10013676 -0.15157938 -0.39996576 -0.81664467
0:  -0.95358324 -0.79279137 -0.1469531   0.8042507   1.7892418   2.619075
0:   2.167395    2.7374043 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.4076505  3.610054   4.2708335  5.2046204  6.0649395  6.7324276
0:   7.3163953  7.939427   8.427252   8.948922   9.418035   9.639007
0:   9.929705  10.31847   11.016113  12.03768   13.21731   14.244455
0:  12.514963  12.997644 ]
0: validation loss for strategy=forecast at epoch 48 : 0.30193936824798584
0: validation loss for velocity_u : 0.1763124018907547
0: validation loss for velocity_v : 0.28283336758613586
0: validation loss for specific_humidity : 0.16776947677135468
0: validation loss for velocity_z : 0.5014379620552063
0: validation loss for temperature : 0.12654505670070648
0: validation loss for total_precip : 0.5567380785942078
0: 49 : 21:13:25 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 49, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8480, 0.8627, 0.8668, 0.8610, 0.8454, 0.8229, 0.7971, 0.7720, 0.7513, 0.7358, 0.7274, 0.7267, 0.7323, 0.7420,
0:         0.7514, 0.7560, 0.7542, 0.7470, 0.7737, 0.7788, 0.7747, 0.7630, 0.7450, 0.7242, 0.7048], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5574, 0.5610, 0.5693, 0.5824, 0.5969, 0.6103, 0.6175, 0.6171, 0.6132, 0.6071, 0.6016, 0.6003, 0.6058, 0.6247,
0:         0.6593, 0.7078, 0.7657, 0.8233, 0.5334, 0.5364, 0.5451, 0.5587, 0.5714, 0.5799, 0.5822], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4618,  0.3858,  0.3182,  0.2563,  0.2074,  0.1699,  0.1405,  0.1129,  0.0828,  0.0522,  0.0240, -0.0010,
0:         -0.0102, -0.0052,  0.0097,  0.0315,  0.0577,  0.0769,  0.4385,  0.3350,  0.2443,  0.1692,  0.1130,  0.0746,
0:          0.0451], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0397,  0.1174,  0.1907,  0.2209,  0.2101,  0.1605,  0.1044,  0.1411,  0.1842,  0.1691,  0.1260,  0.0095,
0:         -0.1005, -0.1329, -0.1092,  0.0268,  0.2360,  0.4345,  0.0505,  0.1368,  0.1950,  0.2166,  0.1993,  0.1281,
0:          0.0677], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.6795, -1.7377, -1.7972, -1.8590, -1.9293, -2.0004, -2.0718, -2.1430, -2.2059, -2.2647, -2.3183, -2.3609,
0:         -2.3986, -2.4277, -2.4468, -2.4565, -2.4545, -2.4414, -2.4153, -2.3799, -2.3417, -2.3013, -2.2649, -2.2332,
0:         -2.2058], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0980, -0.0591, -0.0780,  0.1488,  0.1288, -0.0691, -0.1414, -0.1558, -0.0980, -0.1447, -0.1091, -0.0658,
0:         -0.0558, -0.1069, -0.1314, -0.1214, -0.1358, -0.1292, -0.1303, -0.1002, -0.1369, -0.1658, -0.1525, -0.1747,
0:         -0.0658], device='cuda:0')
0: [DEBUG] Epoch 49, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2381,     nan, -0.2381,     nan, -0.2381,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2381,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan, -0.2381,     nan,     nan, -0.2259,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan, -0.2381, -0.2381,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2381,     nan, -0.2381,     nan, -0.2381,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2381,     nan,     nan, -0.1536, -0.2381,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381, -0.2381,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 49, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1537, -0.1560, -0.1346, -0.1031, -0.0764, -0.0614, -0.0510, -0.0444, -0.0490, -0.0541, -0.0678, -0.1008,
0:         -0.1189, -0.1201, -0.0889, -0.0144,  0.0832,  0.1764, -0.0982, -0.1171, -0.1155, -0.1157, -0.1197, -0.1196,
0:         -0.1154], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6771, 0.7127, 0.7057, 0.6750, 0.6256, 0.5708, 0.5120, 0.4591, 0.4327, 0.4211, 0.4385, 0.4729, 0.4870, 0.4768,
0:         0.4524, 0.4259, 0.4308, 0.4629, 0.6883, 0.7256, 0.7241, 0.6968, 0.6528, 0.6009, 0.5438], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5338, -0.5446, -0.5585, -0.5666, -0.5674, -0.5659, -0.5708, -0.5763, -0.5847, -0.5963, -0.6094, -0.6186,
0:         -0.6208, -0.6194, -0.6161, -0.6079, -0.6033, -0.5893, -0.5703, -0.5825, -0.5949, -0.5986, -0.6006, -0.6025,
0:         -0.5996], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0868, 0.2731, 0.4374, 0.3841, 0.2975, 0.3390, 0.3551, 0.3162, 0.3206, 0.2972, 0.2200, 0.2475, 0.3831, 0.4017,
0:         0.3496, 0.4902, 0.5906, 0.4283, 0.1677, 0.2031, 0.2579, 0.2361, 0.2125, 0.2915, 0.3632], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.6756, -0.7914, -0.8839, -0.9586, -1.0311, -1.1229, -1.2273, -1.3333, -1.4174, -1.4804, -1.5347, -1.5775,
0:         -1.6174, -1.6410, -1.6654, -1.6980, -1.7541, -1.8265, -1.8975, -1.9502, -1.9866, -2.0210, -2.0666, -2.1296,
0:         -2.1976], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2270, -0.2293, -0.2403, -0.2387, -0.2330, -0.2350, -0.2402, -0.2426, -0.2384, -0.2379, -0.2350, -0.2436,
0:         -0.2381, -0.2335, -0.2329, -0.2358, -0.2409, -0.2478, -0.2337, -0.2377, -0.2325, -0.2376, -0.2326, -0.2377,
0:         -0.2443], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23865842819213867; velocity_v: 0.3691893219947815; specific_humidity: 0.1689390242099762; velocity_z: 0.7223107814788818; temperature: 0.1572585105895996; total_precip: 0.6219700574874878; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21681517362594604; velocity_v: 0.3677642047405243; specific_humidity: 0.1999206393957138; velocity_z: 0.5570814609527588; temperature: 0.13469523191452026; total_precip: 0.5244058966636658; 
0: epoch: 49 [1/5 (20%)]	Loss: 0.57319 : 0.32155 :: 0.21117 (2.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22769296169281006; velocity_v: 0.3792541027069092; specific_humidity: 0.21536505222320557; velocity_z: 0.7209281921386719; temperature: 0.18841122090816498; total_precip: 0.8877307176589966; 
0: epoch: 49 [2/5 (40%)]	Loss: 0.88773 : 0.40057 :: 0.21601 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2445181906223297; velocity_v: 0.3703201115131378; specific_humidity: 0.23223567008972168; velocity_z: 0.5138548016548157; temperature: 0.16381624341011047; total_precip: 0.47642335295677185; 
0: epoch: 49 [3/5 (60%)]	Loss: 0.47642 : 0.29748 :: 0.21892 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24912968277931213; velocity_v: 0.3595314919948578; specific_humidity: 0.22857411205768585; velocity_z: 0.6647148728370667; temperature: 0.1857065111398697; total_precip: 0.8923696875572205; 
0: epoch: 49 [4/5 (80%)]	Loss: 0.89237 : 0.39438 :: 0.20688 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 200):
0: [1.43051147e-06 9.53674316e-07 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  1.90734863e-06 4.29153442e-06 5.72204590e-06 4.00543213e-05
0:  1.08718872e-04 1.69277191e-04 9.82284546e-05 3.19480896e-05
0:  2.38418579e-06 1.43051147e-06 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 9.53674316e-07 9.53674316e-07 9.53674316e-07
0:  4.76837158e-07 4.76837158e-07 9.53674316e-07 1.90734863e-06
0:  5.24520874e-06 1.71661377e-05 2.38418579e-05 3.05175781e-05
0:  3.76701355e-05 4.43458557e-05 3.91006470e-05 2.43186951e-05
0:  1.04904175e-05 5.72204590e-06 4.29153442e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 8.58306885e-06 8.58306885e-06 8.58306885e-06
0:  8.58306885e-06 1.85966492e-05 2.38418579e-05 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 9.53674316e-07 6.67572021e-06
0:  5.81741333e-05 7.77244568e-05 8.96453857e-05 1.02996826e-04
0:  1.15394592e-04 7.24792480e-05 6.10351562e-05 4.76837158e-05
0:  3.48091125e-05 2.38418579e-05 1.33514404e-05 5.72204590e-06
0:  3.33786011e-06 2.38418579e-06 1.04904175e-05 2.28881836e-05
0:  2.52723694e-05 2.47955322e-05 1.23977661e-05 9.53674316e-07
0:  4.76837158e-07 0.00000000e+00 4.76837158e-07 0.00000000e+00
0:  9.53674316e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  4.76837158e-07 1.43051147e-06 2.86102295e-06 2.47955322e-05
0:  6.38961792e-05 9.34600830e-05 6.58035278e-05 2.67028809e-05
0:  2.86102295e-06 1.43051147e-06 4.76837158e-07 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 1.43051147e-06 2.86102295e-06
0:  5.24520874e-06 1.90734863e-05 3.29017639e-05 4.10079956e-05
0:  4.57763635e-05 5.14984131e-05 4.91142273e-05 3.24249268e-05
0:  1.62124634e-05 9.53674316e-06 4.76837158e-06 4.76837158e-07
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07
0:  4.76837158e-07 4.76837158e-07 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
0:  0.00000000e+00 4.76837158e-07 1.90734863e-06 1.90734863e-06
0:  4.29153442e-06 1.38282776e-05 2.38418579e-05 2.38418579e-05
0:  5.81741333e-05 9.58442688e-05 1.24454498e-04 7.72476196e-05
0:  3.00407410e-05 3.33786011e-05 4.24385071e-05 5.05447388e-05
0:  9.20295715e-05 1.58786774e-04 2.05039978e-04 1.67369843e-04
0:  1.18732452e-04 9.34600830e-05 8.24928284e-05 7.15255737e-05]
0: Target values (first 200):
0: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7656555e-05
0:  9.0599060e-05 1.3017654e-04 1.2588501e-04 7.4863434e-05 1.1920929e-05
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  4.7683716e-07 1.4305115e-06 2.3841858e-06 2.3841858e-06 1.9073486e-06
0:  4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7683716e-07
0:  4.7683716e-07 4.7683716e-07 4.7683716e-07 4.7683716e-07 0.0000000e+00
0:  5.2452087e-06 1.7166138e-05 3.0040741e-05 9.1552734e-05 1.7547607e-04
0:  2.8228760e-04 2.1696091e-04 1.6450882e-04 1.1396408e-04 8.4877014e-05
0:  5.5313110e-05 3.0040741e-05 2.4318695e-05 8.5830688e-06 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 8.1062317e-06 4.3392181e-05
0:  9.2983246e-05 1.8644333e-04 3.1471252e-04 3.6716461e-04 4.3201447e-04
0:  3.9577484e-04 3.3998492e-04 2.2888184e-04 1.1444092e-04 5.1498413e-05
0:  8.6784363e-05 1.0681152e-04 1.2302399e-04 1.3065338e-04 1.1968613e-04
0:  9.6797943e-05 1.0347366e-04 1.1730194e-04 1.6021729e-04 2.1600723e-04
0:  2.2268295e-04 2.8514862e-04 2.8038025e-04 2.8085709e-04 3.7527084e-04
0:  4.6730042e-04 5.0973892e-04 4.1723251e-04 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 2.3841858e-06 2.0503998e-05 3.8623810e-05
0:  4.7683716e-05 3.0040741e-05 1.4305115e-05 1.4305115e-05 1.4305115e-05
0:  5.2452087e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  4.7683716e-07 4.7683716e-07 9.5367432e-07 9.5367432e-07 9.5367432e-07
0:  9.5367432e-07 4.7683716e-07 0.0000000e+00 1.4305115e-05 2.4795532e-05
0:  3.5285950e-05 1.3208389e-04 2.4747849e-04 3.6287308e-04 2.5129318e-04
0:  1.5926361e-04 8.3446503e-05 6.0558319e-05 2.6226044e-05 1.4305115e-06
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 3.8146973e-06 2.5749207e-05 4.7683716e-05 6.1035156e-05
0:  7.0095062e-05 9.1552734e-05 1.8072128e-04 2.6512146e-04 2.8705597e-04
0:  2.0599365e-04 1.2683868e-04 1.0728836e-04 1.4781952e-04 1.8787384e-04]
0: Prediction values (first 20):
0: [4.756468  4.7587204 5.100378  5.6653295 6.135942  6.368977  6.532834
0:  6.529101  6.263477  5.9318485 5.37495   4.6516523 4.0302377 3.6608398
0:  3.659519  4.017188  4.439521  4.708625  2.8916063 2.8992248]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.502, max = 2.774, mean = -0.085
0:          sample (first 20): tensor([-0.1805, -0.1803, -0.1510, -0.1025, -0.0620, -0.0420, -0.0280, -0.0283, -0.0511, -0.0796, -0.1274, -0.1895,
0:         -0.2429, -0.2746, -0.2747, -0.2440, -0.2077, -0.1846, -0.0775, -0.0988])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.344183  18.477243  18.795357  19.148685  19.140581  18.726332
0:  18.062027  17.409721  16.621742  16.026676  15.390014  14.5187025
0:  13.644167  12.925032  12.788454  13.341511  14.498128  15.774556
0:  14.832029  15.203292 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.00371  19.652145 19.42931  19.192411 18.894213 18.37513  18.38794
0:  18.302513 18.302372 18.239002 17.649529 16.977234 16.23851  16.019384
0:  16.275349 16.913666 17.552702 17.888023 17.329618 17.145836]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.584318  12.332251  12.202356  11.949179  11.549107  10.8137865
0:  10.7132635 10.578722  10.722744  10.87978   10.494553  10.03416
0:   9.345322   9.163467   9.46262   10.198256  10.944501  11.316702
0:  13.60197   13.703704 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.8513293  5.8788443  6.2939696  6.900867   7.38485    7.6283045
0:   7.8654175  8.067063   8.20606    8.40621    8.489107   8.351896
0:   8.242815   8.330176   8.751205   9.570375  10.583108  11.444674
0:  10.065857  10.40122  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.37521  19.32104  19.593576 19.954542 20.186193 20.082    20.101305
0:  20.030052 19.94888  19.937805 19.746597 19.422136 19.141533 19.057215
0:  19.36615  20.077263 21.06829  21.970419 19.9407   19.935055]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.6787815  -9.169991   -9.264324   -8.754889   -7.967627   -7.165872
0:  -6.7450576  -6.362663   -6.476234   -6.422381   -6.192592   -5.979152
0:  -5.6297526  -5.152601   -4.562077   -3.5299602  -2.2691045  -0.95338345
0:  -1.3811946  -1.7544761 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.0892935 -12.676419  -12.649831  -12.223914  -11.796353  -11.648433
0:  -11.541669  -11.662103  -11.940541  -12.29824   -12.8466425 -13.605121
0:  -14.150134  -14.268392  -13.905976  -13.012195  -12.037457  -11.172
0:  -11.917571  -12.457608 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.909151  5.92442   6.285722  6.740288  6.983628  6.8962393 6.8459144
0:  6.6742477 6.4688215 6.349697  6.064498  5.6560607 5.291822  5.2688527
0:  5.640115  6.4745283 7.453703  8.3046    8.013132  8.340884 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.524353   0.51253605 0.8230233  1.434267   2.112759   2.635027
0:  2.8927612  2.972837   2.5518875  2.089334   1.5334587  0.92630243
0:  0.5226531  0.30412102 0.40187263 0.7499118  1.1250982  1.4348388
0:  0.15380096 0.03095818]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.416336  2.3654127 2.6384263 3.220339  3.8040195 4.2537193 4.72285
0:  5.0796905 5.2321615 5.3795176 5.3648667 5.2090535 5.199685  5.437505
0:  6.069646  7.0142283 8.010716  8.79812   8.24106   8.646414 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.065742  -12.149163  -11.987029  -11.329871  -10.543902   -9.8878565
0:   -9.285205   -8.950623   -9.134225   -9.475966  -10.2396345 -11.182565
0:  -11.9387865 -12.429421  -12.275937  -11.819253  -11.305295  -11.01142
0:  -11.501315  -11.719253 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1514053  1.4699159  1.0094213  0.72016716 0.55487967 0.37302065
0:  0.67475367 0.9893031  1.3789301  1.7404962  1.7991991  1.7616496
0:  1.7691727  2.2618423  3.1855602  4.4941387  5.7703257  6.7691307
0:  6.4665995  6.6914654 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6409144  -2.5261798  -2.0556989  -1.2789769  -0.5807862  -0.146554
0:   0.01590967  0.11095572 -0.10111189 -0.21459055 -0.3168416  -0.6063018
0:  -0.8032222  -0.8731456  -0.652102    0.02320862  0.9899287   1.9949403
0:   2.4430041   2.4357066 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.872458  2.0474968 2.6468916 3.5465508 4.4073257 5.0416837 5.3227654
0:  5.499277  5.2392116 5.0133276 4.795063  4.3803616 4.1820803 4.0964127
0:  4.3429985 5.065616  6.0717554 7.1475587 5.2465515 5.262793 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.5705633 -6.2339573 -5.4892454 -4.41335   -3.3437996 -2.5608253
0:  -2.0669117 -1.8221078 -1.9626431 -2.0969105 -2.2879353 -2.5833445
0:  -2.6980457 -2.6618004 -2.3482218 -1.7862015 -1.1748433 -0.6572194
0:  -1.2779903 -1.0562692]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.767577   8.262178   9.199556  10.249034  11.005055  11.38451
0:  11.647538  11.976013  12.1859455 12.52589   12.796913  12.785301
0:  12.866072  13.06592   13.656902  14.646278  15.78474   16.750038
0:  14.081454  14.232563 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.737089  -5.4200726 -5.933467  -6.212248  -6.4378014 -6.591175
0:  -6.0652    -5.4817934 -4.878091  -4.525362  -4.876581  -5.6407824
0:  -6.409524  -6.447074  -5.7916927 -4.7676144 -3.8983002 -3.5594692
0:  -2.957059  -2.9480734]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.04674864 0.39516115 1.2103271  2.3446274  3.4857333  4.4184723
0:  5.2349653  5.779483   5.8312726  5.9071903  5.7852516  5.5683646
0:  5.5695276  5.7903275  6.3732247  7.155594   7.8798313  8.448397
0:  6.993956   7.1651545 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.377323  -11.621088  -11.462841  -11.0680275 -10.717262  -10.58695
0:  -10.365583  -10.296314  -10.367367  -10.452202  -10.847946  -11.459887
0:  -12.000221  -12.163725  -11.975332  -11.455809  -11.02718   -10.75956
0:  -12.027376  -12.240892 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.698071  -9.653511  -9.274609  -8.479868  -7.5654073 -6.763396
0:  -6.250653  -5.9452786 -6.288854  -6.6813636 -7.2274585 -7.861693
0:  -8.1957035 -8.326347  -8.108046  -7.5575004 -6.9432874 -6.3301005
0:  -8.676823  -8.947174 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.983633  13.732214  13.877556  14.176172  14.317104  14.127841
0:  14.046753  13.938018  13.813387  13.758265  13.509231  12.99663
0:  12.536228  12.232144  12.399046  13.053488  13.933012  14.720601
0:  11.817745  11.5735035]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.657562 34.1622   34.66837  35.0905   35.37021  35.24475  35.631813
0:  35.78604  35.970116 35.91754  35.236347 34.52533  33.75643  33.461727
0:  33.613445 33.92132  34.161274 34.070667 33.425213 33.35971 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.869904  -11.4950905 -11.526701  -10.945885  -10.094385   -9.375286
0:   -9.145248   -9.199684   -9.941799  -10.737806  -11.548298  -12.449631
0:  -12.981108  -13.282991  -13.238998  -12.792288  -12.17602   -11.4987545
0:  -13.344511  -14.091566 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.2161255  6.538604   7.3469343  8.28185    8.966175   9.250035
0:   9.327499   9.415203   9.412746   9.623594   9.835863   9.804558
0:   9.765959   9.753252   9.993925  10.67144   11.594434  12.4805155
0:  10.050498  10.155292 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.07046604 -0.17469406  0.10020876  0.85622406  1.7600126   2.576613
0:   3.0330362   3.3123226   2.955011    2.521325    2.0415506   1.4219732
0:   1.0358047   0.80255604  0.87578535  1.3604789   2.1225696   2.9420075
0:   2.1789393   2.1986127 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.975052   9.02324    9.309124   9.780182  10.147719  10.311977
0:  10.28346   10.151117   9.664343   9.172013   8.536541   7.685599
0:   6.854329   6.198654   5.86835    5.974391   6.3057013  6.643471
0:   3.979557   3.8447938]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.151457 22.871017 22.905107 22.937052 22.70748  22.065441 21.468124
0:  20.771976 20.105413 19.571121 18.946854 18.258627 17.817162 17.547886
0:  17.736795 18.332262 19.116892 19.887264 17.151278 17.004894]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.139269  12.006901  12.225021  12.566535  12.880097  12.994362
0:  13.452288  13.844448  14.25583   14.546843  14.335306  13.734986
0:  12.868266  12.147881  11.724594  11.701675  11.922943  12.060158
0:   7.8036814  7.111353 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.3029814 1.1677299 1.4398961 1.8985877 2.333005  2.5454862 2.9463463
0:  3.1925702 3.3902962 3.5935202 3.541797  3.390072  3.3007445 3.5310116
0:  4.1055884 5.0552707 5.984423  6.7280974 5.6219    5.80051  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.177746 20.100576 20.045475 19.889051 19.506226 18.902256 18.617363
0:  18.339737 18.280996 18.32293  18.13102  17.935898 17.815033 18.210089
0:  19.108566 20.307852 21.534483 22.439983 22.016928 22.189054]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.607393 20.47371  20.607847 20.968506 21.440075 21.695412 22.442392
0:  23.117123 23.971983 24.738626 25.152737 25.416655 25.62472  26.014256
0:  26.64821  27.504816 28.443607 29.22073  28.100492 28.25012 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.025608   8.290148   8.825805   9.526792  10.129936  10.495002
0:  11.014753  11.502974  11.840588  12.070246  12.052406  11.813253
0:  11.63587   11.748541  12.28838   13.155287  14.123056  14.817038
0:  13.324663  13.4760685]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.720627 39.689297 39.175167 38.484497 37.792027 36.982502 37.207783
0:  37.47586  37.950344 38.268585 37.727486 37.32926  36.82815  36.88364
0:  37.54581  38.23606  38.7802   38.657238 38.217686 38.479233]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.8778   29.35517  28.806515 28.291344 27.848763 27.116014 27.165098
0:  27.07736  27.21465  27.240644 26.655367 26.043482 25.335835 25.018349
0:  25.208344 25.760105 26.495945 26.957228 27.222723 26.96661 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.824558  -8.772308  -8.220092  -7.224617  -6.167658  -5.3465214
0:  -4.9048443 -4.732604  -5.1615973 -5.6455507 -6.219881  -6.946954
0:  -7.4045153 -7.650876  -7.5435066 -7.0277424 -6.407962  -5.78347
0:  -6.3916473 -6.232836 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.38103485 -0.29272366  0.21139193  1.0531368   1.9421244   2.652635
0:   3.24434     3.6723294   3.7488136   3.822851    3.762771    3.579943
0:   3.5821257   3.7475846   4.2755156   5.038356    5.8864627   6.6022153
0:   5.2997584   6.2198663 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.724707 14.244438 14.330942 14.771008 15.240622 15.516489 15.966755
0:  16.258013 16.578926 16.931181 17.140572 17.2192   17.45161  17.888351
0:  18.731625 19.926336 21.271671 22.50746  20.900423 20.885433]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8925347 -3.2648149 -3.264874  -2.7628293 -2.0692296 -1.4725595
0:  -1.291472  -1.222054  -1.7780299 -2.3383765 -2.9060426 -3.602759
0:  -4.013269  -4.238404  -4.0916505 -3.4960122 -2.6772046 -1.7905784
0:  -2.5865684 -3.062158 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.211922  5.0666275 5.2729473 5.7474537 6.16805   6.339183  6.316689
0:  6.147289  5.6565247 5.112131  4.4382505 3.556776  2.8296242 2.343264
0:  2.2318292 2.5772867 3.130219  3.6701148 2.2357306 2.1944408]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.454987   9.57234   10.054626  10.565136  10.905165  10.8665085
0:  10.856298  10.701109  10.642749  10.747724  10.722128  10.635948
0:  10.604319  10.771154  11.345838  12.243231  13.276794  14.203346
0:  12.81695   12.973816 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.163383 -14.27374  -13.879893 -12.995301 -11.916547 -11.016372
0:  -10.432294 -10.152211 -10.512801 -10.973073 -11.583266 -12.289142
0:  -12.658799 -12.734084 -12.467496 -11.787026 -11.093085 -10.448642
0:  -11.172795 -11.420587]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6903129  -3.8493743  -3.5677562  -2.7923694  -1.8794522  -1.11237
0:  -0.7952976  -0.65153027 -1.1453643  -1.6276393  -2.1378608  -2.8076692
0:  -3.2683625  -3.571836   -3.5357337  -2.9735093  -2.1003776  -1.0905557
0:  -2.791019   -3.1032615 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.9015183 -4.3489385 -4.348437  -3.7991157 -3.0394988 -2.4134984
0:  -2.2222538 -2.2182493 -2.8156447 -3.446999  -4.091204  -4.8274093
0:  -5.3447604 -5.632299  -5.54367   -5.060785  -4.349109  -3.631351
0:  -5.462911  -6.0163465]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.003317 29.503775 29.010485 28.570389 28.12829  27.394548 27.494493
0:  27.482841 27.70561  27.839933 27.242237 26.694376 26.012003 25.862268
0:  26.252054 26.93941  27.673244 27.971716 29.345953 29.185633]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.0727565 3.1109157 3.6022217 4.326053  4.919487  5.175788  5.2386594
0:  5.2050757 4.968137  4.8465104 4.6672916 4.304737  3.9720674 3.7858126
0:  3.8664474 4.305529  4.923438  5.411457  3.7167199 3.6382744]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.655298 25.165422 24.911987 24.6301   23.989248 22.810793 21.727762
0:  20.63959  19.777103 19.232117 18.724644 18.189598 17.801674 17.669569
0:  17.909685 18.551228 19.32556  19.92931  18.177528 17.917236]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.144433    3.9250274   4.0317087   4.392824    4.607666    4.541709
0:   4.0999737   3.6537595   2.9312787   2.3746266   1.8485413   1.087717
0:   0.31057882 -0.41243124 -0.78204536 -0.5152154   0.34314775  1.4559956
0:   0.21386051  0.07786655]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.79915    -3.0118232  -2.8115935  -2.1713614  -1.444982   -0.8513031
0:  -0.64785814 -0.5796342  -1.0493011  -1.5154085  -1.9873748  -2.5915484
0:  -2.9643273  -3.16851    -3.0379386  -2.4963984  -1.7221437  -0.93149376
0:  -2.1071887  -2.502689  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7923427  -1.9746766  -1.836823   -1.325109   -0.7316241  -0.28143787
0:  -0.20384741 -0.32131958 -0.999578   -1.676435   -2.3608565  -3.0191092
0:  -3.3426576  -3.4114213  -3.092464   -2.4792094  -1.8456941  -1.3134542
0:  -2.850348   -3.0702214 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.143106  6.285836  6.795327  7.398802  7.815183  7.924913  7.897929
0:  7.6933594 7.2629704 6.9035444 6.391613  5.6969404 5.0635977 4.57966
0:  4.42933   4.696189  5.171542  5.6441545 3.2487726 3.3864622]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.823307 19.987213 20.45861  20.957026 21.265528 21.346058 21.546753
0:  21.667934 21.754498 21.961359 21.965816 21.856876 21.84302  22.067898
0:  22.701393 23.592442 24.504326 25.077991 23.722473 24.06596 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.726398  -16.780842  -16.251999  -15.250997  -14.21643   -13.4365015
0:  -12.897666  -12.711417  -13.015426  -13.4252615 -14.013288  -14.793916
0:  -15.224986  -15.224296  -14.686561  -13.699367  -12.704928  -11.789269
0:  -11.717011  -11.72071  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.8277957  3.7057414  3.821669   4.0532837  4.250642   4.2450323
0:   4.688716   5.0701113  5.583843   6.092556   6.229263   6.212568
0:   6.0985804  6.41074    7.152625   8.312889   9.559907  10.532764
0:  10.694289  10.782052 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.885286 17.096292 17.1752   17.289314 17.127737 16.825634 17.436283
0:  18.400467 20.029837 21.842083 23.233458 24.255667 25.02641  26.071053
0:  27.556126 29.44324  31.487354 33.1233   32.611065 32.87487 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.019824  7.9679284 8.377996  8.988911  9.452687  9.6355095 9.733805
0:  9.770754  9.642601  9.486634  9.142728  8.523842  7.948744  7.5641046
0:  7.595927  8.070308  8.728077  9.300329  6.766389  6.7001247]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.516642 25.237679 24.711483 24.078447 23.556585 22.934341 23.45218
0:  23.828102 24.472378 24.909262 24.595974 24.361586 24.169518 24.702337
0:  25.91878  27.576002 29.344915 30.573286 33.76315  34.28871 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.037468  8.928887  9.13578   9.400005  9.53402   9.463657  9.39359
0:  9.317526  9.146769  9.037355  8.77188   8.345949  7.989632  7.8393087
0:  8.038563  8.513731  9.058201  9.416846  7.0550094 7.281338 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.006824 40.04094  41.173237 42.13137  42.68856  42.687233 42.964283
0:  42.960224 42.852806 42.47511  41.519238 40.484814 39.65268  39.112114
0:  38.989178 38.866024 38.49831  37.791176 32.255814 31.607758]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.773754 26.076447 26.615389 27.106707 27.422813 27.422512 27.712208
0:  27.915785 28.145174 28.373947 28.249104 27.99519  27.759562 27.80246
0:  28.308455 29.133415 30.11452  30.867352 29.176508 29.536652]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.847673 28.723322 28.594767 28.373257 28.086615 27.461658 27.417553
0:  27.131964 26.935337 26.605587 25.725418 24.84173  23.963034 23.624191
0:  23.876343 24.561079 25.443062 26.093683 26.741459 26.798517]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.7007103  -0.81926537 -0.53489685  0.16866684  0.98748255  1.6570435
0:   1.8596368   1.8820419   1.2664509   0.6243744  -0.04675293 -0.836874
0:  -1.3883219  -1.7575793  -1.7680459  -1.3026733  -0.5174866   0.3668666
0:  -0.82612514 -1.0395684 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.559226  -6.9600754 -6.8083744 -6.0383196 -5.042651  -4.172346
0:  -3.6243906 -3.406845  -3.856451  -4.567938  -5.5354214 -6.7757463
0:  -7.761179  -8.450615  -8.650179  -8.317587  -7.7281923 -7.091119
0:  -8.771501  -9.19312  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.675564  -9.571487  -8.870216  -7.6820607 -6.396567  -5.379252
0:  -4.816235  -4.5366244 -4.857613  -5.1845794 -5.578467  -6.1312337
0:  -6.4375367 -6.561745  -6.348517  -5.7213025 -5.004581  -4.3627257
0:  -6.70248   -6.796682 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.937079   8.590746   8.676159   9.121296   9.515934   9.660182
0:  9.445728   9.1740265  8.524324   7.966837   7.3722143  6.5069895
0:  5.6556835  4.8372984  4.3085794  4.325738   4.774684   5.3774896
0:  1.6102462  0.73526716]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.470762 20.582815 20.934397 21.49542  22.21787  22.787605 24.05678
0:  25.13088  26.347128 27.399668 27.964363 28.347647 28.72221  29.434198
0:  30.660345 32.23419  33.995575 35.47199  35.40127  36.11918 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.413221 26.387627 26.38253  26.240042 26.003834 25.503624 25.46134
0:  25.268711 25.22228  25.17567  24.724358 24.313192 23.89851  23.919516
0:  24.406286 25.173973 26.090225 26.766365 26.35584  26.593882]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.1934254 2.628608  3.548902  4.6855674 5.717604  6.47961   6.996851
0:  7.2693195 7.1584864 6.988315  6.6811695 6.249477  5.9925456 5.9909234
0:  6.3873525 7.1250677 7.940832  8.558273  6.4420877 6.6456428]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9804199 3.6658647 3.6702552 3.978422  4.273726  4.4356556 4.394068
0:  4.3323073 3.9766738 3.7007556 3.4303951 3.019011  2.7592716 2.6215458
0:  2.7592797 3.310637  4.1014957 4.9365244 2.2630014 1.7943077]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2982063 3.7447226 4.4925013 5.342255  6.0186715 6.414485  6.7597256
0:  6.9172544 6.8954544 6.8977723 6.724649  6.4091434 6.1532536 6.1202426
0:  6.407289  6.9871054 7.6488123 8.196434  7.3185635 7.672421 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.194879  -14.247464  -13.913026  -13.2829275 -12.768718  -12.449159
0:  -11.7053175 -11.144525  -10.487525  -10.100337  -10.316159  -10.966783
0:  -11.6705675 -11.938306  -11.720431  -11.037353  -10.297805   -9.80771
0:  -11.785403  -11.726048 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.24666    -8.396082   -8.131613   -7.425101   -6.610518   -6.0420785
0:   -5.940636   -6.138347   -7.0038075  -7.892019   -8.842274   -9.871637
0:  -10.504183  -10.840103  -10.751575  -10.288395   -9.838383   -9.3988495
0:  -10.229088  -10.491638 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.695022   -8.714923   -8.302884   -7.561331   -6.859379   -6.488819
0:   -6.4541607  -6.73372    -7.4719677  -8.215355   -9.014948   -9.924563
0:  -10.530702  -10.804666  -10.722907  -10.213252   -9.660231   -9.195564
0:  -10.66484   -10.740745 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.302125  11.238903  11.540348  11.9565735 12.129749  11.971934
0:  11.760088  11.466726  11.060019  10.773822  10.3373165  9.685102
0:   9.043978   8.53305    8.369059   8.597348   9.035248   9.503971
0:   7.4081783  7.167531 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.366077 24.283176 24.240477 24.161886 23.990505 23.534937 23.699274
0:  23.758614 24.001774 24.167616 23.767113 23.309517 22.765953 22.678553
0:  23.067696 23.747686 24.570488 25.094032 24.749744 24.557657]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.836485 17.757938 19.167362 20.604477 21.505878 21.812267 21.937899
0:  22.086777 22.228432 22.445297 22.362534 21.732641 20.79522  19.639528
0:  18.82555  18.542181 18.768776 19.221298 14.562922 15.108301]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.431017 -13.941542 -13.905064 -13.240549 -12.317722 -11.487402
0:  -11.20961  -11.170952 -11.861933 -12.616653 -13.343534 -14.161922
0:  -14.548582 -14.71447  -14.49119  -13.911123 -13.15078  -12.330908
0:  -12.96336  -13.68079 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.466961 15.644918 16.17343  16.774454 17.24403  17.443293 17.798067
0:  18.033054 18.226122 18.475634 18.46438  18.277742 18.1079   18.116213
0:  18.418447 18.989866 19.570162 19.895773 16.874998 17.154978]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.548153  -8.479933  -8.038525  -7.2602353 -6.466129  -5.9015603
0:  -5.763679  -5.907187  -6.645497  -7.390303  -8.2220745 -9.1084385
0:  -9.670401  -9.936466  -9.753555  -9.177921  -8.512289  -7.81163
0:  -9.312689  -9.469004 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.867247  -10.100026   -9.967354   -9.502293   -9.091598   -8.996102
0:   -9.162735   -9.5844    -10.4621725 -11.282418  -12.234616  -13.248235
0:  -13.938727  -14.239266  -14.07227   -13.486173  -12.823846  -12.271689
0:  -12.936303  -13.072257 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.818302 36.000713 36.16849  36.114483 35.79313  35.135277 34.791927
0:  34.356495 34.080215 33.872005 33.271408 32.540684 31.843613 31.371843
0:  31.358952 31.60113  31.987047 32.241142 28.571981 28.619942]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.95571   13.028267  13.3148575 13.5914955 13.715076  13.5682
0:  13.678425  13.767777  13.867022  13.958935  13.765883  13.431516
0:  13.153782  13.241299  13.724392  14.543997  15.373905  15.945168
0:  14.7539015 14.862257 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.010105 34.034363 34.07441  34.053635 34.06958  33.8652   34.384438
0:  34.769993 35.32344  35.760635 35.64657  35.428593 35.19527  35.23488
0:  35.724453 36.489567 37.34027  37.945343 35.298256 35.21866 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.83160734 -0.8752732  -0.47670507  0.21951294  0.8426318   1.1709142
0:   1.255055    1.1270523   0.654871    0.20665264 -0.30897474 -0.95960045
0:  -1.4137878  -1.6412048  -1.5444994  -1.0681872  -0.491683   -0.01832867
0:  -2.2168741  -2.0993748 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.0971117 7.1201754 7.5535135 8.263333  8.833231  9.125013  9.022807
0:  8.886641  8.403368  8.067182  7.7709556 7.2365794 6.751608  6.2414484
0:  6.0102777 6.281047  7.0075026 7.9357834 5.7676625 5.5958757]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.78841  15.027126 15.468053 15.943346 16.269018 16.345356 16.6536
0:  16.869583 17.045132 17.228619 17.095287 16.80389  16.494308 16.440403
0:  16.759382 17.387703 18.135067 18.717522 16.4971   16.48922 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.372944  15.134262  15.280868  15.55609   15.7608385 15.639431
0:  15.7846985 15.763582  15.7996025 15.863411  15.593522  15.1776
0:  14.779123  14.634424  14.935345  15.691084  16.680351  17.611301
0:  15.877082  15.770655 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.956892  -10.179568   -9.9486685  -9.252869   -8.447245   -7.726798
0:   -7.3838406  -7.169936   -7.464861   -7.7253838  -7.9522696  -8.286081
0:   -8.389017   -8.335659   -7.9534826  -7.09219    -5.951996   -4.6938906
0:   -5.5375814  -5.6272154]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.053783   -1.7671638  -1.0074239   0.10571432  1.1215434   1.8325372
0:   2.272286    2.4195223   2.1294336   1.754921    1.1277165   0.24448538
0:  -0.6468072  -1.4213262  -1.8908248  -1.965529   -1.8323417  -1.6409879
0:  -3.687632   -3.6050506 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.3282046   1.1020885   1.2835326   1.7586446   2.071261    2.0201197
0:   1.610672    1.1081324   0.23143625 -0.5585928  -1.3940554  -2.528494
0:  -3.539669   -4.3812733  -4.772703   -4.438428   -3.6100016  -2.5671768
0:  -3.1887474  -3.0471277 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.375506  2.5904627 3.1804326 4.114836  5.077461  5.8429403 6.1382284
0:  6.235654  5.678336  5.0630827 4.4054394 3.5742104 2.9602866 2.4782052
0:  2.3007965 2.5598245 3.1062055 3.7528684 1.9468737 1.7668672]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.9609556 3.201357  3.8484166 4.7627068 5.6313424 6.2683644 6.7205915
0:  7.0091505 6.990436  6.83207   6.515588  5.979854  5.57815   5.4504037
0:  5.7115803 6.3647394 7.1273804 7.714077  5.6882586 5.671076 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.953262 32.689777 32.444443 32.17611  31.913534 31.415947 31.627676
0:  31.70221  31.956781 32.12642  31.662956 31.12093  30.525568 30.302292
0:  30.586319 31.212357 31.987118 32.514233 31.021893 30.890095]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.8771033 1.0078459 1.5641551 2.4057906 3.1917253 3.7252126 3.9834235
0:  4.077854  3.7700574 3.4845912 3.1241107 2.595477  2.262717  2.0885801
0:  2.24937   2.7865493 3.4483044 4.036193  2.0848594 2.2618036]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.626375  12.743164  13.057056  13.321133  13.392892  13.248247
0:  13.110819  12.952689  12.7573395 12.640055  12.408468  12.08193
0:  11.874203  11.919931  12.3535    13.065881  13.822518  14.45532
0:  12.595567  12.836684 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.065844 12.771533 12.717149 12.65946  12.405153 11.817114 11.441624
0:  10.982907 10.574177 10.245397  9.669119  9.01293   8.329473  7.998205
0:   8.107992  8.623172  9.298922  9.879656  8.72046   8.689114]
0: validation loss for strategy=forecast at epoch 49 : 0.316749632358551
0: validation loss for velocity_u : 0.1733110398054123
0: validation loss for velocity_v : 0.3133915364742279
0: validation loss for specific_humidity : 0.17810393869876862
0: validation loss for velocity_z : 0.5795932412147522
0: validation loss for temperature : 0.10940229892730713
0: validation loss for total_precip : 0.5466954112052917
0: Finished training at 21:17:23 with test loss = 0.316749632358551.
0: [1;34mwandb[0m:
0: [1;34mwandb[0m: You can sync this run to the cloud by running:
0: [1;34mwandb[0m: [1mwandb sync /work/ab1412/atmorep/wandb/offline-run-20250604_180047-3xl7ehv0[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250604_180047-3xl7ehv0/logs[0m
0: l50018:398398:398790 [2] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50018:398398:411173 [2] NCCL INFO comm 0x555583260a50 rank 0 nranks 1 cudaDev 2 busId 84000 - Abort COMPLETE
0: l50018:398398:398795 [3] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50018:398398:411173 [3] NCCL INFO comm 0x555585538570 rank 0 nranks 1 cudaDev 3 busId c4000 - Abort COMPLETE
0: l50018:398398:398785 [1] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50018:398398:411173 [1] NCCL INFO comm 0x55556eada540 rank 0 nranks 1 cudaDev 1 busId 44000 - Abort COMPLETE
0: l50018:398398:398758 [0] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50018:398398:411173 [0] NCCL INFO comm 0x55555f268f10 rank 0 nranks 1 cudaDev 0 busId 3000 - Abort COMPLETE
