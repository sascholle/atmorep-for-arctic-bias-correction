0: Wandb run: atmorep-qbyzbeit-18350700
0: l50069:327628:327628 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.152<0>
0: l50069:327628:327628 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50069:327628:327628 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50069:327628:327628 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50069:327628:327628 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
1: l50078:2446184:2446184 [0] NCCL INFO cudaDriverVersion 12050
1: l50078:2446184:2446184 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.158<0>
1: l50078:2446184:2446184 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
1: l50078:2446184:2446184 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
1: l50078:2446184:2446184 [0] NCCL INFO NET/Plugin: Using internal network plugin.
1: l50078:2446184:2446438 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.158<0>
1: l50078:2446184:2446438 [0] NCCL INFO Using non-device net plugin version 0
1: l50078:2446184:2446438 [0] NCCL INFO Using network IB
0: l50069:327628:328031 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.152<0>
0: l50069:327628:328031 [0] NCCL INFO Using non-device net plugin version 0
0: l50069:327628:328031 [0] NCCL INFO Using network IB
0: l50069:327628:328031 [0] NCCL INFO ncclCommInitRank comm 0x55555f292290 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x3904317260710dd5 - Init START
1: l50078:2446184:2446438 [0] NCCL INFO ncclCommInitRank comm 0x55555ee4d6f0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x3904317260710dd5 - Init START
1: l50078:2446184:2446438 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
0: l50069:327628:328031 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
0: l50069:327628:328031 [0] NCCL INFO comm 0x55555f292290 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50069:327628:328031 [0] NCCL INFO Channel 00/04 :    0   1
0: l50069:327628:328031 [0] NCCL INFO Channel 01/04 :    0   1
0: l50069:327628:328031 [0] NCCL INFO Channel 02/04 :    0   1
0: l50069:327628:328031 [0] NCCL INFO Channel 03/04 :    0   1
0: l50069:327628:328031 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
0: l50069:327628:328031 [0] NCCL INFO P2P Chunksize set to 131072
1: l50078:2446184:2446438 [0] NCCL INFO comm 0x55555ee4d6f0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
1: l50078:2446184:2446438 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
1: l50078:2446184:2446438 [0] NCCL INFO P2P Chunksize set to 131072
0: l50069:327628:328031 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/IB/0
0: l50069:327628:328031 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IB/1
0: l50069:327628:328031 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/0
0: l50069:327628:328031 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/1
0: l50069:327628:328031 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/0
0: l50069:327628:328031 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/1
0: l50069:327628:328031 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/0
0: l50069:327628:328031 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/1
1: l50078:2446184:2446438 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/0
1: l50078:2446184:2446438 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/1
1: l50078:2446184:2446438 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/0
1: l50078:2446184:2446438 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/1
1: l50078:2446184:2446438 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/IB/0
1: l50078:2446184:2446438 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/IB/1
1: l50078:2446184:2446438 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/0
1: l50078:2446184:2446438 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/1
1: l50078:2446184:2446441 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 250.
1: l50078:2446184:2446441 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 50.
0: l50069:327628:328034 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 250.
0: l50069:327628:328034 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 50.
0: l50069:327628:328031 [0] NCCL INFO Connected all rings
1: l50078:2446184:2446438 [0] NCCL INFO Connected all rings
0: l50069:327628:328031 [0] NCCL INFO Connected all trees
1: l50078:2446184:2446438 [0] NCCL INFO Connected all trees
1: l50078:2446184:2446438 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
1: l50078:2446184:2446438 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50069:327628:328031 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
0: l50069:327628:328031 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50069:327628:328031 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50069:327628:328031 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50069:327628:328031 [0] NCCL INFO ncclCommInitRank comm 0x55555f292290 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x3904317260710dd5 - Init COMPLETE
1: l50078:2446184:2446438 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
1: l50078:2446184:2446438 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
1: l50078:2446184:2446438 [0] NCCL INFO ncclCommInitRank comm 0x55555ee4d6f0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x3904317260710dd5 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 1
0: par_rank : 0
0: par_size : 2
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 0, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 0, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.5, 0.9, 0.2, 0.05]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 0, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.5, 0.9, 0.2, 0.05], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [0.25, 0.9, 0.1, 0.05]], ['t2m', [1, 1024, ['velocity_u', '
0: velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 2, 4], [3, 27, 27], [0.5, 0.9, 0.2, 0.05], 'Local']]
0: fields_prediction : [['velocity_u', 0.25], ['velocity_v', 0.25], ['specific_humidity', 0.15], ['velocity_z', 0.025], ['temperature', 0.1], ['total_precip', 0.025], ['t2m', 0.2]]
0: fields_targets : []
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[0.0, 360.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 128
0: num_samples_per_epoch : 480
0: num_samples_validate : 128
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : BERT
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 18350700
0: wandb_id : qbyzbeit
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25_with_t2m.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : t2m
0: sparse_target_sparsity : 0.9
0: years_test : [2021]
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded AtmoRep: ignoring 208 elements: ['embeds_token_info.6.weight', 'embeds_token_info.6.bias', 'embeds.6.weight', 'embeds.6.bias', 'encoders.6.embed.weight', 'encoders.6.embed.bias', 'encoders.6.heads.0.proj_out.weight', 'encoders.6.heads.0.proj_heads.weight', 'encoders.6.heads.0.proj_heads_other.0.weight', 'encoders.6.heads.0.proj_heads_other.1.weight', 'encoders.6.heads.0.proj_heads_other.2.weight', 'encoders.6.heads.0.proj_heads_other.3.weight', 'encoders.6.heads.0.proj_heads_other.4.weight', 'encoders.6.heads.1.proj_out.weight', 'encoders.6.heads.1.proj_heads.weight', 'encoders.6.heads.1.proj_heads_other.0.weight', 'encoders.6.heads.1.proj_heads_other.1.weight', 'encoders.6.heads.1.proj_heads_other.2.weight', 'encoders.6.heads.1.proj_heads_other.3.weight', 'encoders.6.heads.1.proj_heads_other.4.weight', 'encoders.6.heads.2.proj_out.weight', 'encoders.6.heads.2.proj_heads.weight', 'encoders.6.heads.2.proj_heads_other.0.weight', 'encoders.6.heads.2.proj_heads_other.1.weight', 'encoders.6.heads.2.proj_hea
0: ds_other.2.weight', 'encoders.6.heads.2.proj_heads_other.3.weight', 'encoders.6.heads.2.proj_heads_other.4.weight', 'encoders.6.heads.3.proj_out.weight', 'encoders.6.heads.3.proj_heads.weight', 'encoders.6.heads.3.proj_heads_other.0.weight', 'encoders.6.heads.3.proj_heads_other.1.weight', 'encoders.6.heads.3.proj_heads_other.2.weight', 'encoders.6.heads.3.proj_heads_other.3.weight', 'encoders.6.heads.3.proj_heads_other.4.weight', 'encoders.6.heads.4.proj_out.weight', 'encoders.6.heads.4.proj_heads.weight', 'encoders.6.heads.4.proj_heads_other.0.weight', 'encoders.6.heads.4.proj_heads_other.1.weight', 'encoders.6.heads.4.proj_heads_other.2.weight', 'encoders.6.heads.4.proj_heads_other.3.weight', 'encoders.6.heads.4.proj_heads_other.4.weight', 'encoders.6.heads.5.proj_out.weight', 'encoders.6.heads.5.proj_heads.weight', 'encoders.6.heads.5.proj_heads_other.0.weight', 'encoders.6.heads.5.proj_heads_other.1.weight', 'encoders.6.heads.5.proj_heads_other.2.weight', 'encoders.6.heads.5.proj_heads_other.3.weight', 'e
0: ncoders.6.heads.5.proj_heads_other.4.weight', 'encoders.6.mlps.0.blocks.0.weight', 'encoders.6.mlps.0.blocks.0.bias', 'encoders.6.mlps.0.blocks.3.weight', 'encoders.6.mlps.0.blocks.3.bias', 'encoders.6.mlps.1.blocks.0.weight', 'encoders.6.mlps.1.blocks.0.bias', 'encoders.6.mlps.1.blocks.3.weight', 'encoders.6.mlps.1.blocks.3.bias', 'encoders.6.mlps.2.blocks.0.weight', 'encoders.6.mlps.2.blocks.0.bias', 'encoders.6.mlps.2.blocks.3.weight', 'encoders.6.mlps.2.blocks.3.bias', 'encoders.6.mlps.3.blocks.0.weight', 'encoders.6.mlps.3.blocks.0.bias', 'encoders.6.mlps.3.blocks.3.weight', 'encoders.6.mlps.3.blocks.3.bias', 'encoders.6.mlps.4.blocks.0.weight', 'encoders.6.mlps.4.blocks.0.bias', 'encoders.6.mlps.4.blocks.3.weight', 'encoders.6.mlps.4.blocks.3.bias', 'encoders.6.mlps.5.blocks.0.weight', 'encoders.6.mlps.5.blocks.0.bias', 'encoders.6.mlps.5.blocks.3.weight', 'encoders.6.mlps.5.blocks.3.bias', 'decoders.6.blocks.0.proj_heads.weight', 'decoders.6.blocks.0.proj_heads_o_q.weight', 'decoders.6.blocks.0.proj_he
0: ads_o_kv.weight', 'decoders.6.blocks.0.ln_q.weight', 'decoders.6.blocks.0.ln_q.bias', 'decoders.6.blocks.0.ln_k.weight', 'decoders.6.blocks.0.ln_k.bias', 'decoders.6.blocks.0.proj_out.weight', 'decoders.6.blocks.1.blocks.0.weight', 'decoders.6.blocks.1.blocks.0.bias', 'decoders.6.blocks.1.blocks.3.weight', 'decoders.6.blocks.1.blocks.3.bias', 'decoders.6.blocks.2.proj_heads.weight', 'decoders.6.blocks.2.proj_heads_o_q.weight', 'decoders.6.blocks.2.proj_heads_o_kv.weight', 'decoders.6.blocks.2.ln_q.weight', 'decoders.6.blocks.2.ln_q.bias', 'decoders.6.blocks.2.ln_k.weight', 'decoders.6.blocks.2.ln_k.bias', 'decoders.6.blocks.2.proj_out.weight', 'decoders.6.blocks.3.blocks.0.weight', 'decoders.6.blocks.3.blocks.0.bias', 'decoders.6.blocks.3.blocks.3.weight', 'decoders.6.blocks.3.blocks.3.bias', 'decoders.6.blocks.4.proj_heads.weight', 'decoders.6.blocks.4.proj_heads_o_q.weight', 'decoders.6.blocks.4.proj_heads_o_kv.weight', 'decoders.6.blocks.4.ln_q.weight', 'decoders.6.blocks.4.ln_q.bias', 'decoders.6.blocks.4
0: .ln_k.weight', 'decoders.6.blocks.4.ln_k.bias', 'decoders.6.blocks.4.proj_out.weight', 'decoders.6.blocks.5.blocks.0.weight', 'decoders.6.blocks.5.blocks.0.bias', 'decoders.6.blocks.5.blocks.3.weight', 'decoders.6.blocks.5.blocks.3.bias', 'decoders.6.blocks.6.proj_heads.weight', 'decoders.6.blocks.6.proj_heads_o_q.weight', 'decoders.6.blocks.6.proj_heads_o_kv.weight', 'decoders.6.blocks.6.ln_q.weight', 'decoders.6.blocks.6.ln_q.bias', 'decoders.6.blocks.6.ln_k.weight', 'decoders.6.blocks.6.ln_k.bias', 'decoders.6.blocks.6.proj_out.weight', 'decoders.6.blocks.7.blocks.0.weight', 'decoders.6.blocks.7.blocks.0.bias', 'decoders.6.blocks.7.blocks.3.weight', 'decoders.6.blocks.7.blocks.3.bias', 'decoders.6.blocks.8.proj_heads.weight', 'decoders.6.blocks.8.proj_heads_o_q.weight', 'decoders.6.blocks.8.proj_heads_o_kv.weight', 'decoders.6.blocks.8.ln_q.weight', 'decoders.6.blocks.8.ln_q.bias', 'decoders.6.blocks.8.ln_k.weight', 'decoders.6.blocks.8.ln_k.bias', 'decoders.6.blocks.8.proj_out.weight', 'decoders.6.blocks.
0: 9.blocks.0.weight', 'decoders.6.blocks.9.blocks.0.bias', 'decoders.6.blocks.9.blocks.3.weight', 'decoders.6.blocks.9.blocks.3.bias', 'decoders.6.blocks.10.proj_heads.weight', 'decoders.6.blocks.10.proj_heads_o_q.weight', 'decoders.6.blocks.10.proj_heads_o_kv.weight', 'decoders.6.blocks.10.ln_q.weight', 'decoders.6.blocks.10.ln_q.bias', 'decoders.6.blocks.10.ln_k.weight', 'decoders.6.blocks.10.ln_k.bias', 'decoders.6.blocks.10.proj_out.weight', 'decoders.6.blocks.11.blocks.0.weight', 'decoders.6.blocks.11.blocks.0.bias', 'decoders.6.blocks.11.blocks.3.weight', 'decoders.6.blocks.11.blocks.3.bias', 'tails.6.tail_nets.0.0.weight', 'tails.6.tail_nets.0.0.bias', 'tails.6.tail_nets.0.1.weight', 'tails.6.tail_nets.0.1.bias', 'tails.6.tail_nets.1.0.weight', 'tails.6.tail_nets.1.0.bias', 'tails.6.tail_nets.1.1.weight', 'tails.6.tail_nets.1.1.bias', 'tails.6.tail_nets.2.0.weight', 'tails.6.tail_nets.2.0.bias', 'tails.6.tail_nets.2.1.weight', 'tails.6.tail_nets.2.1.bias', 'tails.6.tail_nets.3.0.weight', 'tails.6.tail_ne
0: ts.3.0.bias', 'tails.6.tail_nets.3.1.weight', 'tails.6.tail_nets.3.1.bias', 'tails.6.tail_nets.4.0.weight', 'tails.6.tail_nets.4.0.bias', 'tails.6.tail_nets.4.1.weight', 'tails.6.tail_nets.4.1.bias', 'tails.6.tail_nets.5.0.weight', 'tails.6.tail_nets.5.0.bias', 'tails.6.tail_nets.5.1.weight', 'tails.6.tail_nets.5.1.bias', 'tails.6.tail_nets.6.0.weight', 'tails.6.tail_nets.6.0.bias', 'tails.6.tail_nets.6.1.weight', 'tails.6.tail_nets.6.1.bias', 'tails.6.tail_nets.7.0.weight', 'tails.6.tail_nets.7.0.bias', 'tails.6.tail_nets.7.1.weight', 'tails.6.tail_nets.7.1.bias', 'tails.6.tail_nets.8.0.weight', 'tails.6.tail_nets.8.0.bias', 'tails.6.tail_nets.8.1.weight', 'tails.6.tail_nets.8.1.bias', 'tails.6.tail_nets.9.0.weight', 'tails.6.tail_nets.9.0.bias', 'tails.6.tail_nets.9.1.weight', 'tails.6.tail_nets.9.1.bias', 'tails.6.tail_nets.10.0.weight', 'tails.6.tail_nets.10.0.bias', 'tails.6.tail_nets.10.1.weight', 'tails.6.tail_nets.10.1.bias', 'tails.6.tail_nets.11.0.weight', 'tails.6.tail_nets.11.0.bias', 'tails.6.tai
0: l_nets.11.1.weight', 'tails.6.tail_nets.11.1.bias', 'tails.6.tail_nets.12.0.weight', 'tails.6.tail_nets.12.0.bias', 'tails.6.tail_nets.12.1.weight', 'tails.6.tail_nets.12.1.bias', 'tails.6.tail_nets.13.0.weight', 'tails.6.tail_nets.13.0.bias', 'tails.6.tail_nets.13.1.weight', 'tails.6.tail_nets.13.1.bias', 'tails.6.tail_nets.14.0.weight', 'tails.6.tail_nets.14.0.bias', 'tails.6.tail_nets.14.1.weight', 'tails.6.tail_nets.14.1.bias', 'tails.6.tail_nets.15.0.weight', 'tails.6.tail_nets.15.0.bias', 'tails.6.tail_nets.15.1.weight', 'tails.6.tail_nets.15.1.bias']
1: Loaded AtmoRep: ignoring 208 elements: ['embeds_token_info.6.weight', 'embeds_token_info.6.bias', 'embeds.6.weight', 'embeds.6.bias', 'encoders.6.embed.weight', 'encoders.6.embed.bias', 'encoders.6.heads.0.proj_out.weight', 'encoders.6.heads.0.proj_heads.weight', 'encoders.6.heads.0.proj_heads_other.0.weight', 'encoders.6.heads.0.proj_heads_other.1.weight', 'encoders.6.heads.0.proj_heads_other.2.weight', 'encoders.6.heads.0.proj_heads_other.3.weight', 'encoders.6.heads.0.proj_heads_other.4.weight', 'encoders.6.heads.1.proj_out.weight', 'encoders.6.heads.1.proj_heads.weight', 'encoders.6.heads.1.proj_heads_other.0.weight', 'encoders.6.heads.1.proj_heads_other.1.weight', 'encoders.6.heads.1.proj_heads_other.2.weight', 'encoders.6.heads.1.proj_heads_other.3.weight', 'encoders.6.heads.1.proj_heads_other.4.weight', 'encoders.6.heads.2.proj_out.weight', 'encoders.6.heads.2.proj_heads.weight', 'encoders.6.heads.2.proj_heads_other.0.weight', 'encoders.6.heads.2.proj_heads_other.1.weight', 'encoders.6.heads.2.proj_hea
1: ds_other.2.weight', 'encoders.6.heads.2.proj_heads_other.3.weight', 'encoders.6.heads.2.proj_heads_other.4.weight', 'encoders.6.heads.3.proj_out.weight', 'encoders.6.heads.3.proj_heads.weight', 'encoders.6.heads.3.proj_heads_other.0.weight', 'encoders.6.heads.3.proj_heads_other.1.weight', 'encoders.6.heads.3.proj_heads_other.2.weight', 'encoders.6.heads.3.proj_heads_other.3.weight', 'encoders.6.heads.3.proj_heads_other.4.weight', 'encoders.6.heads.4.proj_out.weight', 'encoders.6.heads.4.proj_heads.weight', 'encoders.6.heads.4.proj_heads_other.0.weight', 'encoders.6.heads.4.proj_heads_other.1.weight', 'encoders.6.heads.4.proj_heads_other.2.weight', 'encoders.6.heads.4.proj_heads_other.3.weight', 'encoders.6.heads.4.proj_heads_other.4.weight', 'encoders.6.heads.5.proj_out.weight', 'encoders.6.heads.5.proj_heads.weight', 'encoders.6.heads.5.proj_heads_other.0.weight', 'encoders.6.heads.5.proj_heads_other.1.weight', 'encoders.6.heads.5.proj_heads_other.2.weight', 'encoders.6.heads.5.proj_heads_other.3.weight', 'e
1: ncoders.6.heads.5.proj_heads_other.4.weight', 'encoders.6.mlps.0.blocks.0.weight', 'encoders.6.mlps.0.blocks.0.bias', 'encoders.6.mlps.0.blocks.3.weight', 'encoders.6.mlps.0.blocks.3.bias', 'encoders.6.mlps.1.blocks.0.weight', 'encoders.6.mlps.1.blocks.0.bias', 'encoders.6.mlps.1.blocks.3.weight', 'encoders.6.mlps.1.blocks.3.bias', 'encoders.6.mlps.2.blocks.0.weight', 'encoders.6.mlps.2.blocks.0.bias', 'encoders.6.mlps.2.blocks.3.weight', 'encoders.6.mlps.2.blocks.3.bias', 'encoders.6.mlps.3.blocks.0.weight', 'encoders.6.mlps.3.blocks.0.bias', 'encoders.6.mlps.3.blocks.3.weight', 'encoders.6.mlps.3.blocks.3.bias', 'encoders.6.mlps.4.blocks.0.weight', 'encoders.6.mlps.4.blocks.0.bias', 'encoders.6.mlps.4.blocks.3.weight', 'encoders.6.mlps.4.blocks.3.bias', 'encoders.6.mlps.5.blocks.0.weight', 'encoders.6.mlps.5.blocks.0.bias', 'encoders.6.mlps.5.blocks.3.weight', 'encoders.6.mlps.5.blocks.3.bias', 'decoders.6.blocks.0.proj_heads.weight', 'decoders.6.blocks.0.proj_heads_o_q.weight', 'decoders.6.blocks.0.proj_he
1: ads_o_kv.weight', 'decoders.6.blocks.0.ln_q.weight', 'decoders.6.blocks.0.ln_q.bias', 'decoders.6.blocks.0.ln_k.weight', 'decoders.6.blocks.0.ln_k.bias', 'decoders.6.blocks.0.proj_out.weight', 'decoders.6.blocks.1.blocks.0.weight', 'decoders.6.blocks.1.blocks.0.bias', 'decoders.6.blocks.1.blocks.3.weight', 'decoders.6.blocks.1.blocks.3.bias', 'decoders.6.blocks.2.proj_heads.weight', 'decoders.6.blocks.2.proj_heads_o_q.weight', 'decoders.6.blocks.2.proj_heads_o_kv.weight', 'decoders.6.blocks.2.ln_q.weight', 'decoders.6.blocks.2.ln_q.bias', 'decoders.6.blocks.2.ln_k.weight', 'decoders.6.blocks.2.ln_k.bias', 'decoders.6.blocks.2.proj_out.weight', 'decoders.6.blocks.3.blocks.0.weight', 'decoders.6.blocks.3.blocks.0.bias', 'decoders.6.blocks.3.blocks.3.weight', 'decoders.6.blocks.3.blocks.3.bias', 'decoders.6.blocks.4.proj_heads.weight', 'decoders.6.blocks.4.proj_heads_o_q.weight', 'decoders.6.blocks.4.proj_heads_o_kv.weight', 'decoders.6.blocks.4.ln_q.weight', 'decoders.6.blocks.4.ln_q.bias', 'decoders.6.blocks.4
1: .ln_k.weight', 'decoders.6.blocks.4.ln_k.bias', 'decoders.6.blocks.4.proj_out.weight', 'decoders.6.blocks.5.blocks.0.weight', 'decoders.6.blocks.5.blocks.0.bias', 'decoders.6.blocks.5.blocks.3.weight', 'decoders.6.blocks.5.blocks.3.bias', 'decoders.6.blocks.6.proj_heads.weight', 'decoders.6.blocks.6.proj_heads_o_q.weight', 'decoders.6.blocks.6.proj_heads_o_kv.weight', 'decoders.6.blocks.6.ln_q.weight', 'decoders.6.blocks.6.ln_q.bias', 'decoders.6.blocks.6.ln_k.weight', 'decoders.6.blocks.6.ln_k.bias', 'decoders.6.blocks.6.proj_out.weight', 'decoders.6.blocks.7.blocks.0.weight', 'decoders.6.blocks.7.blocks.0.bias', 'decoders.6.blocks.7.blocks.3.weight', 'decoders.6.blocks.7.blocks.3.bias', 'decoders.6.blocks.8.proj_heads.weight', 'decoders.6.blocks.8.proj_heads_o_q.weight', 'decoders.6.blocks.8.proj_heads_o_kv.weight', 'decoders.6.blocks.8.ln_q.weight', 'decoders.6.blocks.8.ln_q.bias', 'decoders.6.blocks.8.ln_k.weight', 'decoders.6.blocks.8.ln_k.bias', 'decoders.6.blocks.8.proj_out.weight', 'decoders.6.blocks.
1: 9.blocks.0.weight', 'decoders.6.blocks.9.blocks.0.bias', 'decoders.6.blocks.9.blocks.3.weight', 'decoders.6.blocks.9.blocks.3.bias', 'decoders.6.blocks.10.proj_heads.weight', 'decoders.6.blocks.10.proj_heads_o_q.weight', 'decoders.6.blocks.10.proj_heads_o_kv.weight', 'decoders.6.blocks.10.ln_q.weight', 'decoders.6.blocks.10.ln_q.bias', 'decoders.6.blocks.10.ln_k.weight', 'decoders.6.blocks.10.ln_k.bias', 'decoders.6.blocks.10.proj_out.weight', 'decoders.6.blocks.11.blocks.0.weight', 'decoders.6.blocks.11.blocks.0.bias', 'decoders.6.blocks.11.blocks.3.weight', 'decoders.6.blocks.11.blocks.3.bias', 'tails.6.tail_nets.0.0.weight', 'tails.6.tail_nets.0.0.bias', 'tails.6.tail_nets.0.1.weight', 'tails.6.tail_nets.0.1.bias', 'tails.6.tail_nets.1.0.weight', 'tails.6.tail_nets.1.0.bias', 'tails.6.tail_nets.1.1.weight', 'tails.6.tail_nets.1.1.bias', 'tails.6.tail_nets.2.0.weight', 'tails.6.tail_nets.2.0.bias', 'tails.6.tail_nets.2.1.weight', 'tails.6.tail_nets.2.1.bias', 'tails.6.tail_nets.3.0.weight', 'tails.6.tail_ne
1: ts.3.0.bias', 'tails.6.tail_nets.3.1.weight', 'tails.6.tail_nets.3.1.bias', 'tails.6.tail_nets.4.0.weight', 'tails.6.tail_nets.4.0.bias', 'tails.6.tail_nets.4.1.weight', 'tails.6.tail_nets.4.1.bias', 'tails.6.tail_nets.5.0.weight', 'tails.6.tail_nets.5.0.bias', 'tails.6.tail_nets.5.1.weight', 'tails.6.tail_nets.5.1.bias', 'tails.6.tail_nets.6.0.weight', 'tails.6.tail_nets.6.0.bias', 'tails.6.tail_nets.6.1.weight', 'tails.6.tail_nets.6.1.bias', 'tails.6.tail_nets.7.0.weight', 'tails.6.tail_nets.7.0.bias', 'tails.6.tail_nets.7.1.weight', 'tails.6.tail_nets.7.1.bias', 'tails.6.tail_nets.8.0.weight', 'tails.6.tail_nets.8.0.bias', 'tails.6.tail_nets.8.1.weight', 'tails.6.tail_nets.8.1.bias', 'tails.6.tail_nets.9.0.weight', 'tails.6.tail_nets.9.0.bias', 'tails.6.tail_nets.9.1.weight', 'tails.6.tail_nets.9.1.bias', 'tails.6.tail_nets.10.0.weight', 'tails.6.tail_nets.10.0.bias', 'tails.6.tail_nets.10.1.weight', 'tails.6.tail_nets.10.1.bias', 'tails.6.tail_nets.11.0.weight', 'tails.6.tail_nets.11.0.bias', 'tails.6.tai
1: l_nets.11.1.weight', 'tails.6.tail_nets.11.1.bias', 'tails.6.tail_nets.12.0.weight', 'tails.6.tail_nets.12.0.bias', 'tails.6.tail_nets.12.1.weight', 'tails.6.tail_nets.12.1.bias', 'tails.6.tail_nets.13.0.weight', 'tails.6.tail_nets.13.0.bias', 'tails.6.tail_nets.13.1.weight', 'tails.6.tail_nets.13.1.bias', 'tails.6.tail_nets.14.0.weight', 'tails.6.tail_nets.14.0.bias', 'tails.6.tail_nets.14.1.weight', 'tails.6.tail_nets.14.1.bias', 'tails.6.tail_nets.15.0.weight', 'tails.6.tail_nets.15.0.bias', 'tails.6.tail_nets.15.1.weight', 'tails.6.tail_nets.15.1.bias']
1: Loaded model id = wc5e2i3t.
1: Loaded run 'wc5e2i3t' at epoch -2.
0: Loaded model id = wc5e2i3t.
0: Loaded run 'wc5e2i3t' at epoch -2.
1: -1 : 15:20:59 :: batch_size = 96, lr = 1e-05
0: Number of trainable parameters: 886,234,640
0: -1 : 15:20:59 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch -1, first input batch shapes / sample data:
0:   â””â”€ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1: [DEBUG] INPUT BATCH
1: Epoch -1, first input batch shapes / sample data:
1:   â””â”€ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0:   â””â”€ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.6493, -1.6444, -1.6363, -1.6256, -1.6156, -1.6082, -1.6041, -1.6043, -1.6084, -1.6145, -1.6212, -1.6263,
0:         -1.6288, -1.6295, -1.6288, -1.6269, -1.6246, -1.6218, -1.7031, -1.7000, -1.6923, -1.6812, -1.6686, -1.6565,
0:         -1.6473], device='cuda:0')
0:   â””â”€ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5614, -0.5592, -0.5545, -0.5494, -0.5445, -0.5415, -0.5397, -0.5407, -0.5441, -0.5488, -0.5545, -0.5612,
0:         -0.5660, -0.5702, -0.5730, -0.5733, -0.5732, -0.5696, -0.5477, -0.5493, -0.5475, -0.5425, -0.5371, -0.5321,
0:         -0.5302], device='cuda:0')
0:   â””â”€ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0615,  0.1339,  0.2107,  0.2469,  0.2173,  0.1603,  0.0879,  0.0122, -0.0339, -0.0536, -0.0536, -0.0328,
0:         -0.0131,  0.0045,  0.0264,  0.0319,  0.0286,  0.0330, -0.3334, -0.2028, -0.0888, -0.0164,  0.0122,  0.0111,
0:         -0.0295], device='cuda:0')
0:   â””â”€ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.6754, -0.6800, -0.6813, -0.6787, -0.6722, -0.6631, -0.6526, -0.6417, -0.6308, -0.6203, -0.6080, -0.5928,
0:         -0.5771, -0.5602, -0.5420, -0.5244, -0.5080, -0.4926, -0.4804, -0.4705, -0.4622, -0.4562, -0.4502, -0.4414,
0:         -0.4279], device='cuda:0')
0:   â””â”€ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337,
0:         -0.2337], device='cuda:0')
0:   â””â”€ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.6434, 0.6700, 0.6882, 0.6779, 0.6597, 0.6153, 0.5768, 0.5379, 0.5610, 0.6092, 0.6722, 0.6906, 0.7262, 0.7560,
0:         0.7871, 0.7881, 0.7837, 0.7793, 0.7990, 0.8167, 0.8310, 0.8227, 0.8236, 0.8249, 0.8547], device='cuda:0')
1:      first 25 values: tensor([1.5894, 1.5879, 1.5882, 1.5894, 1.5900, 1.5899, 1.5876, 1.5838, 1.5789, 1.5736, 1.5688, 1.5642, 1.5590, 1.5526,
1:         1.5448, 1.5346, 1.5229, 1.5106, 1.6439, 1.6409, 1.6407, 1.6417, 1.6433, 1.6452, 1.6462], device='cuda:0')
1:   â””â”€ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] TARGET BATCH
0: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([9864, 243])
1:      first 25 values: tensor([-1.6493, -1.6444, -1.6363, -1.6256, -1.6156, -1.6082, -1.6041, -1.6043, -1.6084, -1.6145, -1.6212, -1.6263,
1:         -1.6288, -1.6295, -1.6288, -1.6269, -1.6246, -1.6218, -1.7031, -1.7000, -1.6923, -1.6812, -1.6686, -1.6565,
1:         -1.6473], device='cuda:0')
1:   â””â”€ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.5614, -0.5592, -0.5545, -0.5494, -0.5445, -0.5415, -0.5397, -0.5407, -0.5441, -0.5488, -0.5545, -0.5612,
1:         -0.5660, -0.5702, -0.5730, -0.5733, -0.5732, -0.5696, -0.5477, -0.5493, -0.5475, -0.5425, -0.5371, -0.5321,
1:         -0.5302], device='cuda:0')
1:   â””â”€ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.0615,  0.1339,  0.2107,  0.2469,  0.2173,  0.1603,  0.0879,  0.0122, -0.0339, -0.0536, -0.0536, -0.0328,
1:         -0.0131,  0.0045,  0.0264,  0.0319,  0.0286,  0.0330, -0.3334, -0.2028, -0.0888, -0.0164,  0.0122,  0.0111,
1:         -0.0295], device='cuda:0')
1:   â””â”€ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2337, -0.2326, -0.2292, -0.2180, -0.2057, -0.2035, -0.2102, -0.2079, -0.1878, -0.2337, -0.2337, -0.2337,
0:         -0.2314, -0.2258, -0.2214, -0.2191, -0.2158, -0.1979, -0.2337, -0.2337, -0.2337, -0.2337, -0.2326, -0.2292,
0:         -0.2281, -0.2247, -0.2079, -0.2337, -0.2337, -0.2337, -0.2337, -0.2326, -0.2326, -0.2314, -0.2292, -0.2225,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2314, -0.2247, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2326, -0.2314, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2326, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2314,
0:         -0.2247, -0.2169, -0.2102, -0.2079, -0.2113, -0.2057, -0.2337, -0.2337, -0.2337, -0.2314, -0.2270, -0.2202,
0:         -0.2147, -0.2147, -0.2102, -0.2337, -0.2337, -0.2337, -0.2337, -0.2326, -0.2270, -0.2236, -0.2191, -0.2102,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2292, -0.2258, -0.2079, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2314, -0.2158, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2326, -0.2314, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2326, -0.2314,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2326, -0.2303, -0.2236, -0.2236,
0:         -0.2180, -0.2102, -0.2023, -0.2337, -0.2337, -0.2337, -0.2326, -0.2292, -0.2247, -0.2225, -0.2169, -0.2147,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2292, -0.2281, -0.2236, -0.2214, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2337, -0.2314, -0.2292, -0.2258, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2314, -0.2270, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2326, -0.2314,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2326, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337, -0.2337,
0:         -0.2337, -0.2337, -0.2337])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch -1, first predictions sample:
0:   â””â”€ Predictions for 'velocity_u' shape: torch.Size([26062, 972])
1:      first 25 values: tensor([-0.6754, -0.6800, -0.6813, -0.6787, -0.6722, -0.6631, -0.6526, -0.6417, -0.6308, -0.6203, -0.6080, -0.5928,
1:         -0.5771, -0.5602, -0.5420, -0.5244, -0.5080, -0.4926, -0.4804, -0.4705, -0.4622, -0.4562, -0.4502, -0.4414,
1:         -0.4279], device='cuda:0')
1:   â””â”€ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],
1:        device='cuda:0')
1:   â””â”€ Field: 't2m' shape: torch.Size([96, 1, 12, 2, 4, 3, 27, 27])
0:      first 25 pred values: tensor([1.3624, 1.3553, 1.3558, 1.3672, 1.3937, 1.4302, 1.4759, 1.5291, 1.5821, 1.6305, 1.6666, 1.6901, 1.6979, 1.6888,
0:         1.6720, 1.6443, 1.6070, 1.5667, 1.4318, 1.4140, 1.4016, 1.3983, 1.4097, 1.4370, 1.4746], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   â””â”€ Predictions for 'velocity_v' shape: torch.Size([26165, 972])
0:      first 25 pred values: tensor([-0.7322, -0.6720, -0.6118, -0.5530, -0.4961, -0.4440, -0.3922, -0.3451, -0.3012, -0.2632, -0.2339, -0.2139,
0:         -0.2022, -0.1963, -0.1962, -0.1944, -0.1949, -0.1940, -0.6832, -0.6219, -0.5643, -0.5111, -0.4607, -0.4150,
0:         -0.3689], device='cuda:0', grad_fn=<SliceBackward0>)
0:   â””â”€ Predictions for 'specific_humidity' shape: torch.Size([26473, 972])
1:      first 25 values: tensor([0.6434, 0.6700, 0.6882, 0.6779, 0.6597, 0.6153, 0.5768, 0.5379, 0.5610, 0.6092, 0.6722, 0.6906, 0.7262, 0.7560,
1:         0.7871, 0.7881, 0.7837, 0.7793, 0.7990, 0.8167, 0.8310, 0.8227, 0.8236, 0.8249, 0.8547], device='cuda:0')
0:      first 25 pred values: tensor([-0.6241, -0.6136, -0.6036, -0.5933, -0.5839, -0.5765, -0.5670, -0.5583, -0.5484, -0.5404, -0.5296, -0.5211,
0:         -0.5126, -0.5042, -0.4965, -0.4909, -0.4860, -0.4838, -0.6349, -0.6260, -0.6205, -0.6142, -0.6076, -0.5992,
0:         -0.5917], device='cuda:0', grad_fn=<SliceBackward0>)
0:   â””â”€ Predictions for 'velocity_z' shape: torch.Size([27550, 972])
0:      first 25 pred values: tensor([-0.0308, -0.0260, -0.0279, -0.0126, -0.0131, -0.0198, -0.0228, -0.0188, -0.0141, -0.0165, -0.0267, -0.0158,
0:          0.0127,  0.0213,  0.0405,  0.0571,  0.0440,  0.0679,  0.0299,  0.0328,  0.0367,  0.0652,  0.0873,  0.1105,
0:          0.1357], device='cuda:0', grad_fn=<SliceBackward0>)
0:   â””â”€ Predictions for 'temperature' shape: torch.Size([11706, 2187])
0:      first 25 pred values: tensor([0.2719, 0.2969, 0.3198, 0.3426, 0.3606, 0.3724, 0.3788, 0.3826, 0.3863, 0.3930, 0.3986, 0.4016, 0.3970, 0.3879,
0:         0.3752, 0.3633, 0.3503, 0.3392, 0.3300, 0.3237, 0.3208, 0.3238, 0.3331, 0.3454, 0.3631], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   â””â”€ Predictions for 'total_precip' shape: torch.Size([9864, 243])
1: [DEBUG] TARGET BATCH
1: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([11094, 243])
0:      first 25 pred values: tensor([-0.2248, -0.2214, -0.2205, -0.2182, -0.2159, -0.2081, -0.2035, -0.1970, -0.1912, -0.2234, -0.2260, -0.2240,
0:         -0.2243, -0.2198, -0.2144, -0.2105, -0.2048, -0.1976, -0.2249, -0.2254, -0.2261, -0.2254, -0.2234, -0.2206,
0:         -0.2176], device='cuda:0', grad_fn=<SliceBackward0>)
0:   â””â”€ Predictions for 't2m' shape: torch.Size([2560, 2187])
0:      first 25 pred values: tensor([-0.1028, -0.1113, -0.2420, -0.0709, -0.1679,  0.2053,  0.0613, -0.1525, -0.0079,  0.1718, -0.2241,  0.0776,
0:          0.1148, -0.0830,  0.1134, -0.0397, -0.0568, -0.0777, -0.0414,  0.1742,  0.1508,  0.0847, -0.1768, -0.0889,
0:          0.1656], device='cuda:0', grad_fn=<SliceBackward0>)
1: [DEBUG] First 243 batch values:
1: tensor([-5.4575e-03, -1.0167e-01, -1.9116e-01, -2.2472e-01, -2.2920e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01,
1:         -2.3367e-01,  4.9572e-01,  1.8920e-01,  9.7464e-02, -9.0479e-02, -1.9564e-01, -2.1577e-01, -2.2696e-01,
1:         -2.3367e-01, -2.3367e-01,  7.7987e-01,  4.9796e-01,  2.8988e-01,  8.1802e-02, -2.5594e-02, -1.5536e-01,
1:         -2.1801e-01, -2.2472e-01, -2.3367e-01,  5.5390e-01,  4.3979e-01,  2.3395e-01,  9.7464e-02,  1.4679e-02,
1:         -1.1509e-01, -1.9340e-01, -2.2025e-01, -2.3367e-01,  8.8514e-02,  3.0341e-02,  1.6917e-02, -5.0206e-02,
1:         -1.2404e-01, -1.5536e-01, -1.9340e-01, -2.2249e-01, -2.3144e-01, -1.8669e-01, -1.8669e-01, -1.9340e-01,
1:         -2.0235e-01, -2.0906e-01, -2.1801e-01, -2.2472e-01, -2.2696e-01, -2.3144e-01, -2.3367e-01, -2.3144e-01,
1:         -2.2920e-01, -2.2696e-01, -2.2920e-01, -2.3144e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01,
1:         -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3144e-01,
1:         -2.3144e-01, -2.3144e-01, -2.3144e-01, -2.2920e-01, -2.3144e-01, -2.3144e-01, -2.3144e-01, -2.2920e-01,
1:         -2.2920e-01,  2.9436e-01,  1.2879e-01, -6.1393e-02, -1.6655e-01, -1.9564e-01, -2.2920e-01, -2.2920e-01,
1:         -2.3367e-01, -2.3367e-01,  3.2030e+00,  4.3308e-01,  2.2500e-01,  1.4679e-02, -1.0614e-01, -1.5089e-01,
1:         -1.9116e-01, -2.2696e-01, -2.3367e-01,  4.1047e+00,  1.8337e+00,  1.0170e+00,  4.3084e-01,  3.0331e-01,
1:          6.3902e-02, -7.0342e-02, -1.0614e-01, -2.0459e-01,  2.3752e+00,  2.1022e+00,  1.2900e+00,  6.5010e-01,
1:          4.9796e-01,  2.5856e-01,  9.5226e-02, -4.3494e-02, -1.5760e-01,  6.1207e-01,  5.4047e-01,  5.2705e-01,
1:          3.9951e-01,  2.6527e-01,  1.8920e-01,  7.9564e-02, -5.9155e-02, -1.4641e-01, -1.2170e-02, -9.8266e-04,
1:         -3.2201e-03, -2.5594e-02, -5.0206e-02, -9.7191e-02, -1.4865e-01, -1.6431e-01, -1.9116e-01, -2.2249e-01,
1:         -2.1801e-01, -2.1577e-01, -2.1354e-01, -2.1354e-01, -2.1577e-01, -2.1801e-01, -2.2025e-01, -2.2249e-01,
1:         -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3144e-01, -2.3144e-01,
1:         -2.3144e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3367e-01, -2.3144e-01, -2.3144e-01,
1:         -2.3144e-01, -2.3367e-01,  1.3146e+00,  7.5303e-01,  2.6303e-01,  2.3629e-02, -5.0206e-02, -1.3970e-01,
1:         -1.9116e-01, -2.0906e-01, -2.2920e-01,  3.6684e+00,  1.0215e+00,  5.1810e-01,  2.7198e-01,  5.4953e-02,
1:         -5.9155e-02, -1.3299e-01, -1.9788e-01, -2.2696e-01,  3.4357e+00,  1.4175e+00,  1.6637e+00,  1.7755e+00,
1:          1.5182e+00,  2.7198e-01,  7.5089e-02,  1.2442e-02, -1.2404e-01,  1.9903e+00,  1.9411e+00,  2.1872e+00,
1:          2.4423e+00,  2.0418e+00,  8.0001e-01,  5.3152e-01,  2.6974e-01,  1.1089e-01,  1.5205e+00,  1.4354e+00,
1:          1.4578e+00,  1.3437e+00,  1.1267e+00,  9.9243e-01,  7.3736e-01,  4.7559e-01,  2.8093e-01,  4.9349e-01,
1:          5.4942e-01,  5.9193e-01,  5.9641e-01,  5.7403e-01,  4.9796e-01,  3.7043e-01,  3.0107e-01,  1.4221e-01,
1:         -1.3746e-01, -1.0614e-01, -8.8242e-02, -6.8105e-02, -5.6918e-02, -5.0206e-02, -6.1393e-02, -8.1530e-02,
1:         -1.0614e-01, -2.2696e-01, -2.2249e-01, -2.1801e-01, -2.1354e-01, -2.0906e-01, -2.0459e-01, -2.0235e-01,
1:         -2.0459e-01, -2.0459e-01, -2.3367e-01, -2.3144e-01, -2.3144e-01, -2.3144e-01, -2.3144e-01, -2.3144e-01,
1:         -2.2920e-01, -2.2920e-01, -2.2920e-01])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch -1, first predictions sample:
1:   â””â”€ Predictions for 'velocity_u' shape: torch.Size([25392, 972])
1:      first 25 pred values: tensor([1.2379, 1.2046, 1.1749, 1.1490, 1.1305, 1.1171, 1.1076, 1.1002, 1.0971, 1.0939, 1.0944, 1.0973, 1.1009, 1.1084,
1:         1.1145, 1.1217, 1.1261, 1.1344, 1.2975, 1.2617, 1.2307, 1.2047, 1.1842, 1.1680, 1.1577], device='cuda:0',
1:        grad_fn=<SliceBackward0>)
1:   â””â”€ Predictions for 'velocity_v' shape: torch.Size([26334, 972])
1:      first 25 pred values: tensor([-1.6747, -1.6762, -1.6749, -1.6674, -1.6639, -1.6629, -1.6588, -1.6523, -1.6452, -1.6309, -1.6174, -1.6025,
1:         -1.5877, -1.5651, -1.5415, -1.5193, -1.4994, -1.4815, -1.6245, -1.6292, -1.6241, -1.6157, -1.6103, -1.6063,
1:         -1.6064], device='cuda:0', grad_fn=<SliceBackward0>)
1:   â””â”€ Predictions for 'specific_humidity' shape: torch.Size([26983, 972])
1:      first 25 pred values: tensor([-0.2324, -0.2643, -0.2976, -0.3368, -0.3747, -0.4059, -0.4369, -0.4644, -0.4906, -0.5144, -0.5392, -0.5613,
1:         -0.5795, -0.5939, -0.6040, -0.6018, -0.5906, -0.5666, -0.2502, -0.2768, -0.3084, -0.3430, -0.3799, -0.4091,
1:         -0.4367], device='cuda:0', grad_fn=<SliceBackward0>)
1:   â””â”€ Predictions for 'velocity_z' shape: torch.Size([26512, 972])
1:      first 25 pred values: tensor([1.3043, 1.3455, 1.3160, 1.2528, 1.2180, 1.2284, 1.2573, 1.2933, 1.3412, 1.3411, 1.3000, 1.2789, 1.2682, 1.2470,
1:         1.2348, 1.2231, 1.2111, 1.1812, 1.5288, 1.6006, 1.5836, 1.5392, 1.5114, 1.5112, 1.5177], device='cuda:0',
1:        grad_fn=<SliceBackward0>)
1:   â””â”€ Predictions for 'temperature' shape: torch.Size([13013, 2187])
1:      first 25 pred values: tensor([0.2952, 0.3061, 0.3185, 0.3298, 0.3422, 0.3542, 0.3673, 0.3794, 0.3915, 0.4045, 0.4192, 0.4357, 0.4537, 0.4714,
1:         0.4899, 0.5090, 0.5290, 0.5482, 0.5663, 0.5817, 0.5933, 0.6029, 0.6100, 0.6130, 0.6130], device='cuda:0',
1:        grad_fn=<SliceBackward0>)
1:   â””â”€ Predictions for 'total_precip' shape: torch.Size([11094, 243])
1:      first 25 pred values: tensor([ 0.0425, -0.1038, -0.2086, -0.2659, -0.2533, -0.2077, -0.1688, -0.1692, -0.2002,  0.2707,  0.0965, -0.0576,
1:         -0.1354, -0.1745, -0.1785, -0.1663, -0.1817, -0.1911,  0.4610,  0.2468,  0.0831, -0.0242, -0.1111, -0.1457,
1:         -0.1603], device='cuda:0', grad_fn=<SliceBackward0>)
1:   â””â”€ Predictions for 't2m' shape: torch.Size([2534, 2187])
1:      first 25 pred values: tensor([-0.1374, -0.1729, -0.1926, -0.0086, -0.2854,  0.1017,  0.2467, -0.1760,  0.1668,  0.2944, -0.2326, -0.0311,
1:          0.0032, -0.0296, -0.1359, -0.0189,  0.0534,  0.1274, -0.0088,  0.0489,  0.0270,  0.0081, -0.1496, -0.1670,
1:          0.0029], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for t2m with 10.0% data retained
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [1/5 (20%)]	Loss: nan : nan :: 0.06701 (1.72 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [2/5 (40%)]	Loss: nan : nan :: 0.07009 (10.14 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [3/5 (60%)]	Loss: nan : nan :: 0.07178 (10.16 s/sec)
0: Created sparse mask for t2m with 10.0% data retained
0: epoch: -1 [4/5 (80%)]	Loss: nan : nan :: 0.06840 (10.17 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idqbyzbeit/AtmoRep_idqbyzbeit_epoch-1.mod
0: > /work/ab1412/atmorep/pyenv/lib/python3.10/site-packages/torch/serialization.py(690)__exit__()
0: -> self.file_like.write_end_of_file()
0: (Pdb) 
0: [1;34mwandb[0m:
0: [1;34mwandb[0m: You can sync this run to the cloud by running:
0: [1;34mwandb[0m: [1mwandb sync /work/ab1412/atmorep/wandb/offline-run-20250714_151950-qbyzbeit[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250714_151950-qbyzbeit/logs[0m
0: l50069:327628:328034 [0] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50069:327628:328627 [0] NCCL INFO comm 0x55555f292290 rank 0 nranks 2 cudaDev 0 busId 3000 - Abort COMPLETE
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
1: Created sparse mask for t2m with 10.0% data retained
