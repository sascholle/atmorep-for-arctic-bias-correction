0: Wandb run: atmorep-kyy926zn-17432727
0: l50021:664812:664812 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.120<0>
0: l50021:664812:664812 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50021:664812:664812 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50021:664812:664812 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50021:664812:664812 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
0: l50021:664812:665052 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.120<0>
0: l50021:664812:665052 [0] NCCL INFO Using non-device net plugin version 0
0: l50021:664812:665052 [0] NCCL INFO Using network IB
0: l50021:664812:665052 [0] NCCL INFO ncclCommInitRank comm 0x55555f261440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0xe1b26f166233acc5 - Init START
0: l50021:664812:665052 [0] NCCL INFO Setting affinity for GPU 0 to 0f0000,00000000,00000000,00000000,000f0000,00000000
0: l50021:664812:665052 [0] NCCL INFO comm 0x55555f261440 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50021:664812:665052 [0] NCCL INFO Channel 00/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 01/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 02/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 03/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 04/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 05/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 06/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 07/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 08/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 09/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 10/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 11/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 12/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 13/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 14/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 15/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 16/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 17/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 18/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 19/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 20/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 21/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 22/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 23/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 24/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 25/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 26/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 27/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 28/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 29/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 30/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Channel 31/32 :    0
0: l50021:664812:665052 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50021:664812:665052 [0] NCCL INFO P2P Chunksize set to 131072
0: l50021:664812:665052 [0] NCCL INFO Connected all rings
0: l50021:664812:665052 [0] NCCL INFO Connected all trees
0: l50021:664812:665052 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50021:664812:665052 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50021:664812:665052 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50021:664812:665052 [0] NCCL INFO ncclCommInitRank comm 0x55555f261440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0xe1b26f166233acc5 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 4
0: par_rank : 0
0: par_size : 1
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 1, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 2, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 3, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 3, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17432727
0: wandb_id : kyy926zn
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: years_test : [2021]
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 1
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 2
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded model id = wc5e2i3t at epoch = 0.
0: Loaded run 'wc5e2i3t' at epoch 0.
0: l50021:664812:665187 [1] NCCL INFO Using non-device net plugin version 0
0: l50021:664812:665187 [1] NCCL INFO Using network IB
0: l50021:664812:665187 [1] NCCL INFO ncclCommInitRank comm 0x55557eb47380 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0x714f34dc486d64a9 - Init START
0: l50021:664812:665187 [1] NCCL INFO Setting affinity for GPU 1 to 0f0000,00000000,00000000,00000000,000f0000
0: l50021:664812:665187 [1] NCCL INFO comm 0x55557eb47380 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50021:664812:665187 [1] NCCL INFO Channel 00/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 01/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 02/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 03/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 04/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 05/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 06/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 07/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 08/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 09/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 10/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 11/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 12/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 13/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 14/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 15/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 16/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 17/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 18/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 19/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 20/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 21/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 22/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 23/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 24/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 25/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 26/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 27/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 28/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 29/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 30/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Channel 31/32 :    0
0: l50021:664812:665187 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50021:664812:665187 [1] NCCL INFO P2P Chunksize set to 131072
0: l50021:664812:665187 [1] NCCL INFO Connected all rings
0: l50021:664812:665187 [1] NCCL INFO Connected all trees
0: l50021:664812:665187 [1] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50021:664812:665187 [1] NCCL INFO ncclCommInitRank comm 0x55557eb47380 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0x714f34dc486d64a9 - Init COMPLETE
0: l50021:664812:665192 [2] NCCL INFO Using non-device net plugin version 0
0: l50021:664812:665192 [2] NCCL INFO Using network IB
0: l50021:664812:665192 [2] NCCL INFO ncclCommInitRank comm 0x555580e19e50 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0x96860ccdd6c05d95 - Init START
0: l50021:664812:665192 [2] NCCL INFO Setting affinity for GPU 2 to 1f0000,00000000,00000000,00000000,001f0000,00000000,00000000,00000000
0: l50021:664812:665192 [2] NCCL INFO comm 0x555580e19e50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50021:664812:665192 [2] NCCL INFO Channel 00/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 01/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 02/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 03/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 04/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 05/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 06/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 07/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 08/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 09/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 10/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 11/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 12/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 13/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 14/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 15/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 16/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 17/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 18/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 19/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 20/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 21/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 22/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 23/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 24/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 25/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 26/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 27/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 28/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 29/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 30/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Channel 31/32 :    0
0: l50021:664812:665192 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50021:664812:665192 [2] NCCL INFO P2P Chunksize set to 131072
0: l50021:664812:665192 [2] NCCL INFO Connected all rings
0: l50021:664812:665192 [2] NCCL INFO Connected all trees
0: l50021:664812:665192 [2] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50021:664812:665192 [2] NCCL INFO ncclCommInitRank comm 0x555580e19e50 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0x96860ccdd6c05d95 - Init COMPLETE
0: l50021:664812:665197 [3] NCCL INFO Using non-device net plugin version 0
0: l50021:664812:665197 [3] NCCL INFO Using network IB
0: l50021:664812:665197 [3] NCCL INFO ncclCommInitRank comm 0x555589dade00 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0xf003121a4310e830 - Init START
0: l50021:664812:665197 [3] NCCL INFO Setting affinity for GPU 3 to 0f0000,00000000,00000000,00000000,000f0000,00000000,00000000
0: l50021:664812:665197 [3] NCCL INFO comm 0x555589dade00 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50021:664812:665197 [3] NCCL INFO Channel 00/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 01/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 02/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 03/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 04/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 05/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 06/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 07/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 08/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 09/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 10/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 11/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 12/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 13/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 14/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 15/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 16/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 17/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 18/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 19/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 20/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 21/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 22/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 23/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 24/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 25/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 26/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 27/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 28/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 29/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 30/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Channel 31/32 :    0
0: l50021:664812:665197 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50021:664812:665197 [3] NCCL INFO P2P Chunksize set to 131072
0: l50021:664812:665197 [3] NCCL INFO Connected all rings
0: l50021:664812:665197 [3] NCCL INFO Connected all trees
0: l50021:664812:665197 [3] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50021:664812:665197 [3] NCCL INFO ncclCommInitRank comm 0x555589dade00 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0xf003121a4310e830 - Init COMPLETE
0: Number of trainable parameters: 741,136,272
0: 1 : 08:55:29 :: batch_size = 96, lr = 1.5000000000000002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3558, -0.3408, -0.3337, -0.3343, -0.3388, -0.3434, -0.3482, -0.3555, -0.3641, -0.3698, -0.3718, -0.3731,
0:         -0.3779, -0.3908, -0.4151, -0.4496, -0.4872, -0.5206, -0.3325, -0.3279, -0.3300, -0.3376, -0.3469, -0.3553,
0:         -0.3647], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8139, -0.8064, -0.7952, -0.7790, -0.7580, -0.7353, -0.7110, -0.6829, -0.6495, -0.6120, -0.5741, -0.5391,
0:         -0.5101, -0.4885, -0.4720, -0.4571, -0.4417, -0.4263, -0.8064, -0.7981, -0.7874, -0.7763, -0.7654, -0.7549,
0:         -0.7428], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0738, 0.1690, 0.2646, 0.3556, 0.4239, 0.4583, 0.5009, 0.5528, 0.5984, 0.6410, 0.6168, 0.6153, 0.7137, 0.8573,
0:         1.0588, 1.2054, 1.3152, 1.3407, 0.0437, 0.1710, 0.2959, 0.3870, 0.4528, 0.4757, 0.4877], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.8023,  0.7670,  0.8067,  0.7494,  0.5708,  0.4639,  0.4408,  0.3041,  0.0782, -0.0342, -0.0949, -0.3032,
0:         -0.5059, -0.4310, -0.2470, -0.2359, -0.2161,  0.0153,  0.7317,  0.7549,  0.8254,  0.8023,  0.6469,  0.5201,
0:          0.4749], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.8311, 0.7921, 0.7423, 0.6839, 0.6229, 0.5650, 0.5138, 0.4729, 0.4462, 0.4356, 0.4370, 0.4382, 0.4284, 0.4046,
0:         0.3717, 0.3365, 0.3049, 0.2796, 0.2620, 0.2523, 0.2528, 0.2642, 0.2843, 0.3079, 0.3275], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2470, -0.2470, -0.2470, -0.2470, -0.2470, -0.2470, -0.2470, -0.2470, -0.2459, -0.2470, -0.2470, -0.2470,
0:         -0.2437, -0.2425, -0.2380, -0.2403, -0.2470, -0.2470, -0.2470, -0.2470, -0.2470, -0.2459, -0.2459, -0.2459,
0:         -0.2448], device='cuda:0')
0: [DEBUG] Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2155,     nan,     nan,     nan, -0.2470,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2437, -0.2346,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2391,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1140,     nan, -0.1478,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1794,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2290,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2132,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2470,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1591,     nan,     nan,     nan, -0.1636,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1703,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2470,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2470,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2470])
0: [DEBUG] Epoch 1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1976, -0.2027, -0.2071, -0.2088, -0.2084, -0.2126, -0.2184, -0.2311, -0.2447, -0.2580, -0.2739, -0.2922,
0:         -0.3132, -0.3328, -0.3531, -0.3661, -0.3749, -0.3781, -0.1808, -0.1880, -0.1931, -0.1951, -0.1989, -0.2057,
0:         -0.2167], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0763, -1.0878, -1.0941, -1.0967, -1.1006, -1.1058, -1.1135, -1.1268, -1.1429, -1.1574, -1.1752, -1.1923,
0:         -1.2107, -1.2290, -1.2504, -1.2742, -1.3013, -1.3273, -1.0703, -1.0843, -1.0936, -1.1059, -1.1168, -1.1303,
0:         -1.1444], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0569, 0.0942, 0.1264, 0.1530, 0.1638, 0.1737, 0.1839, 0.2037, 0.2191, 0.2500, 0.2834, 0.3192, 0.3387, 0.3547,
0:         0.3548, 0.3445, 0.3329, 0.3191, 0.0020, 0.0354, 0.0736, 0.1051, 0.1219, 0.1291, 0.1441], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2534, 0.2750, 0.2651, 0.2622, 0.3053, 0.3651, 0.3731, 0.3298, 0.3087, 0.2684, 0.2012, 0.1937, 0.1732, 0.1033,
0:         0.0863, 0.0978, 0.1080, 0.1314, 0.2866, 0.3027, 0.3010, 0.3251, 0.3690, 0.3972, 0.3819], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.6594,  0.6311,  0.6058,  0.5727,  0.5329,  0.4863,  0.4415,  0.3992,  0.3625,  0.3305,  0.3039,  0.2841,
0:          0.2749,  0.2723,  0.2694,  0.2658,  0.2563,  0.2392,  0.2095,  0.1599,  0.0882, -0.0030, -0.1020, -0.1982,
0:         -0.2886], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2328, -0.2338, -0.2299, -0.2288, -0.2242, -0.2290, -0.2233, -0.2199, -0.2212, -0.2385, -0.2358, -0.2348,
0:         -0.2327, -0.2289, -0.2308, -0.2303, -0.2294, -0.2276, -0.2393, -0.2380, -0.2344, -0.2332, -0.2317, -0.2308,
0:         -0.2332], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.05873318761587143; velocity_v: 0.09693939238786697; specific_humidity: 0.03372339531779289; velocity_z: 0.5363221168518066; temperature: 0.08277527987957001; total_precip: 0.5883583426475525; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07031149417161942; velocity_v: 0.11134923994541168; specific_humidity: 0.03943576291203499; velocity_z: 0.5675840973854065; temperature: 0.13099372386932373; total_precip: 0.5458003282546997; 
0: epoch: 1 [1/5 (20%)]	Loss: 0.56708 : 0.21170 :: 0.16054 (1.64 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.055541377514600754; velocity_v: 0.09502124786376953; specific_humidity: 0.0350777767598629; velocity_z: 0.5128427147865295; temperature: 0.09359229356050491; total_precip: 0.8562798500061035; 
0: epoch: 1 [2/5 (40%)]	Loss: 0.85628 : 0.25062 :: 0.13573 (16.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06082611903548241; velocity_v: 0.10998920351266861; specific_humidity: 0.03669530898332596; velocity_z: 0.45335653424263; temperature: 0.10421906411647797; total_precip: 0.44687598943710327; 
0: epoch: 1 [3/5 (60%)]	Loss: 0.44688 : 0.17580 :: 0.14959 (16.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06672155112028122; velocity_v: 0.1121080219745636; specific_humidity: 0.04350293055176735; velocity_z: 0.48235633969306946; temperature: 0.10777503997087479; total_precip: 0.7227082848548889; 
0: epoch: 1 [4/5 (80%)]	Loss: 0.72271 : 0.22905 :: 0.15232 (16.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 1 : 0.24733798205852509
0: validation loss for velocity_u : 0.032034244388341904
0: validation loss for velocity_v : 0.054596468806266785
0: validation loss for specific_humidity : 0.023477813228964806
0: validation loss for velocity_z : 0.4277362525463104
0: validation loss for temperature : 0.0780080035328865
0: validation loss for total_precip : 0.8681750893592834
0: 2 : 08:59:57 :: batch_size = 96, lr = 2e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 2, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0134,  0.0216,  0.0576,  0.0848,  0.1061,  0.1253,  0.1488,  0.1795,  0.2138,  0.2502,  0.2913,  0.3343,
0:          0.3710,  0.3987,  0.4114,  0.4119,  0.3979,  0.3677, -0.0124,  0.0313,  0.0741,  0.1066,  0.1287,  0.1480,
0:          0.1738], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1277, 0.1052, 0.0803, 0.0623, 0.0582, 0.0663, 0.0811, 0.1034, 0.1266, 0.1419, 0.1458, 0.1426, 0.1369, 0.1349,
0:         0.1393, 0.1476, 0.1605, 0.1723, 0.1747, 0.1413, 0.1061, 0.0800, 0.0656, 0.0623, 0.0695], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0787, 1.1003, 1.1001, 1.0871, 1.0671, 1.0594, 1.0666, 1.0970, 1.1359, 1.1375, 1.1428, 1.1024, 1.0553, 1.0308,
0:         1.0282, 1.0765, 1.1549, 1.3040, 1.2004, 1.2221, 1.2736, 1.2631, 1.2686, 1.2706, 1.2822], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4276,  0.4208,  0.2269,  0.0674, -0.1895, -0.3547, -0.3972, -0.4775, -0.4212, -0.3180, -0.2388, -0.2434,
0:         -0.2125, -0.2388, -0.1666, -0.0358,  0.0824,  0.3462,  0.6307,  0.7775,  0.6708,  0.5492,  0.4127,  0.2418,
0:         -0.0048], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.1042,  0.1353,  0.1699,  0.1734,  0.1623,  0.1398,  0.0692, -0.0787, -0.2786, -0.4047, -0.3938, -0.2841,
0:         -0.1051,  0.0829,  0.2609,  0.3866,  0.4205,  0.3871,  0.2595,  0.0302, -0.2700, -0.5427, -0.7026, -0.7052,
0:         -0.5955], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2363, -0.2257, -0.2269, -0.2281, -0.2292, -0.2351, -0.2328, -0.2328, -0.2316, -0.2363, -0.2257, -0.2092,
0:         -0.2080, -0.1927, -0.1997, -0.1997, -0.2080, -0.2056, -0.2399, -0.2363, -0.2233, -0.1974, -0.1655, -0.1478,
0:         -0.1702], device='cuda:0')
0: [DEBUG] Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([        nan,         nan,  1.0522e+00,         nan,         nan,         nan,         nan,  7.9923e-02,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  6.6165e-01,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:          2.2051e+00,         nan,         nan,         nan,         nan,         nan,         nan,  1.3236e+00,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,  1.8959e+00,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,  4.5516e-01,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,  1.7708e+00,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,  1.5490e+00,         nan,
0:                 nan,         nan,         nan,         nan, -6.0495e-02,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,  2.8222e+00,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan, -8.7635e-02,
0:                 nan,  2.3214e-01,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,  1.3378e+00,  5.7434e-01,
0:                 nan,         nan,         nan,  4.5398e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan, -2.6761e-03,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  2.0924e-02,
0:                 nan,         nan,         nan,         nan,         nan,         nan,  1.9148e+00,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,  1.0310e+00,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,  2.1024e+00,         nan,  5.6372e-01,         nan,         nan,
0:         -1.7377e-01,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan])
0: [DEBUG] Epoch 2, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2853, -0.2911, -0.2990, -0.3043, -0.3082, -0.3108, -0.3138, -0.3175, -0.3200, -0.3219, -0.3242, -0.3258,
0:         -0.3248, -0.3227, -0.3189, -0.3153, -0.3122, -0.3106, -0.2897, -0.2943, -0.3005, -0.3037, -0.3092, -0.3128,
0:         -0.3155], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1023,  0.0907,  0.0729,  0.0526,  0.0335,  0.0233,  0.0215,  0.0249,  0.0291,  0.0316,  0.0278,  0.0210,
0:          0.0134,  0.0048, -0.0062, -0.0203, -0.0381, -0.0578,  0.0853,  0.0769,  0.0633,  0.0482,  0.0356,  0.0292,
0:          0.0297], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4459, 0.4274, 0.4153, 0.4147, 0.4224, 0.4693, 0.5406, 0.6603, 0.8089, 0.9778, 1.1638, 1.3748, 1.5980, 1.8318,
0:         2.0608, 2.2645, 2.4111, 2.4960, 0.6664, 0.6671, 0.6785, 0.6811, 0.7074, 0.7498, 0.8171], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3298, -0.2107, -0.1211, -0.0519,  0.1181,  0.2624,  0.3276,  0.3683,  0.3904,  0.4290,  0.4065,  0.3454,
0:          0.3435,  0.2769,  0.1649,  0.1018,  0.1337,  0.2541, -0.3414, -0.2041, -0.1298, -0.0801,  0.0916,  0.2003,
0:          0.2334], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5005, -0.4995, -0.4540, -0.3889, -0.3312, -0.3068, -0.3281, -0.3871, -0.4664, -0.5442, -0.6155, -0.6748,
0:         -0.7302, -0.7855, -0.8371, -0.8754, -0.8921, -0.8836, -0.8621, -0.8478, -0.8589, -0.9005, -0.9703, -1.0566,
0:         -1.1514], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.2488, 0.2743, 0.3704, 0.4955, 0.5311, 0.5108, 0.3940, 0.2771, 0.1795, 0.5034, 0.6446, 0.8399, 0.9805, 1.0554,
0:         1.0059, 0.8600, 0.6593, 0.4805, 0.8600, 1.1083, 1.4170, 1.6080, 1.7054, 1.6952, 1.4639], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.05350678041577339; velocity_v: 0.09541299194097519; specific_humidity: 0.036817245185375214; velocity_z: 0.506853461265564; temperature: 0.0937948003411293; total_precip: 0.697558581829071; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.052894629538059235; velocity_v: 0.09552647173404694; specific_humidity: 0.042290303856134415; velocity_z: 0.48579925298690796; temperature: 0.11420101672410965; total_precip: 0.9339253306388855; 
0: epoch: 2 [1/5 (20%)]	Loss: 0.81574 : 0.24223 :: 0.14087 (1.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.0631064847111702; velocity_v: 0.0903969258069992; specific_humidity: 0.04576505348086357; velocity_z: 0.5830901861190796; temperature: 0.11946709454059601; total_precip: 47.20023727416992; 
0: epoch: 2 [2/5 (40%)]	Loss: 47.20024 : 7.99075 :: 0.14592 (15.79 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05598554015159607; velocity_v: 0.08357565104961395; specific_humidity: 0.05014424026012421; velocity_z: 0.505064070224762; temperature: 0.10568325966596603; total_precip: 48.17169952392578; 
0: epoch: 2 [3/5 (60%)]	Loss: 48.17170 : 8.13575 :: 0.14583 (16.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07605037838220596; velocity_v: 0.1138029545545578; specific_humidity: 0.043212395161390305; velocity_z: 0.5241451263427734; temperature: 0.12003498524427414; total_precip: 40.52083969116211; 
0: epoch: 2 [4/5 (80%)]	Loss: 40.52084 : 6.87158 :: 0.16807 (16.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 2 : 7.589948654174805
0: validation loss for velocity_u : 0.03645646944642067
0: validation loss for velocity_v : 0.058778878301382065
0: validation loss for specific_humidity : 0.028042852878570557
0: validation loss for velocity_z : 0.4572448432445526
0: validation loss for temperature : 0.06643720716238022
0: validation loss for total_precip : 44.89274215698242
0: 3 : 09:04:08 :: batch_size = 96, lr = 1.9512195121951222e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 3, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5589, 0.5661, 0.5670, 0.5616, 0.5505, 0.5339, 0.5146, 0.4930, 0.4709, 0.4530, 0.4391, 0.4288, 0.4187, 0.4066,
0:         0.3939, 0.3819, 0.3734, 0.3681, 0.5587, 0.5658, 0.5666, 0.5610, 0.5502, 0.5342, 0.5141], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1480, -0.1593, -0.1678, -0.1714, -0.1682, -0.1601, -0.1462, -0.1282, -0.1090, -0.0912, -0.0775, -0.0674,
0:         -0.0621, -0.0613, -0.0645, -0.0720, -0.0811, -0.0898, -0.1762, -0.1906, -0.1969, -0.1950, -0.1864, -0.1734,
0:         -0.1558], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4906, -0.5044, -0.5219, -0.5371, -0.5526, -0.5660, -0.5771, -0.5852, -0.5901, -0.5931, -0.5949, -0.5950,
0:         -0.5950, -0.5943, -0.5931, -0.5917, -0.5899, -0.5886, -0.5054, -0.5178, -0.5277, -0.5429, -0.5542, -0.5671,
0:         -0.5780], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4712,  0.5533,  0.5828,  0.5970,  0.5609,  0.4449,  0.3037,  0.1286,  0.0279,  0.0312,  0.0761,  0.0903,
0:          0.0684,  0.0115, -0.0213,  0.0017,  0.0378,  0.1264,  0.5139,  0.6529,  0.6638,  0.6715,  0.6200,  0.4252,
0:          0.2096], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.6355, 0.6180, 0.5948, 0.5694, 0.5402, 0.5045, 0.4603, 0.4090, 0.3564, 0.3065, 0.2677, 0.2375, 0.2161, 0.1955,
0:         0.1722, 0.1545, 0.1431, 0.1431, 0.1504, 0.1604, 0.1848, 0.2193, 0.2566, 0.2907, 0.3278], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1748, -0.1916, -0.2444, -0.2492, -0.2504, -0.2504, -0.2504, -0.2492, -0.2240, -0.2288, -0.2444, -0.2456,
0:         -0.2504, -0.2504, -0.2504, -0.2504, -0.2504, -0.2204, -0.2480, -0.2492, -0.2504, -0.2504, -0.2504, -0.2504,
0:         -0.2504], device='cuda:0')
0: [DEBUG] Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.1952,     nan,     nan,     nan,     nan,     nan, -0.1844,     nan,     nan,
0:         -0.1748,     nan,     nan,     nan, -0.2372,     nan,     nan,     nan,     nan,     nan,     nan, -0.0619,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2492,     nan,
0:             nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,  0.2022,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2504,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2348, -0.2288,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1591,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1291,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2504,     nan,     nan,     nan, -0.2504,
0:             nan, -0.2288,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2504,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2504, -0.2504,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2504,     nan,     nan,     nan,     nan, -0.2432,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 3, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8729, 0.8710, 0.8796, 0.8868, 0.9013, 0.9175, 0.9336, 0.9439, 0.9504, 0.9481, 0.9398, 0.9240, 0.9060, 0.8893,
0:         0.8723, 0.8583, 0.8469, 0.8321, 0.8809, 0.8869, 0.8914, 0.8976, 0.9075, 0.9186, 0.9307], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4321, 0.4171, 0.4032, 0.3871, 0.3672, 0.3528, 0.3374, 0.3221, 0.3091, 0.2953, 0.2830, 0.2684, 0.2570, 0.2439,
0:         0.2319, 0.2217, 0.2142, 0.2160, 0.4788, 0.4774, 0.4692, 0.4567, 0.4376, 0.4167, 0.3957], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4978, -0.4770, -0.3975, -0.2743, -0.1627, -0.0567,  0.0177,  0.0876,  0.1365,  0.1666,  0.1771,  0.1913,
0:          0.2065,  0.2474,  0.3217,  0.4184,  0.5294,  0.6290,  0.0136,  0.0838,  0.2206,  0.3596,  0.4896,  0.5820,
0:          0.6244], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1190,  0.0592,  0.0972, -0.0908, -0.1915, -0.2471, -0.2871, -0.1966, -0.0522,  0.0179,  0.0385,  0.0311,
0:         -0.0233, -0.0446, -0.0406, -0.0397,  0.0452,  0.1091, -0.2267,  0.0073,  0.1263, -0.0062, -0.0662, -0.1050,
0:         -0.1376], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.2467,  0.2177,  0.1708,  0.1050,  0.0300, -0.0446, -0.1162, -0.1763, -0.2271, -0.2711, -0.3100, -0.3465,
0:         -0.3812, -0.4164, -0.4493, -0.4800, -0.5087, -0.5327, -0.5521, -0.5693, -0.5852, -0.6011, -0.6123, -0.6110,
0:         -0.5997], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([3.7710, 3.2499, 2.6014, 2.1023, 1.6402, 1.1705, 0.9526, 0.7569, 0.5897, 4.8482, 4.2855, 3.5377, 2.7475, 2.0053,
0:         1.2997, 0.9583, 0.6705, 0.4659, 5.8713, 5.2684, 4.4240, 3.3816, 2.4387, 1.5443, 0.8694], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.058222975581884384; velocity_v: 0.09478257596492767; specific_humidity: 0.03419079631567001; velocity_z: 0.4460175633430481; temperature: 0.08386975526809692; total_precip: 40.10078430175781; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.0830908864736557; velocity_v: 0.12259382009506226; specific_humidity: 0.05090766400098801; velocity_z: 0.6914580464363098; temperature: 0.12233009934425354; total_precip: 47.486026763916016; 
0: epoch: 3 [1/5 (20%)]	Loss: 43.79340 : 7.42130 :: 0.16855 (1.94 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07791084796190262; velocity_v: 0.12104501575231552; specific_humidity: 0.05448675900697708; velocity_z: 0.6940168142318726; temperature: 0.1514807790517807; total_precip: 46.20732879638672; 
0: epoch: 3 [2/5 (40%)]	Loss: 46.20733 : 7.85565 :: 0.16299 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.0634828582406044; velocity_v: 0.10176214575767517; specific_humidity: 0.042752642184495926; velocity_z: 0.5067088007926941; temperature: 0.1177438274025917; total_precip: 44.09611129760742; 
0: epoch: 3 [3/5 (60%)]	Loss: 44.09611 : 7.46220 :: 0.14857 (16.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06051655858755112; velocity_v: 0.09847041964530945; specific_humidity: 0.03327108547091484; velocity_z: 0.586762011051178; temperature: 0.0820014476776123; total_precip: 40.1461296081543; 
0: epoch: 3 [4/5 (80%)]	Loss: 40.14613 : 6.80961 :: 0.14992 (15.43 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 3 : 0.41997435688972473
0: validation loss for velocity_u : 0.04535537585616112
0: validation loss for velocity_v : 0.06652934104204178
0: validation loss for specific_humidity : 0.028909510001540184
0: validation loss for velocity_z : 0.47962984442710876
0: validation loss for temperature : 0.08836891502141953
0: validation loss for total_precip : 1.8110531568527222
0: 4 : 09:08:16 :: batch_size = 96, lr = 1.903628792385485e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 4, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7344, -0.7544, -0.7761, -0.8014, -0.8246, -0.8456, -0.8635, -0.8783, -0.8923, -0.9034, -0.9134, -0.9214,
0:         -0.9257, -0.9277, -0.9274, -0.9242, -0.9196, -0.9122, -0.7026, -0.7236, -0.7488, -0.7782, -0.8062, -0.8311,
0:         -0.8519], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0146, -0.0205, -0.0358, -0.0515, -0.0689, -0.0872, -0.1024, -0.1190, -0.1313, -0.1393, -0.1442, -0.1431,
0:         -0.1355, -0.1215, -0.0963, -0.0640, -0.0267,  0.0121, -0.0256, -0.0280, -0.0358, -0.0468, -0.0587, -0.0732,
0:         -0.0874], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1189, 1.1412, 1.1571, 1.1754, 1.2044, 1.2186, 1.2258, 1.2317, 1.2368, 1.2425, 1.2488, 1.2498, 1.2511, 1.2511,
0:         1.2615, 1.2668, 1.2809, 1.2722, 1.0897, 1.1223, 1.1384, 1.1619, 1.1878, 1.2093, 1.2138], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2704, 0.3655, 0.4628, 0.5324, 0.5722, 0.5369, 0.5125, 0.5004, 0.4827, 0.5026, 0.4783, 0.4849, 0.4230, 0.4020,
0:         0.3832, 0.3434, 0.3478, 0.2660, 0.2958, 0.3898, 0.5048, 0.5347, 0.5955, 0.5822, 0.5756], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4411, -0.5024, -0.5780, -0.6584, -0.7170, -0.7488, -0.7495, -0.7289, -0.6916, -0.6466, -0.6010, -0.5615,
0:         -0.5387, -0.5334, -0.5575, -0.6053, -0.6575, -0.7057, -0.7430, -0.7657, -0.7811, -0.7749, -0.7596, -0.7342,
0:         -0.6976], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1848, -0.1438, -0.1560, -0.1416, -0.1382, -0.1538, -0.1449, -0.1515, -0.1460, -0.1582, -0.1094, -0.0728,
0:         -0.0728, -0.0927, -0.1083, -0.1438, -0.1560, -0.1327, -0.0572, -0.1171, -0.1615,  0.0193, -0.1027, -0.1405,
0:         -0.1382], device='cuda:0')
0: [DEBUG] Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.1837, -0.1449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1737,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1171,     nan,     nan,     nan, -0.1782,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1693,     nan,     nan,     nan, -0.2148,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2070,     nan,     nan,     nan,     nan, -0.1959,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1371,     nan,     nan,     nan,     nan, -0.1382,     nan, -0.1072,     nan,     nan,
0:         -0.1205,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2215,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2237,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2192,     nan,     nan,     nan,     nan,     nan, -0.1427,
0:         -0.1749,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1183,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 4, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8896, -0.8685, -0.8296, -0.8030, -0.7739, -0.7417, -0.7117, -0.6732, -0.6428, -0.6142, -0.5897, -0.5772,
0:         -0.5632, -0.5491, -0.5340, -0.5091, -0.4787, -0.4514, -0.8443, -0.8246, -0.7985, -0.7782, -0.7582, -0.7316,
0:         -0.6970], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3732, -0.3571, -0.3391, -0.3229, -0.3092, -0.2903, -0.2701, -0.2457, -0.2177, -0.1945, -0.1734, -0.1568,
0:         -0.1401, -0.1275, -0.1120, -0.0880, -0.0533, -0.0123, -0.3687, -0.3478, -0.3291, -0.3081, -0.2920, -0.2738,
0:         -0.2510], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0632,  0.0587,  0.0353,  0.0124, -0.0178, -0.0461, -0.0771, -0.1042, -0.1316, -0.1573, -0.1805, -0.2051,
0:         -0.2263, -0.2473, -0.2651, -0.2795, -0.2931, -0.2985,  0.0339,  0.0254,  0.0040, -0.0194, -0.0491, -0.0797,
0:         -0.1046], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1245, -0.1458, -0.1469, -0.1505, -0.1194, -0.0846, -0.0644, -0.0623, -0.0500, -0.0221, -0.0007,  0.0507,
0:          0.0952,  0.1085,  0.1242,  0.1390,  0.1758,  0.2161, -0.1167, -0.1360, -0.1037, -0.0792, -0.0421, -0.0268,
0:         -0.0281], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.2011, 1.1491, 1.1031, 1.0679, 1.0555, 1.0712, 1.1114, 1.1682, 1.2296, 1.2898, 1.3437, 1.3965, 1.4506, 1.5068,
0:         1.5670, 1.6263, 1.6800, 1.7207, 1.7444, 1.7474, 1.7339, 1.7096, 1.6844, 1.6652, 1.6563], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1175, -0.1323, -0.1332, -0.1442, -0.1467, -0.1548, -0.1467, -0.1355, -0.1287, -0.1134, -0.1278, -0.1367,
0:         -0.1514, -0.1562, -0.1585, -0.1422, -0.1374, -0.1259, -0.1233, -0.1295, -0.1377, -0.1420, -0.1555, -0.1506,
0:         -0.1464], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.059991706162691116; velocity_v: 0.092972531914711; specific_humidity: 0.043033864349126816; velocity_z: 0.5747683048248291; temperature: 0.10377293080091476; total_precip: 1.4921711683273315; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07595028728246689; velocity_v: 0.09689538925886154; specific_humidity: 0.05005645751953125; velocity_z: 0.5452913641929626; temperature: 0.10644574463367462; total_precip: 0.7942417860031128; 
0: epoch: 4 [1/5 (20%)]	Loss: 1.14321 : 0.30925 :: 0.15851 (2.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08888744562864304; velocity_v: 0.11529406905174255; specific_humidity: 0.04717595875263214; velocity_z: 0.5599613785743713; temperature: 0.11214128881692886; total_precip: 0.8157738447189331; 
0: epoch: 4 [2/5 (40%)]	Loss: 0.81577 : 0.26336 :: 0.16110 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11058396846055984; velocity_v: 0.1306145340204239; specific_humidity: 0.057697657495737076; velocity_z: 0.6552489995956421; temperature: 0.11230900138616562; total_precip: 0.9067668318748474; 
0: epoch: 4 [3/5 (60%)]	Loss: 0.90677 : 0.30066 :: 0.17539 (16.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13415396213531494; velocity_v: 0.17376011610031128; specific_humidity: 0.08434740453958511; velocity_z: 0.5745593905448914; temperature: 0.16637979447841644; total_precip: 1.1609660387039185; 
0: epoch: 4 [4/5 (80%)]	Loss: 1.16097 : 0.35288 :: 0.17402 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 4 : 0.2765984833240509
0: validation loss for velocity_u : 0.08577416092157364
0: validation loss for velocity_v : 0.12611792981624603
0: validation loss for specific_humidity : 0.05796288326382637
0: validation loss for velocity_z : 0.43956854939460754
0: validation loss for temperature : 0.08474213629961014
0: validation loss for total_precip : 0.865425169467926
0: 5 : 09:12:27 :: batch_size = 96, lr = 1.857198821839498e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 5, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3614, 1.3828, 1.4048, 1.4278, 1.4517, 1.4771, 1.5043, 1.5325, 1.5611, 1.5904, 1.6197, 1.6489, 1.6786, 1.7091,
0:         1.7402, 1.7720, 1.8041, 1.8366, 1.4070, 1.4278, 1.4497, 1.4729, 1.4972, 1.5228, 1.5494], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1859, 1.2136, 1.2403, 1.2645, 1.2861, 1.3052, 1.3214, 1.3348, 1.3459, 1.3543, 1.3596, 1.3625, 1.3635, 1.3625,
0:         1.3590, 1.3543, 1.3490, 1.3434, 1.2555, 1.2853, 1.3134, 1.3387, 1.3609, 1.3802, 1.3960], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3713, -0.3662, -0.3560, -0.3491, -0.3419, -0.3358, -0.3316, -0.3311, -0.3348, -0.3380, -0.3460, -0.3594,
0:         -0.3694, -0.3794, -0.3945, -0.4052, -0.4132, -0.4266, -0.3485, -0.3364, -0.3257, -0.3141, -0.3051, -0.2989,
0:         -0.2958], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4095, 0.3613, 0.3349, 0.3130, 0.2779, 0.2428, 0.2165, 0.2121, 0.2516, 0.3108, 0.3569, 0.3898, 0.4095, 0.4029,
0:         0.3898, 0.3810, 0.3569, 0.3196, 0.3262, 0.2867, 0.2582, 0.2362, 0.2077, 0.1880, 0.1617], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.5450,  0.5332,  0.5197,  0.5047,  0.4883,  0.4702,  0.4513,  0.4317,  0.4118,  0.3912,  0.3699,  0.3492,
0:          0.3287,  0.3072,  0.2849,  0.2612,  0.2363,  0.2092,  0.1809,  0.1515,  0.1207,  0.0891,  0.0563,  0.0227,
0:         -0.0120], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1101, -0.1078, -0.1393, -0.1907, -0.1896, -0.1942, -0.1837, -0.1814, -0.1791, -0.1195, -0.1452, -0.1779,
0:         -0.1989, -0.2071, -0.2129, -0.2129, -0.2106, -0.2059, -0.2433, -0.2421, -0.2410, -0.2398, -0.2374, -0.2363,
0:         -0.2351], device='cuda:0')
0: [DEBUG] Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2421,     nan,     nan, -0.2398,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2374,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2293,     nan,     nan, -0.2421,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1896,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2410,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2351,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2398,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2293,     nan,
0:             nan, -0.1101,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1942,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2374,
0:             nan,     nan,     nan,     nan, -0.2339,     nan, -0.2374,     nan, -0.2223,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1872,     nan,     nan,     nan,     nan,     nan,     nan, -0.1358,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0331,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 5, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4795, 1.4626, 1.4559, 1.4502, 1.4377, 1.3947, 1.3645, 1.3351, 1.3190, 1.3400, 1.3594, 1.4003, 1.4283, 1.4446,
0:         1.4562, 1.4686, 1.4860, 1.4832, 1.5259, 1.4753, 1.4410, 1.4133, 1.3806, 1.3441, 1.3083], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1749, 1.1891, 1.1960, 1.1995, 1.2018, 1.2144, 1.2162, 1.2052, 1.1930, 1.1755, 1.1637, 1.1698, 1.1746, 1.1673,
0:         1.1539, 1.1555, 1.1766, 1.1964, 1.1490, 1.1547, 1.1660, 1.1628, 1.1629, 1.1638, 1.1634], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5743, 0.5885, 0.5878, 0.5844, 0.5764, 0.5643, 0.5447, 0.5267, 0.5000, 0.4757, 0.4590, 0.4472, 0.4290, 0.4136,
0:         0.3903, 0.3699, 0.3517, 0.3516, 0.5102, 0.5185, 0.5149, 0.5184, 0.5083, 0.4940, 0.4810], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0509,  0.0308,  0.0855,  0.0626,  0.0022, -0.0082, -0.0118, -0.0319,  0.0141,  0.0750,  0.1099,  0.1924,
0:          0.2672,  0.2671,  0.2316,  0.2579,  0.3099,  0.2514, -0.0603, -0.0455, -0.0172, -0.0127, -0.0417, -0.0541,
0:         -0.0481], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.5436, 0.5345, 0.5229, 0.5096, 0.4988, 0.4930, 0.4914, 0.4932, 0.4967, 0.4978, 0.4914, 0.4823, 0.4749, 0.4728,
0:         0.4790, 0.4905, 0.4985, 0.5013, 0.4988, 0.4920, 0.4854, 0.4768, 0.4649, 0.4469, 0.4189], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2026, -0.2060, -0.2092, -0.2201, -0.2236, -0.2289, -0.2275, -0.2240, -0.2173, -0.1962, -0.2035, -0.2127,
0:         -0.2195, -0.2270, -0.2269, -0.2243, -0.2274, -0.2108, -0.2017, -0.2101, -0.2096, -0.2188, -0.2266, -0.2280,
0:         -0.2212], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.12425579875707626; velocity_v: 0.17898930609226227; specific_humidity: 0.07225213944911957; velocity_z: 0.6058835387229919; temperature: 0.10677214711904526; total_precip: 0.7764065861701965; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14377547800540924; velocity_v: 0.18256212770938873; specific_humidity: 0.08625591546297073; velocity_z: 0.6022725105285645; temperature: 0.12725929915905; total_precip: 1.1003801822662354; 
0: epoch: 5 [1/5 (20%)]	Loss: 0.93839 : 0.31484 :: 0.18119 (1.80 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13765154778957367; velocity_v: 0.24756839871406555; specific_humidity: 0.08439154177904129; velocity_z: 0.5755285620689392; temperature: 0.11729025095701218; total_precip: 1.0913505554199219; 
0: epoch: 5 [2/5 (40%)]	Loss: 1.09135 : 0.34843 :: 0.18066 (15.88 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15177834033966064; velocity_v: 0.2653552293777466; specific_humidity: 0.11025316268205643; velocity_z: 0.5797789096832275; temperature: 0.1231890395283699; total_precip: 1.189386248588562; 
0: epoch: 5 [3/5 (60%)]	Loss: 1.18939 : 0.37491 :: 0.18595 (15.91 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16215310990810394; velocity_v: 0.21222122013568878; specific_humidity: 0.10979387909173965; velocity_z: 0.5876668691635132; temperature: 0.13121840357780457; total_precip: 1.015615463256836; 
0: epoch: 5 [4/5 (80%)]	Loss: 1.01562 : 0.34155 :: 0.18678 (15.85 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 5 : 0.3109146058559418
0: validation loss for velocity_u : 0.12989336252212524
0: validation loss for velocity_v : 0.2211524099111557
0: validation loss for specific_humidity : 0.09864424914121628
0: validation loss for velocity_z : 0.5797901153564453
0: validation loss for temperature : 0.09943649917840958
0: validation loss for total_precip : 0.7365712523460388
0: 6 : 09:16:50 :: batch_size = 96, lr = 1.8119012895995104e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 6, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1077,  0.1274,  0.1456,  0.1637,  0.1773,  0.1847,  0.1800,  0.1720,  0.1634,  0.1271,  0.0752,  0.0337,
0:         -0.0146, -0.0548, -0.0516, -0.0287, -0.0353, -0.0562,  0.0270,  0.0479,  0.0587,  0.0656,  0.0714,  0.0785,
0:          0.0796], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0119, 0.0326, 0.0445, 0.0506, 0.0513, 0.0528, 0.0570, 0.0732, 0.1013, 0.1313, 0.1702, 0.2168, 0.2631, 0.3123,
0:         0.3808, 0.4761, 0.5556, 0.6003, 0.0036, 0.0341, 0.0545, 0.0713, 0.0847, 0.0936, 0.0949], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([3.2733, 3.2174, 3.1631, 3.1188, 3.0761, 3.0633, 3.0730, 3.0728, 3.0616, 3.0418, 3.0216, 2.9580, 2.8743, 2.7814,
0:         2.6410, 2.4702, 2.3528, 2.2809, 3.4156, 3.3912, 3.3379, 3.3032, 3.2642, 3.2388, 3.1900], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3102, -0.3124, -0.2415, -0.0685,  0.1022,  0.2286,  0.2663,  0.2286,  0.0734,  0.0978,  0.3151,  0.2375,
0:          0.0889, -0.1239, -0.2038,  0.0069, -0.2747, -0.6118, -0.3235, -0.4277, -0.4499, -0.3146, -0.0929,  0.0712,
0:          0.0601], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.0300,  0.0393,  0.0846,  0.1397,  0.1608,  0.1504,  0.1593,  0.2074,  0.2968,  0.4339,  0.5750,  0.6971,
0:          0.8184,  0.9090,  0.9783,  1.0803,  1.1044,  0.9509,  0.7458,  0.5469,  0.3376,  0.1646,  0.0222, -0.1199,
0:         -0.2544], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2405, -0.2451, -0.2451, -0.2451,
0:         -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2428, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451, -0.2451,
0:         -0.2451], device='cuda:0')
0: [DEBUG] Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451, -0.2451,     nan,     nan,
0:             nan,     nan,     nan, -0.2451,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,
0:         -0.2451,     nan,     nan, -0.2451,     nan, -0.2451,     nan, -0.2451,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2451, -0.2451,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan, -0.2451,
0:             nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 6, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0245,  0.1073,  0.1844,  0.2149,  0.1961,  0.1454,  0.0950,  0.0595,  0.0370,  0.0216, -0.0046, -0.0531,
0:         -0.0986, -0.1278, -0.1301, -0.0981, -0.0477,  0.0019,  0.0338,  0.1065,  0.1718,  0.1848,  0.1438,  0.0879,
0:          0.0423], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0359,  0.0526,  0.0609,  0.0574,  0.0385,  0.0311,  0.0164,  0.0105,  0.0043,  0.0023,  0.0091,  0.0259,
0:          0.0341,  0.0176, -0.0126, -0.0243,  0.0043,  0.0663,  0.0480,  0.0724,  0.0894,  0.0834,  0.0583,  0.0371,
0:          0.0173], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.6450, 2.7026, 2.7417, 2.7571, 2.7354, 2.6864, 2.6462, 2.6253, 2.6209, 2.6210, 2.6122, 2.5643, 2.4925, 2.4332,
0:         2.3813, 2.3881, 2.4588, 2.5755, 2.6055, 2.6879, 2.7661, 2.8128, 2.8077, 2.7691, 2.7408], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2463,  0.3389,  0.4774,  0.1829, -0.0886, -0.1725, -0.4250, -0.5010, -0.4399, -0.4417, -0.4791, -0.4899,
0:         -0.3894, -0.4689, -0.7202, -0.5468, -0.1272, -0.0440,  0.3016,  0.2781,  0.2874, -0.0382, -0.1848, -0.2118,
0:         -0.5080], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0363, -0.0556, -0.0420, -0.0227, -0.0188, -0.0461, -0.1020, -0.1616, -0.2044, -0.2123, -0.1862, -0.1388,
0:         -0.0823, -0.0261,  0.0179,  0.0407,  0.0297, -0.0178, -0.1026, -0.2141, -0.3405, -0.4648, -0.5836, -0.6997,
0:         -0.8176], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2014, -0.2029, -0.2040, -0.2181, -0.2232, -0.2270, -0.2274, -0.2234, -0.2140, -0.1971, -0.2064, -0.2153,
0:         -0.2188, -0.2250, -0.2257, -0.2207, -0.2235, -0.2060, -0.2096, -0.2122, -0.2113, -0.2214, -0.2211, -0.2268,
0:         -0.2203], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.17115244269371033; velocity_v: 0.29279807209968567; specific_humidity: 0.12237633019685745; velocity_z: 0.7195809483528137; temperature: 0.17070665955543518; total_precip: 1.3440542221069336; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1936274915933609; velocity_v: 0.2621594965457916; specific_humidity: 0.11234214156866074; velocity_z: 0.4996202886104584; temperature: 0.13392002880573273; total_precip: 0.7536863088607788; 
0: epoch: 6 [1/5 (20%)]	Loss: 1.04887 : 0.36890 :: 0.19213 (1.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16215431690216064; velocity_v: 0.36239683628082275; specific_humidity: 0.13382035493850708; velocity_z: 0.685448408126831; temperature: 0.12917062640190125; total_precip: 1.2968437671661377; 
0: epoch: 6 [2/5 (40%)]	Loss: 1.29684 : 0.43164 :: 0.19722 (15.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18482472002506256; velocity_v: 0.4031094014644623; specific_humidity: 0.13386647403240204; velocity_z: 0.6512724161148071; temperature: 0.13060228526592255; total_precip: 0.5399042963981628; 
0: epoch: 6 [3/5 (60%)]	Loss: 0.53990 : 0.31086 :: 0.19591 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18179067969322205; velocity_v: 0.4179995357990265; specific_humidity: 0.14019657671451569; velocity_z: 0.5016265511512756; temperature: 0.1733427196741104; total_precip: 0.8173895478248596; 
0: epoch: 6 [4/5 (80%)]	Loss: 0.81739 : 0.34197 :: 0.19543 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 6 : 0.3111778199672699
0: validation loss for velocity_u : 0.1534026712179184
0: validation loss for velocity_v : 0.3424731194972992
0: validation loss for specific_humidity : 0.1114642545580864
0: validation loss for velocity_z : 0.4850933849811554
0: validation loss for temperature : 0.10720018297433853
0: validation loss for total_precip : 0.6674333214759827
0: 7 : 09:21:05 :: batch_size = 96, lr = 1.7677085752190346e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 7, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3012, -0.3054, -0.3097, -0.3138, -0.3179, -0.3221, -0.3262, -0.3304, -0.3345, -0.3386, -0.3426, -0.3468,
0:         -0.3507, -0.3547, -0.3587, -0.3628, -0.3668, -0.3706, -0.4871, -0.4910, -0.4950, -0.4988, -0.5028, -0.5066,
0:         -0.5104], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.4345, -1.4349, -1.4353, -1.4355, -1.4359, -1.4362, -1.4366, -1.4368, -1.4370, -1.4372, -1.4374, -1.4376,
0:         -1.4378, -1.4378, -1.4380, -1.4380, -1.4382, -1.4382, -1.3863, -1.3859, -1.3853, -1.3847, -1.3841, -1.3832,
0:         -1.3826], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5105, -0.5106, -0.5108, -0.5109, -0.5110, -0.5112, -0.5114, -0.5116, -0.5117, -0.5119, -0.5120, -0.5122,
0:         -0.5122, -0.5124, -0.5124, -0.5125, -0.5126, -0.5126, -0.5059, -0.5060, -0.5061, -0.5062, -0.5063, -0.5065,
0:         -0.5066], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4863, -0.4863, -0.4885, -0.4885, -0.4885, -0.4885, -0.4885, -0.4885, -0.4885, -0.4885, -0.4907, -0.4907,
0:         -0.4907, -0.4907, -0.4907, -0.4907, -0.4907, -0.4907, -0.6301, -0.6323, -0.6323, -0.6323, -0.6323, -0.6345,
0:         -0.6345], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.3523, 0.3517, 0.3512, 0.3507, 0.3497, 0.3491, 0.3485, 0.3479, 0.3471, 0.3465, 0.3459, 0.3454, 0.3447, 0.3438,
0:         0.3432, 0.3426, 0.3420, 0.3410, 0.3405, 0.3399, 0.3390, 0.3384, 0.3377, 0.3371, 0.3361], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0.3874, 0.3874, 0.3874, 0.3862, 0.3862, 0.3850, 0.3850, 0.3850, 0.3838, 0.2369, 0.2369, 0.2357, 0.2357, 0.2345,
0:         0.2345, 0.2333, 0.2333, 0.2333, 0.0457, 0.0457, 0.0457, 0.0445, 0.0445, 0.0433, 0.0433], device='cuda:0')
0: [DEBUG] Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2422,     nan,     nan,
0:         -0.2422, -0.2422,     nan,     nan, -0.2422,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2279,     nan,     nan,     nan, -0.2255,
0:             nan,     nan,     nan,     nan,     nan, -0.2255, -0.2183,     nan,     nan,     nan, -0.2207,     nan,
0:             nan,     nan,     nan, -0.2279,     nan,     nan,     nan, -0.2302,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2446,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2446,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2422,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2398,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2350,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2302,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2446,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2446,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2422,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2398,     nan,     nan,     nan,     nan, -0.2398,     nan, -0.2350,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2374,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2374])
0: [DEBUG] Epoch 7, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.4614, -1.3788, -1.3042, -1.2832, -1.2964, -1.3206, -1.3167, -1.2741, -1.2146, -1.1562, -1.1297, -1.1356,
0:         -1.1659, -1.1853, -1.1937, -1.1662, -1.1207, -1.0481, -1.4079, -1.3110, -1.2298, -1.2124, -1.2525, -1.2738,
0:         -1.2634], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0041,  0.0013, -0.0123, -0.0282, -0.0522, -0.0642, -0.0823, -0.0960, -0.1052, -0.1049, -0.0912, -0.0732,
0:         -0.0597, -0.0675, -0.0954, -0.1083, -0.1015, -0.0779, -0.0246, -0.0089, -0.0081, -0.0231, -0.0441, -0.0624,
0:         -0.0766], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6410, -0.6388, -0.6359, -0.6322, -0.6254, -0.6225, -0.6250, -0.6273, -0.6307, -0.6342, -0.6366, -0.6348,
0:         -0.6282, -0.6209, -0.6253, -0.6303, -0.6526, -0.6622, -0.6504, -0.6372, -0.6290, -0.6202, -0.6161, -0.6200,
0:         -0.6215], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0339,  0.1314,  0.1902,  0.1369,  0.0735,  0.0896,  0.0908,  0.0839,  0.1024,  0.0975,  0.0839,  0.1411,
0:          0.2021,  0.1377,  0.0301,  0.0780,  0.1787,  0.1135,  0.0015, -0.0089,  0.0078, -0.0136, -0.0333,  0.0225,
0:          0.0374], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4781, -0.4576, -0.4446, -0.4423, -0.4350, -0.4279, -0.4219, -0.4222, -0.4217, -0.4147, -0.3986, -0.3847,
0:         -0.3756, -0.3768, -0.3861, -0.3977, -0.4042, -0.4038, -0.3944, -0.3891, -0.3927, -0.3998, -0.3954, -0.3784,
0:         -0.3547], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1144, -0.1145, -0.1146, -0.1384, -0.1445, -0.1464, -0.1511, -0.1490, -0.1387, -0.1057, -0.1182, -0.1290,
0:         -0.1410, -0.1464, -0.1461, -0.1463, -0.1508, -0.1300, -0.1193, -0.1252, -0.1264, -0.1415, -0.1442, -0.1498,
0:         -0.1465], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.17131225764751434; velocity_v: 0.3290725350379944; specific_humidity: 0.14015337824821472; velocity_z: 0.6715811491012573; temperature: 0.155329167842865; total_precip: 0.9259687662124634; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16441471874713898; velocity_v: 0.36839592456817627; specific_humidity: 0.1349601149559021; velocity_z: 0.5590819120407104; temperature: 0.15155337750911713; total_precip: 0.9840322136878967; 
0: epoch: 7 [1/5 (20%)]	Loss: 0.95500 : 0.36604 :: 0.19068 (1.72 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22986029088497162; velocity_v: 0.36126869916915894; specific_humidity: 0.16798986494541168; velocity_z: 0.6511208415031433; temperature: 0.22450456023216248; total_precip: 0.8277420401573181; 
0: epoch: 7 [2/5 (40%)]	Loss: 0.82774 : 0.37623 :: 0.20402 (15.78 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.181807741522789; velocity_v: 0.4562107026576996; specific_humidity: 0.16066467761993408; velocity_z: 0.6084221005439758; temperature: 0.1880764365196228; total_precip: 0.9642528295516968; 
0: epoch: 7 [3/5 (60%)]	Loss: 0.96425 : 0.39453 :: 0.20078 (15.42 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19976401329040527; velocity_v: 0.43755143880844116; specific_humidity: 0.15937256813049316; velocity_z: 0.6371812224388123; temperature: 0.1323433667421341; total_precip: 0.9088829755783081; 
0: epoch: 7 [4/5 (80%)]	Loss: 0.90888 : 0.38092 :: 0.20024 (15.77 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 7 : 0.342571884393692
0: validation loss for velocity_u : 0.15528303384780884
0: validation loss for velocity_v : 0.37697193026542664
0: validation loss for specific_humidity : 0.13851669430732727
0: validation loss for velocity_z : 0.5529423952102661
0: validation loss for temperature : 0.12300193309783936
0: validation loss for total_precip : 0.7087154388427734
0: 8 : 09:25:28 :: batch_size = 96, lr = 1.7245937319210094e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 8, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4564, -0.4651, -0.4523, -0.4106, -0.3463, -0.2736, -0.2053, -0.1486, -0.1004, -0.0562, -0.0166,  0.0116,
0:          0.0203,  0.0108, -0.0129, -0.0445, -0.0785, -0.1154, -0.4638, -0.4615, -0.4380, -0.3908, -0.3244, -0.2534,
0:         -0.1886], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0147,  0.0107, -0.0262, -0.0833, -0.1491, -0.2074, -0.2456, -0.2600, -0.2546, -0.2361, -0.2043, -0.1583,
0:         -0.0954, -0.0161,  0.0732,  0.1591,  0.2245,  0.2586,  0.0304,  0.0336, -0.0082, -0.0796, -0.1580, -0.2245,
0:         -0.2718], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.9974, 3.0417, 3.0335, 2.8713, 2.6966, 2.5993, 2.6067, 2.6396, 2.6579, 2.6544, 2.6071, 2.5292, 2.4171, 2.2433,
0:         1.9827, 1.6737, 1.3744, 1.1943, 2.9181, 3.0219, 3.0835, 3.0450, 2.8707, 2.7110, 2.5899], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2149,  0.2608,  0.3833,  0.5188,  0.5286,  0.4565,  0.3264,  0.2084,  0.0980, -0.0485, -0.1097, -0.0911,
0:          0.0401,  0.2248,  0.3680,  0.4598,  0.3942,  0.2685,  0.0794,  0.1472,  0.2587,  0.3089,  0.2619,  0.2302,
0:          0.1450], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.5190, -0.6539, -0.8456, -1.0267, -1.1536, -1.2201, -1.2254, -1.1409, -0.9795, -0.7899, -0.6080, -0.4511,
0:         -0.3282, -0.2376, -0.1763, -0.1392, -0.0982, -0.0653, -0.0825, -0.1711, -0.3242, -0.4827, -0.5850, -0.6196,
0:         -0.6005], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359,
0:         -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359,
0:         -0.2359], device='cuda:0')
0: [DEBUG] Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2182,     nan, -0.2071,     nan, -0.0952,
0:             nan,     nan, -0.2359, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2215,     nan,     nan,     nan, -0.2359,
0:             nan, -0.2270,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2314, -0.2348,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2004,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2348,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2171,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2359,     nan, -0.2303,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2337,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2082,     nan,     nan,     nan, -0.2348,     nan,     nan,     nan, -0.2314,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2015,     nan,
0:             nan,     nan, -0.2259,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2015,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 8, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2717, -0.2057, -0.1397, -0.1140, -0.1212, -0.1460, -0.1473, -0.1307, -0.1176, -0.1186, -0.1575, -0.2330,
0:         -0.3011, -0.3272, -0.2887, -0.1840, -0.0801,  0.0007, -0.1929, -0.1239, -0.0704, -0.0638, -0.1062, -0.1307,
0:         -0.1252], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0069, -0.0035, -0.0212, -0.0528, -0.0925, -0.1097, -0.1197, -0.1240, -0.1219, -0.1170, -0.1061, -0.0928,
0:         -0.0918, -0.1205, -0.1740, -0.2119, -0.2158, -0.1862, -0.0282, -0.0124, -0.0146, -0.0445, -0.0840, -0.1129,
0:         -0.1273], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.8104, 2.8925, 2.9063, 2.8795, 2.8269, 2.7578, 2.7030, 2.6488, 2.6022, 2.5657, 2.5272, 2.4903, 2.4451, 2.4142,
0:         2.3761, 2.3534, 2.3423, 2.3529, 2.6463, 2.7636, 2.8147, 2.8296, 2.7999, 2.7334, 2.6745], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1026,  0.1498,  0.1902,  0.0531,  0.0224,  0.0874,  0.0270,  0.0160,  0.0221, -0.0337, -0.0553, -0.0297,
0:          0.0073,  0.0082, -0.1114, -0.0952,  0.1001,  0.0632,  0.0050, -0.0551, -0.0686, -0.2006, -0.1549, -0.0342,
0:         -0.0513], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0038, -0.0656, -0.1050, -0.1053, -0.0759, -0.0369, -0.0033,  0.0276,  0.0567,  0.0735,  0.0603,  0.0212,
0:         -0.0405, -0.1023, -0.1630, -0.2313, -0.3165, -0.3996, -0.4493, -0.4387, -0.3630, -0.2595, -0.1649, -0.0950,
0:         -0.0309], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0297, -0.0322, -0.0272, -0.0642, -0.0697, -0.0790, -0.0794, -0.0744, -0.0622, -0.0154, -0.0279, -0.0463,
0:         -0.0596, -0.0700, -0.0721, -0.0675, -0.0641, -0.0451, -0.0185, -0.0301, -0.0338, -0.0483, -0.0584, -0.0626,
0:         -0.0563], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.22679506242275238; velocity_v: 0.3493032157421112; specific_humidity: 0.18071088194847107; velocity_z: 0.6188973784446716; temperature: 0.1555604636669159; total_precip: 0.9763855338096619; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19950881600379944; velocity_v: 0.42712870240211487; specific_humidity: 0.1849791705608368; velocity_z: 0.5145511031150818; temperature: 0.16534779965877533; total_precip: 0.5371334552764893; 
0: epoch: 8 [1/5 (20%)]	Loss: 0.75676 : 0.34601 :: 0.19797 (1.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22215425968170166; velocity_v: 0.4349438548088074; specific_humidity: 0.15891827642917633; velocity_z: 0.5962433815002441; temperature: 0.15108148753643036; total_precip: 0.9518751502037048; 
0: epoch: 8 [2/5 (40%)]	Loss: 0.95188 : 0.38772 :: 0.20371 (15.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2030288577079773; velocity_v: 0.42564594745635986; specific_humidity: 0.133429616689682; velocity_z: 0.6492010951042175; temperature: 0.14540398120880127; total_precip: 0.9456186890602112; 
0: epoch: 8 [3/5 (60%)]	Loss: 0.94562 : 0.38596 :: 0.20103 (15.87 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21662607789039612; velocity_v: 0.5491719841957092; specific_humidity: 0.1580468714237213; velocity_z: 0.7353888154029846; temperature: 0.16073232889175415; total_precip: 1.144953727722168; 
0: epoch: 8 [4/5 (80%)]	Loss: 1.14495 : 0.46150 :: 0.20435 (15.92 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 8 : 0.3891724646091461
0: validation loss for velocity_u : 0.1934191733598709
0: validation loss for velocity_v : 0.46988606452941895
0: validation loss for specific_humidity : 0.15572325885295868
0: validation loss for velocity_z : 0.575564980506897
0: validation loss for temperature : 0.11846711486577988
0: validation loss for total_precip : 0.8219740986824036
0: 9 : 09:29:42 :: batch_size = 96, lr = 1.6825304701668386e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 9, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.1888, 2.1812, 2.1725, 2.1617, 2.1503, 2.1381, 2.1239, 2.1085, 2.0919, 2.0751, 2.0589, 2.0436, 2.0308, 2.0196,
0:         2.0090, 1.9987, 1.9884, 1.9793, 2.2923, 2.2832, 2.2722, 2.2590, 2.2448, 2.2283, 2.2100], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2780, -0.2672, -0.2575, -0.2486, -0.2401, -0.2319, -0.2241, -0.2170, -0.2107, -0.2059, -0.2014, -0.1964,
0:         -0.1906, -0.1836, -0.1756, -0.1668, -0.1577, -0.1490, -0.3025, -0.2936, -0.2867, -0.2804, -0.2748, -0.2694,
0:         -0.2635], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2432, -0.2453, -0.2486, -0.2535, -0.2579, -0.2654, -0.2711, -0.2789, -0.2861, -0.2934, -0.2998, -0.3047,
0:         -0.3104, -0.3145, -0.3186, -0.3211, -0.3211, -0.3225, -0.2092, -0.2117, -0.2159, -0.2199, -0.2239, -0.2272,
0:         -0.2329], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1944,  0.1741,  0.2056,  0.2686,  0.3069,  0.3631,  0.3946,  0.4216,  0.4306,  0.3991,  0.4216,  0.4644,
0:          0.5408,  0.6578,  0.7411,  0.8086,  0.8198,  0.8288, -0.0149, -0.0778, -0.0464, -0.0126,  0.0054,  0.0819,
0:          0.1044], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.3459, 0.3352, 0.3236, 0.3101, 0.2964, 0.2831, 0.2692, 0.2558, 0.2419, 0.2281, 0.2169, 0.2065, 0.1981, 0.1924,
0:         0.1889, 0.1857, 0.1817, 0.1768, 0.1716, 0.1683, 0.1664, 0.1636, 0.1632, 0.1636, 0.1636], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2505, -0.2505, -0.2505, -0.2505, -0.2505, -0.2436, -0.2413, -0.2413, -0.2505, -0.2505, -0.2505, -0.2505,
0:         -0.2505, -0.2505, -0.2505, -0.2482, -0.2436, -0.2505, -0.2505, -0.2505, -0.2505, -0.2505, -0.2505, -0.2505,
0:         -0.2390], device='cuda:0')
0: [DEBUG] Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2505,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1852,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1600,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2505,     nan,     nan,
0:             nan, -0.2001,     nan,     nan,     nan,     nan, -0.2012,     nan,     nan,     nan, -0.1715,     nan,
0:             nan, -0.0673,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1234,     nan,     nan,     nan,     nan, -0.2104,     nan,     nan,     nan,     nan,
0:             nan, -0.2493,     nan,     nan, -0.1738,     nan, -0.0879,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1864,     nan, -0.1978,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1738,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2390,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1497,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2482,
0:             nan,     nan,     nan, -0.1864,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2207,     nan,     nan, -0.2505,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 9, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([3.0806, 3.1740, 3.2685, 3.3498, 3.3706, 3.3079, 3.2243, 3.1389, 3.1063, 3.1339, 3.1713, 3.2264, 3.2533, 3.2723,
0:         3.3033, 3.3202, 3.3492, 3.3288, 3.2047, 3.2235, 3.2583, 3.2857, 3.2666, 3.1911, 3.0879], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.5588,  0.5531,  0.5167,  0.4727,  0.4092,  0.3619,  0.2931,  0.2357,  0.1976,  0.1891,  0.2028,  0.2202,
0:          0.2100,  0.1557,  0.0653,  0.0090, -0.0149,  0.0107,  0.5259,  0.5116,  0.5002,  0.4677,  0.4243,  0.3609,
0:          0.2927], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6604, -0.6555, -0.6512, -0.6383, -0.6234, -0.6114, -0.6106, -0.6096, -0.6113, -0.6168, -0.6183, -0.6184,
0:         -0.6168, -0.6151, -0.6197, -0.6259, -0.6404, -0.6528, -0.6641, -0.6564, -0.6488, -0.6338, -0.6186, -0.6073,
0:         -0.6026], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4796, 1.7751, 1.9849, 1.8989, 1.7164, 1.6083, 1.6104, 1.5485, 1.3769, 1.3292, 1.2588, 1.2449, 1.3036, 1.1258,
0:         0.9204, 0.9173, 0.9766, 0.9421, 1.5233, 1.5949, 1.7194, 1.6955, 1.6757, 1.6607, 1.6458], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.0433, 0.1519, 0.2543, 0.3455, 0.4206, 0.4754, 0.5075, 0.5144, 0.5125, 0.5124, 0.5308, 0.5616, 0.6107, 0.6566,
0:         0.7010, 0.7338, 0.7716, 0.8074, 0.8402, 0.8671, 0.8847, 0.9137, 0.9608, 1.0221, 1.0884], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1341, -0.1250, -0.1263, -0.1392, -0.1337, -0.1327, -0.1304, -0.1294, -0.1162, -0.1341, -0.1388, -0.1437,
0:         -0.1410, -0.1413, -0.1336, -0.1326, -0.1321, -0.1183, -0.1436, -0.1429, -0.1414, -0.1441, -0.1436, -0.1469,
0:         -0.1359], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2086051106452942; velocity_v: 0.40944328904151917; specific_humidity: 0.1930341273546219; velocity_z: 0.5184606909751892; temperature: 0.15757429599761963; total_precip: 0.7069942951202393; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21122601628303528; velocity_v: 0.4709789752960205; specific_humidity: 0.17747390270233154; velocity_z: 0.5032525062561035; temperature: 0.18007861077785492; total_precip: 0.6773532629013062; 
0: epoch: 9 [1/5 (20%)]	Loss: 0.69217 : 0.33514 :: 0.20287 (1.72 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2473311424255371; velocity_v: 0.3602445721626282; specific_humidity: 0.19366182386875153; velocity_z: 0.5677326917648315; temperature: 0.15364482998847961; total_precip: 0.8038610219955444; 
0: epoch: 9 [2/5 (40%)]	Loss: 0.80386 : 0.35432 :: 0.20841 (16.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21978925168514252; velocity_v: 0.4951843321323395; specific_humidity: 0.18059512972831726; velocity_z: 0.5809011459350586; temperature: 0.17326532304286957; total_precip: 0.7959142327308655; 
0: epoch: 9 [3/5 (60%)]	Loss: 0.79591 : 0.37385 :: 0.21119 (15.65 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2303946316242218; velocity_v: 0.43413618206977844; specific_humidity: 0.1775369495153427; velocity_z: 0.5615382194519043; temperature: 0.1511482149362564; total_precip: 0.6303478479385376; 
0: epoch: 9 [4/5 (80%)]	Loss: 0.63035 : 0.33175 :: 0.20777 (16.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 9 : 0.40898290276527405
0: validation loss for velocity_u : 0.1900472491979599
0: validation loss for velocity_v : 0.42147961258888245
0: validation loss for specific_humidity : 0.1894991546869278
0: validation loss for velocity_z : 0.5163094997406006
0: validation loss for temperature : 0.15275739133358002
0: validation loss for total_precip : 0.9838051199913025
0: 10 : 09:34:03 :: batch_size = 96, lr = 1.6414931416261842e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 10, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5188, 0.4946, 0.5027, 0.5225, 0.5414, 0.5578, 0.5800, 0.6188, 0.6826, 0.7701, 0.8691, 0.9627, 1.0370, 1.0873,
0:         1.1172, 1.1333, 1.1406, 1.1420, 0.4426, 0.4176, 0.4108, 0.4100, 0.4162, 0.4367, 0.4781], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-4.7237, -4.8489, -4.8015, -4.6394, -4.4253, -4.2074, -4.0149, -3.8600, -3.7420, -3.6578, -3.6053, -3.5813,
0:         -3.5747, -3.5694, -3.5515, -3.5120, -3.4498, -3.3724, -4.7742, -4.8055, -4.6875, -4.4817, -4.2497, -4.0380,
0:         -3.8673], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5966, -0.4880, -0.3893, -0.3357, -0.3032, -0.2858, -0.2680, -0.2519, -0.2356, -0.2188, -0.2012, -0.1824,
0:         -0.1652, -0.1487, -0.1356, -0.1224, -0.1104, -0.0990, -0.5502, -0.4460, -0.3705, -0.3350, -0.3065, -0.2889,
0:         -0.2721], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1587, -0.7885, -1.4931, -2.0436, -2.3071, -2.3451, -2.3875, -2.6622, -3.1770, -3.6684, -3.8180, -3.5120,
0:         -2.9068, -2.2602, -1.7231, -1.2832, -0.8778, -0.5283, -0.3061, -0.8499, -1.3535, -1.6818, -1.8214, -1.8895,
0:         -2.1006], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-3.0444e-01, -2.5320e-01, -1.9330e-01, -1.2906e-01, -6.5458e-02,  8.7154e-04,  7.8129e-02,  1.6445e-01,
0:          2.4557e-01,  3.1161e-01,  3.6729e-01,  4.2299e-01,  4.8143e-01,  5.3798e-01,  5.8929e-01,  6.3540e-01,
0:          6.7922e-01,  7.2425e-01,  7.7210e-01,  8.1770e-01,  8.5284e-01,  8.7397e-01,  8.8690e-01,  8.9998e-01,
0:          9.1658e-01], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0247,  0.2233,  0.5643,  1.0448,  1.6205,  1.8530,  2.1098,  2.3600,  2.4995,  0.0218,  0.2676,  0.5997,
0:          1.0713,  1.6094,  1.8862,  2.2538,  2.8273,  3.0199,  0.1236,  0.5178,  0.9518,  1.3791,  1.6227,  1.8973,
0:          2.2538], device='cuda:0')
0: [DEBUG] Epoch 10, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0340,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.5942,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1875,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2306,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0196,     nan,
0:             nan,     nan, -0.0181,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.5444,     nan,     nan,     nan,     nan,  0.0993,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3008,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.1303,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0085,
0:             nan,     nan,     nan, -0.1620,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1170,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.1414,     nan,  0.0661,     nan,     nan,     nan,
0:             nan,     nan,  0.0816,     nan,     nan,     nan,  0.2211,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.3052,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.5289,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.3539,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0336, -0.0136,     nan,
0:             nan,     nan, -0.0601])
0: [DEBUG] Epoch 10, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.5975, 1.6437, 1.7091, 1.7618, 1.7688, 1.7216, 1.6189, 1.5016, 1.3650, 1.2554, 1.1463, 1.0542, 0.9687, 0.9004,
0:         0.8702, 0.8508, 0.8322, 0.7897, 1.6456, 1.6587, 1.6966, 1.7283, 1.7297, 1.6812, 1.5888], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0587,  0.0339,  0.0880,  0.1188,  0.1274,  0.1589,  0.1749,  0.1817,  0.1651,  0.1298,  0.0921,  0.0445,
0:         -0.0269, -0.1333, -0.2553, -0.3443, -0.3666, -0.3204, -0.0812,  0.0120,  0.0747,  0.0949,  0.1095,  0.1254,
0:          0.1488], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6940, -0.6999, -0.7001, -0.6975, -0.6888, -0.6851, -0.6920, -0.6967, -0.7023, -0.7085, -0.7123, -0.7117,
0:         -0.7065, -0.7013, -0.7040, -0.7076, -0.7208, -0.7217, -0.6903, -0.6925, -0.6913, -0.6808, -0.6788, -0.6796,
0:         -0.6857], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4180, -0.3757, -0.3549, -0.3935, -0.4257, -0.3242, -0.2198, -0.1529, -0.0539, -0.0168, -0.0228,  0.0468,
0:          0.1666,  0.2244,  0.1763,  0.2282,  0.3077,  0.1262, -0.4846, -0.5229, -0.5753, -0.6219, -0.6207, -0.4551,
0:         -0.3285], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-2.2728, -2.2820, -2.2977, -2.3278, -2.3742, -2.4360, -2.5148, -2.6114, -2.7088, -2.7939, -2.8563, -2.8932,
0:         -2.9139, -2.9305, -2.9517, -2.9821, -3.0042, -3.0065, -2.9876, -2.9582, -2.9277, -2.8912, -2.8374, -2.7544,
0:         -2.6491], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.0958, 0.1081, 0.1095, 0.0818, 0.0698, 0.0580, 0.0450, 0.0418, 0.0428, 0.1334, 0.1226, 0.0999, 0.0832, 0.0629,
0:         0.0459, 0.0515, 0.0347, 0.0445, 0.1367, 0.1269, 0.1193, 0.0927, 0.0657, 0.0507, 0.0368], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.2043050229549408; velocity_v: 0.42637157440185547; specific_humidity: 0.15210646390914917; velocity_z: 0.48546791076660156; temperature: 0.1296670138835907; total_precip: 0.3918541371822357; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22048848867416382; velocity_v: 0.41330039501190186; specific_humidity: 0.17436794936656952; velocity_z: 0.6068114042282104; temperature: 0.13467636704444885; total_precip: 0.7831718921661377; 
0: epoch: 10 [1/5 (20%)]	Loss: 0.58751 : 0.31282 :: 0.20094 (1.67 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22080479562282562; velocity_v: 0.4018549621105194; specific_humidity: 0.18368470668792725; velocity_z: 0.5997463464736938; temperature: 0.15748533606529236; total_precip: 0.48841145634651184; 
0: epoch: 10 [2/5 (40%)]	Loss: 0.48841 : 0.30996 :: 0.20144 (15.81 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.25076746940612793; velocity_v: 0.39471033215522766; specific_humidity: 0.1726483851671219; velocity_z: 0.47859126329421997; temperature: 0.1475563496351242; total_precip: 0.5921121835708618; 
0: epoch: 10 [3/5 (60%)]	Loss: 0.59211 : 0.30827 :: 0.20405 (15.92 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23395897448062897; velocity_v: 0.4749850928783417; specific_humidity: 0.18638013303279877; velocity_z: 0.643791139125824; temperature: 0.23277419805526733; total_precip: 1.124293327331543; 
0: epoch: 10 [4/5 (80%)]	Loss: 1.12429 : 0.44982 :: 0.20716 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 10 : 0.31268441677093506
0: validation loss for velocity_u : 0.16273733973503113
0: validation loss for velocity_v : 0.45006439089775085
0: validation loss for specific_humidity : 0.1439451426267624
0: validation loss for velocity_z : 0.4759407341480255
0: validation loss for temperature : 0.1059664711356163
0: validation loss for total_precip : 0.5374526977539062
0: 11 : 09:38:33 :: batch_size = 96, lr = 1.601456723537741e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 11, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.5177, 1.4866, 1.4611, 1.4423, 1.4305, 1.4249, 1.4249, 1.4295, 1.4374, 1.4471, 1.4573, 1.4668, 1.4756, 1.4828,
0:         1.4882, 1.4922, 1.4953, 1.4988, 1.4282, 1.4073, 1.3931, 1.3861, 1.3854, 1.3897, 1.3976], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1985, 0.2153, 0.2346, 0.2576, 0.2839, 0.3134, 0.3447, 0.3776, 0.4110, 0.4442, 0.4757, 0.5053, 0.5325, 0.5574,
0:         0.5800, 0.6005, 0.6200, 0.6399, 0.1504, 0.1736, 0.2002, 0.2302, 0.2632, 0.2984, 0.3351], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7272, -0.7390, -0.7510, -0.7542, -0.7558, -0.7561, -0.7565, -0.7565, -0.7563, -0.7558, -0.7556, -0.7541,
0:         -0.7526, -0.7491, -0.7459, -0.7416, -0.7354, -0.7274, -0.7432, -0.7481, -0.7537, -0.7543, -0.7548, -0.7546,
0:         -0.7544], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4357, -0.4728, -0.4553, -0.3899, -0.2938, -0.1738, -0.0407,  0.0858,  0.1753,  0.2189,  0.2298,  0.2211,
0:          0.2036,  0.1731,  0.1404,  0.1164,  0.1142,  0.1295, -0.4902, -0.4531, -0.3855, -0.2982, -0.2044, -0.0996,
0:          0.0094], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4902, -0.5033, -0.5175, -0.5325, -0.5479, -0.5637, -0.5800, -0.5973, -0.6152, -0.6335, -0.6518, -0.6704,
0:         -0.6895, -0.7090, -0.7284, -0.7482, -0.7679, -0.7880, -0.8078, -0.8275, -0.8466, -0.8653, -0.8832, -0.8993,
0:         -0.9138], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.2073,  0.4891,  0.2689, -0.0483, -0.2023, -0.2138, -0.2240, -0.2309, -0.2366,  0.0783,  0.1947,  0.1011,
0:         -0.0974, -0.2092, -0.2217, -0.2320, -0.2377, -0.2434,  0.0270,  0.1890,  0.1057, -0.0997, -0.2138, -0.2240,
0:         -0.2320], device='cuda:0')
0: [DEBUG] Epoch 11, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2491,     nan,     nan,     nan,
0:             nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2377,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2320,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2240,     nan,
0:             nan, -0.2160,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2400,
0:             nan,     nan,     nan, -0.2446,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2320,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan,     nan,     nan,
0:             nan, -0.2309,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2229,     nan, -0.2206,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2343,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2297,     nan,     nan,     nan, -0.2252,     nan,     nan,
0:         -0.2263,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2172,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2103,     nan, -0.2058,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 11, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0411, 1.0652, 1.1206, 1.1816, 1.2153, 1.2007, 1.1655, 1.1170, 1.0664, 1.0489, 1.0366, 1.0216, 1.0026, 0.9892,
0:         0.9834, 0.9974, 1.0257, 1.0456, 1.0256, 1.0057, 1.0224, 1.0446, 1.0368, 0.9966, 0.9435], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2680, 0.2714, 0.2740, 0.2738, 0.2528, 0.2298, 0.1928, 0.1636, 0.1456, 0.1522, 0.1707, 0.2008, 0.2190, 0.2050,
0:         0.1745, 0.1664, 0.1954, 0.2507, 0.1964, 0.2178, 0.2443, 0.2554, 0.2403, 0.2104, 0.1700], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8117, -0.8044, -0.7982, -0.7899, -0.7778, -0.7714, -0.7761, -0.7817, -0.7869, -0.7943, -0.7963, -0.7934,
0:         -0.7870, -0.7794, -0.7832, -0.7937, -0.8183, -0.8341, -0.8060, -0.7936, -0.7882, -0.7762, -0.7697, -0.7663,
0:         -0.7684], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9711, 1.1063, 1.1894, 1.1269, 1.0438, 1.1094, 1.1416, 1.0938, 1.0781, 1.0041, 0.9467, 1.0652, 1.1542, 1.0986,
0:         1.0361, 1.1008, 1.1544, 0.9672, 0.9253, 0.8922, 0.9128, 0.9304, 0.9397, 1.0464, 1.0919], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4976, -0.4886, -0.4845, -0.4920, -0.5078, -0.5285, -0.5567, -0.5956, -0.6336, -0.6679, -0.6964, -0.7150,
0:         -0.7313, -0.7544, -0.7908, -0.8397, -0.8861, -0.9197, -0.9372, -0.9484, -0.9597, -0.9761, -0.9866, -0.9914,
0:         -1.0032], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1982, -0.1822, -0.1919, -0.1856, -0.1782, -0.1713, -0.1704, -0.1758, -0.1696, -0.2037, -0.2073, -0.2008,
0:         -0.1882, -0.1872, -0.1771, -0.1777, -0.1780, -0.1776, -0.2257, -0.2190, -0.2089, -0.1940, -0.1861, -0.1898,
0:         -0.1880], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2370147854089737; velocity_v: 0.4659653604030609; specific_humidity: 0.17273692786693573; velocity_z: 0.6798076033592224; temperature: 0.1664441078901291; total_precip: 0.7723029255867004; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21870146691799164; velocity_v: 0.4855732023715973; specific_humidity: 0.18208804726600647; velocity_z: 0.6869574189186096; temperature: 0.21748529374599457; total_precip: 0.8561099767684937; 
0: epoch: 11 [1/5 (20%)]	Loss: 0.81421 : 0.39638 :: 0.20543 (2.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23731201887130737; velocity_v: 0.4109804928302765; specific_humidity: 0.19110621511936188; velocity_z: 0.5764051079750061; temperature: 0.16014504432678223; total_precip: 0.8870633840560913; 
0: epoch: 11 [2/5 (40%)]	Loss: 0.88706 : 0.37827 :: 0.20614 (15.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24140596389770508; velocity_v: 0.4750059247016907; specific_humidity: 0.1938459426164627; velocity_z: 0.6151830554008484; temperature: 0.17864708602428436; total_precip: 0.6483975648880005; 
0: epoch: 11 [3/5 (60%)]	Loss: 0.64840 : 0.35973 :: 0.20549 (16.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2361619919538498; velocity_v: 0.40172576904296875; specific_humidity: 0.2136455774307251; velocity_z: 0.5656104683876038; temperature: 0.17827123403549194; total_precip: 0.5490977168083191; 
0: epoch: 11 [4/5 (80%)]	Loss: 0.54910 : 0.32422 :: 0.20952 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 11 : 0.33270397782325745
0: validation loss for velocity_u : 0.20975108444690704
0: validation loss for velocity_v : 0.42542362213134766
0: validation loss for specific_humidity : 0.14354734122753143
0: validation loss for velocity_z : 0.5719371438026428
0: validation loss for temperature : 0.11671615391969681
0: validation loss for total_precip : 0.5288490653038025
0: 12 : 09:42:34 :: batch_size = 96, lr = 1.5623968034514547e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 12, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2151, -1.2175, -1.2207, -1.2248, -1.2300, -1.2363, -1.2433, -1.2508, -1.2585, -1.2661, -1.2732, -1.2797,
0:         -1.2849, -1.2889, -1.2911, -1.2916, -1.2902, -1.2862, -1.2197, -1.2237, -1.2287, -1.2349, -1.2424, -1.2511,
0:         -1.2607], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.8658, 1.8756, 1.8841, 1.8907, 1.8951, 1.8968, 1.8957, 1.8918, 1.8849, 1.8745, 1.8610, 1.8442, 1.8236, 1.7993,
0:         1.7712, 1.7396, 1.7043, 1.6656, 1.8109, 1.8221, 1.8323, 1.8410, 1.8483, 1.8539, 1.8577], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7516, -0.7507, -0.7499, -0.7499, -0.7499, -0.7501, -0.7501, -0.7507, -0.7516, -0.7522, -0.7531, -0.7543,
0:         -0.7554, -0.7564, -0.7573, -0.7581, -0.7587, -0.7596, -0.7461, -0.7451, -0.7440, -0.7438, -0.7438, -0.7438,
0:         -0.7436], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3300, 1.3101, 1.2793, 1.2374, 1.1889, 1.1339, 1.0766, 1.0127, 0.9510, 0.8893, 0.8298, 0.7747, 0.7262, 0.6822,
0:         0.6447, 0.6116, 0.5808, 0.5566, 1.1845, 1.1757, 1.1581, 1.1339, 1.1030, 1.0656, 1.0237], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.6732, 1.6876, 1.7007, 1.7123, 1.7218, 1.7292, 1.7343, 1.7374, 1.7375, 1.7350, 1.7304, 1.7231, 1.7137, 1.7027,
0:         1.6900, 1.6757, 1.6602, 1.6437, 1.6268, 1.6088, 1.5899, 1.5707, 1.5502, 1.5287, 1.5061], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423,
0:         -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423, -0.2423,
0:         -0.2423], device='cuda:0')
0: [DEBUG] Epoch 12, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2423, -0.2423, -0.2423,
0:         -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2423,
0:             nan,     nan,     nan,     nan, -0.2423, -0.2423, -0.2423,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2423,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2423, -0.2423, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2423, -0.2423,     nan,
0:             nan, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2423,     nan])
0: [DEBUG] Epoch 12, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4263, 0.4563, 0.5011, 0.5255, 0.5383, 0.5398, 0.5540, 0.5742, 0.5888, 0.5919, 0.5647, 0.5175, 0.4730, 0.4423,
0:         0.4296, 0.4267, 0.4263, 0.4113, 0.5019, 0.5112, 0.5218, 0.5223, 0.4920, 0.4778, 0.4883], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1104, 0.1656, 0.2198, 0.2400, 0.2290, 0.2066, 0.1699, 0.1612, 0.1607, 0.1880, 0.2099, 0.2340, 0.2317, 0.1815,
0:         0.1189, 0.0729, 0.0731, 0.0895, 0.0570, 0.1164, 0.1906, 0.2275, 0.2247, 0.2084, 0.1753], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7835, -0.7844, -0.7800, -0.7750, -0.7628, -0.7540, -0.7511, -0.7525, -0.7538, -0.7618, -0.7697, -0.7710,
0:         -0.7667, -0.7596, -0.7623, -0.7671, -0.7889, -0.8027, -0.7847, -0.7755, -0.7706, -0.7608, -0.7559, -0.7527,
0:         -0.7499], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1621,  0.2556,  0.3703,  0.2875,  0.1824,  0.2319,  0.2915,  0.2667,  0.2409,  0.1901,  0.0906,  0.0898,
0:          0.1136,  0.0664, -0.0178,  0.0129,  0.1786,  0.1782,  0.1028,  0.0843,  0.1700,  0.1567,  0.1317,  0.1780,
0:          0.2175], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.6434, 1.5963, 1.5729, 1.5871, 1.6222, 1.6580, 1.6806, 1.6898, 1.6922, 1.6916, 1.6870, 1.6886, 1.6986, 1.7276,
0:         1.7704, 1.8126, 1.8381, 1.8463, 1.8472, 1.8490, 1.8579, 1.8622, 1.8577, 1.8443, 1.8368], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2170, -0.2068, -0.2131, -0.2045, -0.1956, -0.1874, -0.1865, -0.1912, -0.1877, -0.2262, -0.2237, -0.2190,
0:         -0.2056, -0.2027, -0.1927, -0.1910, -0.1926, -0.1970, -0.2456, -0.2360, -0.2269, -0.2101, -0.2027, -0.2096,
0:         -0.2049], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23863276839256287; velocity_v: 0.5688168406486511; specific_humidity: 0.17031078040599823; velocity_z: 0.5925275683403015; temperature: 0.16314131021499634; total_precip: 0.9150830507278442; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22414349019527435; velocity_v: 0.4433734714984894; specific_humidity: 0.21296413242816925; velocity_z: 0.6740073561668396; temperature: 0.17219217121601105; total_precip: 0.9709799885749817; 
0: epoch: 12 [1/5 (20%)]	Loss: 0.94303 : 0.41291 :: 0.20421 (2.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23133209347724915; velocity_v: 0.5265854597091675; specific_humidity: 0.19639833271503448; velocity_z: 0.6335481405258179; temperature: 0.20374834537506104; total_precip: 0.8023025989532471; 
0: epoch: 12 [2/5 (40%)]	Loss: 0.80230 : 0.39822 :: 0.20646 (15.50 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22848200798034668; velocity_v: 0.6254975199699402; specific_humidity: 0.18013113737106323; velocity_z: 0.7168947458267212; temperature: 0.17274034023284912; total_precip: 0.6857295036315918; 
0: epoch: 12 [3/5 (60%)]	Loss: 0.68573 : 0.40176 :: 0.20629 (15.83 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1997472047805786; velocity_v: 0.44951191544532776; specific_humidity: 0.16277822852134705; velocity_z: 0.5399163961410522; temperature: 0.16450235247612; total_precip: 0.5456938147544861; 
0: epoch: 12 [4/5 (80%)]	Loss: 0.54569 : 0.31251 :: 0.20181 (15.83 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 12 : 0.38892191648483276
0: validation loss for velocity_u : 0.19048058986663818
0: validation loss for velocity_v : 0.49608251452445984
0: validation loss for specific_humidity : 0.1512148082256317
0: validation loss for velocity_z : 0.581037163734436
0: validation loss for temperature : 0.1352037936449051
0: validation loss for total_precip : 0.7795125842094421
0: 13 : 09:46:41 :: batch_size = 96, lr = 1.5242895643428828e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 13, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2886, -0.3369, -0.3944, -0.4587, -0.5265, -0.5950, -0.6614, -0.7221, -0.7729, -0.8131, -0.8459, -0.8744,
0:         -0.8964, -0.9061, -0.9007, -0.8859, -0.8728, -0.8688, -0.3531, -0.3990, -0.4520, -0.5117, -0.5759, -0.6401,
0:         -0.6988], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3341,  0.2840,  0.2353,  0.1887,  0.1474,  0.1142,  0.0848,  0.0503,  0.0028, -0.0586, -0.1290, -0.2034,
0:         -0.2787, -0.3533, -0.4250, -0.4921, -0.5550, -0.6150,  0.2507,  0.2021,  0.1548,  0.1129,  0.0788,  0.0507,
0:          0.0211], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0666,  0.0577,  0.0588,  0.0657,  0.0722,  0.0783,  0.0750,  0.0647,  0.0506,  0.0303,  0.0015, -0.0175,
0:         -0.0376, -0.0588, -0.0716, -0.0888, -0.1106, -0.1332,  0.0113,  0.0052,  0.0088,  0.0159,  0.0226,  0.0275,
0:          0.0236], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0488,  0.1036,  0.1124, -0.0104, -0.2605, -0.5204, -0.6706, -0.6915, -0.6432, -0.5697, -0.4546, -0.2725,
0:         -0.0477,  0.1376,  0.2023,  0.1354,  0.0093, -0.0795, -0.0598, -0.0093, -0.0346, -0.1727, -0.3833, -0.5610,
0:         -0.6202], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.8845, 0.9031, 0.9179, 0.9280, 0.9357, 0.9465, 0.9638, 0.9859, 1.0092, 1.0288, 1.0426, 1.0497, 1.0469, 1.0303,
0:         0.9995, 0.9622, 0.9314, 0.9158, 0.9146, 0.9208, 0.9291, 0.9384, 0.9488, 0.9595, 0.9691], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0564, -0.1499, -0.1791, -0.1907, -0.2106, -0.2246, -0.2258, -0.2293, -0.2374,  0.0242, -0.0915, -0.2141,
0:         -0.2141, -0.2071, -0.2223, -0.2328, -0.2374, -0.2421, -0.0004, -0.1884, -0.1756, -0.2316, -0.2129, -0.2316,
0:         -0.2386], device='cuda:0')
0: [DEBUG] Epoch 13, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.6350,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0856,     nan,     nan,  0.3150,     nan,     nan,
0:             nan,     nan,  0.2227,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2199,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2491,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2503,     nan, -0.2503,     nan,     nan,
0:         -0.2503,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2472,  0.2134,  0.1818,     nan,     nan,
0:         -0.0062,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2503,     nan,     nan, -0.2503,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1113,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.5894,     nan,
0:             nan,     nan,     nan,  0.1585,     nan,     nan,     nan,     nan,  0.1958,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1078,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2363,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2503,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.0055,     nan])
0: [DEBUG] Epoch 13, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9355, -0.8705, -0.8145, -0.8083, -0.8428, -0.9051, -0.9424, -0.9537, -0.9552, -0.9541, -0.9874, -1.0435,
0:         -1.0988, -1.1197, -1.1027, -1.0388, -0.9676, -0.9033, -0.8624, -0.7857, -0.7275, -0.7388, -0.8151, -0.8877,
0:         -0.9336], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2460,  0.2560,  0.2158,  0.1410,  0.0557, -0.0140, -0.0854, -0.1414, -0.1838, -0.2042, -0.1959, -0.1688,
0:         -0.1512, -0.1638, -0.2011, -0.2311, -0.2207, -0.1808,  0.2199,  0.2367,  0.2094,  0.1292,  0.0309, -0.0574,
0:         -0.1428], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4001, -0.3734, -0.3417, -0.3035, -0.2682, -0.2362, -0.2222, -0.2231, -0.2389, -0.2625, -0.2928, -0.3322,
0:         -0.3645, -0.3980, -0.4408, -0.4702, -0.5029, -0.5218, -0.3698, -0.3372, -0.2957, -0.2547, -0.2232, -0.2028,
0:         -0.1967], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7109, -0.7271, -0.6569, -0.8224, -1.0154, -1.0928, -1.1911, -1.2218, -1.2004, -1.1577, -1.1546, -1.1673,
0:         -1.0050, -0.9161, -0.9991, -0.9306, -0.7856, -0.6962, -0.6755, -0.9106, -1.0451, -1.2345, -1.3409, -1.3799,
0:         -1.5108], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1651, -0.1326, -0.1084, -0.0977, -0.0878, -0.0757, -0.0585, -0.0424, -0.0247, -0.0101, -0.0043, -0.0094,
0:         -0.0256, -0.0512, -0.0783, -0.1055, -0.1249, -0.1399, -0.1561, -0.1841, -0.2242, -0.2635, -0.2843, -0.2818,
0:         -0.2749], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1833, -0.1713, -0.1802, -0.1671, -0.1598, -0.1561, -0.1554, -0.1647, -0.1649, -0.1958, -0.1901, -0.1862,
0:         -0.1685, -0.1664, -0.1592, -0.1619, -0.1633, -0.1735, -0.2185, -0.2071, -0.2016, -0.1778, -0.1715, -0.1787,
0:         -0.1714], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.21200698614120483; velocity_v: 0.3756117820739746; specific_humidity: 0.17016030848026276; velocity_z: 0.5809884071350098; temperature: 0.14324939250946045; total_precip: 0.5601030588150024; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20817668735980988; velocity_v: 0.47253015637397766; specific_humidity: 0.20325395464897156; velocity_z: 0.49942901730537415; temperature: 0.17970219254493713; total_precip: 0.8111646175384521; 
0: epoch: 13 [1/5 (20%)]	Loss: 0.68563 : 0.33620 :: 0.20321 (1.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24739450216293335; velocity_v: 0.5698468089103699; specific_humidity: 0.19891640543937683; velocity_z: 0.5994189381599426; temperature: 0.1956671178340912; total_precip: 0.7048554420471191; 
0: epoch: 13 [2/5 (40%)]	Loss: 0.70486 : 0.38584 :: 0.20981 (16.24 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26323744654655457; velocity_v: 0.5246135592460632; specific_humidity: 0.22058124840259552; velocity_z: 0.6212325692176819; temperature: 0.21899814903736115; total_precip: 0.6754095554351807; 
0: epoch: 13 [3/5 (60%)]	Loss: 0.67541 : 0.38550 :: 0.21467 (16.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21850307285785675; velocity_v: 0.4538234770298004; specific_humidity: 0.1948423683643341; velocity_z: 0.5095841884613037; temperature: 0.17412638664245605; total_precip: 0.9119598269462585; 
0: epoch: 13 [4/5 (80%)]	Loss: 0.91196 : 0.37808 :: 0.20246 (16.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 13 : 0.3770972192287445
0: validation loss for velocity_u : 0.17659802734851837
0: validation loss for velocity_v : 0.46913647651672363
0: validation loss for specific_humidity : 0.15033461153507233
0: validation loss for velocity_z : 0.5332697033882141
0: validation loss for temperature : 0.1397087126970291
0: validation loss for total_precip : 0.7935354113578796
0: 14 : 09:50:47 :: batch_size = 96, lr = 1.4871117700906175e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 14, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2930, -0.3018, -0.3162, -0.3332, -0.3534, -0.3695, -0.3745, -0.3701, -0.3672, -0.3804, -0.4116, -0.4483,
0:         -0.4800, -0.5071, -0.5318, -0.5545, -0.5692, -0.5717, -0.3037, -0.2983, -0.3064, -0.3256, -0.3526, -0.3757,
0:         -0.3853], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0188, -0.0236, -0.0147, -0.0002,  0.0048,  0.0006, -0.0047, -0.0074, -0.0099, -0.0139, -0.0188, -0.0257,
0:         -0.0313, -0.0354, -0.0396, -0.0530, -0.0768, -0.0831,  0.0029, -0.0224, -0.0296, -0.0319, -0.0398, -0.0478,
0:         -0.0460], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4092, -0.3817, -0.3682, -0.3753, -0.3814, -0.3811, -0.4024, -0.4649, -0.5493, -0.6184, -0.6546, -0.6659,
0:         -0.6663, -0.6630, -0.6551, -0.6480, -0.6447, -0.6407, -0.4514, -0.4206, -0.4001, -0.3993, -0.4012, -0.3976,
0:         -0.4139], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3654,  0.3721,  0.3721,  0.4056,  0.4257,  0.4637,  0.4101,  0.1955, -0.0818, -0.3903, -0.4350, -0.3411,
0:         -0.1265,  0.0636,  0.0636, -0.0438, -0.1667, -0.0057,  0.3855,  0.3877,  0.3944,  0.4704,  0.5263,  0.5889,
0:          0.5599], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.2264,  0.1152,  0.0282, -0.0342, -0.1334, -0.3085, -0.5331, -0.6900, -0.6823, -0.5021, -0.2226,  0.0177,
0:          0.1389,  0.1255,  0.0514,  0.0460,  0.1495,  0.2565,  0.1951, -0.1137, -0.5633, -0.8787, -0.9064, -0.7693,
0:         -0.6272], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2431, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2267, -0.2349, -0.2360, -0.2501, -0.2513,
0:         -0.2513, -0.2513, -0.2513, -0.2255, -0.2384, -0.2267, -0.1985, -0.2478, -0.2478, -0.2243, -0.1679, -0.1480,
0:         -0.0576], device='cuda:0')
0: [DEBUG] Epoch 14, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,
0:             nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2513, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2513,     nan,     nan,     nan, -0.2407,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2161,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 14, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7693, -0.7383, -0.7403, -0.7738, -0.8357, -0.8569, -0.8393, -0.7700, -0.7140, -0.7021, -0.7471, -0.8504,
0:         -0.9507, -0.9945, -0.9668, -0.8652, -0.7517, -0.6413, -0.7738, -0.7196, -0.6931, -0.7407, -0.8282, -0.8706,
0:         -0.8537], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0026,  0.0295,  0.0280,  0.0194,  0.0097,  0.0139,  0.0152,  0.0149,  0.0119,  0.0136,  0.0202,  0.0377,
0:          0.0450,  0.0183, -0.0400, -0.0776, -0.0798, -0.0434, -0.0216,  0.0118,  0.0209,  0.0110, -0.0043, -0.0112,
0:         -0.0153], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7552, -0.7524, -0.7520, -0.7379, -0.7245, -0.7185, -0.7204, -0.7293, -0.7418, -0.7543, -0.7576, -0.7534,
0:         -0.7356, -0.7139, -0.7008, -0.6899, -0.6926, -0.6971, -0.7457, -0.7424, -0.7356, -0.7225, -0.7111, -0.7087,
0:         -0.7096], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2610, 0.3890, 0.4767, 0.3291, 0.2171, 0.2745, 0.2317, 0.2154, 0.2644, 0.2084, 0.2385, 0.3345, 0.3288, 0.3302,
0:         0.2829, 0.3230, 0.4620, 0.3647, 0.2881, 0.1840, 0.1586, 0.1122, 0.0897, 0.1804, 0.1946], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.9024, 0.8134, 0.7832, 0.8109, 0.8771, 0.9535, 1.0211, 1.0693, 1.0968, 1.0920, 1.0397, 0.9505, 0.8385, 0.7344,
0:         0.6600, 0.6185, 0.6001, 0.5950, 0.5954, 0.6001, 0.6047, 0.5884, 0.5313, 0.4199, 0.2602], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2063, -0.1948, -0.2100, -0.1983, -0.1914, -0.1926, -0.1875, -0.1917, -0.1910, -0.2211, -0.2158, -0.2125,
0:         -0.2011, -0.1984, -0.1954, -0.1955, -0.1974, -0.2000, -0.2371, -0.2309, -0.2261, -0.2088, -0.2053, -0.2088,
0:         -0.2055], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.24897201359272003; velocity_v: 0.41737082600593567; specific_humidity: 0.2140268087387085; velocity_z: 0.5139923691749573; temperature: 0.25852829217910767; total_precip: 0.8817827105522156; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2143595963716507; velocity_v: 0.5055920481681824; specific_humidity: 0.18607045710086823; velocity_z: 0.5278596878051758; temperature: 0.19854585826396942; total_precip: 0.6670240163803101; 
0: epoch: 14 [1/5 (20%)]	Loss: 0.77440 : 0.36929 :: 0.20217 (2.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20960313081741333; velocity_v: 0.4591011106967926; specific_humidity: 0.1730928272008896; velocity_z: 0.5461645722389221; temperature: 0.24681153893470764; total_precip: 0.6210862994194031; 
0: epoch: 14 [2/5 (40%)]	Loss: 0.62109 : 0.34386 :: 0.20122 (15.89 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22346650063991547; velocity_v: 0.6154206991195679; specific_humidity: 0.17625586688518524; velocity_z: 0.6733483076095581; temperature: 0.17135310173034668; total_precip: 0.6818464994430542; 
0: epoch: 14 [3/5 (60%)]	Loss: 0.68185 : 0.39128 :: 0.20358 (16.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22989694774150848; velocity_v: 0.543887734413147; specific_humidity: 0.17292512953281403; velocity_z: 0.6184707880020142; temperature: 0.15588678419589996; total_precip: 0.6423162817955017; 
0: epoch: 14 [4/5 (80%)]	Loss: 0.64232 : 0.36230 :: 0.20505 (16.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 14 : 0.3438591957092285
0: validation loss for velocity_u : 0.19591838121414185
0: validation loss for velocity_v : 0.4542113244533539
0: validation loss for specific_humidity : 0.14664214849472046
0: validation loss for velocity_z : 0.60062575340271
0: validation loss for temperature : 0.1176043227314949
0: validation loss for total_precip : 0.5481534004211426
0: 15 : 09:54:43 :: batch_size = 96, lr = 1.4508407513079195e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 15, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2827, 0.3019, 0.3043, 0.2448, 0.1583, 0.1165, 0.1274, 0.1494, 0.1560, 0.1629, 0.2039, 0.2587, 0.2634, 0.2217,
0:         0.1888, 0.1704, 0.1509, 0.1355, 0.2399, 0.2556, 0.2539, 0.1954, 0.1042, 0.0581, 0.0765], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9383, -0.9521, -0.9281, -0.8750, -0.8183, -0.7761, -0.7542, -0.7433, -0.7407, -0.7682, -0.8256, -0.8623,
0:         -0.8529, -0.8333, -0.8274, -0.8219, -0.8084, -0.7923, -0.8763, -0.9020, -0.8931, -0.8420, -0.7826, -0.7510,
0:         -0.7516], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0138, -0.0399, -0.1090, -0.2036, -0.2744, -0.3297, -0.3698, -0.4071, -0.4427, -0.4650, -0.4798, -0.4963,
0:         -0.5136, -0.5241, -0.5275, -0.5292, -0.5353, -0.5473, -0.0940, -0.1367, -0.2032, -0.2853, -0.3529, -0.3987,
0:         -0.4300], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1150, -0.3812,  0.5383,  1.2963,  0.9440,  0.4205,  0.2688,  0.3801,  0.5437,  1.0509,  1.7402,  1.4174,
0:          0.6364,  0.8055,  1.0923,  0.7717,  0.5568,  0.5579,  0.3354, -0.0322,  0.4248,  0.8284,  0.6233,  0.3943,
0:          0.4641], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.7168, -1.8149, -1.8101, -1.7158, -1.6163, -1.5662, -1.5486, -1.5274, -1.4877, -1.4342, -1.4019, -1.4078,
0:         -1.4064, -1.3693, -1.3284, -1.2973, -1.2702, -1.2548, -1.2445, -1.2288, -1.2079, -1.1904, -1.2025, -1.2425,
0:         -1.2721], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297,
0:         -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297,
0:         -0.2297], device='cuda:0')
0: [DEBUG] Epoch 15, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan, -0.2297,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2297,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2297,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan,
0:         -0.2297,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan,     nan, -0.2297,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan,     nan,     nan,     nan,
0:             nan, -0.2297,     nan, -0.2297,     nan,     nan, -0.2297,     nan,     nan,     nan,     nan,     nan,
0:         -0.2297,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2297,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2297,
0:             nan, -0.2297,     nan,     nan,     nan, -0.2297,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 15, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3765, -0.3145, -0.2463, -0.2049, -0.1998, -0.2151, -0.2157, -0.1967, -0.1669, -0.1273, -0.1058, -0.0984,
0:         -0.0838, -0.0346,  0.0582,  0.1812,  0.2995,  0.3822, -0.3169, -0.2801, -0.2400, -0.2380, -0.2712, -0.3007,
0:         -0.3082], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1789, -0.1075, -0.0725, -0.0595, -0.0690, -0.0682, -0.0779, -0.0832, -0.0855, -0.0764, -0.0598, -0.0390,
0:         -0.0507, -0.1056, -0.1933, -0.2648, -0.2910, -0.2703, -0.2263, -0.1513, -0.1064, -0.0976, -0.1055, -0.1168,
0:         -0.1299], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3618, -0.3617, -0.3725, -0.3846, -0.3963, -0.4105, -0.4300, -0.4483, -0.4682, -0.4881, -0.5080, -0.5233,
0:         -0.5288, -0.5356, -0.5503, -0.5633, -0.5898, -0.6018, -0.3868, -0.3818, -0.3868, -0.3886, -0.3995, -0.4128,
0:         -0.4231], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.7393, 1.4358, 1.0031, 1.3482, 1.7855, 1.3408, 0.8726, 0.8432, 0.5948, 0.7053, 1.1937, 0.7604, 0.3373, 0.7635,
0:         0.7567, 0.3928, 0.5054, 0.8401, 2.3007, 1.9235, 1.1314, 1.2377, 1.8215, 1.5421, 1.0032], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.0171, 0.0325, 0.0467, 0.0560, 0.0614, 0.0678, 0.0679, 0.0560, 0.0360, 0.0156, 0.0063, 0.0147, 0.0376, 0.0595,
0:         0.0693, 0.0645, 0.0626, 0.0707, 0.0875, 0.0988, 0.0941, 0.0763, 0.0620, 0.0587, 0.0549], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2118, -0.2029, -0.2160, -0.1981, -0.1924, -0.1921, -0.1908, -0.1969, -0.1989, -0.2311, -0.2201, -0.2157,
0:         -0.2004, -0.1995, -0.1952, -0.2014, -0.2014, -0.2076, -0.2419, -0.2335, -0.2273, -0.2076, -0.2040, -0.2048,
0:         -0.2041], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.21671921014785767; velocity_v: 0.446289986371994; specific_humidity: 0.1588129699230194; velocity_z: 0.49405771493911743; temperature: 0.14118227362632751; total_precip: 0.6516165137290955; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2143300175666809; velocity_v: 0.47008776664733887; specific_humidity: 0.18636642396450043; velocity_z: 0.6576361060142517; temperature: 0.4113093614578247; total_precip: 1.4565379619598389; 
0: epoch: 15 [1/5 (20%)]	Loss: 1.05408 : 0.42707 :: 0.19995 (2.21 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2224651426076889; velocity_v: 0.5840211510658264; specific_humidity: 0.18157653510570526; velocity_z: 0.5645931959152222; temperature: 0.23695261776447296; total_precip: 0.5149742960929871; 
0: epoch: 15 [2/5 (40%)]	Loss: 0.51497 : 0.35079 :: 0.20413 (16.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22971275448799133; velocity_v: 0.4755750894546509; specific_humidity: 0.1858890950679779; velocity_z: 0.5803493857383728; temperature: 0.1808369904756546; total_precip: 0.7124406099319458; 
0: epoch: 15 [3/5 (60%)]	Loss: 0.71244 : 0.36137 :: 0.20612 (16.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23448389768600464; velocity_v: 0.5053103566169739; specific_humidity: 0.15999241173267365; velocity_z: 0.6809327006340027; temperature: 0.17047014832496643; total_precip: 0.7236200571060181; 
0: epoch: 15 [4/5 (80%)]	Loss: 0.72362 : 0.37938 :: 0.21224 (16.21 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 15 : 0.36175796389579773
0: validation loss for velocity_u : 0.197462260723114
0: validation loss for velocity_v : 0.3991396427154541
0: validation loss for specific_humidity : 0.16162176430225372
0: validation loss for velocity_z : 0.5713261961936951
0: validation loss for temperature : 0.1309593915939331
0: validation loss for total_precip : 0.7100376486778259
0: 16 : 09:58:42 :: batch_size = 96, lr = 1.4154543915199217e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 16, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4933, -0.4989, -0.5175, -0.5380, -0.5544, -0.5771, -0.6149, -0.6649, -0.7169, -0.7626, -0.7931, -0.8071,
0:         -0.8134, -0.8260, -0.8499, -0.8759, -0.8895, -0.8912, -0.5225, -0.5281, -0.5474, -0.5665, -0.5820, -0.6023,
0:         -0.6358], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5884, -0.6140, -0.6386, -0.6561, -0.6699, -0.6721, -0.6690, -0.6756, -0.6980, -0.7164, -0.7177, -0.7083,
0:         -0.7050, -0.7216, -0.7486, -0.7721, -0.7804, -0.7806, -0.5636, -0.5969, -0.6269, -0.6486, -0.6649, -0.6677,
0:         -0.6611], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.5516, 1.5974, 1.6301, 1.6729, 1.7159, 1.7557, 1.7859, 1.8270, 1.7914, 1.7121, 1.6496, 1.6020, 1.5810, 1.5854,
0:         1.6099, 1.6545, 1.7192, 1.8128, 1.5807, 1.6510, 1.7429, 1.7900, 1.8261, 1.8570, 1.8630], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1480, -0.0992, -0.3046, -0.2819, -0.1401, -0.1072,  0.0006,  0.1378,  0.2717,  0.2184,  0.1424,  0.1310,
0:          0.1242,  0.1288,  0.1140,  0.1685,  0.1696,  0.3170, -0.0913, -0.2705, -0.3454, -0.1605, -0.0085, -0.0142,
0:          0.0607], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1628, -0.2512, -0.3771, -0.4950, -0.5965, -0.6102, -0.5391, -0.4865, -0.4367, -0.3604, -0.2312, -0.0768,
0:          0.0537,  0.1356,  0.1450,  0.1236,  0.1078,  0.1181,  0.1381,  0.1686,  0.1880,  0.1150,  0.0216, -0.0211,
0:         -0.0033], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1365, -0.1917, -0.1953, -0.2035, -0.2000, -0.1741, -0.1247,  0.3081,  0.8692, -0.0576, -0.1623, -0.1917,
0:         -0.1976, -0.1294, -0.0529,  0.2176,  0.4222,  0.7751,  0.0153, -0.1470, -0.1517,  0.2952,  0.4034,  1.0562,
0:          1.5032], device='cuda:0')
0: [DEBUG] Epoch 16, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2600,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2470,     nan,     nan,     nan,     nan, -0.1741,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1682, -0.1776,     nan, -0.0494,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2011,     nan, -0.0659,     nan,     nan,     nan,
0:             nan, -0.2376,     nan,     nan,     nan,     nan,     nan, -0.2317,     nan,     nan, -0.2376, -0.2470,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1388,     nan,
0:         -0.2247,     nan,     nan, -0.2376,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2258,
0:         -0.2270, -0.1988,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1659,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2153,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2411,     nan,     nan,     nan, -0.2400,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0471,     nan,     nan, -0.2223,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0518,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0847,     nan,     nan, -0.2153,     nan,
0:             nan,     nan, -0.1812,     nan,     nan, -0.1823,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1318,     nan])
0: [DEBUG] Epoch 16, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9834, -0.8845, -0.8279, -0.8290, -0.8856, -0.9391, -0.9466, -0.9061, -0.8593, -0.8223, -0.8368, -0.8981,
0:         -0.9603, -0.9801, -0.9362, -0.8430, -0.7510, -0.6632, -0.9102, -0.7973, -0.7298, -0.7544, -0.8490, -0.9070,
0:         -0.9226], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0238,  0.0207,  0.0361,  0.0310, -0.0021, -0.0356, -0.0793, -0.1113, -0.1206, -0.1114, -0.0777, -0.0382,
0:         -0.0203, -0.0476, -0.1101, -0.1562, -0.1672, -0.1290, -0.0306,  0.0237,  0.0519,  0.0457,  0.0124, -0.0355,
0:         -0.0781], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4095, 1.3345, 1.2494, 1.1603, 1.0749, 1.0244, 1.0324, 1.1103, 1.2461, 1.4211, 1.6070, 1.7166, 1.7870, 1.8091,
0:         1.7765, 1.7650, 1.7899, 1.8235, 1.2820, 1.2169, 1.1594, 1.0984, 1.0589, 1.0343, 1.0785], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2546, 0.3362, 0.4597, 0.3478, 0.2363, 0.3614, 0.3580, 0.3143, 0.3962, 0.4453, 0.5300, 0.6377, 0.6312, 0.5519,
0:         0.5129, 0.6185, 0.7377, 0.6617, 0.3731, 0.2862, 0.2608, 0.1601, 0.1374, 0.3066, 0.3198], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.4248,  0.4515,  0.4996,  0.5585,  0.6050,  0.6079,  0.5301,  0.3625,  0.1314, -0.1294, -0.3777, -0.5714,
0:         -0.6817, -0.7080, -0.6664, -0.5884, -0.5034, -0.4314, -0.3820, -0.3588, -0.3564, -0.3758, -0.4055, -0.4303,
0:         -0.4465], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0753, -0.0752, -0.0909, -0.1004, -0.1048, -0.1156, -0.1238, -0.1194, -0.1306, -0.0898, -0.0866, -0.1026,
0:         -0.1050, -0.1095, -0.1194, -0.1269, -0.1259, -0.1286, -0.1052, -0.1087, -0.1162, -0.1119, -0.1195, -0.1211,
0:         -0.1256], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2668115198612213; velocity_v: 0.45401543378829956; specific_humidity: 0.21304985880851746; velocity_z: 0.5084035396575928; temperature: 0.19880440831184387; total_precip: 0.6366524696350098; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23717407882213593; velocity_v: 0.5714558959007263; specific_humidity: 0.1673576831817627; velocity_z: 0.6027270555496216; temperature: 0.15716193616390228; total_precip: 0.6056380271911621; 
0: epoch: 16 [1/5 (20%)]	Loss: 0.62115 : 0.35244 :: 0.20809 (2.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22455981373786926; velocity_v: 0.5174144506454468; specific_humidity: 0.16301268339157104; velocity_z: 0.6585944294929504; temperature: 0.21520230174064636; total_precip: 0.8804221153259277; 
0: epoch: 16 [2/5 (40%)]	Loss: 0.88042 : 0.40901 :: 0.21033 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2575738728046417; velocity_v: 0.4311429262161255; specific_humidity: 0.18909281492233276; velocity_z: 0.6198505759239197; temperature: 0.16671252250671387; total_precip: 0.6799668073654175; 
0: epoch: 16 [3/5 (60%)]	Loss: 0.67997 : 0.35709 :: 0.21553 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2340279072523117; velocity_v: 0.49985471367836; specific_humidity: 0.16676747798919678; velocity_z: 0.7514102458953857; temperature: 0.1682504415512085; total_precip: 0.7259551882743835; 
0: epoch: 16 [4/5 (80%)]	Loss: 0.72596 : 0.39097 :: 0.21033 (16.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 16 : 0.3816162049770355
0: validation loss for velocity_u : 0.17809264361858368
0: validation loss for velocity_v : 0.4457603991031647
0: validation loss for specific_humidity : 0.14992055296897888
0: validation loss for velocity_z : 0.6371854543685913
0: validation loss for temperature : 0.1443122774362564
0: validation loss for total_precip : 0.7344258427619934
0: 17 : 10:02:45 :: batch_size = 96, lr = 1.3809311136779726e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 17, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.9061, 1.9162, 1.9302, 1.9545, 1.9751, 1.9901, 2.0106, 2.0349, 2.0581, 2.0881, 2.1213, 2.1442, 2.1589, 2.1665,
0:         2.1661, 2.1631, 2.1486, 2.1174, 1.8478, 1.8727, 1.9068, 1.9463, 1.9758, 1.9978, 2.0225], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3512, 1.3601, 1.3702, 1.3792, 1.3849, 1.3877, 1.3909, 1.3948, 1.3965, 1.3965, 1.3939, 1.3864, 1.3739, 1.3547,
0:         1.3286, 1.2943, 1.2501, 1.1975, 1.3193, 1.3361, 1.3549, 1.3700, 1.3801, 1.3857, 1.3894], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7304, -0.7376, -0.7445, -0.7496, -0.7540, -0.7582, -0.7616, -0.7654, -0.7689, -0.7713, -0.7731, -0.7744,
0:         -0.7761, -0.7778, -0.7790, -0.7803, -0.7813, -0.7824, -0.7496, -0.7553, -0.7597, -0.7628, -0.7658, -0.7683,
0:         -0.7706], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6048, 0.8423, 0.9192, 0.9764, 0.8994, 0.5279, 0.1564, 0.0794, 0.2663, 0.5916, 0.8972, 0.9698, 0.7455, 0.4114,
0:         0.2113, 0.1849, 0.1915, 0.0618, 0.2729, 0.5015, 0.6862, 0.8686, 0.8599, 0.5653, 0.1937], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 1.2706,  1.2641,  1.2602,  1.2629,  1.2674,  1.2602,  1.2392,  1.2148,  1.1897,  1.1685,  1.1556,  1.1421,
0:          1.1168,  1.0688,  0.9928,  0.8945,  0.7732,  0.6208,  0.4362,  0.2066, -0.0792, -0.4102, -0.7364, -0.9700,
0:         -1.0587], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527,
0:         -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527, -0.2527,
0:         -0.2527], device='cuda:0')
0: [DEBUG] Epoch 17, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,
0:             nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan, -0.2527,
0:             nan,     nan, -0.2527,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2174,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,
0:             nan,     nan, -0.2505,     nan,     nan,     nan,     nan,     nan,     nan, -0.2083,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2527,     nan,     nan,     nan,     nan, -0.2527,     nan,     nan,     nan,
0:         -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 17, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9944, 0.9837, 0.9897, 0.9857, 0.9759, 0.9547, 0.9395, 0.9236, 0.8925, 0.8389, 0.7605, 0.6719, 0.5922, 0.5725,
0:         0.6104, 0.6952, 0.7906, 0.8671, 0.8759, 0.8596, 0.8511, 0.8307, 0.7886, 0.7599, 0.7378], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2104, 0.2229, 0.2187, 0.2063, 0.1845, 0.1671, 0.1410, 0.1130, 0.0838, 0.0766, 0.0835, 0.1023, 0.1149, 0.0888,
0:         0.0349, 0.0053, 0.0109, 0.0495, 0.1676, 0.1786, 0.1819, 0.1591, 0.1314, 0.1093, 0.0844], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8003, -0.7993, -0.7945, -0.7877, -0.7779, -0.7698, -0.7726, -0.7710, -0.7725, -0.7783, -0.7827, -0.7843,
0:         -0.7792, -0.7749, -0.7779, -0.7822, -0.8001, -0.8146, -0.8012, -0.7951, -0.7882, -0.7763, -0.7688, -0.7632,
0:         -0.7586], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9526, 1.1040, 1.1848, 1.0800, 0.9873, 1.0407, 1.0723, 1.0000, 0.7760, 0.5429, 0.4153, 0.3937, 0.5522, 0.6057,
0:         0.3531, 0.2449, 0.2794, 0.1016, 1.0445, 0.9667, 1.0257, 1.0171, 0.9888, 1.0797, 1.1198], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 1.9417,  1.9234,  1.8887,  1.8427,  1.7810,  1.6977,  1.5678,  1.3871,  1.1808,  0.9780,  0.8037,  0.6626,
0:          0.5488,  0.4434,  0.3318,  0.2117,  0.0901, -0.0339, -0.1535, -0.2704, -0.3730, -0.4533, -0.5182, -0.5938,
0:         -0.7187], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1871, -0.1832, -0.1934, -0.1847, -0.1821, -0.1806, -0.1801, -0.1756, -0.1819, -0.2056, -0.1969, -0.2007,
0:         -0.1852, -0.1873, -0.1807, -0.1866, -0.1852, -0.1888, -0.2144, -0.2099, -0.2096, -0.1939, -0.1930, -0.1916,
0:         -0.1902], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2443131059408188; velocity_v: 0.45209550857543945; specific_humidity: 0.1880960613489151; velocity_z: 0.6965178847312927; temperature: 0.16146181523799896; total_precip: 0.9971540570259094; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24309878051280975; velocity_v: 0.4934842586517334; specific_humidity: 0.14553236961364746; velocity_z: 0.7812231183052063; temperature: 0.14861655235290527; total_precip: 1.0687445402145386; 
0: epoch: 17 [1/5 (20%)]	Loss: 1.03295 : 0.43526 :: 0.21357 (2.50 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20877167582511902; velocity_v: 0.45491263270378113; specific_humidity: 0.19105957448482513; velocity_z: 0.6298602819442749; temperature: 0.16508716344833374; total_precip: 0.8285285830497742; 
0: epoch: 17 [2/5 (40%)]	Loss: 0.82853 : 0.37909 :: 0.20824 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22847771644592285; velocity_v: 0.40650904178619385; specific_humidity: 0.14913947880268097; velocity_z: 0.5315407514572144; temperature: 0.15730160474777222; total_precip: 0.6715472936630249; 
0: epoch: 17 [3/5 (60%)]	Loss: 0.67155 : 0.32665 :: 0.20339 (15.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20170173048973083; velocity_v: 0.44852951169013977; specific_humidity: 0.17236700654029846; velocity_z: 0.5491062998771667; temperature: 0.1536325067281723; total_precip: 0.6364366412162781; 
0: epoch: 17 [4/5 (80%)]	Loss: 0.63644 : 0.32823 :: 0.20124 (15.82 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 17 : 0.36803901195526123
0: validation loss for velocity_u : 0.19384445250034332
0: validation loss for velocity_v : 0.4212510585784912
0: validation loss for specific_humidity : 0.1361762434244156
0: validation loss for velocity_z : 0.5644259452819824
0: validation loss for temperature : 0.14815279841423035
0: validation loss for total_precip : 0.7443835139274597
0: 18 : 10:06:44 :: batch_size = 96, lr = 1.3472498670029002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 18, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.3840, -1.4115, -1.4244, -1.4295, -1.4276, -1.4176, -1.4215, -1.4548, -1.5123, -1.5750, -1.6189, -1.6357,
0:         -1.6334, -1.6246, -1.6041, -1.5740, -1.5463, -1.5265, -1.3258, -1.3627, -1.3833, -1.3956, -1.4007, -1.3902,
0:         -1.3845], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6629, 0.6691, 0.6864, 0.7228, 0.7842, 0.8606, 0.9310, 0.9744, 0.9812, 0.9478, 0.8939, 0.8567, 0.8499, 0.8699,
0:         0.8899, 0.8823, 0.8472, 0.7852, 0.7292, 0.7096, 0.7005, 0.7065, 0.7356, 0.7920, 0.8592], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([3.4634, 3.3205, 3.1439, 2.9490, 2.8944, 2.9271, 2.9702, 3.0634, 3.1488, 3.3154, 3.4993, 3.5795, 3.5802, 3.3089,
0:         2.8237, 2.3638, 2.1203, 1.9692, 3.5239, 3.3794, 3.2039, 3.0020, 2.9313, 2.9523, 2.9748], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-4.5380, -4.9241, -4.1722, -2.7805, -1.0208, -0.0646,  0.0768,  0.2721,  0.4292,  0.6604,  0.7884,  1.0241,
0:          1.0443,  1.2238,  0.5168, -1.4181, -1.7390, -1.9814, -2.9735, -4.4976, -4.6368, -3.5773, -1.6111, -0.4349,
0:         -0.0511], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.0292, 0.0266, 0.0405, 0.0956, 0.1269, 0.1462, 0.2291, 0.3545, 0.4371, 0.4473, 0.3750, 0.2728, 0.1877, 0.1701,
0:         0.3317, 0.5533, 0.6785, 0.7537, 0.7898, 0.7994, 0.8467, 0.9243, 0.9942, 1.0048, 0.8917], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([2.6363, 0.4431, 0.0578, 0.1291, 0.1895, 0.5735, 0.8489, 0.7100, 0.8102, 5.1290, 2.9141, 0.5288, 0.3018, 0.3441,
0:         0.5349, 0.7269, 1.0542, 1.1653, 8.0988, 4.4950, 1.5445, 0.3163, 0.5337, 0.6194, 0.6158], device='cuda:0')
0: [DEBUG] Epoch 18, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2344,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2813,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  2.4370,
0:             nan,     nan,     nan,     nan,  5.8887,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  2.0119,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  3.5035,     nan,     nan,  4.5131,     nan,  3.7716,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.2172,
0:          2.9165,  4.7474,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  6.2897,     nan,     nan,     nan,     nan,     nan,     nan,  9.8814,  9.2486,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, 10.1990,     nan,     nan,     nan,     nan,     nan,
0:             nan,  2.9672,     nan,  8.5034,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  3.6749,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.1967,  1.3682,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2499,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0666,     nan,     nan,     nan,     nan,
0:          1.7378,     nan,     nan,     nan,     nan,     nan,  2.2329,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  1.6701,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  2.4262])
0: [DEBUG] Epoch 18, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1962, -1.1220, -1.0725, -1.0692, -1.1029, -1.1339, -1.1314, -1.0889, -1.0579, -1.0566, -1.0918, -1.1647,
0:         -1.2119, -1.1987, -1.1166, -0.9904, -0.8857, -0.7956, -1.1320, -1.0513, -1.0161, -1.0451, -1.1255, -1.1579,
0:         -1.1385], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1693, 0.1737, 0.1685, 0.1650, 0.1617, 0.1663, 0.1595, 0.1433, 0.1233, 0.1057, 0.0968, 0.0994, 0.0994, 0.0772,
0:         0.0372, 0.0259, 0.0384, 0.0609, 0.1317, 0.1443, 0.1502, 0.1471, 0.1370, 0.1256, 0.1132], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4935, 1.4746, 1.4902, 1.5284, 1.5895, 1.6646, 1.7765, 1.8950, 2.0048, 2.0904, 2.1669, 2.1787, 2.1977, 2.2270,
0:         2.2446, 2.2976, 2.3707, 2.4281, 1.3702, 1.3691, 1.4090, 1.4654, 1.5590, 1.6558, 1.7933], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0853,  0.0962,  0.2664,  0.2286,  0.1697,  0.0614, -0.3038, -0.4945, -0.4533, -0.4274, -0.3119, -0.1025,
0:          0.1566,  0.2301,  0.1014,  0.0648,  0.1033,  0.0803,  0.1122,  0.0816,  0.0910,  0.0533,  0.0104, -0.0972,
0:         -0.4234], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 1.2290,  1.1544,  1.0668,  0.9731,  0.8911,  0.8221,  0.7609,  0.6981,  0.6259,  0.5294,  0.4023,  0.2596,
0:          0.1219,  0.0171, -0.0453, -0.0746, -0.0927, -0.1116, -0.1305, -0.1466, -0.1504, -0.1469, -0.1470, -0.1551,
0:         -0.1697], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([1.3107, 1.4273, 1.5474, 1.6229, 1.7608, 1.7260, 1.7247, 1.7081, 1.5889, 1.5015, 1.5957, 1.7114, 1.9276, 1.9952,
0:         2.0206, 1.9885, 1.8720, 1.7415, 1.7234, 1.8352, 2.1419, 2.3552, 2.5028, 2.5660, 2.4531], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.24622640013694763; velocity_v: 0.5231554508209229; specific_humidity: 0.16782645881175995; velocity_z: 0.6830335259437561; temperature: 0.20235328376293182; total_precip: 0.757493793964386; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21554400026798248; velocity_v: 0.45203500986099243; specific_humidity: 0.1849498748779297; velocity_z: 0.5898541808128357; temperature: 0.16746234893798828; total_precip: 0.9074640870094299; 
0: epoch: 18 [1/5 (20%)]	Loss: 0.83248 : 0.39173 :: 0.20283 (2.50 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22797730565071106; velocity_v: 0.4237038493156433; specific_humidity: 0.17504142224788666; velocity_z: 0.5408971309661865; temperature: 0.15567530691623688; total_precip: 0.5397654175758362; 
0: epoch: 18 [2/5 (40%)]	Loss: 0.53977 : 0.31187 :: 0.20624 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24697113037109375; velocity_v: 0.48736995458602905; specific_humidity: 0.1975332498550415; velocity_z: 0.6852244734764099; temperature: 0.20427633821964264; total_precip: 0.9066346883773804; 
0: epoch: 18 [3/5 (60%)]	Loss: 0.90663 : 0.42045 :: 0.20616 (16.39 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23204034566879272; velocity_v: 0.45228779315948486; specific_humidity: 0.16518375277519226; velocity_z: 0.7098550200462341; temperature: 0.1772260069847107; total_precip: 0.5121788382530212; 
0: epoch: 18 [4/5 (80%)]	Loss: 0.51218 : 0.34180 :: 0.21284 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 18 : 0.3065456449985504
0: validation loss for velocity_u : 0.18786652386188507
0: validation loss for velocity_v : 0.41057634353637695
0: validation loss for specific_humidity : 0.14129462838172913
0: validation loss for velocity_z : 0.4591239392757416
0: validation loss for temperature : 0.12722556293010712
0: validation loss for total_precip : 0.5131869912147522
0: 19 : 10:10:41 :: batch_size = 96, lr = 1.3143901141491711e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 19, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8332, -0.8316, -0.8299, -0.8273, -0.8220, -0.8135, -0.8038, -0.7965, -0.7930, -0.7913, -0.7883, -0.7828,
0:         -0.7747, -0.7645, -0.7524, -0.7396, -0.7275, -0.7162, -0.8660, -0.8576, -0.8502, -0.8425, -0.8323, -0.8186,
0:         -0.8044], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8187, -0.8296, -0.8409, -0.8515, -0.8610, -0.8698, -0.8798, -0.8913, -0.9034, -0.9164, -0.9297, -0.9422,
0:         -0.9523, -0.9603, -0.9677, -0.9751, -0.9812, -0.9862, -0.8170, -0.8283, -0.8404, -0.8517, -0.8614, -0.8704,
0:         -0.8798], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5986, -0.6017, -0.6048, -0.6085, -0.6120, -0.6149, -0.6181, -0.6221, -0.6260, -0.6299, -0.6341, -0.6378,
0:         -0.6411, -0.6441, -0.6473, -0.6503, -0.6535, -0.6566, -0.6132, -0.6163, -0.6194, -0.6222, -0.6256, -0.6284,
0:         -0.6312], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5956, 0.5934, 0.6222, 0.6797, 0.7128, 0.6885, 0.6376, 0.6111, 0.6222, 0.6465, 0.6598, 0.6553, 0.6531, 0.6598,
0:         0.6730, 0.6885, 0.7040, 0.7261, 0.5978, 0.6177, 0.6376, 0.6509, 0.6376, 0.6023, 0.5691], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.5550, 0.5296, 0.5058, 0.4815, 0.4538, 0.4229, 0.3921, 0.3643, 0.3394, 0.3161, 0.2924, 0.2678, 0.2420, 0.2165,
0:         0.1929, 0.1718, 0.1520, 0.1325, 0.1144, 0.0985, 0.0843, 0.0711, 0.0583, 0.0465, 0.0360], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2449, -0.2438, -0.2438, -0.2438, -0.2449, -0.2438, -0.2426, -0.2426, -0.2414, -0.2449, -0.2438, -0.2426,
0:         -0.2426, -0.2438, -0.2438, -0.2426, -0.2414, -0.2402, -0.2426, -0.2426, -0.2426, -0.2426, -0.2426, -0.2414,
0:         -0.2414], device='cuda:0')
0: [DEBUG] Epoch 19, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2414,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2391,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2438,     nan, -0.2414,     nan, -0.2402,     nan,     nan,     nan,
0:             nan, -0.2438,     nan,     nan, -0.2402,     nan,     nan, -0.2414,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2402,     nan,     nan,     nan, -0.2438,     nan,     nan, -0.2402,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2426,     nan,     nan, -0.2402,     nan,     nan, -0.2426,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,
0:             nan,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2414,     nan,     nan, -0.2402,     nan,     nan, -0.2449, -0.2438,     nan,
0:         -0.2391,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2402,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438, -0.2426,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 19, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5030, -0.4091, -0.3345, -0.3251, -0.3703, -0.4345, -0.4718, -0.4693, -0.4578, -0.4630, -0.5207, -0.6304,
0:         -0.7496, -0.8279, -0.8442, -0.7922, -0.7200, -0.6534, -0.5090, -0.4108, -0.3389, -0.3413, -0.4018, -0.4705,
0:         -0.5017], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0259,  0.1183,  0.1626,  0.1619,  0.1212,  0.0737,  0.0173, -0.0097, -0.0224, -0.0118,  0.0191,  0.0611,
0:          0.0877,  0.0842,  0.0629,  0.0457,  0.0517,  0.0641, -0.0522,  0.0452,  0.1140,  0.1298,  0.0959,  0.0436,
0:         -0.0111], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6317, -0.6267, -0.6217, -0.6170, -0.6128, -0.6165, -0.6320, -0.6487, -0.6667, -0.6827, -0.6968, -0.7042,
0:         -0.7036, -0.6992, -0.7024, -0.7070, -0.7242, -0.7343, -0.6330, -0.6204, -0.6130, -0.6019, -0.6053, -0.6152,
0:         -0.6298], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2475,  0.3330,  0.4324,  0.3510,  0.2842,  0.4168,  0.4832,  0.3895,  0.2987,  0.2345,  0.2024,  0.1932,
0:          0.1598,  0.0397, -0.1701, -0.1180,  0.1442,  0.1460,  0.2619,  0.1913,  0.2171,  0.1726,  0.1327,  0.2668,
0:          0.3648], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.5155, 0.5378, 0.5490, 0.5482, 0.5433, 0.5294, 0.5125, 0.4864, 0.4620, 0.4462, 0.4406, 0.4462, 0.4588, 0.4700,
0:         0.4697, 0.4564, 0.4374, 0.4181, 0.3990, 0.3805, 0.3551, 0.3331, 0.3213, 0.3115, 0.2908], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1942, -0.1912, -0.2015, -0.1903, -0.1901, -0.1912, -0.1920, -0.1926, -0.1978, -0.2133, -0.2050, -0.2042,
0:         -0.1919, -0.1969, -0.1930, -0.1996, -0.1972, -0.2034, -0.2190, -0.2107, -0.2123, -0.2020, -0.1996, -0.1989,
0:         -0.2016], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2393716275691986; velocity_v: 0.4794606864452362; specific_humidity: 0.18006214499473572; velocity_z: 0.5965908169746399; temperature: 0.18109965324401855; total_precip: 0.9342150688171387; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24808870255947113; velocity_v: 0.5144883990287781; specific_humidity: 0.14981460571289062; velocity_z: 0.5482180118560791; temperature: 0.1437530368566513; total_precip: 0.565488874912262; 
0: epoch: 19 [1/5 (20%)]	Loss: 0.74985 : 0.36623 :: 0.21057 (2.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2455786168575287; velocity_v: 0.5558593273162842; specific_humidity: 0.16412897408008575; velocity_z: 0.7205070853233337; temperature: 0.18283456563949585; total_precip: 0.5798040628433228; 
0: epoch: 19 [2/5 (40%)]	Loss: 0.57980 : 0.37522 :: 0.20746 (15.89 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2170647233724594; velocity_v: 0.45566216111183167; specific_humidity: 0.17513573169708252; velocity_z: 0.5207569003105164; temperature: 0.13803374767303467; total_precip: 0.7316095232963562; 
0: epoch: 19 [3/5 (60%)]	Loss: 0.73161 : 0.34220 :: 0.20468 (15.89 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21536342799663544; velocity_v: 0.5278527140617371; specific_humidity: 0.17167118191719055; velocity_z: 0.7369641065597534; temperature: 0.15989474952220917; total_precip: 0.9107041954994202; 
0: epoch: 19 [4/5 (80%)]	Loss: 0.91070 : 0.42102 :: 0.20660 (15.67 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 19 : 0.4103817641735077
0: validation loss for velocity_u : 0.2120986133813858
0: validation loss for velocity_v : 0.5029826760292053
0: validation loss for specific_humidity : 0.15614719688892365
0: validation loss for velocity_z : 0.5742349624633789
0: validation loss for temperature : 0.12396755814552307
0: validation loss for total_precip : 0.8928589224815369
0: 20 : 10:14:30 :: batch_size = 96, lr = 1.2823318186821183e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 20, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5361, -0.5274, -0.5120, -0.4899, -0.4630, -0.4340, -0.4047, -0.3765, -0.3499, -0.3259, -0.3061, -0.2908,
0:         -0.2796, -0.2692, -0.2565, -0.2389, -0.2164, -0.1899, -0.5693, -0.5765, -0.5747, -0.5635, -0.5448, -0.5220,
0:         -0.4972], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2749,  0.3013,  0.3238,  0.3379,  0.3416,  0.3334,  0.3136,  0.2827,  0.2425,  0.1969,  0.1498,  0.1054,
0:          0.0661,  0.0332,  0.0059, -0.0166, -0.0358, -0.0532,  0.2636,  0.2996,  0.3314,  0.3555,  0.3694,  0.3729,
0:          0.3653], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5897, -0.5868, -0.5850, -0.5834, -0.5816, -0.5793, -0.5788, -0.5777, -0.5775, -0.5771, -0.5743, -0.5715,
0:         -0.5685, -0.5648, -0.5643, -0.5605, -0.5589, -0.5565, -0.5858, -0.5861, -0.5858, -0.5843, -0.5827, -0.5821,
0:         -0.5808], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6938,  0.5254,  0.2657, -0.0091, -0.2199, -0.3296, -0.3481, -0.2873, -0.1547,  0.0387,  0.2538,  0.4352,
0:          0.5363,  0.5297,  0.4091,  0.1908, -0.0656, -0.2862,  0.8741,  0.7981,  0.6047,  0.3407,  0.0908, -0.0841,
0:         -0.1732], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.8407, -1.8351, -1.8274, -1.8181, -1.8075, -1.7957, -1.7824, -1.7665, -1.7475, -1.7247, -1.6974, -1.6642,
0:         -1.6252, -1.5831, -1.5419, -1.5047, -1.4725, -1.4446, -1.4201, -1.3981, -1.3779, -1.3588, -1.3402, -1.3224,
0:         -1.3074], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0874, -0.0874, -0.1101, -0.1260, -0.1351, -0.1532, -0.1419, -0.1260, -0.1237, -0.1487, -0.1351, -0.1260,
0:         -0.1419, -0.1419, -0.1147, -0.1056, -0.0920, -0.0897, -0.2258, -0.2213, -0.2054, -0.1873, -0.1668, -0.1396,
0:         -0.1260], device='cuda:0')
0: [DEBUG] Epoch 20, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.1737, -0.1487,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.0430,     nan,     nan,     nan, -0.2258,     nan,     nan,     nan,
0:             nan,     nan,  0.0589,     nan, -0.2372,     nan,     nan, -0.2338,     nan, -0.1362,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2417,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2417,     nan, -0.2417,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2213,     nan,     nan,     nan,     nan,     nan, -0.2349,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2338,     nan,     nan,  0.0805,
0:             nan,     nan,     nan,     nan, -0.2395, -0.2383,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan, -0.2417,     nan,
0:         -0.2417,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan, -0.2417,     nan,
0:             nan,     nan,     nan, -0.2417,     nan, -0.2417, -0.2395,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2383, -0.2326,     nan, -0.2122,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2417,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2406,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2417, -0.2417,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 20, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1544, -0.1611, -0.1299, -0.0873, -0.0628, -0.0694, -0.0807, -0.1019, -0.1289, -0.1531, -0.2056, -0.2630,
0:         -0.3201, -0.3509, -0.3433, -0.3093, -0.2654, -0.2533, -0.0335, -0.0805, -0.0807, -0.0766, -0.0847, -0.1045,
0:         -0.1235], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0288,  0.0797,  0.1075,  0.1147,  0.0942,  0.0773,  0.0491,  0.0286,  0.0203,  0.0355,  0.0656,  0.1058,
0:          0.1297,  0.1190,  0.0785,  0.0444,  0.0328,  0.0403, -0.0117,  0.0512,  0.0980,  0.1094,  0.0899,  0.0616,
0:          0.0324], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6906, -0.6937, -0.6953, -0.6876, -0.6770, -0.6686, -0.6693, -0.6704, -0.6730, -0.6787, -0.6820, -0.6864,
0:         -0.6807, -0.6769, -0.6828, -0.6865, -0.7067, -0.7194, -0.6931, -0.6904, -0.6878, -0.6779, -0.6720, -0.6690,
0:         -0.6682], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2367, 0.4067, 0.5674, 0.5045, 0.3938, 0.3800, 0.2867, 0.2307, 0.2263, 0.1944, 0.2596, 0.4149, 0.5360, 0.5695,
0:         0.5162, 0.5758, 0.7176, 0.5959, 0.2657, 0.2281, 0.2668, 0.2293, 0.1920, 0.2386, 0.1746], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.3742, -1.3263, -1.2977, -1.2962, -1.3064, -1.3162, -1.3284, -1.3586, -1.3973, -1.4316, -1.4442, -1.4279,
0:         -1.3957, -1.3759, -1.3972, -1.4512, -1.5108, -1.5476, -1.5536, -1.5511, -1.5542, -1.5548, -1.5342, -1.4844,
0:         -1.4221], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1700, -0.1683, -0.1748, -0.1669, -0.1691, -0.1707, -0.1667, -0.1693, -0.1767, -0.1873, -0.1846, -0.1803,
0:         -0.1698, -0.1741, -0.1710, -0.1773, -0.1782, -0.1843, -0.1956, -0.1907, -0.1923, -0.1803, -0.1789, -0.1833,
0:         -0.1822], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.22819451987743378; velocity_v: 0.5373491644859314; specific_humidity: 0.16614386439323425; velocity_z: 0.6447170972824097; temperature: 0.16801463067531586; total_precip: 0.8489044308662415; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2582923173904419; velocity_v: 0.5749304294586182; specific_humidity: 0.17970092594623566; velocity_z: 0.5790175795555115; temperature: 0.30060014128685; total_precip: 0.5535004138946533; 
0: epoch: 20 [1/5 (20%)]	Loss: 0.70120 : 0.38672 :: 0.20900 (2.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2490129917860031; velocity_v: 0.4638538360595703; specific_humidity: 0.1626042127609253; velocity_z: 0.5134978890419006; temperature: 0.14436331391334534; total_precip: 0.6081505417823792; 
0: epoch: 20 [2/5 (40%)]	Loss: 0.60815 : 0.32608 :: 0.20529 (16.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22243356704711914; velocity_v: 0.4253119230270386; specific_humidity: 0.15206342935562134; velocity_z: 0.5412579774856567; temperature: 0.13988088071346283; total_precip: 0.5333849191665649; 
0: epoch: 20 [3/5 (60%)]	Loss: 0.53338 : 0.30462 :: 0.20386 (15.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.229448601603508; velocity_v: 0.5791276693344116; specific_humidity: 0.16614866256713867; velocity_z: 0.5708456635475159; temperature: 0.13083072006702423; total_precip: 0.6229754686355591; 
0: epoch: 20 [4/5 (80%)]	Loss: 0.62298 : 0.35125 :: 0.20799 (15.65 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 20 : 0.402128666639328
0: validation loss for velocity_u : 0.18934257328510284
0: validation loss for velocity_v : 0.5422161817550659
0: validation loss for specific_humidity : 0.1447627991437912
0: validation loss for velocity_z : 0.5720894932746887
0: validation loss for temperature : 0.13139520585536957
0: validation loss for total_precip : 0.8329656720161438
0: 21 : 10:18:24 :: batch_size = 96, lr = 1.2510554328606033e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 21, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1004, 0.1001, 0.0997, 0.0996, 0.0994, 0.0991, 0.0989, 0.0989, 0.0987, 0.0986, 0.0984, 0.0984, 0.0982, 0.0981,
0:         0.0981, 0.0979, 0.0977, 0.0976, 0.0762, 0.0777, 0.0793, 0.0808, 0.0825, 0.0841, 0.0856], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8742, -0.8795, -0.8845, -0.8898, -0.8951, -0.9001, -0.9054, -0.9107, -0.9160, -0.9213, -0.9265, -0.9318,
0:         -0.9371, -0.9426, -0.9479, -0.9532, -0.9585, -0.9640, -0.8120, -0.8170, -0.8221, -0.8271, -0.8324, -0.8377,
0:         -0.8430], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0097, -0.0114, -0.0161, -0.0211, -0.0256, -0.0300, -0.0342, -0.0384, -0.0426, -0.0467, -0.0507, -0.0547,
0:         -0.0587, -0.0626, -0.0666, -0.0703, -0.0754, -0.0804, -0.1185, -0.1238, -0.1291, -0.1330, -0.1369, -0.1415,
0:         -0.1460], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0126, -0.0195, -0.0276, -0.0346, -0.0403, -0.0473, -0.0530, -0.0577, -0.0623, -0.0657, -0.0681, -0.0692,
0:         -0.0692, -0.0692, -0.0681, -0.0657, -0.0634, -0.0600, -0.0577, -0.0600, -0.0623, -0.0669, -0.0704, -0.0750,
0:         -0.0808], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.0052,  0.0044,  0.0048,  0.0049,  0.0044,  0.0047,  0.0041,  0.0047,  0.0040,  0.0045,  0.0041,  0.0038,
0:          0.0033,  0.0030,  0.0026,  0.0024,  0.0021,  0.0019,  0.0007,  0.0005, -0.0007, -0.0009, -0.0012, -0.0023,
0:         -0.0026], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2487, -0.2487, -0.2487, -0.2499, -0.2487, -0.2487, -0.2499, -0.2499, -0.2511, -0.2594, -0.2606, -0.2606,
0:         -0.2618, -0.2618, -0.2606, -0.2606, -0.2594, -0.2594, -0.2546, -0.2558, -0.2570, -0.2570, -0.2582, -0.2594,
0:         -0.2594], device='cuda:0')
0: [DEBUG] Epoch 21, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.2558,     nan, -0.2511,     nan,     nan,     nan,     nan,
0:         -0.2594,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2594,     nan,     nan,     nan, -0.2594,     nan,     nan,     nan,     nan,
0:             nan, -0.2570,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2582,     nan,     nan,
0:             nan,     nan,     nan, -0.2594,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2618,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2630,     nan,     nan,     nan,     nan,     nan, -0.2630,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2582,     nan,     nan,     nan,     nan, -0.2511,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2570,     nan, -0.2594,     nan,     nan,
0:             nan,     nan,     nan, -0.2546,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2594,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2606,     nan, -0.2606,     nan, -0.2618,     nan, -0.2618,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2630,     nan,     nan,     nan, -0.2630, -0.2630,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2582,     nan,     nan,     nan,     nan,
0:         -0.2618,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2534,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2630,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2630,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 21, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5019, 0.5539, 0.6140, 0.6563, 0.6728, 0.6672, 0.6707, 0.6797, 0.6706, 0.6521, 0.6014, 0.5354, 0.4744, 0.4491,
0:         0.4639, 0.5134, 0.5712, 0.6124, 0.5318, 0.5643, 0.6027, 0.6111, 0.5925, 0.5796, 0.5825], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1403, -0.0713, -0.0485, -0.0547, -0.0844, -0.1140, -0.1472, -0.1715, -0.1857, -0.1917, -0.1817, -0.1664,
0:         -0.1680, -0.2060, -0.2622, -0.2795, -0.2440, -0.1489, -0.1749, -0.0956, -0.0631, -0.0641, -0.0923, -0.1308,
0:         -0.1636], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0298, -0.0345, -0.0484, -0.0593, -0.0710, -0.0770, -0.0928, -0.1031, -0.1141, -0.1311, -0.1473, -0.1605,
0:         -0.1694, -0.1778, -0.1930, -0.2023, -0.2238, -0.2202, -0.0578, -0.0580, -0.0687, -0.0718, -0.0865, -0.1015,
0:         -0.1059], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0729,  0.0189,  0.1009, -0.0611, -0.1806, -0.0624, -0.0172, -0.0436, -0.0358, -0.0938, -0.1426, -0.0858,
0:         -0.0658, -0.1464, -0.2287, -0.1853, -0.1007, -0.1793, -0.0343, -0.1073, -0.1047, -0.2179, -0.2601, -0.1452,
0:         -0.1444], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1428, -0.1853, -0.2055, -0.2124, -0.2083, -0.2077, -0.2090, -0.2167, -0.2256, -0.2345, -0.2468, -0.2592,
0:         -0.2688, -0.2716, -0.2746, -0.2750, -0.2798, -0.2820, -0.2806, -0.2788, -0.2806, -0.2897, -0.3010, -0.3120,
0:         -0.3249], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2128, -0.2121, -0.2190, -0.2068, -0.2079, -0.2060, -0.2031, -0.2072, -0.2141, -0.2261, -0.2254, -0.2161,
0:         -0.2084, -0.2089, -0.2019, -0.2095, -0.2114, -0.2193, -0.2331, -0.2232, -0.2235, -0.2117, -0.2071, -0.2099,
0:         -0.2119], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2735354006290436; velocity_v: 0.45896801352500916; specific_humidity: 0.17851868271827698; velocity_z: 0.5883983969688416; temperature: 0.20147842168807983; total_precip: 0.7151445150375366; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22695814073085785; velocity_v: 0.4706452488899231; specific_humidity: 0.15017160773277283; velocity_z: 0.5955432653427124; temperature: 0.14779749512672424; total_precip: 0.7711489200592041; 
0: epoch: 21 [1/5 (20%)]	Loss: 0.74315 : 0.36530 :: 0.21050 (2.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.25246497988700867; velocity_v: 0.5319048762321472; specific_humidity: 0.18130099773406982; velocity_z: 0.7011611461639404; temperature: 0.18159465491771698; total_precip: 1.118360996246338; 
0: epoch: 21 [2/5 (40%)]	Loss: 1.11836 : 0.46167 :: 0.21018 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24277476966381073; velocity_v: 0.4631703794002533; specific_humidity: 0.15543341636657715; velocity_z: 0.6748700141906738; temperature: 0.16330792009830475; total_precip: 0.6943264007568359; 
0: epoch: 21 [3/5 (60%)]	Loss: 0.69433 : 0.36591 :: 0.21508 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23743677139282227; velocity_v: 0.41206568479537964; specific_humidity: 0.15499116480350494; velocity_z: 0.5491891503334045; temperature: 0.14512358605861664; total_precip: 0.8205845952033997; 
0: epoch: 21 [4/5 (80%)]	Loss: 0.82058 : 0.35434 :: 0.20810 (16.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 21 : 0.344668984413147
0: validation loss for velocity_u : 0.19211681187152863
0: validation loss for velocity_v : 0.42498543858528137
0: validation loss for specific_humidity : 0.13153165578842163
0: validation loss for velocity_z : 0.5918275713920593
0: validation loss for temperature : 0.12703107297420502
0: validation loss for total_precip : 0.6005209684371948
0: 22 : 10:22:19 :: batch_size = 96, lr = 1.2205418857176618e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 22, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1887, -0.1866, -0.1839, -0.1801, -0.1749, -0.1679, -0.1590, -0.1480, -0.1350, -0.1197, -0.1025, -0.0839,
0:         -0.0646, -0.0454, -0.0271, -0.0099,  0.0061,  0.0206, -0.1344, -0.1305, -0.1256, -0.1191, -0.1109, -0.1009,
0:         -0.0893], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1279, -1.1631, -1.1981, -1.2299, -1.2540, -1.2684, -1.2706, -1.2597, -1.2353, -1.1973, -1.1466, -1.0850,
0:         -1.0170, -0.9465, -0.8778, -0.8138, -0.7560, -0.7044, -1.1255, -1.1614, -1.1957, -1.2251, -1.2469, -1.2579,
0:         -1.2569], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7249, -0.7413, -0.7539, -0.7600, -0.7617, -0.7619, -0.7593, -0.7554, -0.7505, -0.7461, -0.7421, -0.7387,
0:         -0.7391, -0.7401, -0.7420, -0.7455, -0.7491, -0.7521, -0.7242, -0.7397, -0.7531, -0.7548, -0.7560, -0.7554,
0:         -0.7513], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8927, -0.9259, -0.9591, -0.9967, -1.0343, -1.0631, -1.0719, -1.0608, -1.0321, -0.9878, -0.9281, -0.8484,
0:         -0.7488, -0.6360, -0.5231, -0.4302, -0.3572, -0.3063, -0.8506, -0.8993, -0.9458, -0.9856, -1.0188, -1.0409,
0:         -1.0520], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.1663, 1.1645, 1.1620, 1.1587, 1.1552, 1.1513, 1.1474, 1.1439, 1.1416, 1.1403, 1.1403, 1.1416, 1.1445, 1.1486,
0:         1.1543, 1.1616, 1.1697, 1.1786, 1.1876, 1.1971, 1.2064, 1.2158, 1.2253, 1.2340, 1.2421], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416,
0:         -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416, -0.2416,
0:         -0.2416], device='cuda:0')
0: [DEBUG] Epoch 22, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([   nan,    nan,    nan,    nan,    nan, 0.4513,    nan,    nan,    nan,    nan,    nan,    nan, 0.4020,    nan,
0:            nan,    nan,    nan,    nan,    nan, 0.3190,    nan,    nan,    nan, 0.5209,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 0.4962,    nan,    nan,    nan,    nan, 0.4558,    nan,    nan, 0.4379,    nan, 0.1015,    nan,    nan,
0:         0.2405,    nan,    nan,    nan, 0.4087,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.3908,    nan,
0:         0.4827,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:         0.3930,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 0.5814,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan, 0.2114,    nan,    nan,    nan,    nan,    nan, 0.4368,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan, 0.4401,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan, 0.3033,    nan,    nan,    nan,    nan,    nan,    nan, 0.5422,    nan,    nan, 0.3975,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan, 0.4951,    nan,    nan,    nan,    nan,    nan, 0.5321,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan, 0.5108, 0.4996,    nan])
0: [DEBUG] Epoch 22, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4606, -0.3879, -0.3013, -0.2617, -0.2803, -0.3282, -0.3619, -0.3551, -0.3165, -0.2583, -0.2211, -0.2093,
0:         -0.2083, -0.1862, -0.1195, -0.0147,  0.1035,  0.1988, -0.4346, -0.3565, -0.2697, -0.2366, -0.2664, -0.3186,
0:         -0.3628], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3607, -0.2809, -0.2510, -0.2702, -0.3260, -0.3618, -0.4010, -0.4157, -0.4284, -0.4213, -0.4089, -0.3915,
0:         -0.3925, -0.4244, -0.4738, -0.4941, -0.4631, -0.3911, -0.3428, -0.2499, -0.2020, -0.2225, -0.2769, -0.3286,
0:         -0.3664], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6669, -0.6655, -0.6589, -0.6523, -0.6432, -0.6418, -0.6530, -0.6610, -0.6676, -0.6753, -0.6809, -0.6821,
0:         -0.6802, -0.6804, -0.6875, -0.6955, -0.7137, -0.7197, -0.6760, -0.6658, -0.6584, -0.6490, -0.6486, -0.6561,
0:         -0.6622], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.3135, -1.3618, -1.2976, -1.3340, -1.4016, -1.3807, -1.3791, -1.4474, -1.4219, -1.3862, -1.4895, -1.5078,
0:         -1.4506, -1.5400, -1.6582, -1.5705, -1.4277, -1.4197, -1.3343, -1.4509, -1.4473, -1.5046, -1.5651, -1.5574,
0:         -1.5751], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.0553, 1.0031, 0.9958, 1.0349, 1.0856, 1.1239, 1.1385, 1.1452, 1.1564, 1.1685, 1.1664, 1.1538, 1.1338, 1.1363,
0:         1.1613, 1.1987, 1.2238, 1.2321, 1.2293, 1.2354, 1.2569, 1.2728, 1.2647, 1.2283, 1.1819], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.3203, 0.3328, 0.3356, 0.3312, 0.3057, 0.3279, 0.3454, 0.3327, 0.3568, 0.3323, 0.3461, 0.3333, 0.3342, 0.3393,
0:         0.3499, 0.3609, 0.3846, 0.3877, 0.2931, 0.2983, 0.3208, 0.3351, 0.3456, 0.3680, 0.3906], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.22344224154949188; velocity_v: 0.4528929591178894; specific_humidity: 0.1774768978357315; velocity_z: 0.6570957899093628; temperature: 0.15853282809257507; total_precip: 0.9195773005485535; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26570603251457214; velocity_v: 0.40793371200561523; specific_humidity: 0.167294442653656; velocity_z: 0.6026848554611206; temperature: 0.16339415311813354; total_precip: 0.7764451503753662; 
0: epoch: 22 [1/5 (20%)]	Loss: 0.84801 : 0.38185 :: 0.21193 (2.56 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2744666635990143; velocity_v: 0.49231261014938354; specific_humidity: 0.14528994262218475; velocity_z: 0.5963950157165527; temperature: 0.8898607492446899; total_precip: 0.5461093187332153; 
0: epoch: 22 [2/5 (40%)]	Loss: 0.54611 : 0.45857 :: 0.20885 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2266930341720581; velocity_v: 0.516209602355957; specific_humidity: 0.13047704100608826; velocity_z: 0.540582001209259; temperature: 0.1563209891319275; total_precip: 0.32744336128234863; 
0: epoch: 22 [3/5 (60%)]	Loss: 0.32744 : 0.28527 :: 0.20658 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24168173968791962; velocity_v: 0.6175117492675781; specific_humidity: 0.16348512470722198; velocity_z: 0.6565933227539062; temperature: 0.16041962802410126; total_precip: 0.8040772080421448; 
0: epoch: 22 [4/5 (80%)]	Loss: 0.80408 : 0.40821 :: 0.20632 (15.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 22 : 0.36052587628364563
0: validation loss for velocity_u : 0.2098432332277298
0: validation loss for velocity_v : 0.40715548396110535
0: validation loss for specific_humidity : 0.1246015727519989
0: validation loss for velocity_z : 0.6510342955589294
0: validation loss for temperature : 0.11800577491521835
0: validation loss for total_precip : 0.6525148749351501
0: 23 : 10:26:14 :: batch_size = 96, lr = 1.1907725714318652e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 23, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.9493, 2.9567, 2.9634, 2.9680, 2.9702, 2.9694, 2.9652, 2.9577, 2.9473, 2.9345, 2.9198, 2.9042, 2.8875, 2.8709,
0:         2.8554, 2.8422, 2.8322, 2.8253, 2.8465, 2.8533, 2.8574, 2.8584, 2.8562, 2.8516, 2.8445], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.6339, 1.6575, 1.6822, 1.7085, 1.7360, 1.7651, 1.7949, 1.8254, 1.8561, 1.8869, 1.9169, 1.9453, 1.9712, 1.9928,
0:         2.0095, 2.0209, 2.0275, 2.0303, 1.6949, 1.7134, 1.7350, 1.7604, 1.7896, 1.8223, 1.8564], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7672, -0.7681, -0.7717, -0.7751, -0.7761, -0.7771, -0.7783, -0.7813, -0.7838, -0.7852, -0.7860, -0.7867,
0:         -0.7879, -0.7886, -0.7887, -0.7886, -0.7884, -0.7885, -0.7803, -0.7811, -0.7817, -0.7825, -0.7831, -0.7835,
0:         -0.7846], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 2.6173,  2.0207,  1.2739,  0.5498,  0.0352, -0.1401,  0.0398,  0.4792,  1.0234,  1.5403,  1.9410,  2.1869,
0:          2.2643,  2.1755,  1.9478,  1.6291,  1.2739,  0.9141,  3.7831,  3.1136,  2.3531,  1.6518,  1.1646,  1.0029,
0:          1.1851], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.7773, 1.7722, 1.7593, 1.7375, 1.7074, 1.6703, 1.6287, 1.5859, 1.5446, 1.5068, 1.4729, 1.4421, 1.4139, 1.3875,
0:         1.3620, 1.3360, 1.3086, 1.2785, 1.2462, 1.2115, 1.1753, 1.1368, 1.0942, 1.0449, 0.9866], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598,
0:         -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598, -0.2598,
0:         -0.2598], device='cuda:0')
0: [DEBUG] Epoch 23, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2409,     nan, -0.2385,     nan, -0.2456,     nan,     nan,     nan,     nan, -0.2432,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2550,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2527,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2598,     nan,     nan, -0.2598,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2598,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2598,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2491, -0.2468,     nan,     nan,     nan,     nan,     nan,
0:         -0.2574,     nan,     nan,     nan, -0.2468,     nan,     nan, -0.2420,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2409,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2598,     nan, -0.2586,     nan,     nan,     nan,     nan,     nan,
0:         -0.2562,     nan,     nan,     nan,     nan,     nan,     nan, -0.2479,     nan,     nan,     nan,     nan,
0:         -0.2302,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2373,     nan, -0.2219,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2361,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2196, -0.2066,
0:             nan,     nan,     nan, -0.2243,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2503,     nan,     nan,     nan,     nan,     nan,     nan, -0.2586,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 23, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.5929, 2.5730, 2.5855, 2.6115, 2.6031, 2.5229, 2.3963, 2.2565, 2.1117, 2.0285, 1.9636, 1.9280, 1.9157, 1.9151,
0:         1.9267, 1.9336, 1.9443, 1.9195, 2.6890, 2.6236, 2.6032, 2.5927, 2.5263, 2.4166, 2.2751], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2578,  0.2977,  0.3188,  0.3172,  0.2887,  0.2425,  0.1708,  0.1015,  0.0471,  0.0192,  0.0228,  0.0472,
0:          0.0592,  0.0366, -0.0154, -0.0521, -0.0568, -0.0294,  0.2033,  0.2544,  0.2887,  0.2961,  0.2731,  0.2228,
0:          0.1536], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7859, -0.7805, -0.7742, -0.7667, -0.7602, -0.7588, -0.7695, -0.7785, -0.7854, -0.7963, -0.8023, -0.8054,
0:         -0.8029, -0.7977, -0.7988, -0.7998, -0.8096, -0.8094, -0.7862, -0.7722, -0.7633, -0.7512, -0.7498, -0.7547,
0:         -0.7613], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3709, 0.6464, 0.8247, 0.7379, 0.6187, 0.5859, 0.5635, 0.5225, 0.4831, 0.4363, 0.3901, 0.4344, 0.4902, 0.4263,
0:         0.3181, 0.3148, 0.4152, 0.4052, 0.5708, 0.6586, 0.7017, 0.6595, 0.6603, 0.6383, 0.5826], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5859, -0.5467, -0.5117, -0.4943, -0.4794, -0.4665, -0.4497, -0.4272, -0.3993, -0.3640, -0.3226, -0.2819,
0:         -0.2386, -0.1944, -0.1510, -0.1080, -0.0691, -0.0335, -0.0079,  0.0064,  0.0094,  0.0144,  0.0300,  0.0550,
0:          0.0765], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1942, -0.1942, -0.2058, -0.2034, -0.2029, -0.2041, -0.1992, -0.1991, -0.2010, -0.1980, -0.2023, -0.2018,
0:         -0.2029, -0.2038, -0.1999, -0.2066, -0.1995, -0.2058, -0.2044, -0.2017, -0.2090, -0.2017, -0.2037, -0.2069,
0:         -0.2050], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.21637409925460815; velocity_v: 0.45596757531166077; specific_humidity: 0.17426292598247528; velocity_z: 0.5871583819389343; temperature: 0.15690597891807556; total_precip: 0.7044146656990051; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22090813517570496; velocity_v: 0.49706703424453735; specific_humidity: 0.14107957482337952; velocity_z: 0.6435980200767517; temperature: 0.132976233959198; total_precip: 0.719780445098877; 
0: epoch: 23 [1/5 (20%)]	Loss: 0.71210 : 0.35527 :: 0.21280 (2.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2359151393175125; velocity_v: 0.3794473111629486; specific_humidity: 0.16316112875938416; velocity_z: 0.6527717709541321; temperature: 0.16988614201545715; total_precip: 0.8411181569099426; 
0: epoch: 23 [2/5 (40%)]	Loss: 0.84112 : 0.37417 :: 0.21009 (16.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22821973264217377; velocity_v: 0.3839397728443146; specific_humidity: 0.15987831354141235; velocity_z: 0.6173526644706726; temperature: 0.15152418613433838; total_precip: 0.3840879499912262; 
0: epoch: 23 [3/5 (60%)]	Loss: 0.38409 : 0.28858 :: 0.20991 (16.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2574312090873718; velocity_v: 0.5239675045013428; specific_humidity: 0.13504934310913086; velocity_z: 0.6667479276657104; temperature: 0.1757856160402298; total_precip: 0.5956488847732544; 
0: epoch: 23 [4/5 (80%)]	Loss: 0.59565 : 0.36034 :: 0.21267 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 23 : 0.38572126626968384
0: validation loss for velocity_u : 0.18879763782024384
0: validation loss for velocity_v : 0.4055647552013397
0: validation loss for specific_humidity : 0.14942136406898499
0: validation loss for velocity_z : 0.6077674627304077
0: validation loss for temperature : 0.1139611303806305
0: validation loss for total_precip : 0.8488152027130127
0: 24 : 10:30:12 :: batch_size = 96, lr = 1.1617293379823076e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 24, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6718, -0.6660, -0.6615, -0.6594, -0.6644, -0.6771, -0.6936, -0.7016, -0.6975, -0.6918, -0.6928, -0.7010,
0:         -0.7052, -0.7028, -0.6984, -0.6942, -0.6901, -0.6867, -0.6854, -0.6775, -0.6708, -0.6658, -0.6690, -0.6812,
0:         -0.6976], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3863, -0.3740, -0.3608, -0.3461, -0.3266, -0.3039, -0.2794, -0.2507, -0.2198, -0.1948, -0.1876, -0.1981,
0:         -0.2179, -0.2511, -0.2981, -0.3424, -0.3608, -0.3424, -0.4049, -0.3925, -0.3783, -0.3606, -0.3400, -0.3239,
0:         -0.3086], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0032, 0.9116, 0.8497, 0.7971, 0.7527, 0.6969, 0.5948, 0.4720, 0.3599, 0.2232, 0.1560, 0.0596, 0.1752, 0.3435,
0:         1.0091, 1.6861, 2.0745, 2.2941, 0.8855, 0.8056, 0.7532, 0.7187, 0.6750, 0.6129, 0.5109], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4331, 0.4793, 0.5134, 0.5354, 0.4881, 0.4683, 0.4221, 0.4188, 0.4221, 0.3517, 0.2582, 0.1383, 0.0910, 0.1570,
0:         0.2780, 0.3693, 0.3264, 0.2197, 0.3891, 0.4672, 0.4980, 0.5629, 0.5233, 0.4936, 0.4980], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.8575, 0.9531, 1.0478, 1.1414, 1.2274, 1.2914, 1.3649, 1.4223, 1.4776, 1.4990, 1.4760, 1.4300, 1.3438, 1.2474,
0:         1.0652, 0.8533, 0.7802, 0.8203, 0.9722, 1.1743, 1.3592, 1.5301, 1.6471, 1.7169, 1.7260], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2488, -0.2488, -0.2488, -0.2488, -0.2488, -0.2488, -0.2476, -0.2440, -0.2285, -0.2488, -0.2488, -0.2488,
0:         -0.2488, -0.2488, -0.2488, -0.2476, -0.2428, -0.2178, -0.2488, -0.2488, -0.2488, -0.2488, -0.2488, -0.2488,
0:         -0.2488], device='cuda:0')
0: [DEBUG] Epoch 24, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1807, -0.2309,     nan,     nan,     nan,     nan,
0:         -0.0122,     nan,     nan, -0.1652,     nan,     nan,     nan,     nan,     nan,     nan, -0.1066,     nan,
0:         -0.1676,     nan,     nan,     nan, -0.1723,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0732,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2452, -0.2405,     nan,     nan,     nan,     nan,     nan,     nan, -0.2476,     nan,     nan,
0:             nan,     nan,     nan, -0.2321,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0265,     nan,     nan,     nan, -0.1472,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1532,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1174,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1950,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0923,     nan,     nan, -0.2440,
0:             nan,     nan, -0.2488,     nan,     nan,     nan, -0.0971, -0.2046,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1090,     nan,     nan, -0.2178,     nan,     nan, -0.2070,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0583,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1855, -0.1269,     nan,     nan, -0.1604,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 24, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8885, -0.7489, -0.6585, -0.6598, -0.7255, -0.8023, -0.8279, -0.7939, -0.7471, -0.7110, -0.7237, -0.7781,
0:         -0.8373, -0.8505, -0.8072, -0.7060, -0.5977, -0.4975, -0.8547, -0.6981, -0.6124, -0.6339, -0.7474, -0.8292,
0:         -0.8556], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0652,  0.1141,  0.1271,  0.1202,  0.0888,  0.0622,  0.0224, -0.0132, -0.0360, -0.0321, -0.0024,  0.0426,
0:          0.0647,  0.0374, -0.0379, -0.0970, -0.1156, -0.0887,  0.0070,  0.0542,  0.0803,  0.0740,  0.0486,  0.0127,
0:         -0.0246], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.6153, 1.6165, 1.6097, 1.6015, 1.5918, 1.6080, 1.6496, 1.7055, 1.7660, 1.8167, 1.8855, 1.9130, 1.9619, 2.0356,
0:         2.0886, 2.1710, 2.2598, 2.3450, 1.5042, 1.5398, 1.5738, 1.5860, 1.6223, 1.6456, 1.7009], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0847, -0.0739, -0.0133, -0.0571, -0.1787, -0.1274, -0.0411, -0.0581, -0.0775, -0.0914, -0.1009, -0.0643,
0:          0.0576,  0.1197, -0.0063,  0.0181,  0.1625,  0.0506, -0.0365, -0.1556, -0.2283, -0.2527, -0.2751, -0.1930,
0:         -0.1314], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3199, -0.2880, -0.2426, -0.1973, -0.1620, -0.1478, -0.1566, -0.1861, -0.2144, -0.2338, -0.2501, -0.2663,
0:         -0.2892, -0.3183, -0.3579, -0.4027, -0.4473, -0.4849, -0.5117, -0.5290, -0.5377, -0.5422, -0.5395, -0.5290,
0:         -0.5135], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0496, -0.0314, -0.0279, -0.0319, -0.0517, -0.0531, -0.0502, -0.0589, -0.0510, -0.0546, -0.0508, -0.0587,
0:         -0.0551, -0.0486, -0.0445, -0.0485, -0.0430, -0.0285, -0.0759, -0.0700, -0.0712, -0.0622, -0.0585, -0.0425,
0:         -0.0370], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.24934767186641693; velocity_v: 0.487032949924469; specific_humidity: 0.13109272718429565; velocity_z: 0.5282389521598816; temperature: 0.18036797642707825; total_precip: 0.6886897683143616; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2535261809825897; velocity_v: 0.37706896662712097; specific_humidity: 0.16603389382362366; velocity_z: 0.5147730708122253; temperature: 0.1612955778837204; total_precip: 0.7757615447044373; 
0: epoch: 24 [1/5 (20%)]	Loss: 0.73223 : 0.34402 :: 0.21073 (2.37 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2202804982662201; velocity_v: 0.43863293528556824; specific_humidity: 0.17408981919288635; velocity_z: 0.6647959351539612; temperature: 0.14700207114219666; total_precip: 0.902697741985321; 
0: epoch: 24 [2/5 (40%)]	Loss: 0.90270 : 0.39209 :: 0.20692 (15.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22861455380916595; velocity_v: 0.4382927715778351; specific_humidity: 0.16600468754768372; velocity_z: 0.7516093254089355; temperature: 0.14567206799983978; total_precip: 0.8557941317558289; 
0: epoch: 24 [3/5 (60%)]	Loss: 0.85579 : 0.39844 :: 0.21240 (15.71 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26253268122673035; velocity_v: 0.5254568457603455; specific_humidity: 0.154085174202919; velocity_z: 0.7088699340820312; temperature: 0.16364167630672455; total_precip: 0.9043563604354858; 
0: epoch: 24 [4/5 (80%)]	Loss: 0.90436 : 0.42097 :: 0.20735 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 24 : 0.3908054232597351
0: validation loss for velocity_u : 0.16664822399616241
0: validation loss for velocity_v : 0.3356097936630249
0: validation loss for specific_humidity : 0.13624636828899384
0: validation loss for velocity_z : 0.5664253830909729
0: validation loss for temperature : 0.12363339215517044
0: validation loss for total_precip : 1.016269326210022
0: 25 : 10:34:07 :: batch_size = 96, lr = 1.1333944760803001e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 25, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4610, -0.4741, -0.4849, -0.4953, -0.5078, -0.5231, -0.5401, -0.5577, -0.5753, -0.5914, -0.6036, -0.6116,
0:         -0.6190, -0.6302, -0.6455, -0.6612, -0.6728, -0.6793, -0.4446, -0.4605, -0.4741, -0.4882, -0.5045, -0.5220,
0:         -0.5379], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.4155, -1.3180, -1.2284, -1.1522, -1.0950, -1.0636, -1.0597, -1.0718, -1.0757, -1.0459, -0.9699, -0.8538,
0:         -0.7165, -0.5804, -0.4659, -0.3890, -0.3549, -0.3539, -1.4461, -1.3450, -1.2506, -1.1705, -1.1127, -1.0837,
0:         -1.0821], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6945, -0.6959, -0.6974, -0.6976, -0.6976, -0.6949, -0.6921, -0.6896, -0.6874, -0.6917, -0.6954, -0.6994,
0:         -0.7038, -0.6997, -0.6966, -0.6880, -0.6785, -0.6738, -0.6941, -0.6959, -0.6978, -0.6984, -0.6990, -0.6959,
0:         -0.6928], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-8.2557e-02,  4.4472e-02,  1.1356e-01,  1.4921e-01,  1.8933e-01,  2.1161e-01,  1.6036e-01,  4.8929e-02,
0:         -9.9974e-05,  1.1579e-01,  3.5870e-01,  5.7933e-01,  6.8853e-01,  7.3310e-01,  7.8213e-01,  8.1779e-01,
0:          7.7099e-01,  6.3504e-01,  5.7843e-02,  4.4472e-02,  4.2243e-02,  6.0072e-02,  8.4586e-02,  1.0687e-01,
0:          1.3807e-01], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.3189, 0.3327, 0.3489, 0.3696, 0.3917, 0.4069, 0.4084, 0.3951, 0.3731, 0.3523, 0.3437, 0.3551, 0.3855, 0.4255,
0:         0.4616, 0.4859, 0.4978, 0.5030, 0.5100, 0.5255, 0.5519, 0.5860, 0.6186, 0.6394, 0.6414], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2267, -0.2267, -0.2267, -0.2267, -0.2244, -0.2244, -0.2244, -0.2244, -0.2244, -0.2335, -0.2312, -0.2290,
0:         -0.2267, -0.2244, -0.2267, -0.2290, -0.2312, -0.2335, -0.2358, -0.2335, -0.2312, -0.2267, -0.2222, -0.2267,
0:         -0.2290], device='cuda:0')
0: [DEBUG] Epoch 25, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2176,     nan,     nan,     nan,     nan,     nan, -0.1995,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1995, -0.2018,     nan,     nan,     nan,     nan,     nan,     nan, -0.1927,     nan,     nan,
0:         -0.2109,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2267,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2335,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2335,     nan,     nan,     nan,     nan,
0:         -0.2063,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2018,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2041,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2154,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2335,     nan,     nan,
0:             nan,     nan, -0.2335, -0.2335,     nan,     nan, -0.2335,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2075,     nan,     nan,
0:             nan,     nan, -0.1780,     nan,     nan,     nan, -0.1995,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2097,     nan,     nan, -0.2075, -0.2109,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2301,
0:             nan,     nan,     nan, -0.2233,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2324,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 25, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1384, -1.0615, -1.0019, -1.0063, -1.0483, -1.0880, -1.0679, -0.9917, -0.9006, -0.8276, -0.8079, -0.8473,
0:         -0.8997, -0.9166, -0.8889, -0.8077, -0.7194, -0.6493, -1.1702, -1.0844, -1.0149, -1.0329, -1.0958, -1.1378,
0:         -1.1164], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1497, -0.0746, -0.0502, -0.0597, -0.1029, -0.1495, -0.2028, -0.2374, -0.2542, -0.2449, -0.2097, -0.1642,
0:         -0.1350, -0.1415, -0.1758, -0.1804, -0.1407, -0.0500, -0.2240, -0.1331, -0.0808, -0.0730, -0.0983, -0.1386,
0:         -0.1776], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7455, -0.7325, -0.7230, -0.7138, -0.7007, -0.6924, -0.6952, -0.6979, -0.7028, -0.7116, -0.7178, -0.7166,
0:         -0.7082, -0.6970, -0.6965, -0.6972, -0.7140, -0.7188, -0.7385, -0.7225, -0.7107, -0.6966, -0.6916, -0.6905,
0:         -0.6886], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0132,  0.1375,  0.2661,  0.1476, -0.0276, -0.0360, -0.0309, -0.0954, -0.0694,  0.0826,  0.1903,  0.2147,
0:          0.2629,  0.2984,  0.1968,  0.2004,  0.3623,  0.2705,  0.0624, -0.0583, -0.0653, -0.0319,  0.0018,  0.0073,
0:          0.0058], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.6261, 0.6136, 0.6190, 0.6415, 0.6779, 0.7174, 0.7584, 0.7978, 0.8346, 0.8614, 0.8690, 0.8638, 0.8553, 0.8560,
0:         0.8664, 0.8824, 0.8943, 0.9046, 0.9184, 0.9413, 0.9693, 0.9924, 1.0003, 0.9883, 0.9671], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2222, -0.2173, -0.2286, -0.2221, -0.2204, -0.2166, -0.2226, -0.2225, -0.2289, -0.2252, -0.2276, -0.2207,
0:         -0.2182, -0.2175, -0.2122, -0.2202, -0.2212, -0.2331, -0.2312, -0.2222, -0.2239, -0.2114, -0.2094, -0.2160,
0:         -0.2170], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2350151687860489; velocity_v: 0.40066540241241455; specific_humidity: 0.16204622387886047; velocity_z: 0.5568299293518066; temperature: 0.1626463681459427; total_precip: 0.8446882963180542; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2237812876701355; velocity_v: 0.46200674772262573; specific_humidity: 0.1644153892993927; velocity_z: 0.5908881425857544; temperature: 0.4533149003982544; total_precip: 0.7141987681388855; 
0: epoch: 25 [1/5 (20%)]	Loss: 0.77944 : 0.38194 :: 0.20657 (2.38 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24769236147403717; velocity_v: 0.5129944086074829; specific_humidity: 0.1475054919719696; velocity_z: 0.6600388884544373; temperature: 0.1503346860408783; total_precip: 0.48542696237564087; 
0: epoch: 25 [2/5 (40%)]	Loss: 0.48543 : 0.33580 :: 0.20963 (15.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2534066140651703; velocity_v: 0.5928004384040833; specific_humidity: 0.18777689337730408; velocity_z: 0.7694096565246582; temperature: 0.20420941710472107; total_precip: 1.138597011566162; 
0: epoch: 25 [3/5 (60%)]	Loss: 1.13860 : 0.48935 :: 0.21437 (15.94 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23004665970802307; velocity_v: 0.38756459951400757; specific_humidity: 0.14766965806484222; velocity_z: 0.4904547929763794; temperature: 0.1344866156578064; total_precip: 0.48567819595336914; 
0: epoch: 25 [4/5 (80%)]	Loss: 0.48568 : 0.28245 :: 0.20362 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 25 : 0.4023672342300415
0: validation loss for velocity_u : 0.19246254861354828
0: validation loss for velocity_v : 0.43163347244262695
0: validation loss for specific_humidity : 0.1206611916422844
0: validation loss for velocity_z : 0.6023532748222351
0: validation loss for temperature : 0.11434721201658249
0: validation loss for total_precip : 0.9527457356452942
0: 26 : 10:38:10 :: batch_size = 96, lr = 1.1057507083710246e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 26, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1260, -1.1051, -1.0842, -1.0642, -1.0468, -1.0314, -1.0179, -1.0058, -0.9955, -0.9909, -0.9916, -0.9955,
0:         -0.9983, -0.9976, -0.9963, -0.9955, -0.9960, -0.9981, -1.0940, -1.0675, -1.0420, -1.0187, -0.9983, -0.9808,
0:         -0.9666], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4140, 0.4216, 0.4267, 0.4288, 0.4259, 0.4220, 0.4228, 0.4312, 0.4480, 0.4654, 0.4818, 0.4980, 0.5087, 0.5142,
0:         0.5134, 0.5111, 0.5132, 0.5146, 0.3994, 0.4095, 0.4181, 0.4230, 0.4236, 0.4226, 0.4232], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.4286, 2.1269, 1.7772, 1.4612, 1.2155, 1.0521, 0.9866, 0.9815, 0.9895, 0.9970, 1.0059, 1.0112, 1.0032, 0.9890,
0:         0.9718, 0.9149, 0.8196, 0.6749, 2.3972, 2.1676, 1.9998, 1.7411, 1.5270, 1.3920, 1.3173], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1426, 0.1690, 0.2635, 0.3295, 0.4086, 0.5317, 0.6483, 0.7450, 0.7956, 0.7472, 0.6944, 0.6197, 0.5537, 0.5273,
0:         0.4944, 0.4570, 0.3691, 0.3932, 0.0173, 0.0195, 0.0723, 0.1206, 0.1184, 0.1932, 0.2437], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.7758, 0.9403, 1.0995, 1.2440, 1.3700, 1.4711, 1.5433, 1.6036, 1.6697, 1.7141, 1.7181, 1.6848, 1.6409, 1.6040,
0:         1.5718, 1.5563, 1.5442, 1.4991, 1.4538, 1.4016, 1.3337, 1.2927, 1.2526, 1.2693, 1.4033], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2007, -0.1769, -0.1530, -0.0744,  0.0305,  0.1021, -0.0529, -0.1554, -0.2126, -0.2293, -0.2198, -0.2246,
0:         -0.1793, -0.1411, -0.0911,  0.0997,  0.0138, -0.1507, -0.2413, -0.2365, -0.2389, -0.2436, -0.2150, -0.1864,
0:         -0.0863], device='cuda:0')
0: [DEBUG] Epoch 26, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([        nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,  3.3450e-01,         nan,  5.8602e-01,         nan,
0:                 nan,         nan,         nan, -3.0266e-02,         nan,         nan,         nan,         nan,
0:          6.0629e-01,  4.0960e-01,         nan, -2.3291e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  7.2250e-02,
0:          1.4258e-01,         nan,  4.5490e-01,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan, -2.4841e-01,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,  6.2713e-02,  1.8669e-01,         nan,         nan,         nan,
0:                 nan,         nan,         nan, -1.8642e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan, -4.9339e-02,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,  1.0443e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,  1.0205e-01,         nan,         nan,         nan,
0:                 nan,         nan, -2.4841e-01,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan, -4.6512e-04,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,  1.2828e-01, -2.4722e-01,         nan,
0:                 nan,  2.5582e-01,  3.6549e-01,         nan,         nan,         nan,         nan,         nan,
0:         -2.4841e-01,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,  2.3556e-01,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan])
0: [DEBUG] Epoch 26, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7967, -0.7369, -0.6917, -0.6798, -0.7007, -0.7315, -0.7293, -0.6996, -0.6669, -0.6467, -0.6646, -0.7258,
0:         -0.7917, -0.8266, -0.8100, -0.7265, -0.6282, -0.5260, -0.7408, -0.6895, -0.6583, -0.6779, -0.7487, -0.7806,
0:         -0.7828], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0103,  0.0336,  0.0433,  0.0412,  0.0290,  0.0202, -0.0042, -0.0269, -0.0424, -0.0452, -0.0313, -0.0068,
0:          0.0034, -0.0209, -0.0695, -0.1051, -0.1133, -0.1010, -0.0221,  0.0059,  0.0290,  0.0367,  0.0305,  0.0176,
0:          0.0002], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5625, 0.7217, 0.8556, 0.9846, 1.0847, 1.1547, 1.2042, 1.2300, 1.2464, 1.2650, 1.2781, 1.2969, 1.2938, 1.2779,
0:         1.2152, 1.1291, 0.9961, 0.8578, 0.1441, 0.2684, 0.4007, 0.5371, 0.6675, 0.7835, 0.9016], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2165, 0.3084, 0.4846, 0.4403, 0.3506, 0.3698, 0.2975, 0.2331, 0.1739, 0.1168, 0.1937, 0.2628, 0.2378, 0.2565,
0:         0.2588, 0.3472, 0.5378, 0.4373, 0.2426, 0.1770, 0.2376, 0.2264, 0.2506, 0.3444, 0.3228], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.5113,  0.2881,  0.0814, -0.0744, -0.1801, -0.2457, -0.2823, -0.2842, -0.2513, -0.2019, -0.1678, -0.1549,
0:         -0.1647, -0.1532, -0.1200, -0.0749, -0.0552, -0.0737, -0.1194, -0.1467, -0.1230, -0.0649, -0.0107,  0.0052,
0:         -0.0204], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.4564, 0.4879, 0.5059, 0.5246, 0.5527, 0.5902, 0.6434, 0.6648, 0.6930, 0.3057, 0.3129, 0.3295, 0.3546, 0.4034,
0:         0.4657, 0.5020, 0.5667, 0.6065, 0.1415, 0.1287, 0.1646, 0.1883, 0.2296, 0.3103, 0.3705], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.25003376603126526; velocity_v: 0.4749247431755066; specific_humidity: 0.13712544739246368; velocity_z: 0.6361364126205444; temperature: 0.1519615203142166; total_precip: 0.7162712812423706; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2856874167919159; velocity_v: 0.41630470752716064; specific_humidity: 0.17317438125610352; velocity_z: 0.6533743143081665; temperature: 0.1729854941368103; total_precip: 0.6571873426437378; 
0: epoch: 26 [1/5 (20%)]	Loss: 0.68673 : 0.36149 :: 0.21091 (2.56 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20480424165725708; velocity_v: 0.5000232458114624; specific_humidity: 0.16383060812950134; velocity_z: 0.6486870050430298; temperature: 0.173214852809906; total_precip: 0.9125438928604126; 
0: epoch: 26 [2/5 (40%)]	Loss: 0.91254 : 0.40053 :: 0.20831 (16.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2689356803894043; velocity_v: 0.3963060975074768; specific_humidity: 0.16992096602916718; velocity_z: 0.705211341381073; temperature: 0.16890934109687805; total_precip: 0.452991783618927; 
0: epoch: 26 [3/5 (60%)]	Loss: 0.45299 : 0.32739 :: 0.21230 (15.60 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2674413323402405; velocity_v: 0.4582282304763794; specific_humidity: 0.14526644349098206; velocity_z: 0.505439281463623; temperature: 0.14945171773433685; total_precip: 0.611260712146759; 
0: epoch: 26 [4/5 (80%)]	Loss: 0.61126 : 0.32497 :: 0.21270 (15.73 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 26 : 0.3214045464992523
0: validation loss for velocity_u : 0.19299621880054474
0: validation loss for velocity_v : 0.44198355078697205
0: validation loss for specific_humidity : 0.13477085530757904
0: validation loss for velocity_z : 0.42381906509399414
0: validation loss for temperature : 0.1298539936542511
0: validation loss for total_precip : 0.6050033569335938
0: 27 : 10:41:58 :: batch_size = 96, lr = 1.0787811788985607e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 27, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9205, -0.8878, -0.8934, -0.8994, -0.8984, -0.9312, -0.9912, -1.0345, -1.0455, -1.0277, -1.0079, -1.0199,
0:         -1.0421, -1.0430, -1.0234, -0.9925, -0.9418, -0.8700, -0.9250, -0.9101, -0.9081, -0.8962, -0.9012, -0.9413,
0:         -0.9861], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5929, 0.6242, 0.6626, 0.6572, 0.6134, 0.5683, 0.5026, 0.4231, 0.3780, 0.3668, 0.3580, 0.3131, 0.2257, 0.1709,
0:         0.1821, 0.2205, 0.2622, 0.2946, 0.6144, 0.6275, 0.6371, 0.6157, 0.5864, 0.5658, 0.5053], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0337, -0.0165, -0.0696, -0.0604, -0.0838, -0.1210, -0.0994, -0.0458, -0.0248, -0.0155, -0.0126, -0.0289,
0:         -0.0232, -0.0007, -0.0212, -0.0425, -0.0603, -0.1066,  0.1377,  0.0982,  0.0628,  0.0547,  0.0023, -0.0352,
0:         -0.0231], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0536,  0.2244,  0.1624, -0.4690, -0.5676, -0.4867, -0.8312, -0.6462, -0.5188, -1.0539, -0.6551, -0.4291,
0:         -0.8523, -0.4147, -0.1965, -0.2076, -0.0337,  0.0029, -0.3039, -0.0425, -0.3394, -0.7836, -0.9187, -0.9586,
0:         -1.0683], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.2790, -1.2272, -1.1427, -1.0678, -0.9407, -0.7892, -0.7064, -0.7317, -0.9000, -1.1473, -1.3524, -1.5229,
0:         -1.5812, -1.4982, -1.5572, -1.7328, -1.8605, -1.9200, -1.8579, -1.7034, -1.6040, -1.5772, -1.5425, -1.4281,
0:         -1.2280], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465,
0:         -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465,
0:         -0.2465], device='cuda:0')
0: [DEBUG] Epoch 27, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2465,     nan, -0.2465,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2454,
0:         -0.2249,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2465,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2465, -0.2465,     nan,     nan, -0.2465,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2465,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2465,     nan,     nan,     nan, -0.2465,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2465,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2352,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1559,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2465,
0:             nan, -0.2465,     nan])
0: [DEBUG] Epoch 27, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9555, -0.8551, -0.8087, -0.8120, -0.8525, -0.8735, -0.8474, -0.7719, -0.7011, -0.6590, -0.6743, -0.7258,
0:         -0.7723, -0.7706, -0.7124, -0.6265, -0.5626, -0.5239, -0.9189, -0.8145, -0.7732, -0.8118, -0.9010, -0.9352,
0:         -0.9111], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1711,  0.1890,  0.1918,  0.1882,  0.1627,  0.1348,  0.0971,  0.0608,  0.0299,  0.0093,  0.0084,  0.0268,
0:          0.0419,  0.0349,  0.0027, -0.0178, -0.0173, -0.0020,  0.1236,  0.1522,  0.1743,  0.1718,  0.1468,  0.1130,
0:          0.0792], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1240,  0.0978,  0.0618,  0.0477,  0.0436,  0.0468,  0.0305,  0.0014, -0.0477, -0.0947, -0.1323, -0.1546,
0:         -0.1540, -0.1445, -0.1556, -0.1604, -0.1929, -0.2187,  0.1317,  0.1056,  0.0855,  0.0787,  0.0777,  0.0720,
0:          0.0618], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2006,  0.2597,  0.1566,  0.1300,  0.1931,  0.2080,  0.0865,  0.0592,  0.2701,  0.2571,  0.2264,  0.3893,
0:          0.3043,  0.3258,  0.4071,  0.4064,  0.5958,  0.4575,  0.3959,  0.1101, -0.2603, -0.1603,  0.0206,  0.0903,
0:          0.0387], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.0585, -1.0703, -1.0968, -1.1419, -1.2014, -1.2693, -1.3341, -1.3826, -1.3900, -1.3554, -1.2984, -1.2451,
0:         -1.2157, -1.2112, -1.2287, -1.2413, -1.2350, -1.1955, -1.1309, -1.0621, -1.0082, -0.9778, -0.9701, -0.9791,
0:         -1.0006], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2118, -0.2125, -0.2280, -0.2263, -0.2264, -0.2237, -0.2301, -0.2238, -0.2315, -0.2188, -0.2229, -0.2188,
0:         -0.2250, -0.2250, -0.2207, -0.2282, -0.2234, -0.2315, -0.2210, -0.2150, -0.2216, -0.2183, -0.2150, -0.2230,
0:         -0.2250], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23457194864749908; velocity_v: 0.48830175399780273; specific_humidity: 0.16009944677352905; velocity_z: 0.6627768278121948; temperature: 0.2059047818183899; total_precip: 0.6884623169898987; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23022647202014923; velocity_v: 0.5005470514297485; specific_humidity: 0.14684225618839264; velocity_z: 0.676616907119751; temperature: 0.1419481784105301; total_precip: 0.44798779487609863; 
0: epoch: 27 [1/5 (20%)]	Loss: 0.56823 : 0.34928 :: 0.21021 (2.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2270207554101944; velocity_v: 0.47487977147102356; specific_humidity: 0.15128612518310547; velocity_z: 0.7001808285713196; temperature: 0.16239279508590698; total_precip: 0.779965877532959; 
0: epoch: 27 [2/5 (40%)]	Loss: 0.77997 : 0.38349 :: 0.20882 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26379159092903137; velocity_v: 0.6484739184379578; specific_humidity: 0.14320482313632965; velocity_z: 0.7069165110588074; temperature: 0.19673395156860352; total_precip: 0.7908696532249451; 
0: epoch: 27 [3/5 (60%)]	Loss: 0.79087 : 0.42635 :: 0.21237 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2603206932544708; velocity_v: 0.4793948829174042; specific_humidity: 0.16538169980049133; velocity_z: 0.6787204146385193; temperature: 0.17924764752388; total_precip: 0.6884856820106506; 
0: epoch: 27 [4/5 (80%)]	Loss: 0.68849 : 0.37520 :: 0.21342 (15.94 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 27 : 0.32332557439804077
0: validation loss for velocity_u : 0.21117578446865082
0: validation loss for velocity_v : 0.4098418056964874
0: validation loss for specific_humidity : 0.13960662484169006
0: validation loss for velocity_z : 0.5215813517570496
0: validation loss for temperature : 0.14417536556720734
0: validation loss for total_precip : 0.5135721564292908
0: 28 : 10:45:55 :: batch_size = 96, lr = 1.0524694428278642e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 28, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1195, 1.2028, 1.3036, 1.4042, 1.4904, 1.5538, 1.5886, 1.5985, 1.5899, 1.5659, 1.5346, 1.5034, 1.4755, 1.4560,
0:         1.4460, 1.4423, 1.4451, 1.4536, 1.1636, 1.2596, 1.3654, 1.4591, 1.5263, 1.5621, 1.5685], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1170, 0.2090, 0.2762, 0.3124, 0.3210, 0.3086, 0.2836, 0.2516, 0.2188, 0.1962, 0.1892, 0.1942, 0.2062, 0.2202,
0:         0.2308, 0.2362, 0.2362, 0.2344, 0.2046, 0.2904, 0.3360, 0.3480, 0.3370, 0.3124, 0.2856], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6473, -0.6454, -0.6304, -0.6025, -0.5782, -0.5464, -0.5270, -0.5097, -0.5055, -0.5047, -0.5021, -0.4952,
0:         -0.4876, -0.4793, -0.4692, -0.4604, -0.4517, -0.4424, -0.6531, -0.6385, -0.6138, -0.5849, -0.5538, -0.5253,
0:         -0.5091], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5329,  0.1812, -0.2368, -0.5331, -0.6636, -0.5508, -0.2279,  0.1326,  0.4688,  0.7231,  0.8447,  0.9222,
0:          0.9752,  0.9244,  0.8182,  0.6722,  0.4555,  0.2410, -0.0090, -0.4292, -0.7676, -0.8759, -0.7853, -0.4624,
0:         -0.0090], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.8230, -0.8515, -0.8894, -0.9318, -0.9713, -0.9939, -0.9935, -0.9729, -0.9359, -0.8891, -0.8394, -0.7911,
0:         -0.7468, -0.7068, -0.6684, -0.6306, -0.5934, -0.5555, -0.5172, -0.4819, -0.4506, -0.4237, -0.4006, -0.3813,
0:         -0.3653], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.0436,  0.1328,  0.0636, -0.0233, -0.1169, -0.1326, -0.1771, -0.1883, -0.1749,  0.0012, -0.0122, -0.0434,
0:         -0.1058, -0.0924, -0.0924, -0.0679, -0.0835, -0.1103, -0.1482, -0.1705, -0.1259, -0.1147, -0.1013, -0.0389,
0:         -0.0724], device='cuda:0')
0: [DEBUG] Epoch 28, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,  0.6879,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.7035,  0.6991,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.4940,     nan,     nan,  0.8128,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3825,
0:             nan,  0.1350,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1236,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.0146,     nan,     nan,     nan,  0.5408,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0947,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.1573,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.0525,     nan,     nan,     nan,     nan,     nan,     nan,  0.0904,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.5542,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0659,  0.9533,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.5274,  0.6857,     nan,     nan,  0.6790,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0235,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 28, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.5478, 2.4580, 2.4366, 2.4474, 2.4767, 2.4530, 2.4337, 2.3920, 2.3498, 2.3440, 2.3158, 2.3110, 2.2834, 2.2571,
0:         2.2254, 2.1971, 2.1777, 2.1267, 2.6368, 2.5053, 2.4412, 2.4324, 2.4384, 2.4275, 2.3949], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3531, -0.3050, -0.2963, -0.3089, -0.3448, -0.3850, -0.4427, -0.4972, -0.5454, -0.5695, -0.5614, -0.5359,
0:         -0.5158, -0.5155, -0.5230, -0.5009, -0.4358, -0.3331, -0.3774, -0.3284, -0.3051, -0.3103, -0.3362, -0.3756,
0:         -0.4282], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7883, -0.7855, -0.7815, -0.7719, -0.7575, -0.7491, -0.7514, -0.7533, -0.7552, -0.7616, -0.7647, -0.7640,
0:         -0.7562, -0.7471, -0.7435, -0.7423, -0.7476, -0.7487, -0.7754, -0.7701, -0.7674, -0.7554, -0.7486, -0.7432,
0:         -0.7428], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8325, 0.8918, 1.0247, 0.9860, 0.9964, 1.2346, 1.5403, 1.6976, 1.8182, 1.9863, 2.0040, 2.0838, 2.1523, 1.6990,
0:         1.0694, 0.8337, 0.8481, 0.8087, 1.0461, 0.8137, 0.8586, 0.8846, 0.9506, 1.2711, 1.6794], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.5736, -0.5439, -0.5292, -0.5330, -0.5413, -0.5475, -0.5536, -0.5703, -0.5954, -0.6241, -0.6469, -0.6615,
0:         -0.6636, -0.6694, -0.6805, -0.7036, -0.7232, -0.7383, -0.7469, -0.7580, -0.7797, -0.8029, -0.8178, -0.8231,
0:         -0.8309], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.4293, 0.4162, 0.3652, 0.3425, 0.2858, 0.2617, 0.2643, 0.2686, 0.2772, 0.4666, 0.4366, 0.4038, 0.3468, 0.2957,
0:         0.2761, 0.2326, 0.2223, 0.2788, 0.5715, 0.5043, 0.4588, 0.4060, 0.3402, 0.3114, 0.2546], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.23719260096549988; velocity_v: 0.5877078175544739; specific_humidity: 0.15202750265598297; velocity_z: 0.6599240899085999; temperature: 0.15595705807209015; total_precip: 0.8144510984420776; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2603663206100464; velocity_v: 0.3586661219596863; specific_humidity: 0.1786685287952423; velocity_z: 0.47902753949165344; temperature: 0.16173873841762543; total_precip: 0.637398362159729; 
0: epoch: 28 [1/5 (20%)]	Loss: 0.72592 : 0.35836 :: 0.20872 (2.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22656531631946564; velocity_v: 0.49033671617507935; specific_humidity: 0.15717674791812897; velocity_z: 0.6138731241226196; temperature: 0.17305506765842438; total_precip: 0.6764967441558838; 
0: epoch: 28 [2/5 (40%)]	Loss: 0.67650 : 0.35753 :: 0.20708 (16.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23802480101585388; velocity_v: 0.491533488035202; specific_humidity: 0.16807429492473602; velocity_z: 0.6424098014831543; temperature: 0.22616609930992126; total_precip: 0.6890869736671448; 
0: epoch: 28 [3/5 (60%)]	Loss: 0.68909 : 0.37533 :: 0.20664 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23256951570510864; velocity_v: 0.44017818570137024; specific_humidity: 0.16672231256961823; velocity_z: 0.6932694315910339; temperature: 0.15403173863887787; total_precip: 0.47414520382881165; 
0: epoch: 28 [4/5 (80%)]	Loss: 0.47415 : 0.32817 :: 0.20814 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 28 : 0.3819705545902252
0: validation loss for velocity_u : 0.1858161836862564
0: validation loss for velocity_v : 0.4516744911670685
0: validation loss for specific_humidity : 0.11677499860525131
0: validation loss for velocity_z : 0.5567536354064941
0: validation loss for temperature : 0.13264301419258118
0: validation loss for total_precip : 0.8481614589691162
0: 29 : 10:49:54 :: batch_size = 96, lr = 1.0267994564174285e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 29, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7298, 0.7446, 0.7586, 0.7720, 0.7842, 0.7951, 0.8047, 0.8136, 0.8215, 0.8286, 0.8353, 0.8419, 0.8486, 0.8557,
0:         0.8630, 0.8710, 0.8795, 0.8885, 0.7968, 0.8091, 0.8210, 0.8327, 0.8439, 0.8541, 0.8635], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0240, 0.0274, 0.0274, 0.0244, 0.0189, 0.0122, 0.0067, 0.0040, 0.0054, 0.0130, 0.0289, 0.0554, 0.0931, 0.1420,
0:         0.2009, 0.2681, 0.3404, 0.4156, 0.0307, 0.0332, 0.0323, 0.0293, 0.0246, 0.0197, 0.0169], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2862, 0.2421, 0.1947, 0.1483, 0.1051, 0.0764, 0.0502, 0.0382, 0.0366, 0.0474, 0.0566, 0.0839, 0.1152, 0.1558,
0:         0.1960, 0.2366, 0.2754, 0.3077, 0.2802, 0.2364, 0.1844, 0.1420, 0.1030, 0.0753, 0.0662], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4192, -0.4568, -0.5022, -0.5432, -0.5697, -0.5963, -0.6306, -0.6649, -0.6860, -0.6981, -0.7059, -0.6959,
0:         -0.6550, -0.5941, -0.5277, -0.4469, -0.3406, -0.2233, -0.3827, -0.4203, -0.4712, -0.5277, -0.5720, -0.6074,
0:         -0.6494], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.1433, 1.1325, 1.1199, 1.1057, 1.0898, 1.0724, 1.0540, 1.0351, 1.0161, 0.9971, 0.9786, 0.9601, 0.9424, 0.9254,
0:         0.9101, 0.8966, 0.8848, 0.8743, 0.8648, 0.8557, 0.8469, 0.8387, 0.8315, 0.8251, 0.8192], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1766, -0.1721, -0.1629, -0.1698, -0.1880, -0.1789, -0.1561, -0.1493, -0.1356, -0.1379, -0.1333, -0.1402,
0:         -0.1561, -0.1766, -0.1698, -0.1721, -0.1629, -0.1516, -0.1538, -0.1629, -0.1652, -0.1766, -0.1925, -0.1994,
0:         -0.2017], device='cuda:0')
0: [DEBUG] Epoch 29, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.1698,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1106,     nan,     nan,     nan,     nan,     nan, -0.1766,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2290,     nan,     nan,     nan,     nan,     nan,     nan, -0.1652,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1333,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2222,     nan,     nan,     nan,     nan,     nan,     nan, -0.1948, -0.1857,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1561,
0:             nan,     nan, -0.1743,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1789,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2244,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1561,     nan,
0:             nan,     nan, -0.2085,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1698,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2017,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2335,     nan,     nan,     nan,     nan,     nan, -0.2335,     nan,
0:             nan,     nan,     nan, -0.2290,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2267])
0: [DEBUG] Epoch 29, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6765, 0.6282, 0.6226, 0.6265, 0.6154, 0.5573, 0.4967, 0.4360, 0.3854, 0.3820, 0.3659, 0.3649, 0.3422, 0.3076,
0:         0.2686, 0.2316, 0.2101, 0.1755, 0.7347, 0.6482, 0.6106, 0.6030, 0.5783, 0.5282, 0.4753], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1886, -0.1049, -0.0640, -0.0682, -0.0999, -0.1216, -0.1459, -0.1598, -0.1744, -0.1753, -0.1594, -0.1221,
0:         -0.0917, -0.0930, -0.1291, -0.1567, -0.1507, -0.1032, -0.2231, -0.1269, -0.0637, -0.0573, -0.0836, -0.1050,
0:         -0.1230], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2192, -0.2158, -0.2224, -0.2338, -0.2479, -0.2676, -0.2922, -0.3095, -0.3263, -0.3390, -0.3434, -0.3508,
0:         -0.3455, -0.3476, -0.3657, -0.3708, -0.3839, -0.3704, -0.2583, -0.2496, -0.2498, -0.2456, -0.2557, -0.2734,
0:         -0.2866], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4790, -0.3813, -0.1780, -0.2006, -0.3338, -0.2935, -0.2696, -0.2748, -0.2219, -0.2628, -0.3289, -0.3240,
0:         -0.3115, -0.3015, -0.3632, -0.3493, -0.1433, -0.0782, -0.4270, -0.4797, -0.4395, -0.4577, -0.4834, -0.3704,
0:         -0.3207], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.4349, 1.3554, 1.3030, 1.2952, 1.3226, 1.3610, 1.3855, 1.3955, 1.3976, 1.3956, 1.3838, 1.3691, 1.3572, 1.3630,
0:         1.3826, 1.4039, 1.4097, 1.3988, 1.3820, 1.3731, 1.3756, 1.3740, 1.3537, 1.3194, 1.2867], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1757, -0.1715, -0.1888, -0.1806, -0.1800, -0.1760, -0.1770, -0.1708, -0.1704, -0.1759, -0.1827, -0.1774,
0:         -0.1797, -0.1769, -0.1711, -0.1731, -0.1693, -0.1676, -0.1781, -0.1751, -0.1751, -0.1676, -0.1687, -0.1737,
0:         -0.1737], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.25377246737480164; velocity_v: 0.45187199115753174; specific_humidity: 0.1589551717042923; velocity_z: 0.5524473786354065; temperature: 0.2280436009168625; total_precip: 0.9993674755096436; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2326103150844574; velocity_v: 0.482572078704834; specific_humidity: 0.16178523004055023; velocity_z: 0.5755912065505981; temperature: 0.1697496771812439; total_precip: 0.8577181100845337; 
0: epoch: 29 [1/5 (20%)]	Loss: 0.92854 : 0.39468 :: 0.20703 (2.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26806801557540894; velocity_v: 0.44446223974227905; specific_humidity: 0.1616869419813156; velocity_z: 0.6508383750915527; temperature: 0.1765889823436737; total_precip: 0.799743115901947; 
0: epoch: 29 [2/5 (40%)]	Loss: 0.79974 : 0.38365 :: 0.21334 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2031504064798355; velocity_v: 0.4027807414531708; specific_humidity: 0.1686101108789444; velocity_z: 0.432982861995697; temperature: 0.14485721290111542; total_precip: 0.5950888991355896; 
0: epoch: 29 [3/5 (60%)]	Loss: 0.59509 : 0.29382 :: 0.20086 (15.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.27064400911331177; velocity_v: 0.46136054396629333; specific_humidity: 0.13521526753902435; velocity_z: 0.554802656173706; temperature: 0.1660197228193283; total_precip: 0.6430147886276245; 
0: epoch: 29 [4/5 (80%)]	Loss: 0.64301 : 0.34043 :: 0.21096 (15.74 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 29 : 0.35688114166259766
0: validation loss for velocity_u : 0.18426716327667236
0: validation loss for velocity_v : 0.4025540351867676
0: validation loss for specific_humidity : 0.13552232086658478
0: validation loss for velocity_z : 0.53606778383255
0: validation loss for temperature : 0.12003717571496964
0: validation loss for total_precip : 0.76283860206604
0: 30 : 10:53:50 :: batch_size = 96, lr = 1.0017555672365157e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 30, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2835, 0.3513, 0.3879, 0.4338, 0.5276, 0.6223, 0.6295, 0.5297, 0.3784, 0.2518, 0.2002, 0.2067, 0.2139, 0.2090,
0:         0.2250, 0.2723, 0.3385, 0.4293, 0.2569, 0.3109, 0.3543, 0.4073, 0.5011, 0.5838, 0.5642], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.4993, 2.4750, 2.4625, 2.4457, 2.4216, 2.4007, 2.3855, 2.3777, 2.3937, 2.4516, 2.5521, 2.6880, 2.8566, 3.0485,
0:         3.2428, 3.4202, 3.5677, 3.6735, 2.3635, 2.3608, 2.3576, 2.3387, 2.3159, 2.3035, 2.2966], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2771, -0.2829, -0.2783, -0.2584, -0.2243, -0.1961, -0.1481, -0.1238, -0.1140, -0.1097, -0.0865, -0.0572,
0:         -0.0115,  0.0291,  0.0722,  0.1046,  0.1196,  0.1550, -0.2844, -0.2894, -0.2857, -0.2466, -0.2138, -0.1781,
0:         -0.1329], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6610,  0.8524,  2.1180,  2.5063,  1.3043, -1.0141, -2.5298, -2.0845, -0.7224, -0.1236, -0.7860, -2.1525,
0:         -3.4598, -4.2406, -4.5389, -4.6508, -4.8328, -5.3154,  1.0082,  2.2343,  2.9274,  2.5194,  0.6550, -1.7774,
0:         -3.0847], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-2.6868, -2.6950, -2.6954, -2.6789, -2.6358, -2.5568, -2.4652, -2.4177, -2.4282, -2.4418, -2.4105, -2.3295,
0:         -2.2153, -2.0988, -2.0050, -1.9313, -1.8678, -1.8065, -1.7197, -1.5933, -1.4551, -1.3318, -1.2212, -1.1283,
0:         -1.0783], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2386, -0.2269, -0.2153, -0.1499,  0.2589,  0.3687,  0.5369,  1.2983,  1.7234, -0.2223, -0.1989, -0.1125,
0:          0.0814,  0.7774,  1.5669,  2.1976,  3.1459,  4.1246, -0.0681, -0.0074,  0.3710,  0.9129,  2.2139,  3.0244,
0:          3.8676], device='cuda:0')
0: [DEBUG] Epoch 30, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2503,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2199,
0:             nan, -0.2456,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0389,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2503,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2234,     nan,     nan,     nan,     nan,     nan,     nan, -0.2363,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1001,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1592,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2503,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2503,     nan,     nan,     nan,
0:             nan,     nan, -0.2012,     nan,     nan,     nan,     nan,     nan,  0.2682,     nan,     nan, -0.0226,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0751,     nan,
0:         -0.2503, -0.1919,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 30, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.0551, 2.0172, 2.0195, 2.0568, 2.0933, 2.0873, 2.0614, 2.0094, 1.9528, 1.9334, 1.9163, 1.9334, 1.9517, 1.9912,
0:         2.0310, 2.0635, 2.0832, 2.0577, 2.1439, 2.0310, 1.9765, 1.9638, 1.9600, 1.9395, 1.9004], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-9.2777e-02, -5.4440e-02, -2.5893e-02, -1.5686e-02, -2.6061e-02, -4.3974e-02, -7.8788e-02, -1.1562e-01,
0:         -1.5033e-01, -1.7322e-01, -1.8678e-01, -1.8553e-01, -1.8594e-01, -2.0762e-01, -2.4064e-01, -2.4965e-01,
0:         -2.2758e-01, -1.7554e-01, -1.0372e-01, -5.5865e-02, -6.0643e-03,  9.1212e-03,  7.5754e-05, -2.8337e-02,
0:         -6.5150e-02], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0766, -0.0751, -0.0808, -0.0860, -0.0940, -0.1061, -0.1242, -0.1500, -0.1727, -0.1939, -0.2050, -0.2126,
0:         -0.2091, -0.2071, -0.2190, -0.2310, -0.2592, -0.2839, -0.1001, -0.0991, -0.1078, -0.1114, -0.1214, -0.1386,
0:         -0.1550], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2400, 1.6008, 1.8395, 1.6008, 1.4698, 1.6848, 1.8575, 1.9728, 2.0493, 1.9494, 1.8958, 2.1019, 2.1791, 2.0260,
0:         1.8722, 1.8413, 1.8904, 1.6341, 1.2503, 1.2050, 1.2569, 1.1299, 1.1586, 1.4370, 1.6909], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3497, -0.3379, -0.3341, -0.3394, -0.3508, -0.3690, -0.4058, -0.4724, -0.5463, -0.6084, -0.6417, -0.6424,
0:         -0.6230, -0.6136, -0.6312, -0.6732, -0.7189, -0.7491, -0.7544, -0.7483, -0.7423, -0.7351, -0.7152, -0.6832,
0:         -0.6595], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0502,  0.0364,  0.0031, -0.0480, -0.0752, -0.0985, -0.1171, -0.1134, -0.0994,  0.0576,  0.0097, -0.0307,
0:         -0.0660, -0.1082, -0.1211, -0.1501, -0.1374, -0.0987,  0.0610,  0.0119, -0.0337, -0.0729, -0.1157, -0.1325,
0:         -0.1355], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.22617416083812714; velocity_v: 0.43100622296333313; specific_humidity: 0.12891817092895508; velocity_z: 0.4831515848636627; temperature: 0.13886775076389313; total_precip: 0.35811999440193176; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24324849247932434; velocity_v: 0.40800976753234863; specific_humidity: 0.1475685089826584; velocity_z: 0.49081042408943176; temperature: 0.15101508796215057; total_precip: 0.6927824020385742; 
0: epoch: 30 [1/5 (20%)]	Loss: 0.52545 : 0.29390 :: 0.21121 (2.58 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2735821306705475; velocity_v: 0.3466850519180298; specific_humidity: 0.16594120860099792; velocity_z: 0.6113207936286926; temperature: 0.20269794762134552; total_precip: 0.7492325305938721; 
0: epoch: 30 [2/5 (40%)]	Loss: 0.74923 : 0.35891 :: 0.21410 (15.89 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2696225643157959; velocity_v: 0.4978593587875366; specific_humidity: 0.13842380046844482; velocity_z: 0.7326919436454773; temperature: 0.17288196086883545; total_precip: 0.4957730174064636; 
0: epoch: 30 [3/5 (60%)]	Loss: 0.49577 : 0.35269 :: 0.21235 (16.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.274160236120224; velocity_v: 0.4144930839538574; specific_humidity: 0.14758707582950592; velocity_z: 0.6501977443695068; temperature: 0.18415632843971252; total_precip: 0.7760908603668213; 
0: epoch: 30 [4/5 (80%)]	Loss: 0.77609 : 0.37574 :: 0.21601 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 30 : 0.3768290877342224
0: validation loss for velocity_u : 0.2011173516511917
0: validation loss for velocity_v : 0.4801972210407257
0: validation loss for specific_humidity : 0.12614582479000092
0: validation loss for velocity_z : 0.6026957035064697
0: validation loss for temperature : 0.1279083639383316
0: validation loss for total_precip : 0.7229102253913879
0: 31 : 10:57:34 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 31, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6170, -0.6246, -0.6459, -0.6854, -0.7279, -0.7490, -0.7335, -0.6856, -0.6246, -0.5686, -0.5206, -0.4654,
0:         -0.3851, -0.2774, -0.1617, -0.0638,  0.0038,  0.0457, -0.5970, -0.6076, -0.6276, -0.6618, -0.6973, -0.7127,
0:         -0.6952], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1842, -1.2143, -1.2627, -1.3550, -1.4753, -1.5709, -1.5948, -1.5410, -1.4430, -1.3379, -1.2362, -1.1209,
0:         -0.9729, -0.7895, -0.5875, -0.3905, -0.2164, -0.0705, -1.1747, -1.2113, -1.2568, -1.3297, -1.4177, -1.4801,
0:         -1.4799], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1557,  0.1349,  0.1466,  0.0644,  0.0069,  0.0105, -0.0216,  0.0089,  0.0348, -0.0056, -0.1043, -0.1788,
0:         -0.2575, -0.3305, -0.3566, -0.3728, -0.4088, -0.4470,  0.1909,  0.1993,  0.2112,  0.1284,  0.0566,  0.0124,
0:         -0.0200], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1120,  0.1542,  0.5358,  0.6290,  0.3006, -0.2052, -0.5424, -0.5735, -0.4293, -0.2939, -0.2362, -0.2229,
0:         -0.2251, -0.2651, -0.3716, -0.4914, -0.5113, -0.3383, -0.1076, -0.0055,  0.3250,  0.5425,  0.3628, -0.1164,
0:         -0.5136], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2100, -0.2550, -0.3056, -0.3571, -0.4092, -0.4641, -0.5201, -0.5739, -0.6260, -0.6842, -0.7583, -0.8504,
0:         -0.9535, -1.0606, -1.1704, -1.2841, -1.3968, -1.4977, -1.5752, -1.6262, -1.6536, -1.6618, -1.6552, -1.6396,
0:         -1.6215], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2074,  0.3271,  0.5842,  0.5729,  0.3744,  0.0700, -0.0991, -0.1871, -0.2434, -0.2299, -0.2051,  0.0542,
0:          0.0430,  0.0475,  0.0430, -0.1262, -0.2141, -0.2457, -0.2434, -0.2434, -0.2434, -0.2367, -0.2299, -0.2164,
0:         -0.2164], device='cuda:0')
0: [DEBUG] Epoch 31, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([   nan, 3.2463,    nan,    nan, 0.4568,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.9632,
0:            nan,    nan,    nan,    nan,    nan, 4.4212,    nan,    nan,    nan,    nan,    nan, 2.1424,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.7050,
0:         1.9711, 2.6329,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 7.8309,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.8021,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 4.0728,    nan,    nan,    nan,    nan,    nan,
0:            nan, 4.6039,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan, 2.9712, 4.3073,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan, 0.6124,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.0477,    nan,    nan,    nan, 2.9689, 2.0669,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan, 2.1774,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.1976,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.6000,    nan,    nan, 4.1529,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 5.5600, 3.4865,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan])
0: [DEBUG] Epoch 31, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0320,  0.0760,  0.1368,  0.1893,  0.2084,  0.1958,  0.1876,  0.1680,  0.1396,  0.0943,  0.0229, -0.0620,
0:         -0.1351, -0.1638, -0.1465, -0.0816, -0.0081,  0.0541,  0.0478,  0.0753,  0.1261,  0.1620,  0.1617,  0.1518,
0:          0.1494], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3226, 0.3126, 0.2887, 0.2569, 0.2181, 0.1933, 0.1678, 0.1476, 0.1340, 0.1394, 0.1523, 0.1773, 0.1930, 0.1736,
0:         0.1273, 0.0959, 0.0791, 0.0807, 0.2496, 0.2549, 0.2491, 0.2233, 0.1905, 0.1619, 0.1380], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1878, -0.2142, -0.2467, -0.2738, -0.2964, -0.3122, -0.3337, -0.3506, -0.3682, -0.3857, -0.3998, -0.4092,
0:         -0.4079, -0.4010, -0.3922, -0.3767, -0.3645, -0.3479, -0.2097, -0.2376, -0.2688, -0.2912, -0.3185, -0.3363,
0:         -0.3498], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2865, -0.2138, -0.0808, -0.0340, -0.1424, -0.2400, -0.3558, -0.4603, -0.5340, -0.6313, -0.6623, -0.6151,
0:         -0.5018, -0.4049, -0.3749, -0.2032, -0.0797, -0.2087, -0.5150, -0.5203, -0.3269, -0.0987, -0.0341, -0.0495,
0:         -0.1803], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.8077, -0.7306, -0.6564, -0.5925, -0.5340, -0.4836, -0.4516, -0.4468, -0.4643, -0.4864, -0.5011, -0.5010,
0:         -0.4866, -0.4675, -0.4462, -0.4227, -0.3868, -0.3418, -0.2951, -0.2626, -0.2519, -0.2476, -0.2326, -0.2053,
0:         -0.1768], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([2.5323, 2.6181, 2.6336, 2.6033, 2.5160, 2.4000, 2.2357, 2.1000, 2.0079, 2.7272, 2.8187, 2.8117, 2.7829, 2.6340,
0:         2.4741, 2.3284, 2.1710, 2.0505, 2.6996, 2.7706, 2.8457, 2.7714, 2.6951, 2.5484, 2.3590], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.23084883391857147; velocity_v: 0.42613473534584045; specific_humidity: 0.15456321835517883; velocity_z: 0.6363097429275513; temperature: 0.17740805447101593; total_precip: 0.7956804037094116; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2918621003627777; velocity_v: 0.4237919747829437; specific_humidity: 0.15551060438156128; velocity_z: 0.5504518151283264; temperature: 0.24322643876075745; total_precip: 0.6579516530036926; 
0: epoch: 31 [1/5 (20%)]	Loss: 0.72682 : 0.36327 :: 0.21083 (2.40 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23610202968120575; velocity_v: 0.4379075765609741; specific_humidity: 0.151911199092865; velocity_z: 0.537705659866333; temperature: 0.14107787609100342; total_precip: 0.6004428267478943; 
0: epoch: 31 [2/5 (40%)]	Loss: 0.60044 : 0.31919 :: 0.20961 (16.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2631385326385498; velocity_v: 0.520540177822113; specific_humidity: 0.12409903854131699; velocity_z: 0.5291250348091125; temperature: 0.1459842026233673; total_precip: 0.5311905145645142; 
0: epoch: 31 [3/5 (60%)]	Loss: 0.53119 : 0.32158 :: 0.21202 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2552768886089325; velocity_v: 0.4864942133426666; specific_humidity: 0.1478618085384369; velocity_z: 0.5899096727371216; temperature: 0.14880767464637756; total_precip: 0.6124829649925232; 
0: epoch: 31 [4/5 (80%)]	Loss: 0.61248 : 0.34160 :: 0.21017 (16.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 31 : 0.35232365131378174
0: validation loss for velocity_u : 0.1776641607284546
0: validation loss for velocity_v : 0.3825152814388275
0: validation loss for specific_humidity : 0.13472948968410492
0: validation loss for velocity_z : 0.5707084536552429
0: validation loss for temperature : 0.12496444582939148
0: validation loss for total_precip : 0.7233607172966003
0: 32 : 11:01:22 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 32, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4517, -0.4687, -0.4830, -0.5056, -0.5342, -0.5595, -0.5586, -0.5454, -0.5580, -0.6158, -0.6902, -0.7180,
0:         -0.7089, -0.7030, -0.6823, -0.6594, -0.6712, -0.7054, -0.4943, -0.5242, -0.5524, -0.5750, -0.5950, -0.6098,
0:         -0.5965], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4238, 0.4188, 0.4233, 0.4311, 0.4311, 0.4139, 0.3597, 0.2624, 0.1476, 0.0786, 0.0995, 0.1660, 0.1942, 0.1714,
0:         0.1403, 0.0982, 0.0417, 0.0227, 0.3770, 0.3852, 0.3996, 0.4059, 0.3970, 0.3718, 0.3209], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.8382, 2.0037, 2.2956, 2.7658, 3.2075, 3.5809, 3.6218, 3.7717, 3.9710, 4.1394, 4.2567, 4.1667, 4.1461, 4.0421,
0:         3.8988, 3.7615, 3.7556, 3.7120, 1.8254, 2.0298, 2.4713, 2.9998, 3.4310, 3.6730, 3.6600], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-3.4980e-01, -1.2619e-01,  1.1762e-02,  1.1411e-01,  1.2079e-01,  4.2005e-01,  5.4020e-01,  1.7085e-01,
0:          3.7349e-02, -5.8231e-01, -8.5599e-01, -1.2841e-01,  4.8903e-01,  3.8445e-01, -9.8838e-01, -1.0629e+00,
0:          2.9100e-01,  1.7307e-01, -3.1420e-01, -1.2619e-01, -8.1688e-02, -4.7577e-04, -9.8376e-02,  3.2104e-01,
0:          5.4020e-01], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.2162,  0.2232,  0.2152,  0.1886,  0.0669, -0.0618, -0.1358, -0.2111, -0.3480, -0.4657, -0.4904, -0.5735,
0:         -0.6631, -0.5786, -0.3931, -0.2048, -0.1473, -0.2583, -0.2870, -0.1404,  0.0280,  0.1086,  0.0980,  0.0785,
0:          0.1043], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.1057,  0.0739,  0.1116,  0.0586, -0.0841, -0.1690, -0.1701, -0.1206, -0.1383,  0.0432,  0.0763,  0.1046,
0:         -0.0322, -0.1996, -0.1760, -0.1407, -0.1242, -0.1100, -0.1513, -0.0074,  0.2885,  0.4582,  0.3274,  0.1116,
0:          0.0173], device='cuda:0')
0: [DEBUG] Epoch 32, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1265,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0900,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2991,
0:             nan,     nan,     nan,     nan,     nan,  0.1057,     nan,     nan,     nan,  0.2354,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.1564,     nan,     nan,     nan,     nan,     nan,     nan, -0.2562,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1890,     nan,     nan, -0.2574,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1890,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2373,     nan,     nan,
0:             nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2574,     nan, -0.2456,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2090,     nan,     nan,     nan,     nan,     nan,     nan, -0.2491,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan, -0.2574,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan, -0.2562,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2385, -0.2550,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574, -0.2574,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 32, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6965, -0.5843, -0.5059, -0.4935, -0.5215, -0.5416, -0.5078, -0.4219, -0.3372, -0.2817, -0.2927, -0.3641,
0:         -0.4540, -0.5054, -0.4977, -0.4381, -0.3764, -0.3223, -0.7370, -0.6139, -0.5371, -0.5431, -0.6050, -0.6203,
0:         -0.5786], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0798,  0.0877,  0.0788,  0.0538,  0.0164, -0.0145, -0.0507, -0.0777, -0.0888, -0.0845, -0.0698, -0.0450,
0:         -0.0282, -0.0333, -0.0507, -0.0481, -0.0292, -0.0003,  0.0254,  0.0528,  0.0660,  0.0551,  0.0180, -0.0265,
0:         -0.0680], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([3.5523, 3.6346, 3.7123, 3.7473, 3.7691, 3.7303, 3.7324, 3.7547, 3.8179, 3.9179, 4.0410, 4.0952, 4.1611, 4.1828,
0:         4.1273, 4.1388, 4.1461, 4.1736, 3.5867, 3.7238, 3.8610, 3.9408, 3.9679, 3.9235, 3.8929], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1368,  0.0863,  0.1880,  0.3636,  0.4910,  0.3796,  0.2947,  0.1916, -0.1690, -0.1616,  0.0150,  0.0126,
0:          0.1903,  0.1796, -0.2038, -0.2338,  0.0803,  0.0176,  0.4920,  0.1502,  0.0202,  0.1819,  0.3904,  0.3056,
0:          0.3185], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4319, 0.2946, 0.2051, 0.1955, 0.2898, 0.4613, 0.6653, 0.8514, 0.9771, 1.0207, 0.9817, 0.9036, 0.8302, 0.8176,
0:         0.8575, 0.9243, 0.9701, 0.9933, 1.0110, 1.0489, 1.1179, 1.1892, 1.2387, 1.2745, 1.3281], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([1.0751, 0.9697, 0.8040, 0.6340, 0.5075, 0.4066, 0.3506, 0.3106, 0.3467, 1.1163, 0.9490, 0.7800, 0.6486, 0.5131,
0:         0.4444, 0.3605, 0.3352, 0.3354, 1.1839, 0.9971, 0.8736, 0.7210, 0.5810, 0.5173, 0.4492], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.21057792007923126; velocity_v: 0.46322309970855713; specific_humidity: 0.1564311385154724; velocity_z: 0.5882164835929871; temperature: 0.1777246594429016; total_precip: 0.6043833494186401; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2104427069425583; velocity_v: 0.43367886543273926; specific_humidity: 0.16597309708595276; velocity_z: 0.5489518046379089; temperature: 0.1796417236328125; total_precip: 0.5882588624954224; 
0: epoch: 32 [1/5 (20%)]	Loss: 0.59632 : 0.32856 :: 0.20294 (2.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23108112812042236; velocity_v: 0.40612712502479553; specific_humidity: 0.16188183426856995; velocity_z: 0.5546510815620422; temperature: 0.16368122398853302; total_precip: 0.6204220056533813; 
0: epoch: 32 [2/5 (40%)]	Loss: 0.62042 : 0.32453 :: 0.20887 (15.91 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2638374865055084; velocity_v: 0.6359357833862305; specific_humidity: 0.1484774798154831; velocity_z: 0.6686286926269531; temperature: 0.1800273358821869; total_precip: 0.7282628417015076; 
0: epoch: 32 [3/5 (60%)]	Loss: 0.72826 : 0.40508 :: 0.21237 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22358596324920654; velocity_v: 0.43104755878448486; specific_humidity: 0.16314437985420227; velocity_z: 0.697506844997406; temperature: 0.19690117239952087; total_precip: 1.2458337545394897; 
0: epoch: 32 [4/5 (80%)]	Loss: 1.24583 : 0.45976 :: 0.20855 (16.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 32 : 0.3260074555873871
0: validation loss for velocity_u : 0.18918444216251373
0: validation loss for velocity_v : 0.4621995985507965
0: validation loss for specific_humidity : 0.1278332620859146
0: validation loss for velocity_z : 0.5338246822357178
0: validation loss for temperature : 0.11317893117666245
0: validation loss for total_precip : 0.5298241972923279
0: 33 : 11:05:25 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 33, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0238, -1.0052, -1.0059, -1.0096, -1.0070, -0.9997, -1.0183, -1.0912, -1.2057, -1.2978, -1.3220, -1.2980,
0:         -1.2454, -1.2059, -1.1804, -1.1520, -1.1449, -1.1502, -1.0088, -0.9999, -1.0064, -1.0156, -1.0220, -1.0344,
0:         -1.0780], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2325, -0.1933, -0.1530, -0.1196, -0.0974, -0.0987, -0.1347, -0.1889, -0.2528, -0.3357, -0.4405, -0.5260,
0:         -0.5606, -0.4933, -0.3433, -0.2493, -0.1741, -0.0684, -0.2094, -0.1586, -0.1041, -0.0612, -0.0418, -0.0581,
0:         -0.1157], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.1348, 2.2522, 2.3900, 2.5531, 2.7683, 3.0518, 3.3336, 3.5154, 3.6202, 3.5155, 3.3097, 2.9911, 2.7507, 2.7308,
0:         2.9268, 3.0040, 3.0884, 3.1725, 2.1170, 2.2459, 2.3882, 2.5628, 2.7982, 3.0834, 3.3051], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1782, -0.1340, -0.1053, -0.1075, -0.1495, -0.0589, -0.2136, -0.7374, -1.0402, -0.9739, -0.3882,  0.0052,
0:          0.2086,  0.2859,  0.0119,  0.1577,  0.2041, -0.2246, -0.1407, -0.0810, -0.0655,  0.0119,  0.0406,  0.0141,
0:         -0.1384], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0375,  0.0347,  0.0907,  0.0910,  0.0405, -0.0490, -0.1364, -0.0843,  0.0835,  0.1804,  0.2030,  0.1900,
0:          0.2460,  0.2559,  0.1014,  0.0722,  0.0298, -0.1371, -0.2499, -0.2720, -0.2782, -0.4137, -0.4349, -0.3466,
0:         -0.3584], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1234, -0.1531, -0.0276, -0.0504,  0.0317,  0.1002,  0.1938,  0.1687,  0.0546, -0.1988, -0.1851,  0.0135,
0:         -0.1577, -0.0961, -0.0459,  0.0249, -0.0002, -0.0390, -0.1942, -0.1988, -0.1851, -0.1942, -0.1851, -0.1212,
0:         -0.0573], device='cuda:0')
0: [DEBUG] Epoch 33, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,  0.3296,     nan,     nan,     nan,     nan,     nan,  0.4186,  0.7803,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.5372,     nan,     nan,     nan,     nan, -0.1988,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1748,     nan,     nan,     nan,     nan,     nan,  0.2257,
0:             nan, -0.2102,     nan,     nan,     nan,     nan,     nan,  0.9229,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1303, -0.1885,     nan, -0.1531,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1098, -0.1577,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0653,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.2691,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2193,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2022,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3227,     nan,     nan,
0:             nan,     nan,  1.2013,     nan,     nan,     nan,     nan,     nan,  0.4288,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.2177,     nan,     nan,     nan,     nan,  0.5715,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1953,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.1630,     nan,     nan,     nan,  0.5658,     nan,     nan,  0.1812,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 33, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8337, -0.7166, -0.6421, -0.6575, -0.7339, -0.8118, -0.8448, -0.8143, -0.7643, -0.7160, -0.7036, -0.7574,
0:         -0.8365, -0.8974, -0.9277, -0.8861, -0.8086, -0.7093, -0.7853, -0.6818, -0.6292, -0.6702, -0.7725, -0.8408,
0:         -0.8592], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0420,  0.0965,  0.1121,  0.1105,  0.0959,  0.1047,  0.1061,  0.1039,  0.0896,  0.0759,  0.0614,  0.0525,
0:          0.0355, -0.0108, -0.0860, -0.1297, -0.1358, -0.1091,  0.0039,  0.0563,  0.0812,  0.0703,  0.0538,  0.0525,
0:          0.0658], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([3.4856, 3.4486, 3.4034, 3.3785, 3.3649, 3.3618, 3.3856, 3.4219, 3.4540, 3.4873, 3.5103, 3.4808, 3.4234, 3.3530,
0:         3.2711, 3.2486, 3.2750, 3.3360, 3.2585, 3.2338, 3.2339, 3.2571, 3.2983, 3.3166, 3.3599], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3222, -0.2889, -0.0758, -0.0921, -0.1240,  0.0498, -0.0463, -0.1871, -0.1822, -0.3461, -0.4659, -0.3334,
0:         -0.1937, -0.3912, -0.7785, -0.7466, -0.4552, -0.4587, -0.2130, -0.4200, -0.2973, -0.2169, -0.0799,  0.1957,
0:          0.0964], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0133,  0.0697,  0.1408,  0.1806,  0.2113,  0.2411,  0.2726,  0.3008,  0.3194,  0.3380,  0.3656,  0.4041,
0:          0.4373,  0.4369,  0.3706,  0.2546,  0.1280,  0.0574,  0.0809,  0.1872,  0.3207,  0.4221,  0.4555,  0.4285,
0:          0.3679], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([1.1875, 1.0645, 0.8837, 0.6942, 0.5457, 0.4599, 0.4469, 0.4366, 0.4884, 1.2566, 1.0824, 0.8780, 0.6590, 0.4770,
0:         0.3854, 0.3515, 0.3675, 0.4047, 1.2308, 1.0584, 0.8400, 0.6232, 0.4449, 0.3362, 0.2654], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.27377790212631226; velocity_v: 0.41243216395378113; specific_humidity: 0.1680920124053955; velocity_z: 0.5662124156951904; temperature: 0.20815493166446686; total_precip: 0.6096491813659668; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2566331624984741; velocity_v: 0.4591040015220642; specific_humidity: 0.16718363761901855; velocity_z: 0.5949092507362366; temperature: 0.18519604206085205; total_precip: 0.7280371785163879; 
0: epoch: 33 [1/5 (20%)]	Loss: 0.66884 : 0.35349 :: 0.20517 (2.54 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2327662706375122; velocity_v: 0.5584120750427246; specific_humidity: 0.1548532098531723; velocity_z: 0.6934875845909119; temperature: 0.1777852326631546; total_precip: 0.836503267288208; 
0: epoch: 33 [2/5 (40%)]	Loss: 0.83650 : 0.40901 :: 0.21166 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2519010007381439; velocity_v: 0.4087676405906677; specific_humidity: 0.13415443897247314; velocity_z: 0.47818925976753235; temperature: 0.14696358144283295; total_precip: 0.4668706953525543; 
0: epoch: 33 [3/5 (60%)]	Loss: 0.46687 : 0.28342 :: 0.20891 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22247619926929474; velocity_v: 0.48813992738723755; specific_humidity: 0.14990057051181793; velocity_z: 0.5534563660621643; temperature: 0.17606258392333984; total_precip: 0.6571427583694458; 
0: epoch: 33 [4/5 (80%)]	Loss: 0.65714 : 0.34289 :: 0.20169 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 33 : 0.3108047842979431
0: validation loss for velocity_u : 0.15903958678245544
0: validation loss for velocity_v : 0.5358802676200867
0: validation loss for specific_humidity : 0.11416581273078918
0: validation loss for velocity_z : 0.49874624609947205
0: validation loss for temperature : 0.14057792723178864
0: validation loss for total_precip : 0.41641831398010254
0: 34 : 11:09:17 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 34, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4140, 0.4562, 0.4667, 0.4566, 0.4543, 0.4698, 0.5043, 0.5658, 0.6499, 0.7283, 0.7758, 0.7947, 0.8030, 0.8093,
0:         0.8125, 0.8151, 0.8213, 0.8302, 0.4455, 0.4752, 0.4816, 0.4719, 0.4727, 0.4976, 0.5496], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.6737, 1.6360, 1.5893, 1.5348, 1.4846, 1.4458, 1.4075, 1.3565, 1.2991, 1.2608, 1.2603, 1.2931, 1.3386, 1.3784,
0:         1.4042, 1.4151, 1.4110, 1.3944, 1.6368, 1.6010, 1.5641, 1.5184, 1.4716, 1.4348, 1.4052], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2389, -0.1893, -0.1632, -0.1426, -0.1400, -0.1353, -0.1348, -0.1222, -0.1203, -0.1150, -0.1303, -0.1671,
0:         -0.2190, -0.2679, -0.3206, -0.3654, -0.4121, -0.4496, -0.2028, -0.1619, -0.1404, -0.1241, -0.1220, -0.1150,
0:         -0.1073], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-3.0885, -2.9587, -2.4509, -2.1915, -1.9079, -1.4946, -1.6067, -2.3586, -2.6686, -1.8419, -0.6702, -0.1755,
0:         -0.2569, -0.3118, -0.2481, -0.2393, -0.2129, -0.0920, -2.2157, -2.1981, -1.9255, -1.8771, -1.8639, -1.6902,
0:         -1.7364], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.0160, 0.0444, 0.0773, 0.1178, 0.1559, 0.1819, 0.1945, 0.1978, 0.1980, 0.2097, 0.2505, 0.3237, 0.4141, 0.5052,
0:         0.5917, 0.6740, 0.7529, 0.8296, 0.9032, 0.9700, 1.0270, 1.0761, 1.1212, 1.1641, 1.2062], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([1.3918, 1.0509, 1.0175, 0.9532, 0.5956, 0.4311, 0.3047, 0.8745, 1.0342, 1.9068, 1.0628, 0.8554, 0.6718, 0.4835,
0:         0.3071, 0.1807, 0.6909, 0.8745, 1.4324, 0.6504, 0.4764, 0.3047, 0.1831, 0.1473, 0.1736], device='cuda:0')
0: [DEBUG] Epoch 34, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 3.0369,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.9053,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan, 2.3177,    nan,    nan, 2.5718,    nan, 2.5831,    nan,    nan,    nan,
0:            nan, 1.6982,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.4804, 1.1809,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan, 1.0493, 2.2292,    nan,    nan, 2.1225, 1.9410,    nan,    nan,
0:            nan,    nan, 2.1702,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 2.6240,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan, 2.0431,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.9047,    nan,
0:            nan,    nan,    nan,    nan, 2.4062,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan, 1.9024,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.0953,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.3699,    nan,    nan,    nan, 2.0023,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.3222,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 2.2972, 2.1793,    nan,    nan])
0: [DEBUG] Epoch 34, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9014, 0.8737, 0.9057, 0.9686, 1.0135, 1.0116, 0.9952, 0.9712, 0.9521, 0.9739, 0.9788, 0.9951, 0.9918, 0.9898,
0:         0.9821, 0.9757, 0.9792, 0.9564, 1.0033, 0.9117, 0.8993, 0.9182, 0.9258, 0.9088, 0.8857], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5646, 0.5468, 0.5474, 0.5566, 0.5589, 0.5550, 0.5253, 0.4909, 0.4676, 0.4566, 0.4538, 0.4506, 0.4333, 0.3869,
0:         0.3232, 0.2876, 0.2668, 0.2644, 0.4656, 0.4582, 0.4837, 0.5088, 0.5247, 0.5208, 0.4917], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2116,  0.2165,  0.2075,  0.1972,  0.1807,  0.1572,  0.1306,  0.1043,  0.0823,  0.0575,  0.0397,  0.0181,
0:          0.0042, -0.0121, -0.0501, -0.0606, -0.0765, -0.0561,  0.2069,  0.2166,  0.2134,  0.2080,  0.1880,  0.1616,
0:          0.1507], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1816, -1.2055, -1.1893, -1.2897, -1.3342, -1.2163, -1.1435, -1.0262, -0.6530, -0.2704, -0.0137,  0.2520,
0:          0.4323,  0.4125,  0.2157,  0.1294,  0.1392, -0.0527, -1.1707, -1.3155, -1.3548, -1.4421, -1.5007, -1.3725,
0:         -1.2139], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.0601, 0.0258, 0.0068, 0.0075, 0.0207, 0.0340, 0.0424, 0.0456, 0.0542, 0.0717, 0.0938, 0.1210, 0.1532, 0.1887,
0:         0.2281, 0.2562, 0.2677, 0.2600, 0.2401, 0.2178, 0.1974, 0.1774, 0.1557, 0.1314, 0.1055], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([2.1664, 2.2877, 2.4002, 2.4733, 2.5140, 2.5283, 2.4942, 2.4456, 2.3748, 2.3811, 2.5172, 2.6062, 2.6761, 2.6757,
0:         2.6590, 2.5892, 2.5022, 2.4207, 2.5166, 2.6346, 2.7359, 2.7442, 2.7404, 2.6892, 2.5822], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.2225073277950287; velocity_v: 0.5982726216316223; specific_humidity: 0.13638705015182495; velocity_z: 0.6308372616767883; temperature: 0.17822900414466858; total_precip: 0.46241429448127747; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24028421938419342; velocity_v: 0.5271582007408142; specific_humidity: 0.1241370439529419; velocity_z: 0.5620048642158508; temperature: 0.2777926027774811; total_precip: 0.7076075673103333; 
0: epoch: 34 [1/5 (20%)]	Loss: 0.58501 : 0.35732 :: 0.20807 (2.43 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2310371994972229; velocity_v: 0.36924517154693604; specific_humidity: 0.1759699285030365; velocity_z: 0.6459585428237915; temperature: 0.16163097321987152; total_precip: 0.9103639721870422; 
0: epoch: 34 [2/5 (40%)]	Loss: 0.91036 : 0.38334 :: 0.20419 (16.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2358718365430832; velocity_v: 0.45951467752456665; specific_humidity: 0.1501031517982483; velocity_z: 0.5926063656806946; temperature: 0.16944190859794617; total_precip: 0.9195627570152283; 
0: epoch: 34 [3/5 (60%)]	Loss: 0.91956 : 0.38927 :: 0.20511 (16.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26424866914749146; velocity_v: 0.4075966477394104; specific_humidity: 0.15269534289836884; velocity_z: 0.4853270947933197; temperature: 0.14840154349803925; total_precip: 0.3353543281555176; 
0: epoch: 34 [4/5 (80%)]	Loss: 0.33535 : 0.26739 :: 0.20795 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 34 : 0.3171065151691437
0: validation loss for velocity_u : 0.19114762544631958
0: validation loss for velocity_v : 0.4288698434829712
0: validation loss for specific_humidity : 0.12398514151573181
0: validation loss for velocity_z : 0.47741472721099854
0: validation loss for temperature : 0.15093737840652466
0: validation loss for total_precip : 0.5302841067314148
0: 35 : 11:13:06 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 35, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.5317, -1.5364, -1.5408, -1.5449, -1.5488, -1.5526, -1.5561, -1.5593, -1.5623, -1.5652, -1.5678, -1.5702,
0:         -1.5723, -1.5742, -1.5758, -1.5773, -1.5787, -1.5798, -1.5432, -1.5477, -1.5520, -1.5561, -1.5599, -1.5635,
0:         -1.5669], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6834, -0.6804, -0.6771, -0.6737, -0.6702, -0.6666, -0.6627, -0.6589, -0.6548, -0.6505, -0.6461, -0.6414,
0:         -0.6366, -0.6315, -0.6262, -0.6207, -0.6151, -0.6092, -0.6972, -0.6937, -0.6899, -0.6860, -0.6818, -0.6775,
0:         -0.6728], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7050, -0.7052, -0.7053, -0.7054, -0.7055, -0.7057, -0.7058, -0.7060, -0.7061, -0.7063, -0.7064, -0.7066,
0:         -0.7067, -0.7068, -0.7071, -0.7072, -0.7073, -0.7075, -0.7008, -0.7010, -0.7013, -0.7015, -0.7018, -0.7020,
0:         -0.7022], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7435, 0.7347, 0.7259, 0.7160, 0.7072, 0.6962, 0.6852, 0.6731, 0.6599, 0.6456, 0.6302, 0.6160, 0.6017, 0.5885,
0:         0.5764, 0.5654, 0.5555, 0.5478, 0.6050, 0.6017, 0.5995, 0.5984, 0.5973, 0.5962, 0.5940], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1514, -0.1521, -0.1527, -0.1532, -0.1539, -0.1543, -0.1547, -0.1551, -0.1554, -0.1556, -0.1556, -0.1557,
0:         -0.1557, -0.1557, -0.1555, -0.1552, -0.1548, -0.1543, -0.1539, -0.1532, -0.1525, -0.1520, -0.1512, -0.1506,
0:         -0.1499], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2249, -0.2249, -0.2249, -0.2249, -0.2273, -0.2273, -0.2273, -0.2273, -0.2297, -0.2297, -0.2297, -0.2297,
0:         -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2321, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297, -0.2297,
0:         -0.2297], device='cuda:0')
0: [DEBUG] Epoch 35, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.2441, -0.2441,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2441,     nan, -0.2441,     nan,     nan, -0.2441, -0.2405,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2345, -0.2345,     nan,     nan,     nan,     nan, -0.2345,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2285,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2441,     nan,     nan, -0.2441,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2345,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2321,     nan, -0.2297,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2273,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2321,     nan,     nan,     nan,     nan,     nan, -0.2273,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2441,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2273,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2297,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 35, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.6179, -1.5180, -1.4638, -1.4743, -1.5334, -1.5830, -1.5872, -1.5390, -1.4913, -1.4668, -1.4828, -1.5174,
0:         -1.5329, -1.4884, -1.3848, -1.2694, -1.1904, -1.1519, -1.5836, -1.4657, -1.4080, -1.4419, -1.5508, -1.6138,
0:         -1.6165], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1936, -0.1698, -0.1725, -0.1816, -0.1979, -0.2060, -0.2262, -0.2535, -0.2728, -0.2723, -0.2517, -0.2151,
0:         -0.1919, -0.2093, -0.2626, -0.2902, -0.2725, -0.2069, -0.1825, -0.1538, -0.1420, -0.1458, -0.1645, -0.1845,
0:         -0.2085], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7335, -0.7305, -0.7255, -0.7182, -0.7052, -0.6979, -0.7018, -0.7029, -0.7039, -0.7087, -0.7127, -0.7116,
0:         -0.7033, -0.6967, -0.6986, -0.7059, -0.7214, -0.7346, -0.7303, -0.7236, -0.7170, -0.7055, -0.7012, -0.7006,
0:         -0.6986], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2356, 0.3089, 0.3531, 0.2474, 0.1632, 0.1943, 0.2028, 0.1777, 0.2174, 0.2476, 0.2283, 0.2707, 0.3452, 0.3389,
0:         0.2232, 0.2712, 0.5185, 0.4812, 0.1581, 0.1123, 0.0955, 0.0573, 0.1020, 0.1906, 0.2062], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.0776, 0.0664, 0.0613, 0.0565, 0.0589, 0.0683, 0.0876, 0.1115, 0.1336, 0.1504, 0.1561, 0.1493, 0.1381, 0.1298,
0:         0.1249, 0.1274, 0.1342, 0.1445, 0.1559, 0.1645, 0.1690, 0.1726, 0.1826, 0.1955, 0.2068], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2566, -0.2593, -0.2661, -0.2609, -0.2506, -0.2465, -0.2567, -0.2567, -0.2607, -0.2645, -0.2634, -0.2608,
0:         -0.2514, -0.2504, -0.2462, -0.2479, -0.2541, -0.2693, -0.2622, -0.2557, -0.2559, -0.2438, -0.2360, -0.2485,
0:         -0.2501], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2605893611907959; velocity_v: 0.4936699867248535; specific_humidity: 0.15857599675655365; velocity_z: 0.5638753175735474; temperature: 0.20175564289093018; total_precip: 0.44124412536621094; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2882147431373596; velocity_v: 0.4338919222354889; specific_humidity: 0.15045833587646484; velocity_z: 0.7092862725257874; temperature: 0.16384953260421753; total_precip: 1.1537039279937744; 
0: epoch: 35 [1/5 (20%)]	Loss: 0.79747 : 0.38537 :: 0.21445 (2.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22903096675872803; velocity_v: 0.4739741384983063; specific_humidity: 0.1448526680469513; velocity_z: 0.5519946813583374; temperature: 0.16666187345981598; total_precip: 0.6964726448059082; 
0: epoch: 35 [2/5 (40%)]	Loss: 0.69647 : 0.34597 :: 0.20969 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.27687156200408936; velocity_v: 0.44238346815109253; specific_humidity: 0.12557892501354218; velocity_z: 0.5233738422393799; temperature: 0.17311730980873108; total_precip: 0.646626353263855; 
0: epoch: 35 [3/5 (60%)]	Loss: 0.64663 : 0.33356 :: 0.21194 (16.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2477867752313614; velocity_v: 0.43019282817840576; specific_humidity: 0.17486266791820526; velocity_z: 0.5632909536361694; temperature: 0.15283061563968658; total_precip: 0.6595277786254883; 
0: epoch: 35 [4/5 (80%)]	Loss: 0.65953 : 0.33905 :: 0.21012 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 35 : 0.3524281084537506
0: validation loss for velocity_u : 0.20343132317066193
0: validation loss for velocity_v : 0.39197850227355957
0: validation loss for specific_humidity : 0.13401851058006287
0: validation loss for velocity_z : 0.517262876033783
0: validation loss for temperature : 0.13013531267642975
0: validation loss for total_precip : 0.7377415299415588
0: 36 : 11:17:00 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 36, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3086, -0.3083, -0.3081, -0.3066, -0.3021, -0.2930, -0.2775, -0.2539, -0.2220, -0.1846, -0.1458, -0.1081,
0:         -0.0700, -0.0301,  0.0096,  0.0426,  0.0643,  0.0773, -0.2942, -0.2932, -0.2919, -0.2889, -0.2844, -0.2793,
0:         -0.2718], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5818, -0.6949, -0.8118, -0.9314, -1.0526, -1.1744, -1.2937, -1.4076, -1.5148, -1.6175, -1.7178, -1.8165,
0:         -1.9131, -2.0087, -2.1061, -2.2080, -2.3156, -2.4277, -0.5122, -0.6200, -0.7312, -0.8461, -0.9655, -1.0885,
0:         -1.2125], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5224, -0.5184, -0.5108, -0.5081, -0.5086, -0.5122, -0.5149, -0.5210, -0.5268, -0.5328, -0.5405, -0.5478,
0:         -0.5532, -0.5640, -0.5746, -0.5856, -0.5983, -0.6093, -0.5369, -0.5273, -0.5196, -0.5169, -0.5107, -0.5112,
0:         -0.5140], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0104,  0.0692,  0.1617,  0.2262,  0.2670,  0.3294,  0.4240,  0.5316,  0.6348,  0.7294,  0.8240,  0.9230,
0:          1.0262,  1.1294,  1.1983,  1.1939,  1.1122,  1.0155,  0.0133,  0.0692,  0.1574,  0.2391,  0.3079,  0.3595,
0:          0.3853], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2543, -0.2413, -0.2297, -0.2193, -0.2098, -0.1997, -0.1871, -0.1717, -0.1551, -0.1393, -0.1266, -0.1183,
0:         -0.1153, -0.1187, -0.1279, -0.1390, -0.1477, -0.1518, -0.1533, -0.1554, -0.1592, -0.1634, -0.1684, -0.1798,
0:         -0.2079], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313,
0:         -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313,
0:         -0.2313], device='cuda:0')
0: [DEBUG] Epoch 36, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([ 0.0403,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0753,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.0425,     nan, -0.0780,     nan, -0.0057,     nan,
0:             nan,     nan, -0.0013,     nan,     nan,  0.0381, -0.0473,     nan,     nan,     nan, -0.1656,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1087, -0.1065,     nan,     nan,
0:             nan, -0.0408,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0841,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.3951,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.2966,     nan,  0.6536,  0.6536,     nan,     nan,     nan,  0.0600,     nan, -0.1196,
0:             nan,  0.0490,     nan,     nan,     nan, -0.0605,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0999,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0846,     nan,     nan,     nan,     nan,     nan,     nan,  0.0994,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2050,     nan,     nan,     nan,     nan, -0.1328,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1043,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1393, -0.1525,     nan, -0.1656,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1590,     nan,  0.7916,     nan, -0.1174,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 36, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3520, -0.3185, -0.2634, -0.2110, -0.1778, -0.1640, -0.1445, -0.1268, -0.1161, -0.1101, -0.1278, -0.1706,
0:         -0.2163, -0.2467, -0.2452, -0.2031, -0.1299, -0.0593, -0.2605, -0.2739, -0.2593, -0.2523, -0.2522, -0.2414,
0:         -0.2169], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0806, 0.0944, 0.1007, 0.1098, 0.1125, 0.1086, 0.0907, 0.0649, 0.0434, 0.0304, 0.0440, 0.0857, 0.1218, 0.1213,
0:         0.0860, 0.0546, 0.0357, 0.0529, 0.0577, 0.0909, 0.1138, 0.1337, 0.1333, 0.1204, 0.0969], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3571, -0.3532, -0.3479, -0.3447, -0.3420, -0.3372, -0.3402, -0.3447, -0.3520, -0.3678, -0.3882, -0.4105,
0:         -0.4276, -0.4451, -0.4702, -0.4877, -0.5120, -0.5199, -0.4040, -0.3894, -0.3792, -0.3662, -0.3632, -0.3619,
0:         -0.3617], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0099,  0.0937,  0.3483,  0.2554,  0.1041,  0.2887,  0.2921,  0.0822, -0.0845, -0.2097, -0.2272, -0.2028,
0:         -0.0670, -0.0393, -0.2875, -0.2411,  0.0366,  0.0062,  0.0603,  0.0671,  0.1849,  0.1158,  0.0901,  0.3057,
0:          0.2164], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.8598, -0.7598, -0.6756, -0.6295, -0.6081, -0.6164, -0.6279, -0.6457, -0.6632, -0.6777, -0.6826, -0.6883,
0:         -0.6879, -0.6702, -0.6511, -0.6256, -0.5986, -0.5751, -0.5602, -0.5581, -0.5684, -0.5644, -0.5239, -0.4639,
0:         -0.4127], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0363,  0.0306,  0.0054, -0.0174, -0.0222, -0.0210, -0.0280, -0.0292, -0.0278,  0.0701,  0.0432,  0.0195,
0:          0.0017, -0.0253, -0.0446, -0.0420, -0.0460, -0.0337,  0.0827,  0.0550,  0.0382,  0.0011, -0.0209, -0.0473,
0:         -0.0605], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.21395722031593323; velocity_v: 0.40296855568885803; specific_humidity: 0.15424534678459167; velocity_z: 0.7452760934829712; temperature: 0.18830572068691254; total_precip: 1.058582067489624; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.21270102262496948; velocity_v: 0.4753420054912567; specific_humidity: 0.15986716747283936; velocity_z: 0.5658175349235535; temperature: 0.18977141380310059; total_precip: 0.613257110118866; 
0: epoch: 36 [1/5 (20%)]	Loss: 0.83592 : 0.38247 :: 0.20653 (2.59 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2732292115688324; velocity_v: 0.5054747462272644; specific_humidity: 0.13140445947647095; velocity_z: 0.5598073601722717; temperature: 0.2189672589302063; total_precip: 0.5282543301582336; 
0: epoch: 36 [2/5 (40%)]	Loss: 0.52825 : 0.33665 :: 0.21433 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.25808972120285034; velocity_v: 0.5588586330413818; specific_humidity: 0.13724693655967712; velocity_z: 0.929662823677063; temperature: 0.23300866782665253; total_precip: 0.8196064233779907; 
0: epoch: 36 [3/5 (60%)]	Loss: 0.81961 : 0.45561 :: 0.21262 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24217703938484192; velocity_v: 0.40403902530670166; specific_humidity: 0.1528843641281128; velocity_z: 0.5252213478088379; temperature: 0.19547224044799805; total_precip: 0.6029101014137268; 
0: epoch: 36 [4/5 (80%)]	Loss: 0.60291 : 0.32146 :: 0.20830 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 36 : 0.38658982515335083
0: validation loss for velocity_u : 0.22226978838443756
0: validation loss for velocity_v : 0.4294174611568451
0: validation loss for specific_humidity : 0.13371051847934723
0: validation loss for velocity_z : 0.6296026706695557
0: validation loss for temperature : 0.14581511914730072
0: validation loss for total_precip : 0.7587231993675232
0: 37 : 11:20:57 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 37, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4635, -0.4606, -0.4590, -0.4592, -0.4612, -0.4651, -0.4708, -0.4778, -0.4856, -0.4936, -0.5011, -0.5076,
0:         -0.5132, -0.5174, -0.5205, -0.5225, -0.5235, -0.5240, -0.3820, -0.3795, -0.3787, -0.3800, -0.3832, -0.3882,
0:         -0.3951], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.0339, -2.0058, -1.9783, -1.9518, -1.9267, -1.9028, -1.8801, -1.8588, -1.8385, -1.8190, -1.8004, -1.7822,
0:         -1.7645, -1.7473, -1.7306, -1.7143, -1.6989, -1.6837, -2.1005, -2.0709, -2.0417, -2.0130, -1.9850, -1.9584,
0:         -1.9332], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3774, -0.3782, -0.3826, -0.3870, -0.3914, -0.3994, -0.4114, -0.4254, -0.4335, -0.4416, -0.4496, -0.4654,
0:         -0.4811, -0.4968, -0.5133, -0.5246, -0.5315, -0.5439, -0.3877, -0.3886, -0.3928, -0.3968, -0.3985, -0.4003,
0:         -0.4029], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0742, -0.0731, -0.0776, -0.0934, -0.1204, -0.1599, -0.2106, -0.2703, -0.3312, -0.3886, -0.4382, -0.4732,
0:         -0.4934, -0.4968, -0.4856, -0.4608, -0.4281, -0.3898, -0.2218, -0.2016, -0.1847, -0.1756, -0.1768, -0.1903,
0:         -0.2117], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.3966, 1.4003, 1.4043, 1.4087, 1.4138, 1.4190, 1.4240, 1.4292, 1.4337, 1.4382, 1.4427, 1.4475, 1.4525, 1.4589,
0:         1.4658, 1.4743, 1.4844, 1.4956, 1.5084, 1.5226, 1.5377, 1.5539, 1.5706, 1.5873, 1.6038], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2367, -0.2356, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2367, -0.2332, -0.2321, -0.2321,
0:         -0.2309, -0.2286, -0.2263, -0.2263, -0.2298, -0.2332, -0.2332, -0.2309, -0.2286, -0.2263, -0.2239, -0.2216,
0:         -0.2205], device='cuda:0')
0: [DEBUG] Epoch 37, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.2414,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan, -0.2414, -0.2414,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2414,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2414,     nan, -0.2414,     nan,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan,     nan,
0:         -0.2414,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2414,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan,
0:             nan,     nan,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2414, -0.2414,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2414,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2414,     nan])
0: [DEBUG] Epoch 37, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5851, -0.5298, -0.4644, -0.4215, -0.4159, -0.4326, -0.4307, -0.4108, -0.3768, -0.3461, -0.3516, -0.4093,
0:         -0.4848, -0.5426, -0.5585, -0.5139, -0.4390, -0.3661, -0.4769, -0.4564, -0.4203, -0.4122, -0.4378, -0.4569,
0:         -0.4571], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2184, -0.1643, -0.1650, -0.1871, -0.2331, -0.2689, -0.2998, -0.3178, -0.3195, -0.3097, -0.2788, -0.2400,
0:         -0.2125, -0.2113, -0.2341, -0.2352, -0.2121, -0.1439, -0.2575, -0.1923, -0.1745, -0.1840, -0.2160, -0.2537,
0:         -0.2828], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6681, -0.6529, -0.6369, -0.6233, -0.6087, -0.6044, -0.6120, -0.6205, -0.6321, -0.6462, -0.6591, -0.6660,
0:         -0.6640, -0.6561, -0.6572, -0.6565, -0.6710, -0.6787, -0.6617, -0.6406, -0.6239, -0.6071, -0.6011, -0.6030,
0:         -0.6063], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0067,  0.0523,  0.1384,  0.0079, -0.0756, -0.0420, -0.0123,  0.0400,  0.0729,  0.0923,  0.0657,  0.0519,
0:          0.1185,  0.0873, -0.0553, -0.0452,  0.0484, -0.0605,  0.0038, -0.0456, -0.0225, -0.1166, -0.1039, -0.0518,
0:         -0.0635], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([2.1386, 2.0403, 2.0016, 2.0422, 2.1176, 2.1883, 2.2216, 2.2296, 2.2262, 2.2212, 2.1951, 2.1596, 2.1194, 2.1098,
0:         2.1279, 2.1548, 2.1581, 2.1327, 2.0867, 2.0625, 2.0717, 2.0950, 2.0986, 2.0695, 2.0271], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2558, -0.2597, -0.2694, -0.2664, -0.2517, -0.2484, -0.2541, -0.2505, -0.2548, -0.2619, -0.2631, -0.2585,
0:         -0.2549, -0.2532, -0.2503, -0.2475, -0.2472, -0.2599, -0.2602, -0.2513, -0.2528, -0.2450, -0.2411, -0.2448,
0:         -0.2492], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2782224416732788; velocity_v: 0.6104021072387695; specific_humidity: 0.14619329571723938; velocity_z: 0.5718161463737488; temperature: 0.1751786470413208; total_precip: 0.6742326021194458; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2329387664794922; velocity_v: 0.4519241750240326; specific_humidity: 0.14810146391391754; velocity_z: 0.5092499852180481; temperature: 0.1514570415019989; total_precip: 0.4633048176765442; 
0: epoch: 37 [1/5 (20%)]	Loss: 0.56877 : 0.33566 :: 0.20904 (2.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22457607090473175; velocity_v: 0.5358521938323975; specific_humidity: 0.12244997918605804; velocity_z: 0.51376873254776; temperature: 0.21235151588916779; total_precip: 0.5323121547698975; 
0: epoch: 37 [2/5 (40%)]	Loss: 0.53231 : 0.32522 :: 0.20962 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2764500379562378; velocity_v: 0.3855332136154175; specific_humidity: 0.16273915767669678; velocity_z: 0.5110058188438416; temperature: 0.16435347497463226; total_precip: 0.8296013474464417; 
0: epoch: 37 [3/5 (60%)]	Loss: 0.82960 : 0.35555 :: 0.21039 (16.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.28999748826026917; velocity_v: 0.4598483145236969; specific_humidity: 0.15930883586406708; velocity_z: 0.6431280374526978; temperature: 0.18380534648895264; total_precip: 0.5268979668617249; 
0: epoch: 37 [4/5 (80%)]	Loss: 0.52690 : 0.34442 :: 0.21540 (15.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 37 : 0.31975990533828735
0: validation loss for velocity_u : 0.20749841630458832
0: validation loss for velocity_v : 0.4118003845214844
0: validation loss for specific_humidity : 0.11594286561012268
0: validation loss for velocity_z : 0.4967412054538727
0: validation loss for temperature : 0.10920713096857071
0: validation loss for total_precip : 0.5773691534996033
0: 38 : 11:24:48 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 38, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7459, 0.7376, 0.7311, 0.7265, 0.7238, 0.7228, 0.7226, 0.7233, 0.7240, 0.7241, 0.7236, 0.7221, 0.7197, 0.7168,
0:         0.7138, 0.7115, 0.7104, 0.7110, 0.7509, 0.7394, 0.7289, 0.7197, 0.7117, 0.7051, 0.7001], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.8801, -1.8965, -1.9160, -1.9385, -1.9636, -1.9909, -2.0198, -2.0493, -2.0787, -2.1070, -2.1339, -2.1587,
0:         -2.1821, -2.2042, -2.2262, -2.2494, -2.2745, -2.3028, -1.9032, -1.9134, -1.9257, -1.9405, -1.9578, -1.9772,
0:         -1.9989], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4814, -0.4936, -0.4941, -0.4946, -0.4951, -0.4957, -0.4953, -0.4949, -0.4945, -0.4952, -0.4965, -0.4979,
0:         -0.4992, -0.4969, -0.4936, -0.4902, -0.4866, -0.4771, -0.4568, -0.4729, -0.4891, -0.4970, -0.4991, -0.5012,
0:         -0.5033], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1482, -0.0993, -0.0449,  0.0094,  0.0605,  0.1062,  0.1454,  0.1780,  0.2073,  0.2378,  0.2682,  0.2997,
0:          0.3280,  0.3509,  0.3693,  0.3824,  0.3954,  0.4117,  0.0181,  0.0540,  0.0932,  0.1377,  0.1812,  0.2204,
0:          0.2486], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.2144, 1.1948, 1.1769, 1.1604, 1.1460, 1.1335, 1.1223, 1.1119, 1.1015, 1.0902, 1.0776, 1.0635, 1.0476, 1.0304,
0:         1.0122, 0.9936, 0.9752, 0.9568, 0.9389, 0.9209, 0.9027, 0.8834, 0.8629, 0.8409, 0.8175], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2467, -0.2456, -0.2456, -0.2456, -0.2456, -0.2456, -0.2456, -0.2456, -0.2456, -0.2433, -0.2410, -0.2388,
0:         -0.2388, -0.2410, -0.2433, -0.2456, -0.2444, -0.2444, -0.2467, -0.2467, -0.2467, -0.2467, -0.2467, -0.2456,
0:         -0.2456], device='cuda:0')
0: [DEBUG] Epoch 38, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2467,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2467, -0.2467,     nan,     nan,     nan,     nan,     nan, -0.2467,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2467, -0.2467,     nan,     nan,
0:         -0.2467,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2467,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2467,     nan, -0.2467,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2444,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2467,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2467,     nan,     nan,     nan,     nan,     nan,
0:         -0.2467,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2467,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2444,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2399,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2444, -0.2422,     nan,     nan,     nan,
0:             nan, -0.2467,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2467,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2467,     nan,     nan,     nan,     nan,
0:             nan, -0.2467,     nan])
0: [DEBUG] Epoch 38, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2125, 1.1905, 1.2051, 1.2358, 1.2437, 1.2117, 1.1904, 1.1638, 1.1443, 1.1479, 1.1183, 1.0844, 1.0302, 0.9727,
0:         0.9313, 0.9058, 0.9021, 0.8921, 1.3434, 1.2673, 1.2472, 1.2493, 1.2342, 1.2084, 1.1934], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.8502, -1.8008, -1.7758, -1.7528, -1.7613, -1.7724, -1.8441, -1.9207, -1.9944, -2.0673, -2.0904, -2.0884,
0:         -2.0787, -2.0741, -2.0589, -2.0268, -1.9543, -1.8354, -1.7934, -1.7373, -1.6908, -1.6587, -1.6682, -1.7218,
0:         -1.8050], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5746, -0.5720, -0.5735, -0.5759, -0.5787, -0.5821, -0.5933, -0.6030, -0.6112, -0.6213, -0.6316, -0.6375,
0:         -0.6368, -0.6359, -0.6430, -0.6481, -0.6593, -0.6599, -0.6045, -0.6019, -0.6021, -0.5981, -0.6032, -0.6050,
0:         -0.6083], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.8447, 2.1220, 2.3608, 2.4230, 2.4845, 2.6023, 2.6314, 2.6314, 2.6595, 2.6444, 2.5363, 2.5791, 2.7127, 2.6386,
0:         2.5327, 2.6085, 2.5821, 2.2469, 1.6355, 1.6718, 1.8176, 1.9168, 2.0712, 2.2728, 2.3749], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.3986,  0.3065,  0.2492,  0.2296,  0.2258,  0.2129,  0.1768,  0.1252,  0.0783,  0.0374, -0.0036, -0.0411,
0:         -0.0721, -0.0810, -0.0694, -0.0565, -0.0598, -0.0818, -0.1112, -0.1251, -0.1215, -0.1238, -0.1597, -0.2289,
0:         -0.3042], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2196, -0.2181, -0.2286, -0.2260, -0.2176, -0.2156, -0.2312, -0.2281, -0.2350, -0.2198, -0.2180, -0.2176,
0:         -0.2118, -0.2178, -0.2207, -0.2243, -0.2314, -0.2443, -0.2160, -0.2058, -0.2091, -0.2075, -0.2054, -0.2129,
0:         -0.2255], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23783807456493378; velocity_v: 0.42087075114250183; specific_humidity: 0.14180848002433777; velocity_z: 0.5888535976409912; temperature: 0.15426011383533478; total_precip: 0.8276419043540955; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22828003764152527; velocity_v: 0.4157094955444336; specific_humidity: 0.14676052331924438; velocity_z: 0.5696126222610474; temperature: 0.16606366634368896; total_precip: 0.6043848991394043; 
0: epoch: 38 [1/5 (20%)]	Loss: 0.71601 : 0.34327 :: 0.20772 (2.70 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22702516615390778; velocity_v: 0.3847942054271698; specific_humidity: 0.16195829212665558; velocity_z: 0.6458122730255127; temperature: 0.1866377294063568; total_precip: 0.729045033454895; 
0: epoch: 38 [2/5 (40%)]	Loss: 0.72905 : 0.35606 :: 0.21025 (16.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.25395330786705017; velocity_v: 0.4372602701187134; specific_humidity: 0.14230602979660034; velocity_z: 0.589590847492218; temperature: 0.20042984187602997; total_precip: 0.8489556908607483; 
0: epoch: 38 [3/5 (60%)]	Loss: 0.84896 : 0.37973 :: 0.21010 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2404395043849945; velocity_v: 0.4017455577850342; specific_humidity: 0.1506940722465515; velocity_z: 0.5463804006576538; temperature: 0.141758993268013; total_precip: 0.5762816071510315; 
0: epoch: 38 [4/5 (80%)]	Loss: 0.57628 : 0.31151 :: 0.20995 (15.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 38 : 0.3536590337753296
0: validation loss for velocity_u : 0.1897059828042984
0: validation loss for velocity_v : 0.43894171714782715
0: validation loss for specific_humidity : 0.11519543081521988
0: validation loss for velocity_z : 0.541653573513031
0: validation loss for temperature : 0.15929453074932098
0: validation loss for total_precip : 0.6771627068519592
0: 39 : 11:28:47 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 39, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2469,  0.2512,  0.2471,  0.2299,  0.2034,  0.1751,  0.1430,  0.1042,  0.0670,  0.0437,  0.0312,  0.0192,
0:          0.0049, -0.0139, -0.0329, -0.0463, -0.0567, -0.0614,  0.2574,  0.2641,  0.2641,  0.2504,  0.2238,  0.1909,
0:          0.1508], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6185, 0.6045, 0.5801, 0.5451, 0.5079, 0.4785, 0.4581, 0.4401, 0.4247, 0.4134, 0.4009, 0.3928, 0.3915, 0.3905,
0:         0.3915, 0.3942, 0.3901, 0.3823, 0.5881, 0.5843, 0.5745, 0.5509, 0.5167, 0.4829, 0.4497], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6413, -0.6523, -0.6624, -0.6693, -0.6703, -0.6685, -0.6626, -0.6527, -0.6413, -0.6337, -0.6288, -0.6245,
0:         -0.6185, -0.6096, -0.5932, -0.5736, -0.5494, -0.5231, -0.6375, -0.6517, -0.6640, -0.6727, -0.6765, -0.6750,
0:         -0.6657], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3148, 1.3757, 1.1714, 0.7434, 0.3392, 0.1089, 0.0133, 0.0328, 0.2632, 0.6108, 0.9085, 1.1910, 1.3692, 1.3583,
0:         1.3083, 1.2171, 1.0584, 0.9520, 1.1215, 1.4626, 1.4257, 1.0389, 0.6043, 0.2740, 0.0307], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2826, -0.2804, -0.2852, -0.3011, -0.3306, -0.3746, -0.4313, -0.4898, -0.5360, -0.5637, -0.5768, -0.5854,
0:         -0.5957, -0.6106, -0.6323, -0.6594, -0.6879, -0.7156, -0.7408, -0.7640, -0.7795, -0.7850, -0.7877, -0.7896,
0:         -0.7930], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392,
0:         -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392, -0.2392,
0:         -0.2392], device='cuda:0')
0: [DEBUG] Epoch 39, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2392, -0.2392,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2392,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan, -0.2392, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2392,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 39, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7340, 0.7084, 0.7339, 0.7761, 0.8061, 0.7979, 0.7843, 0.7536, 0.7186, 0.6952, 0.6548, 0.6060, 0.5459, 0.4920,
0:         0.4477, 0.4262, 0.4226, 0.4192, 0.8153, 0.7590, 0.7484, 0.7592, 0.7618, 0.7552, 0.7347], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1249, 0.1519, 0.1418, 0.1196, 0.0942, 0.0844, 0.0712, 0.0535, 0.0359, 0.0246, 0.0238, 0.0361, 0.0467, 0.0363,
0:         0.0159, 0.0136, 0.0374, 0.0765, 0.1416, 0.1924, 0.1981, 0.1844, 0.1480, 0.1119, 0.0778], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5269, -0.5179, -0.5119, -0.5032, -0.4995, -0.5027, -0.5171, -0.5339, -0.5525, -0.5771, -0.5977, -0.6140,
0:         -0.6164, -0.6222, -0.6399, -0.6517, -0.6832, -0.7017, -0.5507, -0.5407, -0.5323, -0.5218, -0.5231, -0.5298,
0:         -0.5357], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0531,  0.1886,  0.3673,  0.2151,  0.0334,  0.1544,  0.1322, -0.0077,  0.0304,  0.0027, -0.0428,  0.0295,
0:          0.0808,  0.1002,  0.0537,  0.1094,  0.2533,  0.1431,  0.0344,  0.0308,  0.0828, -0.0036, -0.0243,  0.1573,
0:          0.1081], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4509, -0.4586, -0.4588, -0.4515, -0.4377, -0.4262, -0.4164, -0.4161, -0.4174, -0.4169, -0.4109, -0.3959,
0:         -0.3729, -0.3404, -0.3086, -0.2823, -0.2622, -0.2470, -0.2367, -0.2338, -0.2380, -0.2452, -0.2459, -0.2441,
0:         -0.2477], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2463, -0.2480, -0.2530, -0.2485, -0.2342, -0.2336, -0.2408, -0.2359, -0.2454, -0.2580, -0.2524, -0.2478,
0:         -0.2406, -0.2384, -0.2364, -0.2331, -0.2361, -0.2514, -0.2533, -0.2446, -0.2444, -0.2349, -0.2289, -0.2281,
0:         -0.2313], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.21186593174934387; velocity_v: 0.38313204050064087; specific_humidity: 0.16536036133766174; velocity_z: 0.5973631739616394; temperature: 0.15149334073066711; total_precip: 0.7031907439231873; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23354461789131165; velocity_v: 0.47032594680786133; specific_humidity: 0.16128011047840118; velocity_z: 0.6914753317832947; temperature: 0.17313535511493683; total_precip: 0.5374323725700378; 
0: epoch: 39 [1/5 (20%)]	Loss: 0.62031 : 0.34074 :: 0.20841 (2.40 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24081116914749146; velocity_v: 0.6070511341094971; specific_humidity: 0.15505528450012207; velocity_z: 0.5692503452301025; temperature: 0.18516482412815094; total_precip: 0.5634124875068665; 
0: epoch: 39 [2/5 (40%)]	Loss: 0.56341 : 0.35344 :: 0.20981 (15.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26129257678985596; velocity_v: 0.5862689018249512; specific_humidity: 0.16043874621391296; velocity_z: 0.5514336824417114; temperature: 0.20060986280441284; total_precip: 0.5267159938812256; 
0: epoch: 39 [3/5 (60%)]	Loss: 0.52672 : 0.34816 :: 0.21058 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.27915051579475403; velocity_v: 0.438368558883667; specific_humidity: 0.14121299982070923; velocity_z: 0.6414164900779724; temperature: 0.19497500360012054; total_precip: 0.6074355840682983; 
0: epoch: 39 [4/5 (80%)]	Loss: 0.60744 : 0.35065 :: 0.21386 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 39 : 0.34609413146972656
0: validation loss for velocity_u : 0.20501627027988434
0: validation loss for velocity_v : 0.37854811549186707
0: validation loss for specific_humidity : 0.12861616909503937
0: validation loss for velocity_z : 0.4872187077999115
0: validation loss for temperature : 0.1452750265598297
0: validation loss for total_precip : 0.7318907380104065
0: 40 : 11:32:44 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 40, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5590, 0.5666, 0.5731, 0.5779, 0.5817, 0.5837, 0.5839, 0.5832, 0.5818, 0.5794, 0.5774, 0.5763, 0.5763, 0.5783,
0:         0.5836, 0.5921, 0.6031, 0.6156, 0.5961, 0.5997, 0.6019, 0.6029, 0.6034, 0.6029, 0.6013], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4331, 0.4451, 0.4532, 0.4594, 0.4663, 0.4748, 0.4852, 0.4981, 0.5122, 0.5267, 0.5412, 0.5553, 0.5690, 0.5838,
0:         0.6014, 0.6243, 0.6552, 0.6959, 0.4439, 0.4582, 0.4680, 0.4748, 0.4806, 0.4866, 0.4943], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1069, 0.1184, 0.1202, 0.1187, 0.1154, 0.1117, 0.1045, 0.0969, 0.0885, 0.0804, 0.0749, 0.0727, 0.0691, 0.0649,
0:         0.0591, 0.0532, 0.0455, 0.0313, 0.1326, 0.1473, 0.1479, 0.1447, 0.1379, 0.1277, 0.1154], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0763, 0.0807, 0.0982, 0.1222, 0.1660, 0.2140, 0.2447, 0.2643, 0.2731, 0.2468, 0.2031, 0.1747, 0.1485, 0.1113,
0:         0.0938, 0.0982, 0.1004, 0.1135, 0.0632, 0.1113, 0.1638, 0.1966, 0.2359, 0.2862, 0.3015], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.1743,  0.1675,  0.1605,  0.1528,  0.1434,  0.1303,  0.1134,  0.0922,  0.0668,  0.0362,  0.0016, -0.0378,
0:         -0.0816, -0.1281, -0.1760, -0.2232, -0.2687, -0.3101, -0.3481, -0.3827, -0.4142, -0.4442, -0.4746, -0.5057,
0:         -0.5379], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1794, -0.1860, -0.1960, -0.2027, -0.2027, -0.1894, -0.1971, -0.2270, -0.1417, -0.1584, -0.1628, -0.1672,
0:         -0.1794, -0.1993, -0.2004, -0.2082, -0.2215, -0.1938, -0.1827, -0.1761, -0.1694, -0.1894, -0.2027, -0.2027,
0:         -0.1993], device='cuda:0')
0: [DEBUG] Epoch 40, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2204, -0.2182,     nan,
0:             nan, -0.1672,     nan,     nan,     nan, -0.1152,     nan,     nan, -0.2326,     nan,     nan, -0.2193,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2348,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2248,     nan,     nan, -0.2182,     nan,
0:             nan,     nan, -0.2270,     nan,     nan,     nan,     nan, -0.2126,     nan, -0.1949,     nan,     nan,
0:         -0.2348, -0.2326,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2348,     nan,     nan,     nan, -0.2348,
0:             nan, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2359,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2337,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2337,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2204,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2359,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2337, -0.2337,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2337,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2359])
0: [DEBUG] Epoch 40, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.6036, 1.6207, 1.6823, 1.7311, 1.7416, 1.6822, 1.6341, 1.5922, 1.5936, 1.6625, 1.7042, 1.7685, 1.7801, 1.7480,
0:         1.6830, 1.5946, 1.5171, 1.4119, 1.6621, 1.6259, 1.6347, 1.6553, 1.6345, 1.5756, 1.5187], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4877, 0.4891, 0.4691, 0.4409, 0.3904, 0.3467, 0.2866, 0.2498, 0.2285, 0.2371, 0.2590, 0.2842, 0.2878, 0.2545,
0:         0.1945, 0.1525, 0.1382, 0.1415, 0.4081, 0.4123, 0.4170, 0.4055, 0.3742, 0.3371, 0.2883], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0770,  0.0482,  0.0147, -0.0177, -0.0427, -0.0704, -0.1088, -0.1423, -0.1732, -0.1974, -0.2146, -0.2197,
0:         -0.2156, -0.2097, -0.2145, -0.2081, -0.2080, -0.1785,  0.1484,  0.1199,  0.0816,  0.0604,  0.0214, -0.0181,
0:         -0.0467], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3769, -0.3209, -0.1679, -0.1856, -0.2321, -0.1904, -0.2261, -0.2384, -0.2427, -0.2982, -0.2938, -0.2802,
0:         -0.2457, -0.2459, -0.4020, -0.4594, -0.4126, -0.5347, -0.3046, -0.3311, -0.2812, -0.3089, -0.2916, -0.2343,
0:         -0.2970], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.2292,  0.2971,  0.3557,  0.3841,  0.3844,  0.3546,  0.3095,  0.2608,  0.2246,  0.2010,  0.1814,  0.1573,
0:          0.1342,  0.1108,  0.0856,  0.0555,  0.0194, -0.0216, -0.0684, -0.1205, -0.1778, -0.2240, -0.2483, -0.2468,
0:         -0.2333], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2028, -0.2053, -0.2100, -0.2110, -0.1994, -0.1980, -0.2053, -0.1953, -0.2122, -0.2130, -0.2080, -0.2058,
0:         -0.2041, -0.2027, -0.1978, -0.2000, -0.2004, -0.2103, -0.2149, -0.2085, -0.2094, -0.2018, -0.1986, -0.1964,
0:         -0.1981], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2559390962123871; velocity_v: 0.5483567118644714; specific_humidity: 0.1464555412530899; velocity_z: 0.572866678237915; temperature: 0.2140556424856186; total_precip: 0.8175612092018127; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2617420256137848; velocity_v: 0.4504126012325287; specific_humidity: 0.1376769095659256; velocity_z: 0.5548287034034729; temperature: 0.1991351842880249; total_precip: 0.687147855758667; 
0: epoch: 40 [1/5 (20%)]	Loss: 0.75235 : 0.37163 :: 0.20652 (2.38 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2521139681339264; velocity_v: 0.5097296833992004; specific_humidity: 0.14864148199558258; velocity_z: 0.5416061282157898; temperature: 0.17845827341079712; total_precip: 0.4928017556667328; 
0: epoch: 40 [2/5 (40%)]	Loss: 0.49280 : 0.32151 :: 0.21206 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2640525996685028; velocity_v: 0.5522147417068481; specific_humidity: 0.1554989516735077; velocity_z: 0.6608647108078003; temperature: 0.23633603751659393; total_precip: 0.7307900190353394; 
0: epoch: 40 [3/5 (60%)]	Loss: 0.73079 : 0.39936 :: 0.21060 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26302099227905273; velocity_v: 0.5336410403251648; specific_humidity: 0.14064016938209534; velocity_z: 0.7108674049377441; temperature: 0.1572590172290802; total_precip: 0.8019087910652161; 
0: epoch: 40 [4/5 (80%)]	Loss: 0.80191 : 0.40253 :: 0.21248 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 40 : 0.3626238703727722
0: validation loss for velocity_u : 0.21735060214996338
0: validation loss for velocity_v : 0.4210599958896637
0: validation loss for specific_humidity : 0.12112834304571152
0: validation loss for velocity_z : 0.5736653208732605
0: validation loss for temperature : 0.12848393619060516
0: validation loss for total_precip : 0.714055061340332
0: 41 : 11:36:40 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 41, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.4286, -1.4025, -1.3859, -1.3809, -1.3847, -1.3918, -1.3985, -1.4043, -1.4155, -1.4242, -1.4167, -1.3946,
0:         -1.3702, -1.3543, -1.3471, -1.3390, -1.3298, -1.3303, -1.4269, -1.4050, -1.3929, -1.3899, -1.3960, -1.4078,
0:         -1.4217], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0006,  0.0142,  0.0296,  0.0420,  0.0481,  0.0516,  0.0534,  0.0577,  0.0692,  0.0881,  0.1098,  0.1273,
0:          0.1423,  0.1553,  0.1681,  0.1817,  0.1961,  0.2138, -0.0089,  0.0138,  0.0345,  0.0497,  0.0567,  0.0615,
0:          0.0664], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([3.1769, 3.2363, 3.2381, 3.1756, 3.0599, 2.9243, 2.7784, 2.6536, 2.5740, 2.5141, 2.4186, 2.2717, 2.0760, 1.8777,
0:         1.7167, 1.5988, 1.4976, 1.4303, 3.2767, 3.3325, 3.3389, 3.3003, 3.2201, 3.1293, 3.0168], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6801,  0.6061,  0.5344,  0.4268,  0.2901,  0.0548, -0.2701, -0.6579, -1.0321, -1.0411, -0.7453, -0.2970,
0:         -0.0416, -0.1379, -0.2970, -0.3665, -0.2231, -0.1267,  0.6868,  0.5591,  0.4336,  0.2475,  0.0302, -0.1939,
0:         -0.5032], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1127, -0.1356, -0.1669, -0.2009, -0.2502, -0.2856, -0.2673, -0.1820,  0.0105,  0.2781,  0.5434,  0.6886,
0:          0.7115,  0.6926,  0.6969,  0.7875,  0.8891,  1.0100,  1.1100,  1.1803,  1.2272,  1.2306,  1.2747,  1.3243,
0:          1.3973], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563,
0:         -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563,
0:         -0.2563], device='cuda:0')
0: [DEBUG] Epoch 41, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2563,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2141,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2340,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2526,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2563,
0:         -0.2563,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2514,     nan, -0.2539, -0.2563,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1868,     nan,     nan, -0.1173,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0528,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1099,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2588,     nan,     nan,     nan,     nan,     nan,     nan, -0.2588,
0:         -0.2526,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2452,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.2910,     nan,     nan,     nan,     nan,     nan,  0.2848,  0.2489,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0805,     nan,  0.2155,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 41, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1345, -1.0048, -0.9333, -0.9318, -0.9982, -1.0679, -1.1132, -1.1144, -1.1224, -1.1445, -1.2045, -1.2853,
0:         -1.3591, -1.3831, -1.3430, -1.2545, -1.1556, -1.0387, -1.0605, -0.9292, -0.8606, -0.8925, -1.0128, -1.0985,
0:         -1.1433], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0841, -0.0626, -0.0448, -0.0179,  0.0034,  0.0257,  0.0243,  0.0078, -0.0166, -0.0266, -0.0197,  0.0060,
0:          0.0210,  0.0072, -0.0283, -0.0395, -0.0244,  0.0114, -0.0906, -0.0747, -0.0548, -0.0356, -0.0149,  0.0031,
0:          0.0112], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 1.0008,  0.7851,  0.5000,  0.2109, -0.0439, -0.2468, -0.3989, -0.5112, -0.5839, -0.6383, -0.6518, -0.6688,
0:         -0.6626, -0.6461, -0.6405, -0.6083, -0.5282, -0.4556,  0.9439,  0.7265,  0.4612,  0.1837, -0.0354, -0.2219,
0:         -0.3418], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0963,  0.1746,  0.2742,  0.2376,  0.0904,  0.1189,  0.0868, -0.0349,  0.0300,  0.0563,  0.0079,  0.0033,
0:          0.0181,  0.0548, -0.0633, -0.0737,  0.1426,  0.0702,  0.1070,  0.0800,  0.1049,  0.1045,  0.0436,  0.1243,
0:          0.1145], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.1497, 1.1054, 1.0751, 1.0679, 1.0784, 1.1062, 1.1290, 1.1464, 1.1560, 1.1673, 1.1809, 1.2118, 1.2652, 1.3504,
0:         1.4415, 1.5202, 1.5599, 1.5682, 1.5604, 1.5567, 1.5632, 1.5675, 1.5586, 1.5428, 1.5368], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2205, -0.2178, -0.2161, -0.2154, -0.2022, -0.2021, -0.2098, -0.2077, -0.2194, -0.2361, -0.2238, -0.2227,
0:         -0.2145, -0.2094, -0.2073, -0.2059, -0.2056, -0.2179, -0.2359, -0.2270, -0.2239, -0.2182, -0.2073, -0.2031,
0:         -0.2008], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.24663370847702026; velocity_v: 0.4753350019454956; specific_humidity: 0.15688925981521606; velocity_z: 0.6226598024368286; temperature: 0.8293887972831726; total_precip: 0.676001250743866; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23972386121749878; velocity_v: 0.35286158323287964; specific_humidity: 0.14636671543121338; velocity_z: 0.4730947017669678; temperature: 0.14422564208507538; total_precip: 0.542668342590332; 
0: epoch: 41 [1/5 (20%)]	Loss: 0.60933 : 0.37654 :: 0.20522 (2.83 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2495722770690918; velocity_v: 0.38597556948661804; specific_humidity: 0.14191046357154846; velocity_z: 0.47186070680618286; temperature: 0.1379614919424057; total_precip: 0.4832810163497925; 
0: epoch: 41 [2/5 (40%)]	Loss: 0.48328 : 0.28024 :: 0.20816 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.256479948759079; velocity_v: 0.5083004832267761; specific_humidity: 0.12074313312768936; velocity_z: 0.5981979370117188; temperature: 0.1465378701686859; total_precip: 0.34712499380111694; 
0: epoch: 41 [3/5 (60%)]	Loss: 0.34712 : 0.29848 :: 0.21254 (16.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.27183401584625244; velocity_v: 0.47337138652801514; specific_humidity: 0.1377352774143219; velocity_z: 0.602971613407135; temperature: 0.2131156325340271; total_precip: 0.5568853616714478; 
0: epoch: 41 [4/5 (80%)]	Loss: 0.55689 : 0.34257 :: 0.21478 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 41 : 0.4335337281227112
0: validation loss for velocity_u : 0.21284671127796173
0: validation loss for velocity_v : 0.4608492851257324
0: validation loss for specific_humidity : 0.12116154283285141
0: validation loss for velocity_z : 0.6504094004631042
0: validation loss for temperature : 0.13290633261203766
0: validation loss for total_precip : 1.0230294466018677
0: 42 : 11:40:30 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 42, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3515, -0.3447, -0.3259, -0.2957, -0.2579, -0.2182, -0.1803, -0.1455, -0.1123, -0.0781, -0.0404,  0.0016,
0:          0.0467,  0.0938,  0.1414,  0.1907,  0.2436,  0.3017, -0.3749, -0.3628, -0.3446, -0.3184, -0.2846, -0.2451,
0:         -0.2023], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5619, 0.5846, 0.6198, 0.6718, 0.7395, 0.8175, 0.8978, 0.9742, 1.0429, 1.1048, 1.1625, 1.2189, 1.2758, 1.3328,
0:         1.3889, 1.4431, 1.4952, 1.5432, 0.6813, 0.7122, 0.7514, 0.8048, 0.8747, 0.9575, 1.0465], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7375, -0.7292, -0.7347, -0.7399, -0.7453, -0.7493, -0.7538, -0.7596, -0.7635, -0.7659, -0.7678, -0.7687,
0:         -0.7700, -0.7723, -0.7742, -0.7753, -0.7763, -0.7764, -0.7434, -0.7469, -0.7516, -0.7567, -0.7590, -0.7622,
0:         -0.7662], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9207, 1.0390, 1.1316, 1.1230, 0.9874, 0.7614, 0.5311, 0.3675, 0.3072, 0.3288, 0.3890, 0.4536, 0.5160, 0.5763,
0:         0.6301, 0.6559, 0.6172, 0.4902, 0.9917, 1.0132, 1.0519, 1.0369, 0.9314, 0.7463, 0.5440], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.0619, 1.0539, 1.0444, 1.0311, 1.0143, 0.9951, 0.9763, 0.9608, 0.9499, 0.9437, 0.9412, 0.9396, 0.9386, 0.9388,
0:         0.9418, 0.9495, 0.9605, 0.9704, 0.9698, 0.9464, 0.8905, 0.7986, 0.6765, 0.5376, 0.3997], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483,
0:         -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483,
0:         -0.2483], device='cuda:0')
0: [DEBUG] Epoch 42, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan, -0.2483,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2483,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2436,     nan, -0.2448, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2471,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2483,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2483, -0.2483,     nan, -0.2483, -0.2483,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2483,     nan, -0.2483,     nan,     nan,     nan, -0.2483,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 42, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8767, -0.7598, -0.6775, -0.6792, -0.7426, -0.8224, -0.8745, -0.8944, -0.8967, -0.9176, -0.9697, -1.0445,
0:         -1.1277, -1.1636, -1.1444, -1.0772, -0.9860, -0.8879, -1.0138, -0.9045, -0.8315, -0.8595, -0.9627, -1.0506,
0:         -1.1042], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1002,  0.1219,  0.1243,  0.1112,  0.0854,  0.0667,  0.0284, -0.0011, -0.0226, -0.0200, -0.0094,  0.0133,
0:          0.0185, -0.0054, -0.0498, -0.0727, -0.0590, -0.0291,  0.0698,  0.0911,  0.1019,  0.0894,  0.0635,  0.0347,
0:         -0.0035], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7852, -0.7861, -0.7815, -0.7779, -0.7693, -0.7691, -0.7793, -0.7884, -0.7972, -0.8098, -0.8158, -0.8147,
0:         -0.8047, -0.7928, -0.7899, -0.7856, -0.7942, -0.7960, -0.7963, -0.7866, -0.7767, -0.7666, -0.7604, -0.7617,
0:         -0.7687], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4387, 0.5354, 0.5821, 0.4858, 0.3235, 0.2197, 0.2836, 0.3683, 0.4254, 0.3957, 0.2631, 0.3495, 0.5785, 0.7147,
0:         0.8106, 1.0119, 1.2086, 1.1467, 0.4866, 0.2984, 0.2705, 0.3474, 0.4624, 0.6520, 0.8975], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.6272, 0.5400, 0.4913, 0.4773, 0.4803, 0.4849, 0.4804, 0.4686, 0.4584, 0.4443, 0.4177, 0.3900, 0.3690, 0.3679,
0:         0.3918, 0.4218, 0.4388, 0.4342, 0.4087, 0.3744, 0.3397, 0.2939, 0.2282, 0.1457, 0.0619], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2623, -0.2638, -0.2661, -0.2636, -0.2530, -0.2532, -0.2575, -0.2579, -0.2695, -0.2720, -0.2630, -0.2618,
0:         -0.2588, -0.2523, -0.2594, -0.2604, -0.2580, -0.2758, -0.2674, -0.2598, -0.2561, -0.2546, -0.2453, -0.2497,
0:         -0.2528], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.24473625421524048; velocity_v: 0.46607738733291626; specific_humidity: 0.13613209128379822; velocity_z: 0.606863260269165; temperature: 0.134529247879982; total_precip: 0.527934193611145; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.25811439752578735; velocity_v: 0.3647170662879944; specific_humidity: 0.17535506188869476; velocity_z: 0.5337619185447693; temperature: 0.18112701177597046; total_precip: 0.587479293346405; 
0: epoch: 42 [1/5 (20%)]	Loss: 0.55771 : 0.31892 :: 0.21169 (2.59 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2649672329425812; velocity_v: 0.39992839097976685; specific_humidity: 0.16511857509613037; velocity_z: 0.5589971542358398; temperature: 0.15388090908527374; total_precip: 0.7394916415214539; 
0: epoch: 42 [2/5 (40%)]	Loss: 0.73949 : 0.34743 :: 0.21223 (16.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.30883628129959106; velocity_v: 0.45997267961502075; specific_humidity: 0.1510831117630005; velocity_z: 0.5859342813491821; temperature: 0.1837003380060196; total_precip: 0.8720184564590454; 
0: epoch: 42 [3/5 (60%)]	Loss: 0.87202 : 0.39404 :: 0.21552 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2736310660839081; velocity_v: 0.4472528398036957; specific_humidity: 0.17336063086986542; velocity_z: 0.5058529376983643; temperature: 0.22497117519378662; total_precip: 0.6098505854606628; 
0: epoch: 42 [4/5 (80%)]	Loss: 0.60985 : 0.33872 :: 0.21089 (16.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 42 : 0.3246956765651703
0: validation loss for velocity_u : 0.2208864539861679
0: validation loss for velocity_v : 0.40977704524993896
0: validation loss for specific_humidity : 0.12211909145116806
0: validation loss for velocity_z : 0.4623701870441437
0: validation loss for temperature : 0.12183348089456558
0: validation loss for total_precip : 0.6111878156661987
0: 43 : 11:44:23 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 43, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6744, 0.6741, 0.6747, 0.6762, 0.6786, 0.6817, 0.6852, 0.6888, 0.6922, 0.6953, 0.6986, 0.7018, 0.7049, 0.7079,
0:         0.7103, 0.7120, 0.7134, 0.7146, 0.6408, 0.6390, 0.6383, 0.6386, 0.6398, 0.6417, 0.6442], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2946, 0.2732, 0.2517, 0.2297, 0.2080, 0.1866, 0.1662, 0.1469, 0.1290, 0.1127, 0.0981, 0.0853, 0.0749, 0.0673,
0:         0.0620, 0.0589, 0.0574, 0.0570, 0.3208, 0.3015, 0.2819, 0.2620, 0.2422, 0.2230, 0.2044], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7869, -0.7870, -0.7871, -0.7872, -0.7872, -0.7873, -0.7874, -0.7874, -0.7875, -0.7876, -0.7876, -0.7877,
0:         -0.7877, -0.7877, -0.7877, -0.7878, -0.7878, -0.7877, -0.7871, -0.7872, -0.7873, -0.7873, -0.7874, -0.7874,
0:         -0.7875], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4953, 0.4685, 0.4317, 0.3904, 0.3525, 0.3201, 0.2922, 0.2643, 0.2331, 0.1985, 0.1673, 0.1461, 0.1394, 0.1405,
0:         0.1438, 0.1427, 0.1382, 0.1371, 0.5287, 0.4986, 0.4607, 0.4161, 0.3703, 0.3301, 0.2944], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.3003, 0.2929, 0.2849, 0.2768, 0.2678, 0.2580, 0.2480, 0.2370, 0.2260, 0.2148, 0.2036, 0.1925, 0.1814, 0.1703,
0:         0.1591, 0.1473, 0.1351, 0.1225, 0.1103, 0.0986, 0.0873, 0.0774, 0.0688, 0.0614, 0.0550], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2549, -0.2549, -0.2549, -0.2537, -0.2537, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549,
0:         -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549, -0.2549,
0:         -0.2549], device='cuda:0')
0: [DEBUG] Epoch 43, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2445, -0.2445,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2526,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2549,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2549,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2549,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2549,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2526, -0.2537,     nan, -0.2537,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2549,     nan,     nan,     nan, -0.2549,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2549,     nan,     nan,     nan, -0.2514,     nan,     nan,
0:             nan,     nan, -0.2514,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2526,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2537,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2549,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 43, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3678, 0.3670, 0.4145, 0.4682, 0.5043, 0.4922, 0.4873, 0.4699, 0.4616, 0.4776, 0.4771, 0.4770, 0.4633, 0.4652,
0:         0.4829, 0.5297, 0.5834, 0.6187, 0.4710, 0.4205, 0.4195, 0.4231, 0.4084, 0.3809, 0.3613], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2015, -0.1364, -0.0865, -0.0780, -0.1004, -0.1226, -0.1695, -0.2129, -0.2509, -0.2656, -0.2575, -0.2254,
0:         -0.2023, -0.2113, -0.2466, -0.2695, -0.2636, -0.2455, -0.1934, -0.1335, -0.0818, -0.0854, -0.1137, -0.1433,
0:         -0.1757], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8467, -0.8481, -0.8440, -0.8395, -0.8284, -0.8216, -0.8204, -0.8211, -0.8225, -0.8267, -0.8299, -0.8300,
0:         -0.8233, -0.8156, -0.8177, -0.8174, -0.8296, -0.8367, -0.8468, -0.8399, -0.8347, -0.8261, -0.8231, -0.8196,
0:         -0.8183], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4695, 0.6152, 0.6253, 0.4731, 0.3642, 0.3838, 0.3607, 0.2163, 0.1852, 0.2165, 0.1513, 0.1686, 0.2159, 0.1777,
0:         0.0821, 0.1265, 0.3131, 0.2963, 0.4520, 0.3989, 0.3341, 0.2848, 0.3271, 0.4049, 0.4093], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.3091, 1.2762, 1.2623, 1.2659, 1.2751, 1.2743, 1.2581, 1.2305, 1.1975, 1.1621, 1.1223, 1.0879, 1.0683, 1.0731,
0:         1.1032, 1.1386, 1.1607, 1.1632, 1.1538, 1.1553, 1.1728, 1.1997, 1.2182, 1.2158, 1.2038], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2674, -0.2669, -0.2713, -0.2668, -0.2571, -0.2575, -0.2585, -0.2629, -0.2713, -0.2752, -0.2676, -0.2638,
0:         -0.2601, -0.2525, -0.2585, -0.2631, -0.2594, -0.2754, -0.2713, -0.2627, -0.2559, -0.2561, -0.2437, -0.2502,
0:         -0.2494], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2204013615846634; velocity_v: 0.516788125038147; specific_humidity: 0.14049765467643738; velocity_z: 0.42409300804138184; temperature: 0.2305009812116623; total_precip: 0.3414371609687805; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22848056256771088; velocity_v: 0.4190167486667633; specific_humidity: 0.13092583417892456; velocity_z: 0.5290966033935547; temperature: 0.15031073987483978; total_precip: 0.5881361961364746; 
0: epoch: 43 [1/5 (20%)]	Loss: 0.46479 : 0.29501 :: 0.20909 (2.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2970419228076935; velocity_v: 0.41481471061706543; specific_humidity: 0.15779000520706177; velocity_z: 0.5180913209915161; temperature: 0.1940332055091858; total_precip: 0.5990995168685913; 
0: epoch: 43 [2/5 (40%)]	Loss: 0.59910 : 0.33080 :: 0.21478 (15.92 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.25421342253685; velocity_v: 0.5010428428649902; specific_humidity: 0.143345907330513; velocity_z: 0.6168704032897949; temperature: 0.14990809559822083; total_precip: 0.732917070388794; 
0: epoch: 43 [3/5 (60%)]	Loss: 0.73292 : 0.36737 :: 0.21205 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.26855209469795227; velocity_v: 0.5904115438461304; specific_humidity: 0.14100247621536255; velocity_z: 0.5730207562446594; temperature: 0.1758420765399933; total_precip: 0.8238391876220703; 
0: epoch: 43 [4/5 (80%)]	Loss: 0.82384 : 0.39588 :: 0.21620 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 43 : 0.3783525228500366
0: validation loss for velocity_u : 0.21715426445007324
0: validation loss for velocity_v : 0.4672631025314331
0: validation loss for specific_humidity : 0.1127658262848854
0: validation loss for velocity_z : 0.6630372405052185
0: validation loss for temperature : 0.1267884075641632
0: validation loss for total_precip : 0.6831066608428955
0: 44 : 11:48:17 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 44, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.9349, 1.9179, 1.8950, 1.8676, 1.8384, 1.8093, 1.7811, 1.7550, 1.7327, 1.7148, 1.7018, 1.6923, 1.6847, 1.6773,
0:         1.6697, 1.6620, 1.6555, 1.6500, 1.9388, 1.9275, 1.9095, 1.8854, 1.8575, 1.8279, 1.7979], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3173, 0.3367, 0.3634, 0.3976, 0.4391, 0.4856, 0.5339, 0.5826, 0.6326, 0.6865, 0.7452, 0.8094, 0.8781, 0.9501,
0:         1.0220, 1.0912, 1.1553, 1.2138, 0.2441, 0.2671, 0.2965, 0.3317, 0.3721, 0.4167, 0.4632], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3826, -0.3831, -0.3836, -0.3850, -0.3877, -0.3900, -0.3919, -0.3941, -0.3956, -0.3969, -0.3989, -0.4019,
0:         -0.4085, -0.4135, -0.4232, -0.4337, -0.4429, -0.4527, -0.3931, -0.3933, -0.3934, -0.3928, -0.3940, -0.3959,
0:         -0.3978], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2370, -0.2633, -0.1866, -0.0158,  0.2142,  0.4594,  0.6719,  0.8274,  0.9150,  0.9413,  0.9194,  0.8909,
0:          0.8865,  0.9084,  0.9281,  0.9194,  0.8843,  0.8339, -0.1406, -0.2523, -0.2545, -0.1538,  0.0258,  0.2492,
0:          0.4748], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.4427, 0.4347, 0.4266, 0.4208, 0.4185, 0.4196, 0.4227, 0.4262, 0.4290, 0.4313, 0.4333, 0.4355, 0.4394, 0.4462,
0:         0.4571, 0.4707, 0.4854, 0.4985, 0.5078, 0.5111, 0.5088, 0.5028, 0.4965, 0.4918, 0.4903], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2532, -0.2509, -0.2509, -0.2487, -0.2487, -0.2464, -0.2464, -0.2464, -0.2487, -0.2509, -0.2509, -0.2487,
0:         -0.2464, -0.2441, -0.2441, -0.2441, -0.2418, -0.2441, -0.2509, -0.2487, -0.2487, -0.2464, -0.2441, -0.2418,
0:         -0.2418], device='cuda:0')
0: [DEBUG] Epoch 44, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.0665,     nan,     nan,     nan,     nan,  0.6988,  0.5758,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  1.0427,  0.9835,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.2273,     nan,     nan,     nan,     nan,     nan,     nan, -0.1712,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2373,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1393,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3594,     nan,
0:          1.6098,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.0290,     nan,     nan,
0:             nan,     nan,     nan,  0.2194,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0425,     nan,     nan,     nan,     nan,  0.1579,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0448,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1294,     nan,
0:             nan,     nan,     nan,  0.9778,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  1.0506,  0.9265,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.8548,     nan,     nan,     nan,     nan,  1.0210,     nan,     nan,     nan,
0:             nan,     nan,  0.3093,     nan,     nan,     nan,     nan,     nan,     nan, -0.1188,     nan,     nan,
0:             nan, -0.0152,     nan])
0: [DEBUG] Epoch 44, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.8330, 1.7839, 1.7955, 1.8457, 1.8830, 1.8460, 1.7919, 1.6977, 1.6238, 1.6081, 1.5817, 1.5907, 1.5711, 1.5413,
0:         1.5037, 1.4548, 1.4276, 1.3775, 1.8920, 1.7821, 1.7431, 1.7630, 1.7716, 1.7385, 1.6746], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6784, -0.5675, -0.5115, -0.4945, -0.5141, -0.5309, -0.5662, -0.5899, -0.6095, -0.6065, -0.5783, -0.5393,
0:         -0.5152, -0.5248, -0.5499, -0.5594, -0.5173, -0.4302, -0.6920, -0.5839, -0.5122, -0.4973, -0.5124, -0.5310,
0:         -0.5509], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4853, -0.4940, -0.5043, -0.5135, -0.5159, -0.5204, -0.5376, -0.5529, -0.5703, -0.5935, -0.6140, -0.6348,
0:         -0.6533, -0.6738, -0.6967, -0.7105, -0.7270, -0.7225, -0.5064, -0.5106, -0.5168, -0.5115, -0.5152, -0.5219,
0:         -0.5310], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5477, -0.4207, -0.3051, -0.3487, -0.4018, -0.4032, -0.4192, -0.3976, -0.3883, -0.4081, -0.3536, -0.3010,
0:         -0.2541, -0.1981, -0.2693, -0.2637, -0.1934, -0.3081, -0.4970, -0.5008, -0.4995, -0.5668, -0.5618, -0.4866,
0:         -0.4714], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.8125, 0.7570, 0.7338, 0.7556, 0.7876, 0.8063, 0.7965, 0.7754, 0.7604, 0.7555, 0.7420, 0.7238, 0.6962, 0.6953,
0:         0.7188, 0.7544, 0.7745, 0.7745, 0.7589, 0.7533, 0.7647, 0.7746, 0.7610, 0.7154, 0.6616], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.5167, 0.5167, 0.4328, 0.3525, 0.2636, 0.2074, 0.1596, 0.1428, 0.1711, 0.6618, 0.6596, 0.6116, 0.5137, 0.4200,
0:         0.3399, 0.2934, 0.2807, 0.2948, 0.7836, 0.7857, 0.7514, 0.6640, 0.5912, 0.5374, 0.4722], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.2607313394546509; velocity_v: 0.3719285726547241; specific_humidity: 0.15511824190616608; velocity_z: 0.6696897149085999; temperature: 0.1833670288324356; total_precip: 1.1199089288711548; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.25349894165992737; velocity_v: 0.4299977123737335; specific_humidity: 0.15255016088485718; velocity_z: 0.6356930732727051; temperature: 0.16306012868881226; total_precip: 0.5430659055709839; 
0: epoch: 44 [1/5 (20%)]	Loss: 0.83149 : 0.37866 :: 0.21347 (2.47 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2386016547679901; velocity_v: 0.5435708165168762; specific_humidity: 0.13720104098320007; velocity_z: 0.6735771894454956; temperature: 0.13853460550308228; total_precip: 0.8533229231834412; 
0: epoch: 44 [2/5 (40%)]	Loss: 0.85332 : 0.39889 :: 0.21056 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2359755039215088; velocity_v: 0.4615437090396881; specific_humidity: 0.12746332585811615; velocity_z: 0.5656455755233765; temperature: 0.16178782284259796; total_precip: 0.6142287850379944; 
0: epoch: 44 [3/5 (60%)]	Loss: 0.61423 : 0.32981 :: 0.20577 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24769844114780426; velocity_v: 0.404639333486557; specific_humidity: 0.1310417503118515; velocity_z: 0.6247763633728027; temperature: 0.15170975029468536; total_precip: 0.8537995219230652; 
0: epoch: 44 [4/5 (80%)]	Loss: 0.85380 : 0.37073 :: 0.21027 (16.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 44 : 0.32534441351890564
0: validation loss for velocity_u : 0.2028215080499649
0: validation loss for velocity_v : 0.45227697491645813
0: validation loss for specific_humidity : 0.11035860329866409
0: validation loss for velocity_z : 0.5142224431037903
0: validation loss for temperature : 0.12537238001823425
0: validation loss for total_precip : 0.5470151305198669
0: 45 : 11:52:16 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 45, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5092, 0.5395, 0.5631, 0.5819, 0.5959, 0.6131, 0.6355, 0.6547, 0.6727, 0.6936, 0.7142, 0.7350, 0.7559, 0.7773,
0:         0.8005, 0.8240, 0.8490, 0.8750, 0.4674, 0.4972, 0.5238, 0.5475, 0.5684, 0.5902, 0.6128], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3096, 0.3304, 0.3547, 0.3776, 0.4007, 0.4316, 0.4651, 0.4948, 0.5224, 0.5476, 0.5688, 0.5869, 0.6026, 0.6189,
0:         0.6378, 0.6576, 0.6786, 0.6990, 0.3401, 0.3654, 0.3943, 0.4213, 0.4478, 0.4748, 0.5004], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6111, -0.6143, -0.6177, -0.6203, -0.6229, -0.6268, -0.6311, -0.6348, -0.6389, -0.6428, -0.6459, -0.6492,
0:         -0.6507, -0.6532, -0.6502, -0.6487, -0.6392, -0.6198, -0.6138, -0.6185, -0.6234, -0.6279, -0.6320, -0.6364,
0:         -0.6396], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8191, 0.7727, 0.6622, 0.4700, 0.3529, 0.3507, 0.3816, 0.4037, 0.4634, 0.5871, 0.7109, 0.7727, 0.7992, 0.8147,
0:         0.7727, 0.6821, 0.6092, 0.5319, 0.9804, 0.9937, 0.9451, 0.8059, 0.6733, 0.5849, 0.5098], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.7781, -1.7374, -1.6985, -1.6641, -1.6333, -1.6078, -1.5849, -1.5602, -1.5313, -1.4983, -1.4599, -1.4164,
0:         -1.3726, -1.3305, -1.2906, -1.2546, -1.2194, -1.1876, -1.1578, -1.1291, -1.1044, -1.0801, -1.0589, -1.0424,
0:         -1.0294], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393,
0:         -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393,
0:         -0.2393], device='cuda:0')
0: [DEBUG] Epoch 45, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2393,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,
0:             nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393, -0.2393,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,
0:             nan,     nan,     nan,     nan,     nan, -0.2393,     nan,     nan, -0.2393,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2393,     nan,
0:             nan,     nan, -0.2393,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2393,     nan,     nan,     nan,     nan, -0.2393,     nan, -0.2393, -0.2393,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 45, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2613, 1.2173, 1.2217, 1.2574, 1.2589, 1.1934, 1.0893, 0.9606, 0.8416, 0.7733, 0.7229, 0.7016, 0.6753, 0.6624,
0:         0.6570, 0.6780, 0.7379, 0.7773, 1.4630, 1.3478, 1.2884, 1.2542, 1.1994, 1.1125, 0.9954], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2242,  0.2451,  0.2390,  0.2156,  0.1820,  0.1489,  0.0921,  0.0322, -0.0250, -0.0606, -0.0718, -0.0643,
0:         -0.0621, -0.0941, -0.1446, -0.1693, -0.1554, -0.1057,  0.2116,  0.2507,  0.2597,  0.2389,  0.2040,  0.1604,
0:          0.1008], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7370, -0.7402, -0.7488, -0.7442, -0.7388, -0.7355, -0.7395, -0.7440, -0.7461, -0.7511, -0.7547, -0.7529,
0:         -0.7407, -0.7313, -0.7277, -0.7245, -0.7331, -0.7372, -0.7306, -0.7311, -0.7310, -0.7213, -0.7196, -0.7183,
0:         -0.7177], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0785,  0.3259,  0.6373,  0.6011,  0.5375,  0.6257,  0.5033,  0.3752,  0.4338,  0.4276,  0.4504,  0.5408,
0:          0.5797,  0.6376,  0.5749,  0.7573,  1.1773,  1.0928, -0.0388, -0.0322,  0.2373,  0.2861,  0.3373,  0.5693,
0:          0.6039], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0453, -0.0609, -0.0528, -0.0149,  0.0517,  0.1389,  0.2259,  0.2970,  0.3453,  0.3741,  0.3921,  0.4174,
0:          0.4566,  0.5033,  0.5528,  0.5894,  0.6123,  0.6296,  0.6513,  0.6830,  0.7217,  0.7556,  0.7765,  0.7847,
0:          0.7897], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2474, -0.2473, -0.2538, -0.2442, -0.2416, -0.2414, -0.2439, -0.2477, -0.2519, -0.2553, -0.2502, -0.2448,
0:         -0.2481, -0.2372, -0.2456, -0.2506, -0.2524, -0.2649, -0.2560, -0.2483, -0.2408, -0.2425, -0.2296, -0.2370,
0:         -0.2367], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2690728008747101; velocity_v: 0.5600104928016663; specific_humidity: 0.14324846863746643; velocity_z: 0.7236511707305908; temperature: 0.1741487681865692; total_precip: 0.9333981275558472; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2673085927963257; velocity_v: 0.4501684010028839; specific_humidity: 0.1496906578540802; velocity_z: 0.6430460810661316; temperature: 0.19411076605319977; total_precip: 0.8289237022399902; 
0: epoch: 45 [1/5 (20%)]	Loss: 0.88116 : 0.41212 :: 0.21373 (2.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.3022589683532715; velocity_v: 0.4181634187698364; specific_humidity: 0.15098807215690613; velocity_z: 0.5750502943992615; temperature: 0.2043691873550415; total_precip: 0.5370944738388062; 
0: epoch: 45 [2/5 (40%)]	Loss: 0.53709 : 0.33119 :: 0.21778 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2519720196723938; velocity_v: 0.38219594955444336; specific_humidity: 0.15410110354423523; velocity_z: 0.566684901714325; temperature: 0.17327739298343658; total_precip: 0.4920998811721802; 
0: epoch: 45 [3/5 (60%)]	Loss: 0.49210 : 0.30419 :: 0.21241 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23959626257419586; velocity_v: 0.4475659132003784; specific_humidity: 0.1443912833929062; velocity_z: 0.5004599094390869; temperature: 0.16071118414402008; total_precip: 0.4355282783508301; 
0: epoch: 45 [4/5 (80%)]	Loss: 0.43553 : 0.28995 :: 0.20412 (16.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 45 : 0.2924126088619232
0: validation loss for velocity_u : 0.223676398396492
0: validation loss for velocity_v : 0.4038691520690918
0: validation loss for specific_humidity : 0.10958630591630936
0: validation loss for velocity_z : 0.450001984834671
0: validation loss for temperature : 0.11008089780807495
0: validation loss for total_precip : 0.457260400056839
0: 46 : 11:56:15 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 46, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6161, 0.6498, 0.6860, 0.7229, 0.7579, 0.7891, 0.8172, 0.8447, 0.8741, 0.9057, 0.9387, 0.9719, 1.0043, 1.0344,
0:         1.0620, 1.0881, 1.1139, 1.1384, 0.5812, 0.6113, 0.6426, 0.6749, 0.7078, 0.7399, 0.7698], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.4145, -2.4435, -2.4708, -2.4923, -2.5051, -2.5083, -2.5010, -2.4834, -2.4578, -2.4301, -2.4062, -2.3865,
0:         -2.3670, -2.3432, -2.3154, -2.2870, -2.2606, -2.2356, -2.2820, -2.3024, -2.3226, -2.3395, -2.3503, -2.3525,
0:         -2.3442], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1390, 0.1240, 0.1162, 0.1123, 0.1227, 0.1268, 0.1456, 0.1712, 0.1957, 0.2182, 0.2264, 0.2350, 0.2351, 0.2388,
0:         0.2410, 0.2381, 0.2413, 0.2514, 0.2057, 0.1908, 0.1810, 0.1689, 0.1668, 0.1682, 0.1837], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1168,  0.1856,  0.2521,  0.2632,  0.2344,  0.1944,  0.1256,  0.0014, -0.1295, -0.1783, -0.1317, -0.0696,
0:         -0.0341,  0.0192,  0.1501,  0.2987,  0.3298,  0.1745,  0.1989,  0.2743,  0.2787,  0.2033,  0.1190,  0.0613,
0:          0.0036], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.4341,  0.4301,  0.4213,  0.4059,  0.3810,  0.3459,  0.3015,  0.2518,  0.1977,  0.1406,  0.0809,  0.0217,
0:         -0.0360, -0.0924, -0.1468, -0.1966, -0.2400, -0.2793, -0.3185, -0.3605, -0.4069, -0.4596, -0.5194, -0.5785,
0:         -0.6195], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0563, -0.0563,  0.0554,  0.0148,  0.1974,  0.2267,  0.1365,  0.0103, -0.0619, -0.0416, -0.0337,  0.1512,
0:          0.2696,  0.2944,  0.2515,  0.1783,  0.1568,  0.2651,  0.0046, -0.0630, -0.0303,  0.0103,  0.1151,  0.3226,
0:          0.2696], device='cuda:0')
0: [DEBUG] Epoch 46, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2547,     nan,     nan,     nan,     nan,     nan, -0.2547,     nan,     nan, -0.2547, -0.2547,     nan,
0:         -0.2547,     nan,     nan, -0.2547,     nan, -0.2547,     nan,     nan, -0.2547,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2547,     nan,     nan,
0:             nan,     nan, -0.2547,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2547,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2547,     nan,     nan,     nan, -0.2536,     nan,     nan,     nan,
0:             nan,     nan, -0.2502,     nan,     nan,     nan,     nan,     nan,     nan, -0.2547,     nan,     nan,
0:             nan, -0.2547,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2547,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2547,     nan,     nan, -0.2547,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2547,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2547, -0.2547,     nan, -0.2547,     nan,     nan,     nan, -0.2547, -0.2547,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2547,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2547,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2547,     nan,     nan,     nan,     nan,     nan,
0:         -0.2547,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2547,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 46, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9613, 0.9210, 0.9258, 0.9697, 1.0039, 1.0022, 0.9957, 0.9691, 0.9409, 0.9364, 0.9186, 0.8951, 0.8761, 0.8679,
0:         0.8682, 0.9041, 0.9578, 0.9982, 1.0540, 0.9764, 0.9520, 0.9572, 0.9623, 0.9537, 0.9431], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8929, -0.8066, -0.7724, -0.7683, -0.8051, -0.8352, -0.8804, -0.9258, -0.9810, -1.0372, -1.0708, -1.0894,
0:         -1.1060, -1.1401, -1.1785, -1.1898, -1.1534, -1.0549, -0.9169, -0.8259, -0.7706, -0.7683, -0.8048, -0.8517,
0:         -0.8978], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1387, -0.2113, -0.2874, -0.3473, -0.3935, -0.4325, -0.4711, -0.5005, -0.5256, -0.5462, -0.5657, -0.5767,
0:         -0.5916, -0.6027, -0.6432, -0.6539, -0.6696, -0.6567, -0.0270, -0.0997, -0.1785, -0.2400, -0.3177, -0.3802,
0:         -0.4285], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1263, -0.0197,  0.1475,  0.0756, -0.0916, -0.0967, -0.0709, -0.0798, -0.0736, -0.0186, -0.0230, -0.0203,
0:          0.1290,  0.1228,  0.0296,  0.1088,  0.1292,  0.0255, -0.1709, -0.1550, -0.0646, -0.0734, -0.0827, -0.0309,
0:         -0.0449], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.8500, -0.8975, -0.9428, -0.9917, -1.0291, -1.0568, -1.0796, -1.0995, -1.1221, -1.1463, -1.1730, -1.1984,
0:         -1.2181, -1.2265, -1.2290, -1.2265, -1.2308, -1.2375, -1.2464, -1.2534, -1.2600, -1.2682, -1.2784, -1.2877,
0:         -1.2998], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1943, -0.1946, -0.2038, -0.1969, -0.2023, -0.2024, -0.2071, -0.2130, -0.2209, -0.2088, -0.2066, -0.2083,
0:         -0.2044, -0.2079, -0.2102, -0.2162, -0.2198, -0.2279, -0.2196, -0.2176, -0.2103, -0.2164, -0.2070, -0.2146,
0:         -0.2101], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.2293107956647873; velocity_v: 0.434013694524765; specific_humidity: 0.14674074947834015; velocity_z: 0.6957563161849976; temperature: 0.16951386630535126; total_precip: 0.7861910462379456; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22455662488937378; velocity_v: 0.45306816697120667; specific_humidity: 0.1534545123577118; velocity_z: 0.6853764653205872; temperature: 0.13534489274024963; total_precip: 1.0366624593734741; 
0: epoch: 46 [1/5 (20%)]	Loss: 0.91143 : 0.39694 :: 0.20904 (2.46 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2212563157081604; velocity_v: 0.4500444233417511; specific_humidity: 0.13334716856479645; velocity_z: 0.5122347474098206; temperature: 0.14360541105270386; total_precip: 0.5159323215484619; 
0: epoch: 46 [2/5 (40%)]	Loss: 0.51593 : 0.29871 :: 0.20524 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2359631359577179; velocity_v: 0.440205842256546; specific_humidity: 0.12086133658885956; velocity_z: 0.5386766791343689; temperature: 0.16222289204597473; total_precip: 0.49706852436065674; 
0: epoch: 46 [3/5 (60%)]	Loss: 0.49707 : 0.30102 :: 0.20750 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2400261014699936; velocity_v: 0.6006069779396057; specific_humidity: 0.1351017951965332; velocity_z: 0.6033077836036682; temperature: 0.16776470839977264; total_precip: 0.7987532019615173; 
0: epoch: 46 [4/5 (80%)]	Loss: 0.79875 : 0.39193 :: 0.20868 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 46 : 0.3833855390548706
0: validation loss for velocity_u : 0.22687523066997528
0: validation loss for velocity_v : 0.475337952375412
0: validation loss for specific_humidity : 0.10520797222852707
0: validation loss for velocity_z : 0.6370707750320435
0: validation loss for temperature : 0.11471325159072876
0: validation loss for total_precip : 0.7411084175109863
0: 47 : 12:00:12 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 47, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7034, -0.7009, -0.6990, -0.6977, -0.6967, -0.6959, -0.6950, -0.6942, -0.6934, -0.6920, -0.6905, -0.6882,
0:         -0.6850, -0.6806, -0.6751, -0.6682, -0.6600, -0.6505, -0.6488, -0.6458, -0.6433, -0.6413, -0.6395, -0.6376,
0:         -0.6359], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.5021, 1.5096, 1.5165, 1.5230, 1.5291, 1.5352, 1.5409, 1.5466, 1.5519, 1.5567, 1.5614, 1.5661, 1.5705, 1.5746,
0:         1.5785, 1.5815, 1.5837, 1.5847, 1.5216, 1.5277, 1.5340, 1.5403, 1.5468, 1.5537, 1.5610], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7323, -0.7312, -0.7299, -0.7284, -0.7269, -0.7257, -0.7247, -0.7236, -0.7225, -0.7220, -0.7220, -0.7220,
0:         -0.7220, -0.7222, -0.7239, -0.7253, -0.7266, -0.7281, -0.7336, -0.7329, -0.7318, -0.7307, -0.7294, -0.7280,
0:         -0.7269], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1705, -0.1749, -0.1682, -0.1503, -0.1301, -0.1077, -0.0920, -0.0831, -0.0808, -0.0786, -0.0763, -0.0696,
0:         -0.0651, -0.0629, -0.0719, -0.0898, -0.1189, -0.1525, -0.2870, -0.2937, -0.2915, -0.2758, -0.2534, -0.2242,
0:         -0.1951], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.6273, -1.6302, -1.6338, -1.6380, -1.6422, -1.6471, -1.6526, -1.6581, -1.6645, -1.6712, -1.6782, -1.6851,
0:         -1.6918, -1.6978, -1.7032, -1.7075, -1.7113, -1.7143, -1.7166, -1.7183, -1.7190, -1.7186, -1.7174, -1.7150,
0:         -1.7124], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563,
0:         -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563, -0.2563,
0:         -0.2563], device='cuda:0')
0: [DEBUG] Epoch 47, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2563,     nan,     nan, -0.2563,     nan, -0.2563,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,     nan,     nan, -0.2563,
0:             nan, -0.2563,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2563,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,     nan,     nan,
0:             nan, -0.2526,     nan,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,     nan,     nan,
0:         -0.2539,     nan,     nan,     nan, -0.2563,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2563,     nan,     nan,     nan,     nan,     nan, -0.2563,
0:         -0.2563,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2563,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2563,     nan,     nan,     nan,     nan, -0.2514,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 47, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2620, 0.2931, 0.3511, 0.4110, 0.4543, 0.4692, 0.4854, 0.4866, 0.4715, 0.4483, 0.4028, 0.3323, 0.2605, 0.2063,
0:         0.1827, 0.2028, 0.2533, 0.3164, 0.3496, 0.3432, 0.3654, 0.3852, 0.3969, 0.4153, 0.4354], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0764, 0.0913, 0.1132, 0.1184, 0.1042, 0.0855, 0.0574, 0.0452, 0.0465, 0.0639, 0.0769, 0.0852, 0.0830, 0.0556,
0:         0.0265, 0.0137, 0.0230, 0.0415, 0.0187, 0.0431, 0.0892, 0.1117, 0.1014, 0.0797, 0.0454], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7875, -0.7866, -0.7832, -0.7792, -0.7732, -0.7703, -0.7787, -0.7795, -0.7831, -0.7917, -0.7961, -0.8000,
0:         -0.7943, -0.7864, -0.7818, -0.7779, -0.7840, -0.7911, -0.7836, -0.7733, -0.7660, -0.7590, -0.7580, -0.7626,
0:         -0.7696], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1520,  0.3100,  0.3366,  0.2743,  0.2452,  0.2696,  0.2525,  0.1607,  0.1698,  0.1970,  0.1401,  0.1825,
0:          0.2513,  0.1950,  0.0624,  0.1238,  0.3335,  0.2896,  0.1140,  0.0954,  0.0205, -0.0227,  0.0440,  0.1406,
0:          0.1775], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1618, -0.1680, -0.1688, -0.1701, -0.1694, -0.1703, -0.1680, -0.1679, -0.1642, -0.1570, -0.1502, -0.1450,
0:         -0.1434, -0.1481, -0.1602, -0.1772, -0.1921, -0.1982, -0.1947, -0.1870, -0.1815, -0.1800, -0.1786, -0.1775,
0:         -0.1801], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2693, -0.2670, -0.2680, -0.2606, -0.2596, -0.2583, -0.2578, -0.2659, -0.2627, -0.2753, -0.2704, -0.2637,
0:         -0.2616, -0.2610, -0.2601, -0.2611, -0.2614, -0.2697, -0.2751, -0.2692, -0.2615, -0.2623, -0.2516, -0.2556,
0:         -0.2455], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.25065600872039795; velocity_v: 0.5281392335891724; specific_humidity: 0.1285017877817154; velocity_z: 0.6090627312660217; temperature: 0.17208798229694366; total_precip: 0.8253924250602722; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.22267040610313416; velocity_v: 0.48391783237457275; specific_humidity: 0.1380934715270996; velocity_z: 0.6411559581756592; temperature: 0.1904555708169937; total_precip: 0.8002723455429077; 
0: epoch: 47 [1/5 (20%)]	Loss: 0.81283 : 0.38344 :: 0.20742 (2.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2931499779224396; velocity_v: 0.5026907920837402; specific_humidity: 0.13197140395641327; velocity_z: 0.618914783000946; temperature: 0.16007688641548157; total_precip: 0.5064958333969116; 
0: epoch: 47 [2/5 (40%)]	Loss: 0.50650 : 0.33673 :: 0.21910 (16.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2476058155298233; velocity_v: 0.3707770109176636; specific_humidity: 0.16523337364196777; velocity_z: 0.6462317109107971; temperature: 0.19976457953453064; total_precip: 0.8376742601394653; 
0: epoch: 47 [3/5 (60%)]	Loss: 0.83767 : 0.37746 :: 0.21157 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24212077260017395; velocity_v: 0.4472062587738037; specific_humidity: 0.13267375528812408; velocity_z: 0.593229353427887; temperature: 0.19563162326812744; total_precip: 0.724031388759613; 
0: epoch: 47 [4/5 (80%)]	Loss: 0.72403 : 0.35768 :: 0.20545 (16.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 47 : 0.3424144983291626
0: validation loss for velocity_u : 0.23755382001399994
0: validation loss for velocity_v : 0.4621945917606354
0: validation loss for specific_humidity : 0.1139707937836647
0: validation loss for velocity_z : 0.5710702538490295
0: validation loss for temperature : 0.13155071437358856
0: validation loss for total_precip : 0.5381466746330261
0: 48 : 12:04:09 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 48, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1552, 0.1773, 0.1918, 0.1941, 0.1872, 0.1791, 0.1756, 0.1803, 0.1913, 0.2069, 0.2255, 0.2421, 0.2553, 0.2627,
0:         0.2655, 0.2643, 0.2582, 0.2482, 0.1724, 0.1954, 0.2179, 0.2321, 0.2321, 0.2232, 0.2145], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5382, -0.5056, -0.4754, -0.4524, -0.4393, -0.4341, -0.4347, -0.4362, -0.4327, -0.4239, -0.4132, -0.4018,
0:         -0.3918, -0.3837, -0.3777, -0.3759, -0.3785, -0.3839, -0.5857, -0.5616, -0.5324, -0.5048, -0.4837, -0.4691,
0:         -0.4619], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2976, -0.3088, -0.3279, -0.3488, -0.3598, -0.3660, -0.3676, -0.3590, -0.3460, -0.3327, -0.3210, -0.3182,
0:         -0.3154, -0.3154, -0.3149, -0.3151, -0.3177, -0.3204, -0.2781, -0.2873, -0.2968, -0.3156, -0.3391, -0.3469,
0:         -0.3561], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1222,  0.3426,  0.5543,  0.6743,  0.6132,  0.4037,  0.1855,  0.0710,  0.0851,  0.2128,  0.3448,  0.4015,
0:          0.3721,  0.3219,  0.3164,  0.3634,  0.4670,  0.5619, -0.1363,  0.0655,  0.3350,  0.5892,  0.7081,  0.6121,
0:          0.3754], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4312, -0.4024, -0.3176, -0.1924, -0.0772, -0.0163, -0.0296, -0.0995, -0.1814, -0.2511, -0.2921, -0.3110,
0:         -0.3287, -0.3469, -0.3640, -0.3705, -0.3588, -0.3414, -0.3266, -0.3297, -0.3574, -0.4029, -0.4567, -0.4997,
0:         -0.5297], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2491, -0.2491, -0.2423, -0.2263, -0.2103, -0.1989, -0.2286, -0.2080, -0.2172, -0.2491, -0.2491, -0.2423,
0:         -0.2263, -0.2012, -0.1487, -0.2126, -0.1784, -0.1898, -0.2491, -0.2491, -0.2446, -0.2286, -0.1966, -0.1556,
0:         -0.1693], device='cuda:0')
0: [DEBUG] Epoch 48, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.1168,     nan,     nan,     nan,     nan,  0.2438,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0430,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.2575,     nan,     nan,     nan,     nan,  0.0270,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1761,     nan,     nan,     nan,  0.1365,
0:             nan,  0.3099,     nan,     nan,     nan,     nan,     nan, -0.2263,     nan,     nan,     nan,     nan,
0:             nan, -0.2400,     nan,     nan, -0.1807,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0932,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1533,     nan, -0.0666,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1875, -0.1236,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1966,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2468,
0:             nan,     nan,     nan, -0.2309,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1875,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1670,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2240,     nan,
0:         -0.2446,     nan,     nan, -0.2468,     nan, -0.1693,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 48, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2181, 0.2194, 0.2675, 0.3362, 0.3906, 0.4006, 0.3838, 0.3405, 0.2887, 0.2563, 0.2071, 0.1612, 0.1026, 0.0606,
0:         0.0358, 0.0304, 0.0383, 0.0185, 0.2968, 0.2381, 0.2421, 0.2692, 0.2981, 0.2994, 0.2745], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1221,  0.1591,  0.1571,  0.1410,  0.1104,  0.0802,  0.0435,  0.0099, -0.0119, -0.0247, -0.0156,  0.0045,
0:          0.0163,  0.0023, -0.0354, -0.0537, -0.0463, -0.0027,  0.0543,  0.1121,  0.1325,  0.1357,  0.1176,  0.0820,
0:          0.0454], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5744, -0.5690, -0.5720, -0.5696, -0.5697, -0.5727, -0.5800, -0.5922, -0.6031, -0.6173, -0.6254, -0.6327,
0:         -0.6280, -0.6203, -0.6197, -0.6120, -0.6166, -0.6180, -0.5405, -0.5312, -0.5287, -0.5248, -0.5264, -0.5324,
0:         -0.5378], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0936,  0.0207,  0.1700, -0.0414, -0.2354, -0.0077,  0.0882,  0.0478,  0.1090,  0.0549,  0.0507,  0.1600,
0:          0.1933,  0.1553,  0.0215,  0.1027,  0.3665,  0.3485,  0.1271,  0.0064,  0.0205, -0.0875, -0.1840, -0.0039,
0:          0.0370], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.5163,  0.4040,  0.3032,  0.2329,  0.1865,  0.1511,  0.1131,  0.0698,  0.0289, -0.0096, -0.0469, -0.0772,
0:         -0.1028, -0.1115, -0.1080, -0.0994, -0.0950, -0.0993, -0.1132, -0.1352, -0.1662, -0.2165, -0.2852, -0.3620,
0:         -0.4294], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.1428,  0.1755,  0.1683,  0.1809,  0.1629,  0.1616,  0.1832,  0.1645,  0.1799,  0.0686,  0.0737,  0.0954,
0:          0.1181,  0.1135,  0.1381,  0.1425,  0.1674,  0.2069,  0.0104, -0.0070,  0.0347,  0.0550,  0.0943,  0.1245,
0:          0.1667], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.26689645648002625; velocity_v: 0.4429759979248047; specific_humidity: 0.13910163938999176; velocity_z: 0.5716415047645569; temperature: 0.14868302643299103; total_precip: 0.5840772986412048; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2180587202310562; velocity_v: 0.4532085359096527; specific_humidity: 0.14245076477527618; velocity_z: 0.5793209671974182; temperature: 0.1845683455467224; total_precip: 0.7512338757514954; 
0: epoch: 48 [1/5 (20%)]	Loss: 0.66766 : 0.34129 :: 0.20569 (2.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.24755583703517914; velocity_v: 0.4354441165924072; specific_humidity: 0.14253222942352295; velocity_z: 0.511789858341217; temperature: 0.21364209055900574; total_precip: 0.39934444427490234; 
0: epoch: 48 [2/5 (40%)]	Loss: 0.39934 : 0.29184 :: 0.21268 (15.91 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2876792550086975; velocity_v: 0.5661271810531616; specific_humidity: 0.11627619713544846; velocity_z: 0.7071852087974548; temperature: 0.16599862277507782; total_precip: 0.5633360743522644; 
0: epoch: 48 [3/5 (60%)]	Loss: 0.56334 : 0.36838 :: 0.21784 (16.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.2402338832616806; velocity_v: 0.3827841877937317; specific_humidity: 0.16123171150684357; velocity_z: 0.6103609800338745; temperature: 0.19310326874256134; total_precip: 0.7082264423370361; 
0: epoch: 48 [4/5 (80%)]	Loss: 0.70823 : 0.35001 :: 0.20848 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 48 : 0.3790279030799866
0: validation loss for velocity_u : 0.21557147800922394
0: validation loss for velocity_v : 0.43151047825813293
0: validation loss for specific_humidity : 0.11745729297399521
0: validation loss for velocity_z : 0.5313859581947327
0: validation loss for temperature : 0.15024466812610626
0: validation loss for total_precip : 0.8279974460601807
0: 49 : 12:08:05 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 49, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0501, -0.0668, -0.0849, -0.1023, -0.1170, -0.1293, -0.1406, -0.1519, -0.1622, -0.1703, -0.1772, -0.1838,
0:         -0.1923, -0.2045, -0.2238, -0.2528, -0.2909, -0.3330, -0.1262, -0.1342, -0.1447, -0.1545, -0.1631, -0.1723,
0:         -0.1836], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8726, -0.9081, -0.9560, -1.0187, -1.0926, -1.1695, -1.2420, -1.3079, -1.3691, -1.4310, -1.4977, -1.5712,
0:         -1.6515, -1.7358, -1.8211, -1.9052, -1.9877, -2.0678, -0.8844, -0.9298, -0.9873, -1.0565, -1.1338, -1.2122,
0:         -1.2865], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4572,  0.4689,  0.4809,  0.4688,  0.4494,  0.4302,  0.4112,  0.3923,  0.3792,  0.3594,  0.3196,  0.2828,
0:          0.2153,  0.1447,  0.0903,  0.0415,  0.0032, -0.0050,  0.4600,  0.4683,  0.4753,  0.4503,  0.4266,  0.4068,
0:          0.3857], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2477, -0.1638, -0.0182,  0.1827,  0.3305,  0.3305,  0.2091,  0.0701, -0.0093, -0.0402, -0.0645, -0.0976,
0:         -0.1285, -0.1572, -0.1859, -0.1881, -0.1042,  0.0922, -0.1726, -0.0733,  0.0503,  0.1650,  0.1827,  0.0745,
0:         -0.0645], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.4615, 1.4524, 1.4450, 1.4359, 1.4228, 1.4079, 1.3948, 1.3860, 1.3800, 1.3731, 1.3626, 1.3487, 1.3351, 1.3254,
0:         1.3210, 1.3198, 1.3190, 1.3173, 1.3155, 1.3150, 1.3147, 1.3122, 1.3060, 1.2974, 1.2891], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396,
0:         -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396,
0:         -0.2396], device='cuda:0')
0: [DEBUG] Epoch 49, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,
0:             nan, -0.2396,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,
0:             nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,
0:             nan,     nan,     nan,     nan, -0.2396,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 49, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6786, -0.6385, -0.6168, -0.6312, -0.6886, -0.7393, -0.7602, -0.7358, -0.7253, -0.7202, -0.7557, -0.8306,
0:         -0.8961, -0.9134, -0.8863, -0.8024, -0.7351, -0.6734, -0.6394, -0.6122, -0.6139, -0.6697, -0.7773, -0.8370,
0:         -0.8504], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0959,  0.1616,  0.1720,  0.1440,  0.0845,  0.0335, -0.0275, -0.0777, -0.1212, -0.1487, -0.1442, -0.1125,
0:         -0.0848, -0.0810, -0.1079, -0.1225, -0.1027, -0.0338,  0.0580,  0.1413,  0.1668,  0.1432,  0.0863,  0.0191,
0:         -0.0397], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1119,  0.1372,  0.1449,  0.1475,  0.1318,  0.1061,  0.0619,  0.0122, -0.0452, -0.1032, -0.1626, -0.2096,
0:         -0.2476, -0.2756, -0.3017, -0.3190, -0.3455, -0.3603,  0.1001,  0.1214,  0.1238,  0.1185,  0.0847,  0.0401,
0:         -0.0064], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0672,  0.1552,  0.2245,  0.1572,  0.0276,  0.0375,  0.0988,  0.2128,  0.3848,  0.4538,  0.4620,  0.4266,
0:          0.4042,  0.4046,  0.2712,  0.3635,  0.6039,  0.4178,  0.0871,  0.0546,  0.0506, -0.0206, -0.0784,  0.0500,
0:          0.1696], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.7850, 1.6989, 1.6497, 1.6488, 1.6770, 1.7076, 1.7148, 1.7034, 1.6809, 1.6581, 1.6328, 1.6132, 1.6018, 1.6092,
0:         1.6299, 1.6517, 1.6551, 1.6436, 1.6311, 1.6321, 1.6525, 1.6703, 1.6701, 1.6532, 1.6356], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2650, -0.2679, -0.2634, -0.2524, -0.2514, -0.2488, -0.2524, -0.2592, -0.2602, -0.2737, -0.2688, -0.2586,
0:         -0.2542, -0.2563, -0.2523, -0.2574, -0.2570, -0.2707, -0.2709, -0.2665, -0.2575, -0.2555, -0.2460, -0.2538,
0:         -0.2445], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.23955710232257843; velocity_v: 0.45334160327911377; specific_humidity: 0.13796137273311615; velocity_z: 0.552808940410614; temperature: 0.16253408789634705; total_precip: 0.6265316605567932; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.23445327579975128; velocity_v: 0.4724583327770233; specific_humidity: 0.14505936205387115; velocity_z: 0.5672869086265564; temperature: 0.1574895679950714; total_precip: 0.6502271294593811; 
0: epoch: 49 [1/5 (20%)]	Loss: 0.63838 : 0.33474 :: 0.20959 (2.39 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.30681970715522766; velocity_v: 0.41585487127304077; specific_humidity: 0.16101400554180145; velocity_z: 0.6288918256759644; temperature: 0.2641323506832123; total_precip: 0.9184475541114807; 
0: epoch: 49 [2/5 (40%)]	Loss: 0.91845 : 0.41455 :: 0.21217 (15.96 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.3205167055130005; velocity_v: 0.4659162163734436; specific_humidity: 0.14246053993701935; velocity_z: 0.5092102289199829; temperature: 0.1624860167503357; total_precip: 0.6345894932746887; 
0: epoch: 49 [3/5 (60%)]	Loss: 0.63459 : 0.34054 :: 0.21317 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.29897886514663696; velocity_v: 0.447872132062912; specific_humidity: 0.13770395517349243; velocity_z: 0.588641345500946; temperature: 0.16970738768577576; total_precip: 0.44881799817085266; 
0: epoch: 49 [4/5 (80%)]	Loss: 0.44882 : 0.31568 :: 0.21752 (16.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 49 : 0.35263246297836304
0: validation loss for velocity_u : 0.19403403997421265
0: validation loss for velocity_v : 0.3990562856197357
0: validation loss for specific_humidity : 0.11557710915803909
0: validation loss for velocity_z : 0.5602394342422485
0: validation loss for temperature : 0.1271437257528305
0: validation loss for total_precip : 0.7197444438934326
0: Finished training at 12:12:01 with test loss = 0.35263246297836304.
0: [1;34mwandb[0m:
0: [1;34mwandb[0m: You can sync this run to the cloud by running:
0: [1;34mwandb[0m: [1mwandb sync /work/ab1412/atmorep/wandb/offline-run-20250604_085457-kyy926zn[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250604_085457-kyy926zn/logs[0m
0: l50021:664812:665193 [2] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50021:664812:681288 [2] NCCL INFO comm 0x555580e19e50 rank 0 nranks 1 cudaDev 2 busId 84000 - Abort COMPLETE
0: l50021:664812:665198 [3] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50021:664812:681288 [3] NCCL INFO comm 0x555589dade00 rank 0 nranks 1 cudaDev 3 busId c4000 - Abort COMPLETE
0: l50021:664812:665188 [1] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50021:664812:681288 [1] NCCL INFO comm 0x55557eb47380 rank 0 nranks 1 cudaDev 1 busId 44000 - Abort COMPLETE
0: l50021:664812:665055 [0] NCCL INFO [Service thread] Connection closed by localRank 0
0: l50021:664812:681288 [0] NCCL INFO comm 0x55555f261440 rank 0 nranks 1 cudaDev 0 busId 3000 - Abort COMPLETE
