0: Wandb run: atmorep-6dspws7j-17438591
0: l50048:2658325:2658325 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.138<0>
0: l50048:2658325:2658325 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50048:2658325:2658325 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50048:2658325:2658325 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50048:2658325:2658325 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
0: l50048:2658325:2658694 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.138<0>
0: l50048:2658325:2658694 [0] NCCL INFO Using non-device net plugin version 0
0: l50048:2658325:2658694 [0] NCCL INFO Using network IB
0: l50048:2658325:2658694 [0] NCCL INFO ncclCommInitRank comm 0x55555f262b00 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0x9d3f9dcf036eaf83 - Init START
0: l50048:2658325:2658694 [0] NCCL INFO Setting affinity for GPU 0 to 0f0000,00000000,00000000,00000000,000f0000,00000000
0: l50048:2658325:2658694 [0] NCCL INFO comm 0x55555f262b00 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 00/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 01/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 02/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 03/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 04/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 05/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 06/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 07/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 08/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 09/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 10/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 11/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 12/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 13/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 14/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 15/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 16/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 17/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 18/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 19/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 20/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 21/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 22/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 23/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 24/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 25/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 26/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 27/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 28/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 29/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 30/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Channel 31/32 :    0
0: l50048:2658325:2658694 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50048:2658325:2658694 [0] NCCL INFO P2P Chunksize set to 131072
0: l50048:2658325:2658694 [0] NCCL INFO Connected all rings
0: l50048:2658325:2658694 [0] NCCL INFO Connected all trees
0: l50048:2658325:2658694 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50048:2658325:2658694 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50048:2658325:2658694 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50048:2658325:2658694 [0] NCCL INFO ncclCommInitRank comm 0x55555f262b00 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0x9d3f9dcf036eaf83 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 4
0: par_rank : 0
0: par_size : 1
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 1, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 2, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 3, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 3, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17438591
0: wandb_id : 6dspws7j
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: years_test : [2021]
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 1
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 2
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded model id = wc5e2i3t at epoch = 0.
0: Loaded run 'wc5e2i3t' at epoch 0.
0: l50048:2658325:2658721 [1] NCCL INFO Using non-device net plugin version 0
0: l50048:2658325:2658721 [1] NCCL INFO Using network IB
0: l50048:2658325:2658721 [1] NCCL INFO ncclCommInitRank comm 0x55558b399890 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0xeaf9e293401a2b2d - Init START
0: l50048:2658325:2658721 [1] NCCL INFO Setting affinity for GPU 1 to 0f0000,00000000,00000000,00000000,000f0000
0: l50048:2658325:2658721 [1] NCCL INFO comm 0x55558b399890 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 00/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 01/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 02/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 03/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 04/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 05/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 06/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 07/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 08/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 09/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 10/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 11/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 12/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 13/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 14/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 15/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 16/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 17/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 18/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 19/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 20/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 21/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 22/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 23/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 24/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 25/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 26/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 27/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 28/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 29/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 30/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Channel 31/32 :    0
0: l50048:2658325:2658721 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50048:2658325:2658721 [1] NCCL INFO P2P Chunksize set to 131072
0: l50048:2658325:2658721 [1] NCCL INFO Connected all rings
0: l50048:2658325:2658721 [1] NCCL INFO Connected all trees
0: l50048:2658325:2658721 [1] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50048:2658325:2658721 [1] NCCL INFO ncclCommInitRank comm 0x55558b399890 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0xeaf9e293401a2b2d - Init COMPLETE
0: l50048:2658325:2658726 [2] NCCL INFO Using non-device net plugin version 0
0: l50048:2658325:2658726 [2] NCCL INFO Using network IB
0: l50048:2658325:2658726 [2] NCCL INFO ncclCommInitRank comm 0x55556a329340 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0x8c89dba47f42b087 - Init START
0: l50048:2658325:2658726 [2] NCCL INFO Setting affinity for GPU 2 to 1f0000,00000000,00000000,00000000,001f0000,00000000,00000000,00000000
0: l50048:2658325:2658726 [2] NCCL INFO comm 0x55556a329340 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 00/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 01/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 02/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 03/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 04/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 05/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 06/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 07/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 08/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 09/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 10/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 11/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 12/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 13/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 14/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 15/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 16/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 17/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 18/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 19/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 20/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 21/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 22/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 23/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 24/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 25/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 26/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 27/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 28/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 29/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 30/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Channel 31/32 :    0
0: l50048:2658325:2658726 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50048:2658325:2658726 [2] NCCL INFO P2P Chunksize set to 131072
0: l50048:2658325:2658726 [2] NCCL INFO Connected all rings
0: l50048:2658325:2658726 [2] NCCL INFO Connected all trees
0: l50048:2658325:2658726 [2] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50048:2658325:2658726 [2] NCCL INFO ncclCommInitRank comm 0x55556a329340 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0x8c89dba47f42b087 - Init COMPLETE
0: l50048:2658325:2658731 [3] NCCL INFO Using non-device net plugin version 0
0: l50048:2658325:2658731 [3] NCCL INFO Using network IB
0: l50048:2658325:2658731 [3] NCCL INFO ncclCommInitRank comm 0x5555854f2a80 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0x12f3c4a1048f0834 - Init START
0: l50048:2658325:2658731 [3] NCCL INFO Setting affinity for GPU 3 to 0f0000,00000000,00000000,00000000,000f0000,00000000,00000000
0: l50048:2658325:2658731 [3] NCCL INFO comm 0x5555854f2a80 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 00/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 01/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 02/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 03/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 04/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 05/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 06/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 07/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 08/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 09/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 10/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 11/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 12/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 13/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 14/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 15/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 16/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 17/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 18/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 19/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 20/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 21/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 22/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 23/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 24/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 25/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 26/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 27/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 28/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 29/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 30/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Channel 31/32 :    0
0: l50048:2658325:2658731 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50048:2658325:2658731 [3] NCCL INFO P2P Chunksize set to 131072
0: l50048:2658325:2658731 [3] NCCL INFO Connected all rings
0: l50048:2658325:2658731 [3] NCCL INFO Connected all trees
0: l50048:2658325:2658731 [3] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50048:2658325:2658731 [3] NCCL INFO ncclCommInitRank comm 0x5555854f2a80 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0x12f3c4a1048f0834 - Init COMPLETE
0: Number of trainable parameters: 741,136,272
0: 1 : 14:21:20 :: batch_size = 96, lr = 1.5000000000000002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0874, 1.0926, 1.0962, 1.0932, 1.0816, 1.0627, 1.0413, 1.0222, 1.0083, 1.0009, 1.0008, 1.0078, 1.0216, 1.0410,
0:         1.0643, 1.0893, 1.1148, 1.1406, 1.1159, 1.1221, 1.1231, 1.1143, 1.0957, 1.0715, 1.0482], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.3763, 2.4462, 2.5205, 2.5982, 2.6756, 2.7463, 2.8043, 2.8469, 2.8752, 2.8940, 2.9084, 2.9239, 2.9432, 2.9692,
0:         3.0016, 3.0401, 3.0825, 3.1270, 2.3823, 2.4560, 2.5343, 2.6158, 2.6947, 2.7646, 2.8199], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9771, 0.9720, 0.9650, 0.9580, 0.9486, 0.9390, 0.8783, 0.8173, 0.7106, 0.6038, 0.5026, 0.4011, 0.3325, 0.2639,
0:         0.2294, 0.1949, 0.1868, 0.1787, 0.9631, 0.9567, 0.9501, 0.9435, 0.9290, 0.9128, 0.8414], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1486, -0.2775, -0.4192, -0.5265, -0.5480, -0.4664, -0.3183, -0.1658, -0.0520,  0.0102,  0.0339,  0.0403,
0:          0.0489,  0.0575,  0.0532,  0.0317, -0.0112, -0.0778, -0.2152, -0.4084, -0.6060, -0.7241, -0.6854, -0.5136,
0:         -0.2989], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.1669, 1.1548, 1.1458, 1.1339, 1.1158, 1.0954, 1.0783, 1.0679, 1.0630, 1.0620, 1.0639, 1.0678, 1.0711, 1.0725,
0:         1.0704, 1.0642, 1.0527, 1.0359, 1.0160, 0.9970, 0.9813, 0.9692, 0.9598, 0.9514, 0.9410], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0958, -0.1070, -0.1328, -0.1575, -0.1575, -0.1575, -0.1497, -0.1429, -0.1250, -0.1519, -0.1463, -0.1351,
0:         -0.1317, -0.1340, -0.1373, -0.1542, -0.1654, -0.1519, -0.1115, -0.1104, -0.0891, -0.0576, -0.1138, -0.1452,
0:         -0.1632], device='cuda:0')
0: [DEBUG] Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2238,     nan,     nan,     nan, -0.2103,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2215,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2227,     nan,     nan,     nan, -0.2283,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2227,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2047,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1519,     nan,     nan,     nan,
0:         -0.1845,     nan,     nan,     nan,     nan, -0.1160,     nan,     nan,     nan, -0.2036,     nan,     nan,
0:             nan,     nan, -0.1216,     nan,     nan,     nan,     nan, -0.2272,     nan, -0.2317,     nan,     nan,
0:         -0.2283, -0.2317,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2305,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2283,     nan,     nan, -0.2238,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2193, -0.2114,     nan,     nan,     nan,     nan,     nan,     nan, -0.2238,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1677,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3169, 0.3304, 0.3415, 0.3526, 0.3617, 0.3716, 0.3865, 0.4011, 0.4187, 0.4413, 0.4608, 0.4817, 0.5024, 0.5214,
0:         0.5394, 0.5534, 0.5677, 0.5859, 0.3473, 0.3676, 0.3821, 0.3944, 0.4033, 0.4158, 0.4319], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8560, 0.9061, 0.9623, 1.0187, 1.0806, 1.1413, 1.2021, 1.2676, 1.3320, 1.3954, 1.4523, 1.5067, 1.5540, 1.5970,
0:         1.6425, 1.6921, 1.7469, 1.8029, 0.8661, 0.9201, 0.9794, 1.0404, 1.1021, 1.1695, 1.2311], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.5999,  0.5611,  0.5084,  0.4468,  0.3720,  0.2996,  0.2232,  0.1576,  0.0954,  0.0393, -0.0047, -0.0442,
0:         -0.0792, -0.1091, -0.1351, -0.1555, -0.1685, -0.1654,  0.5343,  0.4899,  0.4384,  0.3758,  0.3151,  0.2491,
0:          0.1864], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1998, -0.2268, -0.2145, -0.2177, -0.1957, -0.1431, -0.1132, -0.1004, -0.0610, -0.0191,  0.0020,  0.0439,
0:          0.0768,  0.0750,  0.0983,  0.1518,  0.2170,  0.2528, -0.2492, -0.2591, -0.2405, -0.2475, -0.2367, -0.1938,
0:         -0.1599], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.4956, 1.4962, 1.5000, 1.5006, 1.5019, 1.5025, 1.5012, 1.4969, 1.4919, 1.4861, 1.4797, 1.4733, 1.4671, 1.4576,
0:         1.4463, 1.4349, 1.4246, 1.4163, 1.4094, 1.4021, 1.3925, 1.3808, 1.3674, 1.3553, 1.3449], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1890, -0.1921, -0.1934, -0.1978, -0.1983, -0.2073, -0.2038, -0.2045, -0.2032, -0.1915, -0.1910, -0.1968,
0:         -0.2005, -0.1988, -0.2048, -0.2058, -0.2068, -0.2066, -0.1886, -0.1935, -0.1946, -0.1950, -0.2008, -0.1962,
0:         -0.1987], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.054634351283311844; velocity_v: 0.0947088673710823; specific_humidity: 0.039134830236434937; velocity_z: 0.5731692314147949; temperature: 0.10749904811382294; total_precip: 0.9808306097984314; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.0681249126791954; velocity_v: 0.11428975313901901; specific_humidity: 0.04206738620996475; velocity_z: 0.5331628322601318; temperature: 0.12640060484409332; total_precip: 1.0178252458572388; 
0: epoch: 1 [1/5 (20%)]	Loss: 0.99933 : 0.28646 :: 0.15079 (1.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05469140782952309; velocity_v: 0.09114597737789154; specific_humidity: 0.044551972299814224; velocity_z: 0.515720784664154; temperature: 0.11737152189016342; total_precip: 1.0057777166366577; 
0: epoch: 1 [2/5 (40%)]	Loss: 1.00578 : 0.27852 :: 0.14076 (16.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.0562993623316288; velocity_v: 0.08486314862966537; specific_humidity: 0.03816637024283409; velocity_z: 0.5381748676300049; temperature: 0.09797702729701996; total_precip: 1.053702712059021; 
0: epoch: 1 [3/5 (60%)]	Loss: 1.05370 : 0.28570 :: 0.14340 (16.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.0497666597366333; velocity_v: 0.07832269370555878; specific_humidity: 0.03456876426935196; velocity_z: 0.408792644739151; temperature: 0.09397520869970322; total_precip: 0.6418775320053101; 
0: epoch: 1 [4/5 (80%)]	Loss: 0.64188 : 0.19319 :: 0.13861 (16.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 1 : 0.28721415996551514
0: validation loss for velocity_u : 0.0342445932328701
0: validation loss for velocity_v : 0.06801766902208328
0: validation loss for specific_humidity : 0.026096152141690254
0: validation loss for velocity_z : 0.5273733735084534
0: validation loss for temperature : 0.07687915861606598
0: validation loss for total_precip : 0.9906740188598633
0: 2 : 14:25:34 :: batch_size = 96, lr = 2e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 2, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4655, 0.4750, 0.4848, 0.5107, 0.5567, 0.5870, 0.5826, 0.5810, 0.6132, 0.6473, 0.6424, 0.6137, 0.5990, 0.5998,
0:         0.5967, 0.5815, 0.5516, 0.5043, 0.4352, 0.4561, 0.4781, 0.5066, 0.5492, 0.5906, 0.6216], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0396,  0.0105,  0.0530,  0.0916,  0.1358,  0.1848,  0.2327,  0.2888,  0.3658,  0.4516,  0.5142,  0.5315,
0:          0.5075,  0.4573,  0.3928,  0.3196,  0.2392,  0.1557, -0.0120,  0.0323,  0.0780,  0.1263,  0.1850,  0.2527,
0:          0.3172], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5551, -0.5036, -0.4676, -0.4555, -0.4521, -0.4847, -0.5196, -0.4529, -0.4433, -0.4235, -0.2232, -0.1068,
0:          0.0378,  0.1282,  0.2769,  0.3729,  0.4247,  0.4801, -0.4215, -0.3938, -0.3849, -0.3723, -0.3814, -0.3867,
0:         -0.4219], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5830,  0.7109,  0.7554,  0.7398,  0.6030,  0.3182, -0.0256, -0.4283, -0.9289, -1.1837, -0.7242,  0.1513,
0:          0.5563,  0.2993,  0.0924,  0.2693,  0.3238, -0.0333,  0.3049,  0.4995,  0.4350,  0.1791, -0.0278, -0.2759,
0:         -0.6630], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1993, -0.1811, -0.1624, -0.1493, -0.1381, -0.1303, -0.1379, -0.1622, -0.1840, -0.1923, -0.1940, -0.1959,
0:         -0.1926, -0.1788, -0.1560, -0.1281, -0.1018, -0.0845, -0.0761, -0.0695, -0.0619, -0.0549, -0.0441, -0.0280,
0:         -0.0183], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2456, -0.2267, -0.1772, -0.0688, -0.0428,  1.1574,  1.5370,  0.7825,  0.7070, -0.1065, -0.1843, -0.1301,
0:         -0.0145,  0.3085,  1.3012,  1.6407,  1.5959,  0.9947,  0.1199,  0.0939,  0.2001,  0.7942,  1.3578,  2.8008,
0:          3.3526], device='cuda:0')
0: [DEBUG] Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2562,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2550, -0.2574,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2538,     nan,     nan,     nan,     nan,     nan,     nan, -0.2279,     nan,
0:             nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan, -0.2515,     nan,     nan,
0:             nan,     nan,     nan, -0.2067, -0.2538,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.0161,     nan,     nan,     nan, -0.1077,     nan,     nan,     nan,     nan,  0.1788,     nan,
0:             nan, -0.1147,     nan,     nan,     nan,     nan,     nan,  0.0138,     nan,     nan, -0.2574,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,
0:         -0.2173,     nan,     nan,     nan, -0.2550,     nan,     nan,     nan, -0.1619,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2421,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1152,
0:             nan,     nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2468, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2538,     nan,     nan, -0.2421,     nan,     nan,     nan,     nan,
0:             nan, -0.2362,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2020,     nan,     nan,     nan,     nan, -0.1831,     nan,     nan,     nan,
0:             nan, -0.2550,     nan,     nan, -0.2032,     nan,     nan, -0.1796, -0.2197,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 2, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2478, 0.2676, 0.2788, 0.2806, 0.2840, 0.2955, 0.3244, 0.3684, 0.4212, 0.4759, 0.5280, 0.5762, 0.6211, 0.6625,
0:         0.6976, 0.7292, 0.7531, 0.7666, 0.2965, 0.3158, 0.3244, 0.3283, 0.3339, 0.3516, 0.3891], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2643, -0.2661, -0.2682, -0.2576, -0.2241, -0.1484, -0.0374,  0.1090,  0.2742,  0.4389,  0.5775,  0.6832,
0:          0.7530,  0.8009,  0.8438,  0.8928,  0.9585,  1.0349, -0.2347, -0.2295, -0.2155, -0.1828, -0.1209, -0.0220,
0:          0.1145], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7288, -0.7237, -0.7094, -0.6796, -0.6438, -0.5952, -0.5348, -0.4568, -0.3663, -0.2695, -0.1745, -0.0827,
0:         -0.0027,  0.0579,  0.1057,  0.1475,  0.1827,  0.2255, -0.6992, -0.6851, -0.6544, -0.6180, -0.5674, -0.5055,
0:         -0.4309], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3647,  0.5183,  0.6908,  1.0242,  1.1471,  0.8747,  0.6822,  0.8006,  0.7755,  0.3143,  0.1673,  0.5836,
0:          0.5122, -0.5958, -2.0190, -2.8615, -3.3674, -3.9127,  0.6241,  0.8003,  0.9416,  1.1997,  1.2313,  0.8777,
0:          0.7076], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.2308, -1.2002, -1.1639, -1.1258, -1.0852, -1.0415, -0.9933, -0.9385, -0.8785, -0.8150, -0.7504, -0.6865,
0:         -0.6271, -0.5756, -0.5300, -0.4923, -0.4594, -0.4288, -0.4004, -0.3722, -0.3462, -0.3209, -0.2972, -0.2759,
0:         -0.2592], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2245, -0.2306, -0.2278, -0.2293, -0.2260, -0.2343, -0.2233, -0.2190, -0.2119, -0.2197, -0.2205, -0.2192,
0:         -0.2273, -0.2191, -0.2283, -0.2165, -0.2110, -0.1992, -0.2181, -0.2202, -0.2145, -0.2125, -0.2104, -0.1985,
0:         -0.1976], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.05766203626990318; velocity_v: 0.09694328159093857; specific_humidity: 0.03872150555253029; velocity_z: 0.44318538904190063; temperature: 0.11188150942325592; total_precip: 0.6399553418159485; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05666709318757057; velocity_v: 0.0909302607178688; specific_humidity: 0.04596135392785072; velocity_z: 0.678280234336853; temperature: 0.11571600288152695; total_precip: 1.3009018898010254; 
0: epoch: 2 [1/5 (20%)]	Loss: 0.97043 : 0.28027 :: 0.13880 (2.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.054435402154922485; velocity_v: 0.08655636757612228; specific_humidity: 0.03654785081744194; velocity_z: 0.5136784911155701; temperature: 0.09888048470020294; total_precip: 40.08536911010742; 
0: epoch: 2 [2/5 (40%)]	Loss: 40.08537 : 6.78757 :: 0.14645 (15.99 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05453966557979584; velocity_v: 0.09117580205202103; specific_humidity: 0.04166775196790695; velocity_z: 0.5075910687446594; temperature: 0.11466995626688004; total_precip: 39.900291442871094; 
0: epoch: 2 [3/5 (60%)]	Loss: 39.90029 : 6.75892 :: 0.14901 (15.90 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06016092747449875; velocity_v: 0.08869095891714096; specific_humidity: 0.0394209660589695; velocity_z: 0.6124959588050842; temperature: 0.09227516502141953; total_precip: 38.13998031616211; 
0: epoch: 2 [4/5 (80%)]	Loss: 38.13998 : 6.48021 :: 0.14887 (14.85 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 2 : 7.123180866241455
0: validation loss for velocity_u : 0.03765636309981346
0: validation loss for velocity_v : 0.06464608758687973
0: validation loss for specific_humidity : 0.0273982435464859
0: validation loss for velocity_z : 0.5734368562698364
0: validation loss for temperature : 0.0928807333111763
0: validation loss for total_precip : 41.94306945800781
0: 3 : 14:29:34 :: batch_size = 96, lr = 1.9512195121951222e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 3, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1522, -0.1554, -0.1644, -0.1766, -0.1931, -0.2199, -0.2606, -0.3130, -0.3712, -0.4307, -0.4919, -0.5531,
0:         -0.6057, -0.6393, -0.6484, -0.6366, -0.6156, -0.6030, -0.1398, -0.1414, -0.1433, -0.1444, -0.1473, -0.1582,
0:         -0.1820], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4330, 0.4058, 0.3894, 0.3879, 0.3993, 0.4220, 0.4602, 0.5162, 0.5841, 0.6551, 0.7232, 0.7817, 0.8247, 0.8485,
0:         0.8494, 0.8253, 0.7780, 0.7126, 0.4307, 0.4058, 0.3815, 0.3620, 0.3501, 0.3497, 0.3657], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5418, -0.5576, -0.5643, -0.5726, -0.5797, -0.5796, -0.5786, -0.5752, -0.5691, -0.5645, -0.5627, -0.5685,
0:         -0.5719, -0.5712, -0.5742, -0.5855, -0.5955, -0.6010, -0.5236, -0.5439, -0.5606, -0.5701, -0.5780, -0.5796,
0:         -0.5778], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3999, -0.4917, -0.4699, -0.4316, -0.3759, -0.2874, -0.2666, -0.2972, -0.2491, -0.1682, -0.1485, -0.0655,
0:          0.1400,  0.2460,  0.1739,  0.1367,  0.1826,  0.1138, -0.4437, -0.5890, -0.6087, -0.5792, -0.5300, -0.4229,
0:         -0.2928], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.3637, -0.3609, -0.3644, -0.3750, -0.3917, -0.4103, -0.4288, -0.4438, -0.4511, -0.4534, -0.4562, -0.4573,
0:         -0.4505, -0.4321, -0.4025, -0.3705, -0.3536, -0.3576, -0.3635, -0.3502, -0.3198, -0.2902, -0.2709, -0.2610,
0:         -0.2546], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2248, -0.2281, -0.2314, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2314, -0.2326, -0.2326,
0:         -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359, -0.2359,
0:         -0.2359], device='cuda:0')
0: [DEBUG] Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0698,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0997,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0908,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1063,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1085, -0.1240,     nan,     nan, -0.1628,     nan,     nan,     nan, -0.1262,
0:         -0.1307, -0.1462,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2115,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2060,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1860,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1639,     nan,
0:             nan,     nan,     nan,     nan, -0.2015,     nan,     nan,     nan, -0.1495,     nan,     nan,     nan,
0:             nan, -0.2215,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2237,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2259,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2259,
0:             nan,     nan, -0.2359,     nan,     nan,     nan,     nan,     nan,     nan, -0.2270,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2204,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 3, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1785, -0.1854, -0.1983, -0.2152, -0.2307, -0.2483, -0.2615, -0.2742, -0.2841, -0.2921, -0.2974, -0.2989,
0:         -0.2955, -0.2939, -0.2960, -0.3021, -0.3170, -0.3326, -0.1537, -0.1579, -0.1691, -0.1887, -0.2118, -0.2367,
0:         -0.2557], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7077, -0.6533, -0.5969, -0.5473, -0.5159, -0.4996, -0.4995, -0.5144, -0.5390, -0.5708, -0.5987, -0.6287,
0:         -0.6587, -0.6871, -0.7138, -0.7448, -0.7757, -0.7880, -0.7085, -0.6611, -0.6093, -0.5601, -0.5213, -0.4977,
0:         -0.4911], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6304, -0.6273, -0.6276, -0.6258, -0.6260, -0.6254, -0.6246, -0.6223, -0.6216, -0.6204, -0.6181, -0.6151,
0:         -0.6144, -0.6145, -0.6148, -0.6156, -0.6150, -0.6123, -0.6354, -0.6329, -0.6328, -0.6320, -0.6312, -0.6285,
0:         -0.6256], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0145, -0.0904, -0.0191,  0.1138,  0.1741,  0.0987, -0.0868, -0.2913, -0.3620, -0.2811, -0.0969,  0.0117,
0:         -0.1254, -0.3407, -0.4318, -0.3891, -0.2463, -0.1189,  0.1306,  0.0312,  0.0318,  0.0690,  0.0527, -0.0526,
0:         -0.2044], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.5230, -1.5170, -1.5094, -1.5042, -1.4999, -1.4947, -1.4850, -1.4683, -1.4497, -1.4345, -1.4240, -1.4110,
0:         -1.3876, -1.3507, -1.3008, -1.2501, -1.2064, -1.1715, -1.1408, -1.1087, -1.0743, -1.0431, -1.0225, -1.0150,
0:         -1.0196], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.8590, 0.6535, 0.5004, 0.5348, 0.4857, 0.4521, 0.5280, 0.5468, 0.5445, 0.8286, 0.6618, 0.4811, 0.4321, 0.4011,
0:         0.3955, 0.3991, 0.4834, 0.5022, 0.8122, 0.6380, 0.6001, 0.4918, 0.4544, 0.4569, 0.3927], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.06730803102254868; velocity_v: 0.10518553853034973; specific_humidity: 0.04202623665332794; velocity_z: 0.6120637059211731; temperature: 0.10719849914312363; total_precip: 37.97289276123047; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.061364494264125824; velocity_v: 0.09291741997003555; specific_humidity: 0.04489152878522873; velocity_z: 0.7000337839126587; temperature: 0.10772869735956192; total_precip: 42.63252639770508; 
0: epoch: 3 [1/5 (20%)]	Loss: 40.30271 : 6.85300 :: 0.14505 (2.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.056320689618587494; velocity_v: 0.08465312421321869; specific_humidity: 0.03912458196282387; velocity_z: 0.49391064047813416; temperature: 0.09574012458324432; total_precip: 40.025428771972656; 
0: epoch: 3 [2/5 (40%)]	Loss: 40.02543 : 6.77403 :: 0.14885 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06243247166275978; velocity_v: 0.0951339602470398; specific_humidity: 0.04192570969462395; velocity_z: 0.5378720760345459; temperature: 0.10268054157495499; total_precip: 41.76995086669922; 
0: epoch: 3 [3/5 (60%)]	Loss: 41.76995 : 7.07596 :: 0.15260 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06543418765068054; velocity_v: 0.10326356440782547; specific_humidity: 0.04923481121659279; velocity_z: 0.5845158100128174; temperature: 0.13505569100379944; total_precip: 44.13157272338867; 
0: epoch: 3 [4/5 (80%)]	Loss: 44.13157 : 7.48491 :: 0.15226 (15.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 3 : 0.39732134342193604
0: validation loss for velocity_u : 0.03849761560559273
0: validation loss for velocity_v : 0.05851948633790016
0: validation loss for specific_humidity : 0.030228599905967712
0: validation loss for velocity_z : 0.46827933192253113
0: validation loss for temperature : 0.09174901247024536
0: validation loss for total_precip : 1.6966547966003418
0: 4 : 14:33:29 :: batch_size = 96, lr = 1.903628792385485e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 4, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4249,  0.3325,  0.2582,  0.2220,  0.1992,  0.1863,  0.1528,  0.0788,  0.0192,  0.0136,  0.0440,  0.0941,
0:          0.1466,  0.1679,  0.1399,  0.0666, -0.0233, -0.0860,  0.5188,  0.4237,  0.3422,  0.3171,  0.2916,  0.2683,
0:          0.2460], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1372,  0.1542,  0.1794,  0.1207,  0.0633,  0.0764,  0.1340,  0.2244,  0.3323,  0.4242,  0.4739,  0.4659,
0:          0.4017,  0.2948,  0.1882,  0.1235,  0.0891,  0.0631,  0.1104,  0.1157,  0.1272,  0.0455, -0.0279, -0.0150,
0:          0.0398], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4729,  0.4341,  0.3992,  0.4885,  0.7962,  1.3118,  1.7196,  2.3070,  2.5426,  2.5793,  2.5919,  2.3473,
0:          1.8888,  1.2869,  0.7233,  0.1201, -0.2150, -0.3891,  0.4840,  0.4123,  0.3426,  0.3618,  0.8846,  1.4572,
0:          1.9095], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1543,  0.1825,  0.6334,  0.3791, -0.9485, -1.2049, -0.2217, -0.2238, -0.5530, -0.2814, -0.2347, -0.2293,
0:         -0.0967, -0.0359,  0.2118,  0.3639,  0.3531,  0.4117,  0.1130,  0.2933,  0.1836,  0.2053, -0.4846, -1.0321,
0:         -0.3922], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.7764, 0.7663, 0.7819, 0.7872, 0.7797, 0.7738, 0.7627, 0.7305, 0.6716, 0.6272, 0.6161, 0.6325, 0.6933, 0.7742,
0:         0.8294, 0.8445, 0.8100, 0.7612, 0.7546, 0.7724, 0.7683, 0.7091, 0.6084, 0.5127, 0.4269], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2031, -0.1873, -0.1101,  0.0317,  0.6500,  0.3959, -0.2179, -0.2417, -0.2417, -0.1997, -0.1748, -0.1169,
0:          0.2143,  0.0941, -0.1079, -0.2417, -0.2417, -0.2417, -0.1249, -0.0806, -0.0999,  0.2847, -0.0738, -0.2417,
0:         -0.2417], device='cuda:0')
0: [DEBUG] Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2168,     nan,     nan,  0.1531,
0:             nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2054,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.5728,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.7271,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.3006,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,  0.6500,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1668,     nan,     nan,     nan,     nan,  1.5803,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.2166,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,  6.8397,  1.9456,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 4, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5125, 0.5463, 0.5537, 0.5056, 0.4181, 0.3159, 0.2338, 0.1871, 0.1699, 0.1767, 0.1884, 0.1916, 0.1960, 0.2064,
0:         0.2200, 0.2364, 0.2425, 0.2372, 0.5071, 0.5340, 0.5213, 0.4524, 0.3368, 0.2159, 0.1267], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0754,  0.0823,  0.0747,  0.0552,  0.0264,  0.0031, -0.0188, -0.0339, -0.0470, -0.0610, -0.0856, -0.1204,
0:         -0.1654, -0.2202, -0.2719, -0.3083, -0.3180, -0.2966,  0.0484,  0.0621,  0.0634,  0.0518,  0.0293,  0.0049,
0:         -0.0163], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8694, 0.9215, 1.0538, 1.2799, 1.5690, 1.8869, 2.2041, 2.4792, 2.6923, 2.8281, 2.9070, 2.9259, 2.9255, 2.9348,
0:         2.9574, 3.0221, 3.1052, 3.1965, 0.8478, 0.9597, 1.1835, 1.4778, 1.8203, 2.1379, 2.4131], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3935, -0.5807, -0.4799, -0.5025, -1.3238, -0.9966, -0.4383, -0.6829, -0.3776, -0.0875, -0.4040, -0.3906,
0:         -0.1672, -0.0905, -0.0761, -0.2157, -0.4811, -0.6255, -0.0617, -1.0577, -0.3947, -0.0436, -1.1586, -0.7016,
0:          0.2925], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.0912, 1.0480, 1.0072, 0.9775, 0.9749, 0.9990, 1.0385, 1.0803, 1.1096, 1.1232, 1.1254, 1.1214, 1.1191, 1.1216,
0:         1.1242, 1.1297, 1.1367, 1.1494, 1.1657, 1.1844, 1.1944, 1.1899, 1.1677, 1.1332, 1.0973], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 2.5359,  2.1448,  1.7463,  1.2837,  0.9269,  0.5981,  0.4122,  0.2561,  0.2474,  2.2441,  1.8643,  1.3796,
0:          0.9379,  0.5013,  0.2433,  0.0794,  0.0691,  0.0308,  1.9022,  1.4616,  1.0107,  0.4784,  0.1577, -0.0472,
0:         -0.1003], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.07965224981307983; velocity_v: 0.12240040302276611; specific_humidity: 0.03775480389595032; velocity_z: 0.5272740125656128; temperature: 0.09719623625278473; total_precip: 1.1847652196884155; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06746716052293777; velocity_v: 0.10562638938426971; specific_humidity: 0.04500725120306015; velocity_z: 0.4650261104106903; temperature: 0.1067943423986435; total_precip: 0.63251793384552; 
0: epoch: 4 [1/5 (20%)]	Loss: 0.90864 : 0.25965 :: 0.15991 (2.60 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06788522750139236; velocity_v: 0.09875904768705368; specific_humidity: 0.05320070683956146; velocity_z: 0.42492228746414185; temperature: 0.10784602165222168; total_precip: 0.5005335807800293; 
0: epoch: 4 [2/5 (40%)]	Loss: 0.50053 : 0.18055 :: 0.16398 (15.88 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.09738992899656296; velocity_v: 0.13843472301959991; specific_humidity: 0.09275718778371811; velocity_z: 0.6925062537193298; temperature: 0.15058815479278564; total_precip: 1.2975924015045166; 
0: epoch: 4 [3/5 (60%)]	Loss: 1.29759 : 0.37922 :: 0.17794 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.09075859189033508; velocity_v: 0.1553461253643036; specific_humidity: 0.06483606994152069; velocity_z: 0.5411097407341003; temperature: 0.10578572005033493; total_precip: 0.5301415324211121; 
0: epoch: 4 [4/5 (80%)]	Loss: 0.53014 : 0.21831 :: 0.17798 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 4 : 0.27251672744750977
0: validation loss for velocity_u : 0.05993720889091492
0: validation loss for velocity_v : 0.08327945321798325
0: validation loss for specific_humidity : 0.07610684633255005
0: validation loss for velocity_z : 0.5306168794631958
0: validation loss for temperature : 0.09359671920537949
0: validation loss for total_precip : 0.7915632128715515
0: 5 : 14:37:27 :: batch_size = 96, lr = 1.857198821839498e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 5, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9334, -0.9478, -0.9381, -0.9104, -0.8769, -0.8370, -0.7798, -0.7399, -0.7500, -0.7757, -0.8207, -0.8401,
0:         -0.8035, -0.8158, -0.8647, -0.8657, -0.8570, -0.8654, -0.9097, -0.9265, -0.9278, -0.8997, -0.8651, -0.8413,
0:         -0.8012], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1222, -0.1188, -0.0670, -0.0349, -0.0300, -0.0167,  0.0062,  0.0049, -0.0067,  0.0104, -0.0203, -0.0793,
0:         -0.0719,  0.0429,  0.1791,  0.1901,  0.1244,  0.0715, -0.0562, -0.0533, -0.0048,  0.0400,  0.0448,  0.0410,
0:          0.0472], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7660, 0.9043, 1.3394, 1.4670, 1.5155, 1.6493, 1.4957, 1.4598, 1.2930, 1.2531, 1.3507, 1.4899, 1.7454, 1.8719,
0:         2.0901, 2.1043, 1.9625, 1.8519, 0.7790, 0.9468, 1.4378, 1.7848, 1.8334, 1.8080, 1.5919], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ -0.1079,  -0.2286,  -0.1189,  -0.0870,  -0.4503,  -0.2132,   1.1398,   3.2819,   0.8677,  -7.8532, -14.4529,
0:         -14.0293, -12.0442, -12.2592,  -9.0856,  -7.3934, -10.0700,  -4.7432,  -0.1935,  -0.3548,  -0.2034,  -0.4217,
0:          -1.6837,  -1.6969,  -0.1913], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1644, -0.2544, -0.4346, -0.5659, -0.6512, -0.7474, -0.6712, -0.4484, -0.1472,  0.2459,  0.5454,  0.4251,
0:         -0.0278, -0.5202, -0.9626, -1.1479, -1.1175, -1.0514, -0.9867, -1.0150, -1.1032, -1.1696, -1.1103, -0.9790,
0:         -0.8776], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.4195,  0.3593,  0.7828,  0.6925,  1.0681,  0.8474,  1.0859,  1.8315,  2.2093,  0.0774, -0.0129,  0.5599,
0:          2.0298,  2.4054,  3.5521,  3.8185,  2.4065,  0.9667, -0.2135, -0.2246,  0.2178,  1.9307,  4.1595,  5.8378,
0:          4.8415], device='cuda:0')
0: [DEBUG] Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([        nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:          1.4882e+00,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan, -2.5584e-01,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan, -2.5473e-01,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan, -2.4916e-01,         nan,
0:                 nan,         nan,         nan,         nan,  5.5769e-01,         nan,         nan,         nan,
0:                 nan,         nan,         nan,  3.1697e-01,         nan,         nan, -2.1461e-01,         nan,
0:                 nan,         nan,  4.1709e-02,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan, -1.8786e-01,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,  1.7321e-01,         nan,         nan,  6.7582e-01,         nan,         nan,  2.2394e+00,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,  4.5293e-01, -2.0235e-01,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,  1.5900e-03,         nan,  1.4637e+00,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,  1.4869e-01,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,  1.7423e+00,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,  7.5271e-01,
0:                 nan,         nan,         nan,         nan,  2.4993e-02,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,  2.5679e-01,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:          1.3755e-01,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan,         nan, -1.8118e-01,         nan,         nan, -2.1461e-01,
0:                 nan,         nan,         nan,         nan,         nan,         nan,         nan,         nan,
0:                 nan,         nan,         nan])
0: [DEBUG] Epoch 5, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7419, -0.6639, -0.5836, -0.5440, -0.5471, -0.5724, -0.5931, -0.5882, -0.5659, -0.5396, -0.5286, -0.5544,
0:         -0.5840, -0.5948, -0.5766, -0.5190, -0.4586, -0.4074, -0.7176, -0.6601, -0.5925, -0.5656, -0.5887, -0.6190,
0:         -0.6422], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3583, 0.3660, 0.3528, 0.3294, 0.3071, 0.3061, 0.3083, 0.3142, 0.3187, 0.3145, 0.3044, 0.2968, 0.2822, 0.2513,
0:         0.2084, 0.1827, 0.1828, 0.2126, 0.2967, 0.3189, 0.3154, 0.2863, 0.2624, 0.2531, 0.2551], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9102, 1.0205, 1.1388, 1.2538, 1.3580, 1.4537, 1.5565, 1.6704, 1.7846, 1.8935, 1.9710, 2.0133, 2.0112, 1.9955,
0:         1.9717, 1.9686, 1.9766, 2.0153, 0.7055, 0.8310, 0.9736, 1.1198, 1.2534, 1.3825, 1.5135], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3062,  0.6014,  0.7784,  0.6528,  0.4159,  0.1368, -0.1014, -0.1721, -0.1700,  0.1194,  0.4243,  0.4802,
0:          0.7009,  0.8151,  0.6184,  0.6483,  0.7180,  0.4884,  0.5079,  0.6543,  0.7712,  0.6558,  0.4729,  0.2312,
0:         -0.0106], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0124,  0.0899,  0.1627,  0.1998,  0.1761,  0.1047,  0.0150, -0.0609, -0.1000, -0.1151, -0.1261, -0.1536,
0:         -0.2113, -0.2955, -0.3925, -0.4897, -0.5743, -0.6442, -0.6992, -0.7584, -0.8262, -0.8964, -0.9406, -0.9251,
0:         -0.8301], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2179, -0.2150, -0.2083, -0.2077, -0.1959, -0.1912, -0.1830, -0.1645, -0.1559, -0.2191, -0.2243, -0.2227,
0:         -0.2176, -0.2138, -0.2016, -0.1921, -0.1753, -0.1645, -0.2355, -0.2322, -0.2289, -0.2299, -0.2245, -0.2150,
0:         -0.1990], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.08580345660448074; velocity_v: 0.1296868920326233; specific_humidity: 0.09719165414571762; velocity_z: 0.5021096467971802; temperature: 0.14296862483024597; total_precip: 0.8400370478630066; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11180686950683594; velocity_v: 0.16035059094429016; specific_humidity: 0.1016426533460617; velocity_z: 0.5896090269088745; temperature: 0.11369290202856064; total_precip: 0.9884426593780518; 
0: epoch: 5 [1/5 (20%)]	Loss: 0.91424 : 0.29104 :: 0.19111 (2.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11962417513132095; velocity_v: 0.19338840246200562; specific_humidity: 0.13967633247375488; velocity_z: 0.6925791501998901; temperature: 0.15009024739265442; total_precip: 1.2921134233474731; 
0: epoch: 5 [2/5 (40%)]	Loss: 1.29211 : 0.39867 :: 0.19132 (15.75 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1204589307308197; velocity_v: 0.188989520072937; specific_humidity: 0.16486573219299316; velocity_z: 0.6309446692466736; temperature: 0.1507321447134018; total_precip: 1.4227776527404785; 
0: epoch: 5 [3/5 (60%)]	Loss: 1.42278 : 0.41352 :: 0.19163 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11759933084249496; velocity_v: 0.17718422412872314; specific_humidity: 0.13840825855731964; velocity_z: 0.5278510451316833; temperature: 0.12524788081645966; total_precip: 0.9331809878349304; 
0: epoch: 5 [4/5 (80%)]	Loss: 0.93318 : 0.30499 :: 0.19103 (15.98 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 5 : 0.34912872314453125
0: validation loss for velocity_u : 0.08136165142059326
0: validation loss for velocity_v : 0.14114613831043243
0: validation loss for specific_humidity : 0.17114727199077606
0: validation loss for velocity_z : 0.49743005633354187
0: validation loss for temperature : 0.10988689213991165
0: validation loss for total_precip : 1.093800663948059
0: 6 : 14:41:20 :: batch_size = 96, lr = 1.8119012895995104e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 6, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0185, -0.0663, -0.1647, -0.2610, -0.3655, -0.4800, -0.5784, -0.6311, -0.6114, -0.5626, -0.5493, -0.5669,
0:         -0.5835, -0.6428, -0.7121, -0.6902, -0.6032, -0.5284, -0.1010, -0.1766, -0.2710, -0.3630, -0.4627, -0.5642,
0:         -0.6556], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7945, 0.8461, 0.8939, 0.9263, 0.9369, 0.9454, 0.9407, 0.9312, 0.9148, 0.8771, 0.8755, 0.8864, 0.8706, 0.8569,
0:         0.8208, 0.7590, 0.7147, 0.7096, 0.8086, 0.8773, 0.9203, 0.9450, 0.9492, 0.9257, 0.8832], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.6120, 2.5172, 2.4241, 2.3604, 2.3593, 2.3678, 2.4800, 2.8714, 2.9454, 3.0003, 2.9322, 2.8267, 2.6938, 2.5985,
0:         2.6698, 2.7596, 2.6897, 2.6226, 2.5317, 2.4402, 2.3477, 2.3132, 2.3154, 2.3963, 2.6748], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9060, -0.5636, -0.5284, -0.2913,  0.1280,  0.2136,  0.4705,  0.1390, -0.1398, -0.7150, -0.7150,  0.4859,
0:          0.9293,  0.7800,  0.5978,  0.1170, -0.4187, -0.4077,  0.2663,  0.7032,  0.6198,  0.6922,  0.7559,  0.5715,
0:          0.5342], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2345,  0.0763,  0.1883,  0.1289, -0.0291, -0.0364, -0.1040, -0.3184, -0.4322, -0.5240, -0.5828, -0.6733,
0:         -0.8655, -1.0019, -1.3076, -1.6060, -1.5260, -1.2219, -0.7327, -0.1597,  0.4024,  0.9226,  1.3335,  1.6686,
0:          1.9140], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588,
0:         -0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.1941, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588, -0.2588,
0:         -0.1181], device='cuda:0')
0: [DEBUG] Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.2588,     nan,     nan,     nan,     nan,     nan, -0.2588, -0.2588, -0.2588,     nan,
0:             nan,     nan,     nan, -0.2588, -0.2588,     nan,     nan,     nan,     nan,     nan,     nan, -0.2588,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2588,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2588,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2588,     nan,     nan,     nan,     nan,     nan,     nan, -0.2588,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2588,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2588,     nan,     nan,     nan,     nan, -0.2588,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2576,
0:             nan, -0.2588,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2588,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2588,     nan,     nan,
0:             nan, -0.2588,     nan, -0.2588,     nan,     nan,     nan,     nan, -0.2588,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2588, -0.2588,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2588,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2588,     nan,     nan, -0.2588,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2588,     nan,     nan])
0: [DEBUG] Epoch 6, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4519, -0.3417, -0.2179, -0.1396, -0.1195, -0.1340, -0.1334, -0.1029, -0.0636, -0.0341, -0.0547, -0.1363,
0:         -0.2403, -0.3154, -0.3447, -0.3049, -0.2420, -0.1761, -0.4157, -0.3205, -0.2108, -0.1464, -0.1545, -0.1712,
0:         -0.1731], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3402, 0.3755, 0.3787, 0.3464, 0.2953, 0.2607, 0.2327, 0.2241, 0.2264, 0.2236, 0.2082, 0.1830, 0.1499, 0.0982,
0:         0.0466, 0.0206, 0.0258, 0.0505, 0.2697, 0.3098, 0.3150, 0.2700, 0.2118, 0.1565, 0.1260], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.7053, 1.7592, 1.7952, 1.8213, 1.8097, 1.7881, 1.7879, 1.7936, 1.8261, 1.8623, 1.9237, 1.9679, 2.0347, 2.1266,
0:         2.2039, 2.3036, 2.3984, 2.4751, 1.7204, 1.7876, 1.8400, 1.8633, 1.8590, 1.8337, 1.8304], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.4297,  0.4224,  0.5180,  0.4623,  0.3076,  0.3115,  0.3657,  0.3653,  0.1156, -0.1606, -0.2518, -0.1551,
0:          0.0116, -0.8069, -1.8520, -1.4495, -0.7291, -0.5798,  0.5062,  0.3215,  0.2725,  0.2429,  0.2556,  0.3884,
0:          0.3761], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.6429, -0.7365, -0.7848, -0.7793, -0.7341, -0.6872, -0.6711, -0.6989, -0.7487, -0.7858, -0.7799, -0.7244,
0:         -0.6463, -0.5947, -0.6376, -0.7923, -1.0350, -1.2886, -1.4737, -1.5555, -1.5452, -1.4903, -1.4408, -1.4159,
0:         -1.4161], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2194, -0.2218, -0.2216, -0.2269, -0.2268, -0.2291, -0.2360, -0.2248, -0.2197, -0.2135, -0.2189, -0.2238,
0:         -0.2211, -0.2290, -0.2307, -0.2286, -0.2237, -0.2176, -0.2195, -0.2221, -0.2198, -0.2247, -0.2282, -0.2299,
0:         -0.2208], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.13662728667259216; velocity_v: 0.16407354176044464; specific_humidity: 0.17305351793766022; velocity_z: 0.5537919998168945; temperature: 0.1387670934200287; total_precip: 0.9408753514289856; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.12528499960899353; velocity_v: 0.20381072163581848; specific_humidity: 0.15905426442623138; velocity_z: 0.6739564538002014; temperature: 0.15199801325798035; total_precip: 1.1521517038345337; 
0: epoch: 6 [1/5 (20%)]	Loss: 1.04651 : 0.34788 :: 0.19545 (2.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13123473525047302; velocity_v: 0.19078674912452698; specific_humidity: 0.1897963434457779; velocity_z: 0.5802940726280212; temperature: 0.13066977262496948; total_precip: 0.6474759578704834; 
0: epoch: 6 [2/5 (40%)]	Loss: 0.64748 : 0.27917 :: 0.19693 (15.80 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.12114552408456802; velocity_v: 0.19178909063339233; specific_humidity: 0.15222768485546112; velocity_z: 0.5603058934211731; temperature: 0.11451764404773712; total_precip: 0.9215301871299744; 
0: epoch: 6 [3/5 (60%)]	Loss: 0.92153 : 0.31166 :: 0.19623 (15.86 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14872953295707703; velocity_v: 0.2425999641418457; specific_humidity: 0.18452607095241547; velocity_z: 0.6743277907371521; temperature: 0.16450846195220947; total_precip: 1.112051010131836; 
0: epoch: 6 [4/5 (80%)]	Loss: 1.11205 : 0.38556 :: 0.20691 (16.03 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 6 : 0.33406445384025574
0: validation loss for velocity_u : 0.0836343988776207
0: validation loss for velocity_v : 0.18332348763942719
0: validation loss for specific_humidity : 0.19292707741260529
0: validation loss for velocity_z : 0.472082257270813
0: validation loss for temperature : 0.10141237825155258
0: validation loss for total_precip : 0.9710070490837097
0: 7 : 14:45:14 :: batch_size = 96, lr = 1.7677085752190346e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 7, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.0048, 1.9831, 1.9579, 1.9350, 1.9164, 1.8994, 1.8802, 1.8561, 1.8235, 1.7819, 1.7353, 1.6889, 1.6448, 1.5990,
0:         1.5455, 1.4839, 1.4208, 1.3640, 2.0611, 2.0368, 1.9941, 1.9528, 1.9304, 1.9299, 1.9421], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3073, -0.2894, -0.2792, -0.2732, -0.2714, -0.2773, -0.2956, -0.3300, -0.3828, -0.4519, -0.5283, -0.6007,
0:         -0.6602, -0.7013, -0.7212, -0.7224, -0.7093, -0.6853, -0.2884, -0.2383, -0.2088, -0.1985, -0.2014, -0.2149,
0:         -0.2408], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.6941, 2.7277, 2.7893, 2.8612, 2.8573, 2.8815, 2.8458, 2.8073, 2.7639, 2.6987, 2.5192, 2.3791, 2.1559, 1.9398,
0:         1.7122, 1.5386, 1.3944, 1.2665, 2.6870, 2.6669, 2.7126, 2.7792, 2.8324, 2.8694, 2.8596], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-3.0078, -3.2798, -3.1139, -2.6098, -2.0039, -1.5284, -1.2432, -1.0132, -0.7478, -0.4891, -0.3034, -0.2171,
0:         -0.2237, -0.2680, -0.2790, -0.2282, -0.1530, -0.1176, -3.0343, -3.4523, -3.5186, -3.2400, -2.7822, -2.3687,
0:         -2.0790], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.7732, 1.7447, 1.6895, 1.6305, 1.5832, 1.5497, 1.5152, 1.4606, 1.3838, 1.2955, 1.2031, 1.1077, 1.0089, 0.9087,
0:         0.8112, 0.7236, 0.6516, 0.5969, 0.5551, 0.5182, 0.4831, 0.4515, 0.4250, 0.4042, 0.3919], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2049, -0.2049, -0.2002, -0.2049, -0.2179, -0.2249, -0.2296, -0.2285, -0.2273, -0.1050, -0.2179, -0.2226,
0:         -0.2238, -0.2214, -0.2261, -0.2308, -0.2332, -0.2320,  0.0620, -0.0497, -0.1591, -0.2320, -0.2285, -0.2308,
0:         -0.2296], device='cuda:0')
0: [DEBUG] Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2426,     nan,     nan,     nan,     nan, -0.2355,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2426,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2449, -0.2449,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2414, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan, -0.2449,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan, -0.2449,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 7, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([3.0058, 2.9367, 2.8598, 2.7644, 2.6672, 2.5746, 2.4760, 2.4106, 2.3626, 2.3583, 2.3576, 2.3848, 2.3956, 2.3692,
0:         2.3319, 2.2655, 2.2163, 2.1717, 3.0876, 3.0256, 2.9504, 2.8683, 2.7752, 2.6698, 2.5741], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9876, -1.0165, -1.0578, -1.1095, -1.1773, -1.2394, -1.3036, -1.3460, -1.3685, -1.3726, -1.3588, -1.3406,
0:         -1.3234, -1.3156, -1.3023, -1.2580, -1.1811, -1.0586, -0.9665, -0.9610, -0.9687, -1.0027, -1.0587, -1.1303,
0:         -1.1991], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0633, 1.1695, 1.2719, 1.3595, 1.4239, 1.4586, 1.4608, 1.4476, 1.4203, 1.3918, 1.3665, 1.3461, 1.3238, 1.3139,
0:         1.3035, 1.3086, 1.3182, 1.3440, 0.7191, 0.8217, 0.9240, 1.0357, 1.1258, 1.1918, 1.2374], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6289, -0.7010, -0.7385, -0.9352, -1.2116, -1.3210, -1.3566, -1.4655, -1.5121, -1.5280, -1.5807, -1.5142,
0:         -1.4260, -1.4358, -1.3972, -1.1947, -0.9889, -0.9694, -0.0232, -0.1049, -0.2511, -0.4313, -0.6522, -0.8114,
0:         -0.9442], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.2738, 0.2209, 0.2084, 0.2420, 0.2979, 0.3417, 0.3591, 0.3435, 0.3117, 0.2819, 0.2574, 0.2544, 0.2743, 0.3180,
0:         0.3629, 0.3952, 0.3926, 0.3738, 0.3641, 0.3787, 0.4153, 0.4576, 0.4890, 0.4907, 0.4712], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0428, -0.0608, -0.0675, -0.0951, -0.0970, -0.1081, -0.1081, -0.0869, -0.0732, -0.0294, -0.0550, -0.0679,
0:         -0.0797, -0.1001, -0.1044, -0.0902, -0.0728, -0.0498, -0.0351, -0.0552, -0.0622, -0.0880, -0.1037, -0.0944,
0:         -0.0710], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1436442881822586; velocity_v: 0.23567615449428558; specific_humidity: 0.2082461714744568; velocity_z: 0.5320861339569092; temperature: 0.13175101578235626; total_precip: 0.9898347854614258; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1584167182445526; velocity_v: 0.26081573963165283; specific_humidity: 0.1916135847568512; velocity_z: 0.6242708563804626; temperature: 0.14256994426250458; total_precip: 0.8934841752052307; 
0: epoch: 7 [1/5 (20%)]	Loss: 0.94166 : 0.34162 :: 0.21161 (2.74 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15083831548690796; velocity_v: 0.25271525979042053; specific_humidity: 0.15818817913532257; velocity_z: 0.6462734937667847; temperature: 0.11921472102403641; total_precip: 1.022301435470581; 
0: epoch: 7 [2/5 (40%)]	Loss: 1.02230 : 0.35801 :: 0.20470 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15266716480255127; velocity_v: 0.33481258153915405; specific_humidity: 0.22864456474781036; velocity_z: 0.5757738351821899; temperature: 0.14968781173229218; total_precip: 1.3326537609100342; 
0: epoch: 7 [3/5 (60%)]	Loss: 1.33265 : 0.42778 :: 0.20502 (16.00 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15369826555252075; velocity_v: 0.24007178843021393; specific_humidity: 0.1962234079837799; velocity_z: 0.5320590138435364; temperature: 0.14333415031433105; total_precip: 0.9900371432304382; 
0: epoch: 7 [4/5 (80%)]	Loss: 0.99004 : 0.34176 :: 0.20823 (15.94 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 7 : 0.4407113492488861
0: validation loss for velocity_u : 0.10923335701227188
0: validation loss for velocity_v : 0.22245758771896362
0: validation loss for specific_humidity : 0.1986498385667801
0: validation loss for velocity_z : 0.6069948077201843
0: validation loss for temperature : 0.10463467985391617
0: validation loss for total_precip : 1.4022973775863647
0: 8 : 14:49:01 :: batch_size = 96, lr = 1.7245937319210094e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 8, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3343,  0.3062,  0.2960,  0.2784,  0.2451,  0.2227,  0.2403,  0.2387,  0.1565,  0.0547, -0.0047, -0.0493,
0:         -0.0896, -0.1076, -0.1062, -0.0764, -0.0180,  0.0183,  0.3204,  0.3100,  0.3142,  0.3028,  0.2670,  0.2350,
0:          0.2512], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7561, -0.7257, -0.6843, -0.6291, -0.5634, -0.4831, -0.3973, -0.3291, -0.2566, -0.1632, -0.0916, -0.0648,
0:         -0.0747, -0.1148, -0.1419, -0.1267, -0.1243, -0.1712, -0.7285, -0.7069, -0.6727, -0.6277, -0.5681, -0.4910,
0:         -0.4088], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5847, 0.5324, 0.4638, 0.4636, 0.5181, 0.5237, 0.3641, 0.3723, 0.6849, 1.0392, 1.3399, 1.5926, 1.7919, 1.9709,
0:         2.1429, 2.0400, 1.9612, 1.9949, 0.5814, 0.5390, 0.4603, 0.4583, 0.5068, 0.4985, 0.3319], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.1555,  0.8778,  1.0343,  1.1577,  0.8646,  0.9483,  1.3935,  1.4134,  1.1709,  0.8558,  0.4480,  0.1196,
0:         -0.2418, -0.4622, -0.3917, -0.3829, -0.2462,  0.1725,  1.0541,  1.0255,  1.2855,  1.1842,  0.9065,  0.9924,
0:          1.3010], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.4294, 0.4444, 0.4617, 0.4955, 0.5422, 0.5712, 0.5888, 0.6455, 0.7248, 0.7618, 0.7572, 0.7184, 0.6376, 0.5548,
0:         0.4966, 0.4702, 0.5006, 0.5638, 0.6099, 0.6303, 0.6629, 0.7531, 0.8263, 0.7563, 0.6444], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2508, -0.1732,  0.0896, -0.1663, -0.2371, -0.2349, -0.2280, -0.1617, -0.1709, -0.2508, -0.1777,  0.0256,
0:         -0.1297, -0.1983, -0.2143, -0.2371, -0.1366, -0.0429, -0.2508, -0.2257, -0.1777, -0.2211, -0.2440, -0.2463,
0:         -0.2508], device='cuda:0')
0: [DEBUG] Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2508,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2280,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2063,
0:         -0.2371,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2508,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan, -0.2314,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2508,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2394,     nan,     nan,     nan, -0.2429,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2497,     nan, -0.2337,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2337, -0.2474,     nan,     nan,     nan,     nan,     nan,  0.0165,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2451, -0.2451,     nan, -0.2508,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2508,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2497,     nan,     nan,     nan,     nan,  0.1924,     nan,     nan,     nan,     nan,     nan, -0.2486,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2383,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2246])
0: [DEBUG] Epoch 8, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2711, -0.2324, -0.1970, -0.1923, -0.2191, -0.2628, -0.2754, -0.2747, -0.2721, -0.2847, -0.3343, -0.4120,
0:         -0.4963, -0.5452, -0.5760, -0.5829, -0.6127, -0.6617, -0.2490, -0.2177, -0.1880, -0.2066, -0.2600, -0.3110,
0:         -0.3280], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2962, -0.2711, -0.2647, -0.2825, -0.3249, -0.3760, -0.4253, -0.4524, -0.4470, -0.4129, -0.3570, -0.2911,
0:         -0.2381, -0.2232, -0.2390, -0.2398, -0.2121, -0.1456, -0.3532, -0.3201, -0.3090, -0.3344, -0.3885, -0.4600,
0:         -0.5210], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1839, -0.1734, -0.1997, -0.2271, -0.2810, -0.3282, -0.3602, -0.3772, -0.3738, -0.3801, -0.3757, -0.3910,
0:         -0.4031, -0.4013, -0.4067, -0.3820, -0.3430, -0.2925, -0.2273, -0.2135, -0.2242, -0.2514, -0.3017, -0.3645,
0:         -0.3969], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.4116,  0.6632,  0.7727,  0.4212,  0.1296,  0.3285,  0.5112,  0.3063,  0.0115, -0.0778, -0.0610,  0.0485,
0:          0.2428,  0.3240,  0.3718,  0.6116,  0.8341,  0.7194,  0.3804,  0.4077,  0.3906,  0.1582,  0.0200,  0.2086,
0:          0.3758], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.5394, 1.5735, 1.6028, 1.6178, 1.6295, 1.6473, 1.6717, 1.6883, 1.6850, 1.6532, 1.5942, 1.5170, 1.4268, 1.3324,
0:         1.2368, 1.1535, 1.0998, 1.0804, 1.0859, 1.0918, 1.0795, 1.0494, 1.0160, 0.9989, 1.0075], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0437,  0.0324,  0.0415, -0.0041, -0.0232, -0.0494, -0.0520, -0.0235, -0.0064,  0.0449,  0.0260,  0.0083,
0:         -0.0003, -0.0355, -0.0357, -0.0352, -0.0192,  0.0032,  0.0301,  0.0044,  0.0075, -0.0240, -0.0378, -0.0241,
0:         -0.0175], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.12476816028356552; velocity_v: 0.2466970533132553; specific_humidity: 0.1867932826280594; velocity_z: 0.4613056778907776; temperature: 0.10923730581998825; total_precip: 0.6978116631507874; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14506939053535461; velocity_v: 0.24016354978084564; specific_humidity: 0.18836604058742523; velocity_z: 0.6418873071670532; temperature: 0.12718558311462402; total_precip: 0.9366149306297302; 
0: epoch: 8 [1/5 (20%)]	Loss: 0.81721 : 0.30955 :: 0.20205 (2.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14670901000499725; velocity_v: 0.2917886972427368; specific_humidity: 0.1797281950712204; velocity_z: 0.5564419031143188; temperature: 0.12612780928611755; total_precip: 0.6882736086845398; 
0: epoch: 8 [2/5 (40%)]	Loss: 0.68827 : 0.29749 :: 0.21131 (15.87 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16285787522792816; velocity_v: 0.3234212398529053; specific_humidity: 0.24935656785964966; velocity_z: 0.5043937563896179; temperature: 0.15231388807296753; total_precip: 0.8992886543273926; 
0: epoch: 8 [3/5 (60%)]	Loss: 0.89929 : 0.34681 :: 0.21495 (15.93 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16621515154838562; velocity_v: 0.3739696741104126; specific_humidity: 0.2517701983451843; velocity_z: 0.7082503437995911; temperature: 0.1634843945503235; total_precip: 1.0885834693908691; 
0: epoch: 8 [4/5 (80%)]	Loss: 1.08858 : 0.42359 :: 0.21448 (16.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 8 : 0.4049203395843506
0: validation loss for velocity_u : 0.11922910809516907
0: validation loss for velocity_v : 0.3330667316913605
0: validation loss for specific_humidity : 0.20412449538707733
0: validation loss for velocity_z : 0.6720729470252991
0: validation loss for temperature : 0.11163982003927231
0: validation loss for total_precip : 0.989389181137085
0: 9 : 14:53:03 :: batch_size = 96, lr = 1.6825304701668386e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 9, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8876, -0.8463, -0.8038, -0.7600, -0.7155, -0.6710, -0.6290, -0.5911, -0.5566, -0.5247, -0.4953, -0.4685,
0:         -0.4436, -0.4196, -0.3954, -0.3707, -0.3442, -0.3148, -0.7608, -0.7230, -0.6839, -0.6436, -0.6024, -0.5605,
0:         -0.5191], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0928, -0.1270, -0.1609, -0.1947, -0.2275, -0.2579, -0.2869, -0.3155, -0.3441, -0.3694, -0.3872, -0.3953,
0:         -0.3945, -0.3876, -0.3779, -0.3667, -0.3539, -0.3391, -0.1614, -0.1856, -0.2096, -0.2326, -0.2527, -0.2687,
0:         -0.2811], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6428, -0.6408, -0.6377, -0.6342, -0.6323, -0.6306, -0.6280, -0.6268, -0.6245, -0.6220, -0.6206, -0.6180,
0:         -0.6173, -0.6182, -0.6185, -0.6221, -0.6232, -0.6282, -0.6401, -0.6388, -0.6375, -0.6369, -0.6359, -0.6348,
0:         -0.6334], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.3800, -1.3644, -1.3443, -1.3152, -1.2817, -1.2258, -1.1185, -0.9597, -0.7898, -0.6423, -0.5171, -0.3986,
0:         -0.2823, -0.1907, -0.1214, -0.0565,  0.0239,  0.1178, -1.5589, -1.4873, -1.4113, -1.3286, -1.2437, -1.1542,
0:         -1.0402], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.6622, -1.6486, -1.6339, -1.6188, -1.6033, -1.5875, -1.5725, -1.5582, -1.5455, -1.5341, -1.5239, -1.5156,
0:         -1.5088, -1.5041, -1.5025, -1.5045, -1.5099, -1.5164, -1.5223, -1.5264, -1.5288, -1.5282, -1.5243, -1.5165,
0:         -1.5055], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1268, -0.1339, -0.1527, -0.1621, -0.1715, -0.1785, -0.1926, -0.2090, -0.2161, -0.2208, -0.2255, -0.2278,
0:         -0.2302, -0.2372, -0.2419, -0.2466, -0.2466, -0.2490, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513,
0:         -0.2513], device='cuda:0')
0: [DEBUG] Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.3804,    nan,    nan, 0.3945, 0.4227,
0:            nan,    nan,    nan,    nan,    nan, 0.4345,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.2583,
0:            nan,    nan, 0.2513,    nan,    nan, 0.2982,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan, 0.2888,    nan, 0.2724,    nan,    nan,    nan,    nan,    nan, 0.3194,
0:            nan,    nan,    nan,    nan,    nan,    nan, 0.3147,    nan, 0.3757,    nan,    nan, 0.2137,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.2865,    nan,    nan,    nan,    nan,    nan,
0:         0.3969,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 0.2912,    nan,    nan,    nan,    nan, 0.3593,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.3476,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.2348,
0:            nan,    nan,    nan,    nan,    nan,    nan, 0.4814,    nan, 0.2466,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan, 0.2442,    nan,    nan, 0.4133,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan, 0.3757,    nan,    nan,    nan,    nan, 0.3663,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.4298,    nan,
0:         0.4579,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan, 0.5542])
0: [DEBUG] Epoch 9, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2305, 0.3601, 0.4648, 0.4909, 0.4695, 0.4418, 0.4689, 0.5467, 0.6129, 0.6338, 0.5702, 0.4447, 0.3118, 0.2422,
0:         0.2586, 0.3296, 0.3880, 0.4230, 0.2124, 0.3074, 0.3795, 0.3730, 0.2963, 0.2490, 0.2600], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9844, 0.9524, 0.8850, 0.8046, 0.7281, 0.6840, 0.6442, 0.6173, 0.5903, 0.5662, 0.5448, 0.5302, 0.4926, 0.4119,
0:         0.3087, 0.2429, 0.2411, 0.2860, 0.9068, 0.8811, 0.8125, 0.7149, 0.6234, 0.5513, 0.5071], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6429, -0.6435, -0.6424, -0.6403, -0.6385, -0.6374, -0.6470, -0.6548, -0.6662, -0.6802, -0.6893, -0.6917,
0:         -0.6855, -0.6748, -0.6673, -0.6595, -0.6612, -0.6566, -0.6441, -0.6403, -0.6405, -0.6383, -0.6385, -0.6418,
0:         -0.6467], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0245, -0.0280,  0.0307, -0.0414, -0.1278, -0.0883, -0.0750, -0.1132, -0.1087, -0.1014, -0.1141, -0.0604,
0:         -0.0077, -0.0063, -0.0316, -0.0091,  0.0928,  0.0676, -0.0064, -0.1595, -0.1948, -0.2531, -0.2240, -0.1420,
0:         -0.1394], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4871, -0.5049, -0.5017, -0.4893, -0.4741, -0.4675, -0.4692, -0.4822, -0.4974, -0.5144, -0.5370, -0.5568,
0:         -0.5736, -0.5885, -0.5986, -0.6096, -0.6223, -0.6313, -0.6329, -0.6303, -0.6290, -0.6362, -0.6535, -0.6776,
0:         -0.7058], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.2486, 0.2351, 0.2638, 0.2327, 0.2306, 0.2280, 0.2186, 0.2501, 0.2629, 0.3192, 0.3101, 0.2886, 0.2776, 0.2515,
0:         0.2357, 0.2389, 0.2606, 0.2507, 0.3459, 0.3323, 0.3252, 0.2876, 0.2633, 0.2658, 0.2575], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.16312789916992188; velocity_v: 0.31575313210487366; specific_humidity: 0.21301119029521942; velocity_z: 0.5629338026046753; temperature: 0.1393508017063141; total_precip: 0.9496493935585022; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.163609579205513; velocity_v: 0.29334756731987; specific_humidity: 0.19032679498195648; velocity_z: 0.6019396781921387; temperature: 0.1400158554315567; total_precip: 0.6568676829338074; 
0: epoch: 9 [1/5 (20%)]	Loss: 0.80326 : 0.33142 :: 0.21226 (2.76 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16884855926036835; velocity_v: 0.35341978073120117; specific_humidity: 0.19955599308013916; velocity_z: 0.6200180053710938; temperature: 0.149017795920372; total_precip: 0.9586388468742371; 
0: epoch: 9 [2/5 (40%)]	Loss: 0.95864 : 0.37359 :: 0.21704 (15.77 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16787028312683105; velocity_v: 0.30380189418792725; specific_humidity: 0.2165827751159668; velocity_z: 0.4959736764431; temperature: 0.1363259255886078; total_precip: 0.5113945007324219; 
0: epoch: 9 [3/5 (60%)]	Loss: 0.51139 : 0.27133 :: 0.21545 (15.95 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15219739079475403; velocity_v: 0.30771782994270325; specific_humidity: 0.28279218077659607; velocity_z: 0.6006016135215759; temperature: 0.15099431574344635; total_precip: 0.7763117551803589; 
0: epoch: 9 [4/5 (80%)]	Loss: 0.77631 : 0.34282 :: 0.20871 (15.97 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 9 : 0.4022730588912964
0: validation loss for velocity_u : 0.12172213196754456
0: validation loss for velocity_v : 0.2306455820798874
0: validation loss for specific_humidity : 0.25281092524528503
0: validation loss for velocity_z : 0.5581830739974976
0: validation loss for temperature : 0.12006067484617233
0: validation loss for total_precip : 1.1302160024642944
0: 10 : 14:56:58 :: batch_size = 96, lr = 1.6414931416261842e-05
