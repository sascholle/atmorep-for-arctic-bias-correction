0: Wandb run: atmorep-mempca6n-17611889
0: l50154:1550681:1550681 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.198<0>
0: l50154:1550681:1550681 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50154:1550681:1550681 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50154:1550681:1550681 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50154:1550681:1550681 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
1: l50160:180417:180417 [0] NCCL INFO cudaDriverVersion 12050
1: l50160:180417:180417 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.202<0>
1: l50160:180417:180417 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
1: l50160:180417:180417 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
1: l50160:180417:180417 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50154:1550681:1550812 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.198<0>
0: l50154:1550681:1550812 [0] NCCL INFO Using non-device net plugin version 0
0: l50154:1550681:1550812 [0] NCCL INFO Using network IB
1: l50160:180417:180450 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.202<0>
1: l50160:180417:180450 [0] NCCL INFO Using non-device net plugin version 0
1: l50160:180417:180450 [0] NCCL INFO Using network IB
0: l50154:1550681:1550812 [0] NCCL INFO ncclCommInitRank comm 0x55555f266c90 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x92af7a84c32dce47 - Init START
1: l50160:180417:180450 [0] NCCL INFO ncclCommInitRank comm 0x55555ee1f2c0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x92af7a84c32dce47 - Init START
0: l50154:1550681:1550812 [0] NCCL INFO comm 0x55555f266c90 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50154:1550681:1550812 [0] NCCL INFO Channel 00/04 :    0   1
0: l50154:1550681:1550812 [0] NCCL INFO Channel 01/04 :    0   1
1: l50160:180417:180450 [0] NCCL INFO comm 0x55555ee1f2c0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50154:1550681:1550812 [0] NCCL INFO Channel 02/04 :    0   1
0: l50154:1550681:1550812 [0] NCCL INFO Channel 03/04 :    0   1
0: l50154:1550681:1550812 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
0: l50154:1550681:1550812 [0] NCCL INFO P2P Chunksize set to 131072
1: l50160:180417:180450 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
1: l50160:180417:180450 [0] NCCL INFO P2P Chunksize set to 131072
0: l50154:1550681:1550812 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/IB/0
1: l50160:180417:180450 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/0
0: l50154:1550681:1550812 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IB/1
1: l50160:180417:180450 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/1
0: l50154:1550681:1550812 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/0
1: l50160:180417:180450 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/0
0: l50154:1550681:1550812 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/1
1: l50160:180417:180450 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/1
0: l50154:1550681:1550812 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/0
1: l50160:180417:180450 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/IB/0
0: l50154:1550681:1550812 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/1
1: l50160:180417:180450 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/IB/1
0: l50154:1550681:1550812 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/0
1: l50160:180417:180450 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/0
0: l50154:1550681:1550812 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/1
1: l50160:180417:180450 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/1
1: l50160:180417:180453 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 250.
1: l50160:180417:180453 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 50.
0: l50154:1550681:1550815 [0] NCCL INFO NCCL_IB_TIMEOUT set by environment to 250.
0: l50154:1550681:1550815 [0] NCCL INFO NCCL_IB_RETRY_CNT set by environment to 50.
0: l50154:1550681:1550812 [0] NCCL INFO Connected all rings
0: l50154:1550681:1550812 [0] NCCL INFO Connected all trees
1: l50160:180417:180450 [0] NCCL INFO Connected all rings
1: l50160:180417:180450 [0] NCCL INFO Connected all trees
1: l50160:180417:180450 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
1: l50160:180417:180450 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50154:1550681:1550812 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
0: l50154:1550681:1550812 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50154:1550681:1550812 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50154:1550681:1550812 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50154:1550681:1550812 [0] NCCL INFO ncclCommInitRank comm 0x55555f266c90 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x92af7a84c32dce47 - Init COMPLETE
1: l50160:180417:180450 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
1: l50160:180417:180450 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
1: l50160:180417:180450 [0] NCCL INFO ncclCommInitRank comm 0x55555ee1f2c0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 3000 commId 0x92af7a84c32dce47 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 4
0: par_rank : 0
0: par_size : 2
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 1, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 2, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 3, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 3, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17611889
0: wandb_id : mempca6n
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: mask_input_field : total_precip
0: mask_input_value : 0
0: years_test : [2021]
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
1: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
1: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
1: self.lats : (721,)
1: self.lons : (1440,)
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 1
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 1
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 2
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 2
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
1: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
1: Loaded model id = wc5e2i3t.
1: Loaded run 'wc5e2i3t' at epoch -2.
0: Loaded model id = wc5e2i3t.
0: Loaded run 'wc5e2i3t' at epoch -2.
1: l50160:180417:180481 [1] NCCL INFO Using non-device net plugin version 0
1: l50160:180417:180481 [1] NCCL INFO Using network IB
0: l50154:1550681:1550833 [1] NCCL INFO Using non-device net plugin version 0
0: l50154:1550681:1550833 [1] NCCL INFO Using network IB
0: l50154:1550681:1550833 [1] NCCL INFO ncclCommInitRank comm 0x555580dfae80 rank 0 nranks 2 cudaDev 1 nvmlDev 1 busId 44000 commId 0x1ab7e4f40d1e1bf0 - Init START
1: l50160:180417:180481 [1] NCCL INFO ncclCommInitRank comm 0x5555a6b15db0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 44000 commId 0x1ab7e4f40d1e1bf0 - Init START
0: l50154:1550681:1550833 [1] NCCL INFO comm 0x555580dfae80 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
1: l50160:180417:180481 [1] NCCL INFO comm 0x5555a6b15db0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50154:1550681:1550833 [1] NCCL INFO Channel 00/02 :    0   1
0: l50154:1550681:1550833 [1] NCCL INFO Channel 01/02 :    0   1
1: l50160:180417:180481 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
1: l50160:180417:180481 [1] NCCL INFO P2P Chunksize set to 131072
0: l50154:1550681:1550833 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
0: l50154:1550681:1550833 [1] NCCL INFO P2P Chunksize set to 131072
1: l50160:180417:180481 [1] NCCL INFO Channel 00/0 : 0[1] -> 1[1] [receive] via NET/IB/0/GDRDMA
1: l50160:180417:180481 [1] NCCL INFO Channel 01/0 : 0[1] -> 1[1] [receive] via NET/IB/0/GDRDMA
1: l50160:180417:180481 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[1] [send] via NET/IB/0/GDRDMA
1: l50160:180417:180481 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[1] [send] via NET/IB/0/GDRDMA
0: l50154:1550681:1550833 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[1] [receive] via NET/IB/0/GDRDMA
0: l50154:1550681:1550833 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[1] [receive] via NET/IB/0/GDRDMA
0: l50154:1550681:1550833 [1] NCCL INFO Channel 00/0 : 0[1] -> 1[1] [send] via NET/IB/0/GDRDMA
0: l50154:1550681:1550833 [1] NCCL INFO Channel 01/0 : 0[1] -> 1[1] [send] via NET/IB/0/GDRDMA
0: l50154:1550681:1550833 [1] NCCL INFO Connected all rings
0: l50154:1550681:1550833 [1] NCCL INFO Connected all trees
1: l50160:180417:180481 [1] NCCL INFO Connected all rings
1: l50160:180417:180481 [1] NCCL INFO Connected all trees
1: l50160:180417:180481 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
1: l50160:180417:180481 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
0: l50154:1550681:1550833 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
0: l50154:1550681:1550833 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
0: l50154:1550681:1550833 [1] NCCL INFO ncclCommInitRank comm 0x555580dfae80 rank 0 nranks 2 cudaDev 1 nvmlDev 1 busId 44000 commId 0x1ab7e4f40d1e1bf0 - Init COMPLETE
1: l50160:180417:180481 [1] NCCL INFO ncclCommInitRank comm 0x5555a6b15db0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 44000 commId 0x1ab7e4f40d1e1bf0 - Init COMPLETE
1: l50160:180417:180485 [2] NCCL INFO Using non-device net plugin version 0
1: l50160:180417:180485 [2] NCCL INFO Using network IB
0: l50154:1550681:1550838 [2] NCCL INFO Using non-device net plugin version 0
0: l50154:1550681:1550838 [2] NCCL INFO Using network IB
0: l50154:1550681:1550838 [2] NCCL INFO ncclCommInitRank comm 0x555580f61140 rank 0 nranks 2 cudaDev 2 nvmlDev 2 busId 84000 commId 0x75d7c0ddb26169a1 - Init START
1: l50160:180417:180485 [2] NCCL INFO ncclCommInitRank comm 0x55557e72dee0 rank 1 nranks 2 cudaDev 2 nvmlDev 2 busId 84000 commId 0x75d7c0ddb26169a1 - Init START
0: l50154:1550681:1550838 [2] NCCL INFO comm 0x555580f61140 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50154:1550681:1550838 [2] NCCL INFO Channel 00/02 :    0   1
0: l50154:1550681:1550838 [2] NCCL INFO Channel 01/02 :    0   1
1: l50160:180417:180485 [2] NCCL INFO comm 0x55557e72dee0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50154:1550681:1550838 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
0: l50154:1550681:1550838 [2] NCCL INFO P2P Chunksize set to 131072
1: l50160:180417:180485 [2] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
1: l50160:180417:180485 [2] NCCL INFO P2P Chunksize set to 131072
0: l50154:1550681:1550838 [2] NCCL INFO Channel 00/0 : 1[2] -> 0[2] [receive] via NET/IB/1/GDRDMA
0: l50154:1550681:1550838 [2] NCCL INFO Channel 01/0 : 1[2] -> 0[2] [receive] via NET/IB/1/GDRDMA
0: l50154:1550681:1550838 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[2] [send] via NET/IB/1/GDRDMA
0: l50154:1550681:1550838 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[2] [send] via NET/IB/1/GDRDMA
1: l50160:180417:180485 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[2] [receive] via NET/IB/1/GDRDMA
1: l50160:180417:180485 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[2] [receive] via NET/IB/1/GDRDMA
1: l50160:180417:180485 [2] NCCL INFO Channel 00/0 : 1[2] -> 0[2] [send] via NET/IB/1/GDRDMA
1: l50160:180417:180485 [2] NCCL INFO Channel 01/0 : 1[2] -> 0[2] [send] via NET/IB/1/GDRDMA
1: l50160:180417:180485 [2] NCCL INFO Connected all rings
1: l50160:180417:180485 [2] NCCL INFO Connected all trees
1: l50160:180417:180485 [2] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
1: l50160:180417:180485 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
0: l50154:1550681:1550838 [2] NCCL INFO Connected all rings
0: l50154:1550681:1550838 [2] NCCL INFO Connected all trees
0: l50154:1550681:1550838 [2] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
0: l50154:1550681:1550838 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
1: l50160:180417:180485 [2] NCCL INFO ncclCommInitRank comm 0x55557e72dee0 rank 1 nranks 2 cudaDev 2 nvmlDev 2 busId 84000 commId 0x75d7c0ddb26169a1 - Init COMPLETE
0: l50154:1550681:1550838 [2] NCCL INFO ncclCommInitRank comm 0x555580f61140 rank 0 nranks 2 cudaDev 2 nvmlDev 2 busId 84000 commId 0x75d7c0ddb26169a1 - Init COMPLETE
1: l50160:180417:180489 [3] NCCL INFO Using non-device net plugin version 0
1: l50160:180417:180489 [3] NCCL INFO Using network IB
0: l50154:1550681:1550843 [3] NCCL INFO Using non-device net plugin version 0
0: l50154:1550681:1550843 [3] NCCL INFO Using network IB
0: l50154:1550681:1550843 [3] NCCL INFO ncclCommInitRank comm 0x55558c10e500 rank 0 nranks 2 cudaDev 3 nvmlDev 3 busId c4000 commId 0x7113df7b36162407 - Init START
1: l50160:180417:180489 [3] NCCL INFO ncclCommInitRank comm 0x55558730e420 rank 1 nranks 2 cudaDev 3 nvmlDev 3 busId c4000 commId 0x7113df7b36162407 - Init START
0: l50154:1550681:1550843 [3] NCCL INFO comm 0x55558c10e500 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
1: l50160:180417:180489 [3] NCCL INFO comm 0x55558730e420 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
0: l50154:1550681:1550843 [3] NCCL INFO Channel 00/04 :    0   1
0: l50154:1550681:1550843 [3] NCCL INFO Channel 01/04 :    0   1
0: l50154:1550681:1550843 [3] NCCL INFO Channel 02/04 :    0   1
1: l50160:180417:180489 [3] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
1: l50160:180417:180489 [3] NCCL INFO P2P Chunksize set to 131072
0: l50154:1550681:1550843 [3] NCCL INFO Channel 03/04 :    0   1
0: l50154:1550681:1550843 [3] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
0: l50154:1550681:1550843 [3] NCCL INFO P2P Chunksize set to 131072
1: l50160:180417:180489 [3] NCCL INFO Channel 00/0 : 0[3] -> 1[3] [receive] via NET/IB/1
1: l50160:180417:180489 [3] NCCL INFO Channel 01/0 : 0[3] -> 1[3] [receive] via NET/IB/0
1: l50160:180417:180489 [3] NCCL INFO Channel 02/0 : 0[3] -> 1[3] [receive] via NET/IB/1
1: l50160:180417:180489 [3] NCCL INFO Channel 03/0 : 0[3] -> 1[3] [receive] via NET/IB/0
1: l50160:180417:180489 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[3] [send] via NET/IB/1
1: l50160:180417:180489 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[3] [send] via NET/IB/0
1: l50160:180417:180489 [3] NCCL INFO Channel 02/0 : 1[3] -> 0[3] [send] via NET/IB/1
1: l50160:180417:180489 [3] NCCL INFO Channel 03/0 : 1[3] -> 0[3] [send] via NET/IB/0
0: l50154:1550681:1550843 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[3] [receive] via NET/IB/1
0: l50154:1550681:1550843 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[3] [receive] via NET/IB/0
0: l50154:1550681:1550843 [3] NCCL INFO Channel 02/0 : 1[3] -> 0[3] [receive] via NET/IB/1
0: l50154:1550681:1550843 [3] NCCL INFO Channel 03/0 : 1[3] -> 0[3] [receive] via NET/IB/0
0: l50154:1550681:1550843 [3] NCCL INFO Channel 00/0 : 0[3] -> 1[3] [send] via NET/IB/1
0: l50154:1550681:1550843 [3] NCCL INFO Channel 01/0 : 0[3] -> 1[3] [send] via NET/IB/0
0: l50154:1550681:1550843 [3] NCCL INFO Channel 02/0 : 0[3] -> 1[3] [send] via NET/IB/1
0: l50154:1550681:1550843 [3] NCCL INFO Channel 03/0 : 0[3] -> 1[3] [send] via NET/IB/0
0: l50154:1550681:1550843 [3] NCCL INFO Connected all rings
1: l50160:180417:180489 [3] NCCL INFO Connected all rings
0: l50154:1550681:1550843 [3] NCCL INFO Connected all trees
1: l50160:180417:180489 [3] NCCL INFO Connected all trees
1: l50160:180417:180489 [3] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
1: l50160:180417:180489 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
0: l50154:1550681:1550843 [3] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
0: l50154:1550681:1550843 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
1: l50160:180417:180489 [3] NCCL INFO ncclCommInitRank comm 0x55558730e420 rank 1 nranks 2 cudaDev 3 nvmlDev 3 busId c4000 commId 0x7113df7b36162407 - Init COMPLETE
0: l50154:1550681:1550843 [3] NCCL INFO ncclCommInitRank comm 0x55558c10e500 rank 0 nranks 2 cudaDev 3 nvmlDev 3 busId c4000 commId 0x7113df7b36162407 - Init COMPLETE
1: -1 : 09:52:41 :: batch_size = 96, lr = 1e-05
0: Number of trainable parameters: 741,136,272
0: -1 : 09:52:41 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch -1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9954, -0.9902, -1.0108, -1.0751, -1.1818, -1.2972, -1.3723, -1.3991, -1.3788, -1.3375, -1.3092, -1.3203,
0:         -1.3591, -1.3923, -1.4049, -1.3970, -1.3688, -1.3272, -1.0472, -1.0538, -1.0942, -1.2041, -1.3418, -1.4449,
0:         -1.4675], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4760, -0.4577, -0.4402, -0.4515, -0.4790, -0.4499, -0.3331, -0.1742, -0.0177,  0.1120,  0.2179,  0.3154,
0:          0.4085,  0.4924,  0.5462,  0.5512,  0.5098,  0.4278, -0.4702, -0.4484, -0.4638, -0.5031, -0.5173, -0.4412,
0:         -0.2726], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.6156, 2.6368, 2.7164, 2.7870, 2.9271, 3.0868, 3.3770, 3.6493, 3.7044, 3.5329, 3.3970, 3.1925, 2.9681, 2.8955,
0:         2.7838, 2.7048, 2.7167, 2.9018, 2.6108, 2.7079, 2.7937, 2.9642, 3.1041, 3.3650, 3.6061], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.8562,  1.0673,  1.6621,  1.4606,  1.2206,  0.8594,  0.5679,  0.1831,  0.0213, -0.0226, -0.0151,  0.1810,
0:         -0.2981, -0.0773, -0.8457, -1.0097,  0.3118, -0.0183,  0.7340,  1.0684,  1.6975,  1.3417,  0.8701,  0.4339,
0:          0.1564], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0508,  0.2516,  0.3624,  0.3329,  0.2751,  0.2399,  0.1756, -0.0238, -0.1187, -0.0060,  0.1225,  0.2002,
0:          0.2549,  0.3663,  0.5799,  0.7839,  0.7684,  0.5729,  0.4615,  0.5246,  0.4393,  0.0870, -0.1383, -0.1872,
0:         -0.0418], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
1: [DEBUG] INPUT BATCH
1: Epoch -1, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0: [DEBUG] TARGET BATCH
0: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2357,     nan,     nan,     nan,  0.2550,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1269,     nan,     nan,     nan,     nan,     nan,     nan, -0.2426,     nan,     nan,     nan,     nan,
0:         -0.1281,     nan, -0.0390,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2403,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2241,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0525,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2264,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2345, -0.2311,     nan,     nan,     nan, -0.2415,     nan, -0.2369,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2207,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2450,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1026,     nan,
0:             nan,     nan,     nan,     nan, -0.2276,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1802, -0.1570,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2461,     nan, -0.2102,     nan,     nan, -0.2438,
0:             nan,     nan,     nan,     nan,     nan, -0.2276,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
0:             nan,     nan, -0.2068])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch -1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2676, -1.2559, -1.2448, -1.2386, -1.2362, -1.2428, -1.2509, -1.2609, -1.2681, -1.2685, -1.2637, -1.2607,
0:         -1.2582, -1.2454, -1.2341, -1.2081, -1.1734, -1.1323, -1.2676, -1.2620, -1.2584, -1.2573, -1.2584, -1.2619,
0:         -1.2630], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0836,  0.1070,  0.1402,  0.1773,  0.2109,  0.2455,  0.2684,  0.2771,  0.2677,  0.2385,  0.1850,  0.1118,
0:          0.0290, -0.0464, -0.0959, -0.1088, -0.0787, -0.0268,  0.0892,  0.1081,  0.1400,  0.1797,  0.2241,  0.2621,
0:          0.2865], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.3252, 2.3398, 2.3615, 2.3863, 2.4091, 2.4372, 2.4623, 2.5090, 2.5516, 2.6324, 2.7299, 2.8378, 2.9403, 3.0393,
0:         3.1200, 3.1738, 3.2118, 3.2442, 2.3542, 2.3671, 2.3941, 2.4131, 2.4455, 2.4715, 2.5079], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.6010,  0.6496,  0.6755,  0.5929,  0.5090,  0.4740,  0.3784,  0.3554,  0.2122, -0.0339, -0.0360, -0.0121,
0:         -0.0077, -0.1094, -0.4157, -0.6367, -1.3985, -2.5459,  0.5883,  0.6326,  0.6615,  0.5651,  0.4561,  0.4166,
0:          0.2795], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.4090, 1.5612, 1.6735, 1.7128, 1.6939, 1.6351, 1.5632, 1.4979, 1.4388, 1.3957, 1.3918, 1.4755, 1.7127, 2.1147,
0:         2.6521, 3.2126, 3.6544, 3.8831, 3.8806, 3.7249, 3.5323, 3.4074, 3.3854, 3.4379, 3.4923], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1792, -0.1823, -0.1866, -0.1850, -0.1846, -0.1853, -0.1841, -0.1798, -0.1766, -0.1830, -0.1797, -0.1829,
0:         -0.1851, -0.1833, -0.1791, -0.1823, -0.1775, -0.1733, -0.1774, -0.1813, -0.1762, -0.1777, -0.1773, -0.1732,
0:         -0.1748], device='cuda:0', grad_fn=<SliceBackward0>)
1:      first 25 values: tensor([-0.3580, -0.3259, -0.3048, -0.2999, -0.2970, -0.2893, -0.2827, -0.2726, -0.2566, -0.2434, -0.2338, -0.2202,
1:         -0.2043, -0.1890, -0.1704, -0.1467, -0.1209, -0.1004, -0.4082, -0.3823, -0.3684, -0.3677, -0.3601, -0.3397,
1:         -0.3210], device='cuda:0')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.5048, -0.5693, -0.6171, -0.6516, -0.6787, -0.6984, -0.7113, -0.7218, -0.7339, -0.7483, -0.7622, -0.7732,
1:         -0.7826, -0.7929, -0.8082, -0.8333, -0.8746, -0.9256, -0.5511, -0.6027, -0.6381, -0.6673, -0.6903, -0.6998,
1:         -0.7005], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.9507, 0.9634, 1.0363, 1.0715, 1.1446, 1.1290, 1.0729, 1.0125, 0.9532, 0.8918, 0.8469, 0.8008, 0.7698, 0.6727,
1:         0.5837, 0.5323, 0.5763, 0.5735, 0.9280, 0.9730, 1.0396, 1.0749, 1.0025, 0.9385, 0.8733], device='cuda:2')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.3547, -0.3169, -0.2570, -0.3080, -0.3458, -0.3547, -0.4768, -0.5301, -0.4257, -0.4590, -0.5967, -0.5656,
1:         -0.4923, -0.5101, -0.4302, -0.2592, -0.1326,  0.0739, -0.2459, -0.1815, -0.1659, -0.1970, -0.1881, -0.2192,
1:         -0.3347], device='cuda:3')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([-0.6761, -0.6182, -0.5725, -0.5159, -0.4423, -0.3998, -0.3737, -0.3212, -0.2813, -0.2857, -0.2833, -0.2444,
1:         -0.1856, -0.0973,  0.0262,  0.1465,  0.2415,  0.3044,  0.3293,  0.3367,  0.3486,  0.3546,  0.3514,  0.3494,
1:          0.3463], device='cuda:3')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
1:        device='cuda:0')
1: [DEBUG] TARGET BATCH
1: Epoch -1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan, -0.2604,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2604,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,
1:             nan,     nan, -0.2604,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:         -0.2604,     nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2604,     nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan, -0.2604,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2604,     nan,     nan,     nan,     nan, -0.2604,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch -1, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.4592, 0.4537, 0.4455, 0.4298, 0.4162, 0.4016, 0.3823, 0.3631, 0.3415, 0.3262, 0.3171, 0.3129, 0.3116, 0.3136,
1:         0.3112, 0.3111, 0.3102, 0.3091, 0.3704, 0.3678, 0.3557, 0.3367, 0.3196, 0.3020, 0.2863], device='cuda:0',
1:        grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1338, -0.1739, -0.2203, -0.2673, -0.3058, -0.3296, -0.3392, -0.3319, -0.3129, -0.2873, -0.2579, -0.2246,
1:         -0.1933, -0.1648, -0.1450, -0.1337, -0.1351, -0.1468, -0.1321, -0.1664, -0.2065, -0.2390, -0.2622, -0.2731,
1:         -0.2685], device='cuda:1', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-1.4165e-01, -1.2932e-01, -8.2941e-02, -1.2491e-04,  1.1500e-01,  2.7433e-01,  4.5852e-01,  6.5194e-01,
1:          8.2281e-01,  9.3722e-01,  9.8400e-01,  9.6347e-01,  8.7223e-01,  7.4364e-01,  5.9516e-01,  4.6297e-01,
1:          3.5849e-01,  3.0206e-01, -1.1010e-01, -7.0154e-02,  1.4299e-02,  1.4477e-01,  3.0453e-01,  5.0894e-01,
1:          7.2149e-01], device='cuda:2', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1738, -0.1072, -0.0756, -0.1254, -0.0846, -0.0633, -0.1071, -0.0572,  0.0234,  0.0163, -0.0199,  0.0257,
1:          0.0706,  0.0531,  0.0561,  0.1260,  0.2632,  0.3978, -0.0517,  0.0060,  0.0441,  0.0116,  0.1010,  0.1995,
1:          0.1864], device='cuda:3', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([-0.1531, -0.1582, -0.1028, -0.0082,  0.0913,  0.1636,  0.1938,  0.1949,  0.1891,  0.1925,  0.2123,  0.2481,
1:          0.2895,  0.3271,  0.3499,  0.3467,  0.3079,  0.2325,  0.1277,  0.0126, -0.0895, -0.1617, -0.2017, -0.2161,
1:         -0.2204], device='cuda:3', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1625, -0.1686, -0.1748, -0.1767, -0.1792, -0.1818, -0.1837, -0.1805, -0.1781, -0.1695, -0.1671, -0.1739,
1:         -0.1761, -0.1801, -0.1771, -0.1829, -0.1802, -0.1729, -0.1630, -0.1684, -0.1654, -0.1696, -0.1719, -0.1720,
1:         -0.1745], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [1/5 (20%)]	Loss: nan : nan :: 0.14184 (0.66 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [2/5 (40%)]	Loss: nan : nan :: 0.14172 (19.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [3/5 (60%)]	Loss: nan : nan :: 0.14096 (19.37 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: -1 [4/5 (80%)]	Loss: nan : nan :: 0.16297 (18.88 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_embeds_token_info_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_velocity_u_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_velocity_v_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_specific_humidity_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_velocity_z_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_temperature_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_total_precip_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_velocity_u_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_velocity_v_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_specific_humidity_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_velocity_z_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_temperature_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_total_precip_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_velocity_u_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_velocity_v_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_specific_humidity_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_velocity_z_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_temperature_idmempca6n_epoch-1.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_total_precip_idmempca6n_epoch-1.mod
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.207, max = -0.086, mean = -0.155
0:          sample (first 20): tensor([-0.1436, -0.1472, -0.1547, -0.1570, -0.1593, -0.1628, -0.1663, -0.1597, -0.1597, -0.1520, -0.1481, -0.1544,
0:         -0.1594, -0.1640, -0.1618, -0.1658, -0.1644, -0.1568, -0.1521, -0.1531])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch -1 : nan
0: validation loss for velocity_u : 0.03388214856386185
0: validation loss for velocity_v : 0.06392866373062134
0: validation loss for specific_humidity : 0.02560870349407196
0: validation loss for velocity_z : 0.4929990768432617
0: validation loss for temperature : 0.08162995427846909
0: validation loss for total_precip : nan
0: 0 : 10:16:08 :: batch_size = 96, lr = 1e-05
1: 0 : 10:16:08 :: batch_size = 96, lr = 1e-05
1: Created sparse mask for total_precip with 10.0% data retained
1: [DEBUG] INPUT BATCH
1: Epoch 0, first input batch shapes / sample data:
1:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([0.7814, 0.7812, 0.7757, 0.7659, 0.7543, 0.7443, 0.7391, 0.7399, 0.7467, 0.7580, 0.7708, 0.7816, 0.7861, 0.7817,
1:         0.7679, 0.7463, 0.7200, 0.6924, 0.7475, 0.7414, 0.7317, 0.7205, 0.7104, 0.7049, 0.7057], device='cuda:0')
1:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([1.5615, 1.6045, 1.6479, 1.6923, 1.7391, 1.7891, 1.8434, 1.9012, 1.9623, 2.0252, 2.0875, 2.1466, 2.1990, 2.2416,
1:         2.2703, 2.2824, 2.2764, 2.2538, 1.5583, 1.5958, 1.6342, 1.6754, 1.7216, 1.7734, 1.8303], device='cuda:1')
1:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([-0.4032, -0.3653, -0.3165, -0.2698, -0.2277, -0.1966, -0.1823, -0.1689, -0.1747, -0.1814, -0.2074, -0.2335,
1:         -0.2719, -0.3134, -0.3484, -0.3830, -0.4067, -0.4208, -0.3573, -0.3227, -0.2832, -0.2477, -0.2201, -0.1970,
1:         -0.1923], device='cuda:2')
1:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
1:      first 25 values: tensor([ 0.3237,  0.2365,  0.1806,  0.1720,  0.2301,  0.3625,  0.5508,  0.7553,  0.9231,  1.0125,  0.9963,  0.8661,
1:          0.6390,  0.3571,  0.0794, -0.1401, -0.2757, -0.3209,  0.2839,  0.2021,  0.1795,  0.2365,  0.3732,  0.5702,
1:          0.7854], device='cuda:3')
1:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
1:      first 25 values: tensor([0.4228, 0.4057, 0.3842, 0.3612, 0.3398, 0.3235, 0.3150, 0.3155, 0.3247, 0.3403, 0.3589, 0.3774, 0.3939, 0.4066,
1:         0.4153, 0.4203, 0.4226, 0.4233, 0.4227, 0.4216, 0.4205, 0.4203, 0.4215, 0.4236, 0.4252], device='cuda:3')
1:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
1:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
1:        device='cuda:0')
1: [DEBUG] TARGET BATCH
1: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
1: [DEBUG] First 243 batch values:
1: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2343, -0.2354,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2343,     nan, -0.2389,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2343,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan, -0.2307,     nan,     nan,     nan,     nan, -0.2202,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2155,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan, -0.2424, -0.2448,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan, -0.2389,     nan,     nan,     nan,     nan,     nan,     nan, -0.2389,
1:             nan, -0.2343,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2389,
1:         -0.2389,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2343, -0.2343,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2296,     nan,     nan, -0.2319,     nan,
1:             nan,     nan,     nan, -0.2296,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2343,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan, -0.2343,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
1:             nan,     nan,     nan])
1: [DEBUG] PREDICTIONS TRAIN BATCH
1: Epoch 0, first predictions sample:
1:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([0.4191, 0.4248, 0.4323, 0.4384, 0.4477, 0.4577, 0.4640, 0.4678, 0.4716, 0.4751, 0.4796, 0.4793, 0.4755, 0.4705,
1:         0.4582, 0.4458, 0.4359, 0.4301, 0.4037, 0.4081, 0.4132, 0.4182, 0.4249, 0.4302, 0.4366], device='cuda:0',
1:        grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([2.1075, 2.1026, 2.0954, 2.0874, 2.0844, 2.0892, 2.0947, 2.0991, 2.1108, 2.1257, 2.1373, 2.1493, 2.1510, 2.1496,
1:         2.1417, 2.1322, 2.1167, 2.0970, 2.0671, 2.0538, 2.0383, 2.0381, 2.0482, 2.0645, 2.0824], device='cuda:1',
1:        grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.1986, -0.1811, -0.1453, -0.0921, -0.0302,  0.0441,  0.1214,  0.2009,  0.2703,  0.3252,  0.3556,  0.3535,
1:          0.3138,  0.2400,  0.1330,  0.0242, -0.0727, -0.1336, -0.1706, -0.1393, -0.0798, -0.0087,  0.0739,  0.1551,
1:          0.2318], device='cuda:2', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
1:      first 25 pred values: tensor([-0.3459, -0.2672, -0.1580, -0.0609,  0.0010,  0.0512,  0.1007,  0.1319,  0.1813,  0.2167,  0.2022,  0.2036,
1:          0.1847,  0.1060,  0.0458,  0.0389,  0.0629,  0.0791, -0.3386, -0.2520, -0.1200, -0.0056,  0.0620,  0.1150,
1:          0.1669], device='cuda:3', grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
1:      first 25 pred values: tensor([0.8480, 0.8585, 0.8749, 0.8904, 0.9072, 0.9188, 0.9263, 0.9260, 0.9188, 0.9072, 0.8958, 0.8885, 0.8903, 0.8989,
1:         0.9129, 0.9273, 0.9392, 0.9467, 0.9521, 0.9563, 0.9577, 0.9557, 0.9511, 0.9447, 0.9383], device='cuda:3',
1:        grad_fn=<SliceBackward0>)
1:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
1:      first 25 pred values: tensor([-0.1657, -0.1705, -0.1773, -0.1761, -0.1768, -0.1763, -0.1779, -0.1727, -0.1700, -0.1734, -0.1687, -0.1738,
1:         -0.1740, -0.1769, -0.1714, -0.1754, -0.1749, -0.1684, -0.1691, -0.1690, -0.1661, -0.1669, -0.1682, -0.1682,
1:         -0.1675], device='cuda:0', grad_fn=<SliceBackward0>)
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] INPUT BATCH
0: Epoch 0, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7814, 0.7812, 0.7757, 0.7659, 0.7543, 0.7443, 0.7391, 0.7399, 0.7467, 0.7580, 0.7708, 0.7816, 0.7861, 0.7817,
0:         0.7679, 0.7463, 0.7200, 0.6924, 0.7475, 0.7414, 0.7317, 0.7205, 0.7104, 0.7049, 0.7057], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.5615, 1.6045, 1.6479, 1.6923, 1.7391, 1.7891, 1.8434, 1.9012, 1.9623, 2.0252, 2.0875, 2.1466, 2.1990, 2.2416,
0:         2.2703, 2.2824, 2.2764, 2.2538, 1.5583, 1.5958, 1.6342, 1.6754, 1.7216, 1.7734, 1.8303], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4032, -0.3653, -0.3165, -0.2698, -0.2277, -0.1966, -0.1823, -0.1689, -0.1747, -0.1814, -0.2074, -0.2335,
0:         -0.2719, -0.3134, -0.3484, -0.3830, -0.4067, -0.4208, -0.3573, -0.3227, -0.2832, -0.2477, -0.2201, -0.1970,
0:         -0.1923], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.3237,  0.2365,  0.1806,  0.1720,  0.2301,  0.3625,  0.5508,  0.7553,  0.9231,  1.0125,  0.9963,  0.8661,
0:          0.6390,  0.3571,  0.0794, -0.1401, -0.2757, -0.3209,  0.2839,  0.2021,  0.1795,  0.2365,  0.3732,  0.5702,
0:          0.7854], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.4228, 0.4057, 0.3842, 0.3612, 0.3398, 0.3235, 0.3150, 0.3155, 0.3247, 0.3403, 0.3589, 0.3774, 0.3939, 0.4066,
0:         0.4153, 0.4203, 0.4226, 0.4233, 0.4227, 0.4216, 0.4205, 0.4203, 0.4215, 0.4236, 0.4252], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
0:        device='cuda:0')
0: [DEBUG] TARGET BATCH
0: Epoch 0, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2272,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2319,     nan, -0.2343, -0.2296,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2249,     nan,
0:         -0.2401,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2389,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2389,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2366, -0.2366, -0.2343,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2319,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2436, -0.2436, -0.2413,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2343,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2343,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2343,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,
0:             nan,     nan,     nan])
0: [DEBUG] PREDICTIONS TRAIN BATCH
0: Epoch 0, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4217, 0.4268, 0.4368, 0.4475, 0.4603, 0.4724, 0.4776, 0.4775, 0.4762, 0.4754, 0.4762, 0.4759, 0.4746, 0.4699,
0:         0.4598, 0.4459, 0.4305, 0.4177, 0.4041, 0.4088, 0.4152, 0.4254, 0.4342, 0.4419, 0.4475], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.0765, 2.0816, 2.0790, 2.0701, 2.0596, 2.0584, 2.0579, 2.0601, 2.0726, 2.0844, 2.0987, 2.1164, 2.1245, 2.1316,
0:         2.1249, 2.1179, 2.1024, 2.0874, 2.0291, 2.0298, 2.0262, 2.0254, 2.0307, 2.0386, 2.0544], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1803, -0.1712, -0.1423, -0.1056, -0.0550,  0.0092,  0.0826,  0.1635,  0.2430,  0.3141,  0.3618,  0.3721,
0:          0.3419,  0.2724,  0.1724,  0.0739, -0.0135, -0.0674, -0.1422, -0.1106, -0.0590,  0.0025,  0.0729,  0.1453,
0:          0.2169], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3676, -0.3358, -0.2549, -0.1613, -0.0603,  0.0493,  0.1391,  0.1843,  0.2196,  0.2269,  0.1917,  0.1783,
0:          0.1656,  0.1328,  0.1129,  0.1263,  0.1558,  0.1252, -0.3364, -0.2831, -0.1585, -0.0269,  0.0833,  0.1837,
0:          0.2454], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.8473, 0.8673, 0.8882, 0.9029, 0.9160, 0.9237, 0.9287, 0.9268, 0.9201, 0.9103, 0.9004, 0.8934, 0.8936, 0.8989,
0:         0.9090, 0.9205, 0.9317, 0.9408, 0.9478, 0.9522, 0.9531, 0.9505, 0.9454, 0.9370, 0.9253], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1598, -0.1590, -0.1630, -0.1635, -0.1631, -0.1578, -0.1617, -0.1536, -0.1550, -0.1613, -0.1552, -0.1607,
0:         -0.1629, -0.1625, -0.1591, -0.1635, -0.1595, -0.1557, -0.1588, -0.1588, -0.1553, -0.1569, -0.1553, -0.1569,
0:         -0.1578], device='cuda:0', grad_fn=<SliceBackward0>)
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [1/5 (20%)]	Loss: nan : nan :: 0.14698 (0.75 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [2/5 (40%)]	Loss: nan : nan :: 0.13877 (19.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [3/5 (60%)]	Loss: nan : nan :: 0.15621 (18.74 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: epoch: 0 [4/5 (80%)]	Loss: nan : nan :: 0.13619 (18.44 s/sec)
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_embeds_token_info_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_velocity_u_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_velocity_v_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_specific_humidity_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_velocity_z_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_temperature_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_encoder_total_precip_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_velocity_u_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_velocity_v_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_specific_humidity_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_velocity_z_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_temperature_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_decoder_total_precip_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_velocity_u_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_velocity_v_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_specific_humidity_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_velocity_z_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_temperature_idmempca6n_epoch0.mod
0: Model file: /work/ab1412/atmorep/atmorep/config/../../models/idmempca6n/AtmoRep_tail_total_precip_idmempca6n_epoch0.mod
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] PREDICTIONS VALIDATION BATCH
0: Normalised validation prediction values for 'total_precip' with shape: torch.Size([144, 243])
0:          min = -0.210, max = -0.108, mean = -0.159
0:          sample (first 20): tensor([-0.1674, -0.1704, -0.1766, -0.1762, -0.1766, -0.1778, -0.1799, -0.1740, -0.1736, -0.1724, -0.1692, -0.1750,
0:         -0.1764, -0.1787, -0.1750, -0.1797, -0.1773, -0.1712, -0.1695, -0.1706])
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
1: Created sparse mask for total_precip with 10.0% data retained
