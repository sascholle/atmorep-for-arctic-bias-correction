0: Wandb run: atmorep-lbhs8bxh-17613072
0: l50136:688480:688480 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.186<0>
0: l50136:688480:688480 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50136:688480:688480 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50136:688480:688480 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50136:688480:688480 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
1: l50136:688481:688481 [0] NCCL INFO cudaDriverVersion 12050
2: l50136:688482:688482 [0] NCCL INFO cudaDriverVersion 12050
3: l50136:688483:688483 [0] NCCL INFO cudaDriverVersion 12050
1: l50136:688481:688481 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.186<0>
1: l50136:688481:688481 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
1: l50136:688481:688481 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
1: l50136:688481:688481 [0] NCCL INFO NET/Plugin: Using internal network plugin.
2: l50136:688482:688482 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.186<0>
2: l50136:688482:688482 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
2: l50136:688482:688482 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
2: l50136:688482:688482 [0] NCCL INFO NET/Plugin: Using internal network plugin.
3: l50136:688483:688483 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.186<0>
3: l50136:688483:688483 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
3: l50136:688483:688483 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
3: l50136:688483:688483 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50136:688480:688642 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.186<0>
1: l50136:688481:688655 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.186<0>
0: l50136:688480:688642 [0] NCCL INFO Using non-device net plugin version 0
0: l50136:688480:688642 [0] NCCL INFO Using network IB
1: l50136:688481:688655 [0] NCCL INFO Using non-device net plugin version 0
1: l50136:688481:688655 [0] NCCL INFO Using network IB
2: l50136:688482:688656 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.186<0>
2: l50136:688482:688656 [0] NCCL INFO Using non-device net plugin version 0
2: l50136:688482:688656 [0] NCCL INFO Using network IB
3: l50136:688483:688657 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.186<0>
3: l50136:688483:688657 [0] NCCL INFO Using non-device net plugin version 0
3: l50136:688483:688657 [0] NCCL INFO Using network IB
0: l50136:688480:688642 [0] NCCL INFO ncclCommInitRank comm 0x55555f235a30 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId c4000 commId 0xd1e709323526aa21 - Init START
1: l50136:688481:688655 [0] NCCL INFO ncclCommInitRank comm 0x55555edec2a0 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 3000 commId 0xd1e709323526aa21 - Init START
2: l50136:688482:688656 [0] NCCL INFO ncclCommInitRank comm 0x55555edebda0 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 84000 commId 0xd1e709323526aa21 - Init START
3: l50136:688483:688657 [0] NCCL INFO ncclCommInitRank comm 0x55555edeba70 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 44000 commId 0xd1e709323526aa21 - Init START
1: 
1: l50136:688481:688655 [0] misc/nvmlwrap.cc:187 NCCL WARN nvmlDeviceGetHandleByPciBusId() failed: Not Found
1: l50136:688481:688655 [0] NCCL INFO graph/xml.cc:850 -> 2
1: l50136:688481:688655 [0] NCCL INFO graph/topo.cc:696 -> 2
1: l50136:688481:688655 [0] NCCL INFO init.cc:1012 -> 2
1: l50136:688481:688655 [0] NCCL INFO init.cc:1548 -> 2
1: l50136:688481:688655 [0] NCCL INFO group.cc:64 -> 2 [Async thread]
1: l50136:688481:688481 [0] NCCL INFO group.cc:418 -> 2
2: 
2: l50136:688482:688656 [0] misc/nvmlwrap.cc:187 NCCL WARN nvmlDeviceGetHandleByPciBusId() failed: Not Found
2: l50136:688482:688656 [0] NCCL INFO graph/xml.cc:850 -> 2
2: l50136:688482:688656 [0] NCCL INFO graph/topo.cc:696 -> 2
2: l50136:688482:688656 [0] NCCL INFO init.cc:1012 -> 2
2: l50136:688482:688656 [0] NCCL INFO init.cc:1548 -> 2
2: l50136:688482:688656 [0] NCCL INFO group.cc:64 -> 2 [Async thread]
2: l50136:688482:688482 [0] NCCL INFO group.cc:418 -> 2
2: l50136:688482:688482 [0] NCCL INFO init.cc:1929 -> 2
3: 
3: l50136:688483:688657 [0] misc/nvmlwrap.cc:187 NCCL WARN nvmlDeviceGetHandleByPciBusId() failed: Not Found
3: l50136:688483:688657 [0] NCCL INFO graph/xml.cc:850 -> 2
3: l50136:688483:688657 [0] NCCL INFO graph/topo.cc:696 -> 2
3: l50136:688483:688657 [0] NCCL INFO init.cc:1012 -> 2
3: l50136:688483:688657 [0] NCCL INFO init.cc:1548 -> 2
3: l50136:688483:688657 [0] NCCL INFO group.cc:64 -> 2 [Async thread]
1: l50136:688481:688481 [0] NCCL INFO init.cc:1929 -> 2
3: l50136:688483:688483 [0] NCCL INFO group.cc:418 -> 2
3: l50136:688483:688483 [0] NCCL INFO init.cc:1929 -> 2
0: 
0: l50136:688480:688642 [0] misc/nvmlwrap.cc:187 NCCL WARN nvmlDeviceGetHandleByPciBusId() failed: Not Found
0: l50136:688480:688642 [0] NCCL INFO graph/xml.cc:850 -> 2
0: l50136:688480:688642 [0] NCCL INFO graph/topo.cc:696 -> 2
0: l50136:688480:688642 [0] NCCL INFO init.cc:1012 -> 2
0: l50136:688480:688642 [0] NCCL INFO init.cc:1548 -> 2
0: l50136:688480:688642 [0] NCCL INFO group.cc:64 -> 2 [Async thread]
0: l50136:688480:688480 [0] NCCL INFO group.cc:418 -> 2
0: l50136:688480:688480 [0] NCCL INFO init.cc:1929 -> 2
0: > /work/ab1412/atmorep/pyenv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py(2501)all_reduce()
0: -> work = group.allreduce([tensor], opts)
1: > /work/ab1412/atmorep/pyenv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py(2501)all_reduce()
1: -> work = group.allreduce([tensor], opts)
2: > /work/ab1412/atmorep/pyenv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py(2501)all_reduce()
2: -> work = group.allreduce([tensor], opts)
3: > /work/ab1412/atmorep/pyenv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py(2501)all_reduce()
3: -> work = group.allreduce([tensor], opts)
0: (Pdb) 
1: (Pdb) 
2: (Pdb) 
3: (Pdb) 
0: [1;34mwandb[0m:
0: [1;34mwandb[0m: You can sync this run to the cloud by running:
0: [1;34mwandb[0m: [1mwandb sync /work/ab1412/atmorep/wandb/offline-run-20250613_103551-lbhs8bxh[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250613_103551-lbhs8bxh/logs[0m
