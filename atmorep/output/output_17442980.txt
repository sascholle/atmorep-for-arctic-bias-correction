0: Wandb run: atmorep-55oc46s3-17442980
0: l50081:1921848:1921848 [0] NCCL INFO Bootstrap : Using ib0:10.128.11.160<0>
0: l50081:1921848:1921848 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l50081:1921848:1921848 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l50081:1921848:1921848 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l50081:1921848:1921848 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
0: l50081:1921848:1922190 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.11.160<0>
0: l50081:1921848:1922190 [0] NCCL INFO Using non-device net plugin version 0
0: l50081:1921848:1922190 [0] NCCL INFO Using network IB
0: l50081:1921848:1922190 [0] NCCL INFO ncclCommInitRank comm 0x55555f2699f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0xa5b125c85c8e5732 - Init START
0: l50081:1921848:1922190 [0] NCCL INFO Setting affinity for GPU 0 to 0f0000,00000000,00000000,00000000,000f0000,00000000
0: l50081:1921848:1922190 [0] NCCL INFO comm 0x55555f2699f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 00/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 01/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 02/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 03/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 04/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 05/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 06/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 07/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 08/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 09/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 10/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 11/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 12/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 13/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 14/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 15/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 16/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 17/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 18/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 19/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 20/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 21/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 22/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 23/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 24/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 25/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 26/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 27/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 28/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 29/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 30/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Channel 31/32 :    0
0: l50081:1921848:1922190 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50081:1921848:1922190 [0] NCCL INFO P2P Chunksize set to 131072
0: l50081:1921848:1922190 [0] NCCL INFO Connected all rings
0: l50081:1921848:1922190 [0] NCCL INFO Connected all trees
0: l50081:1921848:1922190 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50081:1921848:1922190 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l50081:1921848:1922190 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l50081:1921848:1922190 [0] NCCL INFO ncclCommInitRank comm 0x55555f2699f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0xa5b125c85c8e5732 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 4
0: par_rank : 0
0: par_size : 1
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 1, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 2, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 3, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 3, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17442980
0: wandb_id : 55oc46s3
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: years_test : [2021]
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 1
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 2
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded model id = wc5e2i3t at epoch = 0.
0: Loaded run 'wc5e2i3t' at epoch 0.
0: l50081:1921848:1922385 [1] NCCL INFO Using non-device net plugin version 0
0: l50081:1921848:1922385 [1] NCCL INFO Using network IB
0: l50081:1921848:1922385 [1] NCCL INFO ncclCommInitRank comm 0x55556eb11bc0 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0xa71811ee84ede76b - Init START
0: l50081:1921848:1922385 [1] NCCL INFO Setting affinity for GPU 1 to 0f0000,00000000,00000000,00000000,000f0000
0: l50081:1921848:1922385 [1] NCCL INFO comm 0x55556eb11bc0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 00/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 01/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 02/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 03/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 04/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 05/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 06/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 07/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 08/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 09/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 10/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 11/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 12/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 13/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 14/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 15/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 16/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 17/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 18/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 19/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 20/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 21/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 22/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 23/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 24/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 25/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 26/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 27/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 28/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 29/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 30/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Channel 31/32 :    0
0: l50081:1921848:1922385 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50081:1921848:1922385 [1] NCCL INFO P2P Chunksize set to 131072
0: l50081:1921848:1922385 [1] NCCL INFO Connected all rings
0: l50081:1921848:1922385 [1] NCCL INFO Connected all trees
0: l50081:1921848:1922385 [1] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50081:1921848:1922385 [1] NCCL INFO ncclCommInitRank comm 0x55556eb11bc0 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0xa71811ee84ede76b - Init COMPLETE
0: l50081:1921848:1922390 [2] NCCL INFO Using non-device net plugin version 0
0: l50081:1921848:1922390 [2] NCCL INFO Using network IB
0: l50081:1921848:1922390 [2] NCCL INFO ncclCommInitRank comm 0x55557eb209f0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0xaa09000d2c262d58 - Init START
0: l50081:1921848:1922390 [2] NCCL INFO Setting affinity for GPU 2 to 1f0000,00000000,00000000,00000000,001f0000,00000000,00000000,00000000
0: l50081:1921848:1922390 [2] NCCL INFO comm 0x55557eb209f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 00/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 01/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 02/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 03/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 04/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 05/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 06/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 07/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 08/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 09/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 10/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 11/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 12/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 13/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 14/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 15/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 16/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 17/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 18/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 19/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 20/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 21/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 22/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 23/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 24/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 25/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 26/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 27/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 28/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 29/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 30/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Channel 31/32 :    0
0: l50081:1921848:1922390 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50081:1921848:1922390 [2] NCCL INFO P2P Chunksize set to 131072
0: l50081:1921848:1922390 [2] NCCL INFO Connected all rings
0: l50081:1921848:1922390 [2] NCCL INFO Connected all trees
0: l50081:1921848:1922390 [2] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50081:1921848:1922390 [2] NCCL INFO ncclCommInitRank comm 0x55557eb209f0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0xaa09000d2c262d58 - Init COMPLETE
0: l50081:1921848:1922395 [3] NCCL INFO Using non-device net plugin version 0
0: l50081:1921848:1922395 [3] NCCL INFO Using network IB
0: l50081:1921848:1922395 [3] NCCL INFO ncclCommInitRank comm 0x5555876f1110 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0x3c4a79dee83fa641 - Init START
0: l50081:1921848:1922395 [3] NCCL INFO Setting affinity for GPU 3 to 0f0000,00000000,00000000,00000000,000f0000,00000000,00000000
0: l50081:1921848:1922395 [3] NCCL INFO comm 0x5555876f1110 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 00/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 01/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 02/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 03/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 04/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 05/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 06/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 07/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 08/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 09/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 10/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 11/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 12/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 13/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 14/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 15/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 16/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 17/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 18/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 19/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 20/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 21/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 22/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 23/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 24/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 25/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 26/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 27/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 28/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 29/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 30/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Channel 31/32 :    0
0: l50081:1921848:1922395 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l50081:1921848:1922395 [3] NCCL INFO P2P Chunksize set to 131072
0: l50081:1921848:1922395 [3] NCCL INFO Connected all rings
0: l50081:1921848:1922395 [3] NCCL INFO Connected all trees
0: l50081:1921848:1922395 [3] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l50081:1921848:1922395 [3] NCCL INFO ncclCommInitRank comm 0x5555876f1110 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0x3c4a79dee83fa641 - Init COMPLETE
0: Number of trainable parameters: 741,136,272
0: 1 : 17:30:34 :: batch_size = 96, lr = 1.5000000000000002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.4400, 2.4408, 2.4423, 2.4434, 2.4428, 2.4397, 2.4337, 2.4253, 2.4153, 2.4044, 2.3932, 2.3825, 2.3719, 2.3593,
0:         2.3421, 2.3186, 2.2876, 2.2473, 2.4864, 2.4863, 2.4870, 2.4881, 2.4887, 2.4880, 2.4854], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2999, -1.2813, -1.2601, -1.2375, -1.2145, -1.1921, -1.1709, -1.1509, -1.1303, -1.1069, -1.0787, -1.0449,
0:         -1.0051, -0.9616, -0.9176, -0.8760, -0.8388, -0.8074, -1.3123, -1.2973, -1.2807, -1.2629, -1.2445, -1.2253,
0:         -1.2063], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4488, -0.4644, -0.4799, -0.4954, -0.5113, -0.5272, -0.5423, -0.5564, -0.5681, -0.5774, -0.5873, -0.5932,
0:         -0.5991, -0.6003, -0.6030, -0.6051, -0.6057, -0.6078, -0.4224, -0.4350, -0.4450, -0.4579, -0.4731, -0.4873,
0:         -0.5012], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.1543,  1.1322,  1.0537,  0.9719,  0.9321,  0.9133,  0.8812,  0.8447,  0.8237,  0.7916,  0.7120,  0.5870,
0:          0.4355,  0.2498,  0.0397, -0.1494, -0.2755, -0.3385,  1.0415,  1.0482,  1.0216,  0.9829,  0.9575,  0.9398,
0:          0.9199], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.5564, 0.5655, 0.5744, 0.5810, 0.5850, 0.5879, 0.5916, 0.5970, 0.6044, 0.6145, 0.6271, 0.6419, 0.6584, 0.6754,
0:         0.6903, 0.7012, 0.7075, 0.7083, 0.7035, 0.6929, 0.6794, 0.6659, 0.6557, 0.6514, 0.6534], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2206, -0.2173, -0.2128, -0.2106, -0.2095, -0.2173, -0.2195, -0.2229, -0.2262, -0.2173, -0.2072, -0.2028,
0:         -0.1972, -0.2006, -0.1972, -0.1983, -0.2039, -0.2072, -0.2128, -0.2106, -0.2106, -0.2028, -0.1950, -0.1816,
0:         -0.1805], device='cuda:0')
0: [DEBUG] Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([ 0.9176,  0.7593,     nan,     nan,     nan,     nan,     nan,     nan, -0.2195,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          1.2476,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  3.3791,  3.4147,     nan,     nan,     nan,  2.1193,     nan,  0.6590,
0:             nan,  0.3914,  0.0347,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.1038,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.3078,     nan,     nan,     nan,
0:             nan,     nan,  2.7659,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  1.9878,     nan,     nan,  3.8874,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          3.1517,     nan,     nan,     nan,     nan,     nan,  1.0558,     nan,     nan,  3.9209,     nan,     nan,
0:             nan,     nan,     nan,  0.7102,     nan,     nan,     nan,     nan,     nan,     nan, -0.2173,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.0166,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.9443,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.1227,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2366, 1.2312, 1.2233, 1.2096, 1.1935, 1.1774, 1.1696, 1.1608, 1.1687, 1.1696, 1.1614, 1.1428, 1.1101, 1.0711,
0:         1.0328, 0.9956, 0.9617, 0.9349, 1.2538, 1.2618, 1.2610, 1.2556, 1.2459, 1.2328, 1.2200], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-3.1755, -3.1486, -3.0969, -3.0238, -2.9274, -2.8273, -2.7278, -2.6213, -2.5106, -2.3947, -2.2734, -2.1563,
0:         -2.0441, -1.9245, -1.8128, -1.7121, -1.6293, -1.5628, -3.1743, -3.1383, -3.0806, -3.0009, -2.9035, -2.8078,
0:         -2.7139], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3307, 0.3390, 0.3662, 0.3969, 0.4204, 0.4464, 0.4670, 0.5008, 0.5348, 0.5690, 0.6129, 0.6672, 0.7219, 0.7820,
0:         0.8499, 0.9112, 0.9757, 1.0261, 0.2638, 0.2878, 0.3274, 0.3665, 0.3927, 0.4162, 0.4315], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0627,  0.0183,  0.0948,  0.1281,  0.1043,  0.1199,  0.1535,  0.1389,  0.1164,  0.1054,  0.1188,  0.1659,
0:          0.2260,  0.2616,  0.3254,  0.3987,  0.3490,  0.2368, -0.1491, -0.0585,  0.0431,  0.0964,  0.0979,  0.1117,
0:          0.1434], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.7791, 0.7723, 0.7671, 0.7616, 0.7589, 0.7609, 0.7651, 0.7695, 0.7764, 0.7836, 0.7924, 0.8058, 0.8243, 0.8413,
0:         0.8597, 0.8772, 0.8951, 0.9111, 0.9253, 0.9334, 0.9359, 0.9317, 0.9276, 0.9294, 0.9346], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.7632, 0.8310, 0.9106, 0.9640, 0.9030, 0.7567, 0.6087, 0.4196, 0.2430, 1.0423, 1.1307, 1.2077, 1.2005, 1.1551,
0:         0.9284, 0.7154, 0.4566, 0.2465, 1.4248, 1.5389, 1.5993, 1.5509, 1.4089, 1.1471, 0.7796], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.05275096371769905; velocity_v: 0.087249755859375; specific_humidity: 0.03005777671933174; velocity_z: 0.5652737021446228; temperature: 0.07159624993801117; total_precip: 0.5145319700241089; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07310624420642853; velocity_v: 0.11090127378702164; specific_humidity: 0.04651612415909767; velocity_z: 0.6991256475448608; temperature: 0.11506310850381851; total_precip: 0.8858962059020996; 
0: epoch: 1 [1/5 (20%)]	Loss: 0.70021 : 0.24543 :: 0.15630 (2.06 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05697019398212433; velocity_v: 0.09587462246417999; specific_humidity: 0.04099060595035553; velocity_z: 0.49171343445777893; temperature: 0.0928778275847435; total_precip: 0.7725159525871277; 
0: epoch: 1 [2/5 (40%)]	Loss: 0.77252 : 0.23317 :: 0.14464 (16.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.059332020580768585; velocity_v: 0.10111410915851593; specific_humidity: 0.04453786090016365; velocity_z: 0.4994172751903534; temperature: 0.11377957463264465; total_precip: 0.7951575517654419; 
0: epoch: 1 [3/5 (60%)]	Loss: 0.79516 : 0.24211 :: 0.14656 (16.57 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05499151349067688; velocity_v: 0.08775018900632858; specific_humidity: 0.031537700444459915; velocity_z: 0.47340133786201477; temperature: 0.08969389647245407; total_precip: 0.5034486055374146; 
0: epoch: 1 [4/5 (80%)]	Loss: 0.50345 : 0.18226 :: 0.14050 (16.54 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 20):
0: [4.7683716e-07 4.7683716e-07 4.7683716e-07 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Target values (first 20):
0: [4.7683716e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]
0: Prediction values (first 20):
0: [-4.4789176 -4.5184674 -4.535256  -4.5289207 -4.5224366 -4.562673
0:  -4.579963  -4.67112   -4.7703166 -4.8606544 -4.9465814 -5.0423284
0:  -5.1270375 -5.211271  -5.3309894 -5.382055  -5.4106383 -5.3505282
0:  -6.1121793 -6.218209 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.665, max = 2.900, mean = -0.148
0:          sample (first 20): tensor([-1.0587, -1.0622, -1.0636, -1.0631, -1.0625, -1.0660, -1.0676, -1.0756, -1.0843, -1.0923, -1.0999, -1.1083,
0:         -1.1158, -1.1232, -1.1337, -1.1382, -1.1407, -1.1354, -1.0144, -1.0175])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.752878  11.7855    11.825136  11.846384  11.869507  11.863367
0:  11.861084  11.83831   11.8210945 11.809084  11.784645  11.696888
0:  11.582409  11.429968  11.253011  11.09935   10.985244  10.910794
0:  10.512466  10.302407 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.9662366 -7.918239  -7.8045497 -7.665088  -7.4899087 -7.3388457
0:  -7.191838  -7.1209207 -7.095239  -7.1152864 -7.140845  -7.225759
0:  -7.313725  -7.3728156 -7.466635  -7.53412   -7.605389  -7.639727
0:  -8.261652  -8.348881 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.464062 11.581745 11.720673 11.855589 12.02302  12.20016  12.37632
0:  12.530879 12.708535 12.91004  13.136309 13.368917 13.599138 13.797983
0:  13.939577 14.047872 14.146788 14.251416 13.927409 13.993605]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.617128  -10.510178  -10.353634  -10.1949005 -10.032618   -9.896515
0:   -9.7737255  -9.675674   -9.586599   -9.474602   -9.347523   -9.22444
0:   -9.086851   -8.952024   -8.825321   -8.71755    -8.647779   -8.602837
0:   -8.691126   -8.543219 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.6267614 -7.018031  -7.475465  -7.8310347 -8.02483   -8.082368
0:  -8.017341  -7.934643  -7.871316  -7.8029575 -7.774149  -7.78465
0:  -7.822694  -7.788681  -7.713189  -7.518092  -7.229026  -6.8244095
0:  -5.9113636 -5.5290813]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.1756897   0.04273415 -0.09909344 -0.28076077 -0.46864414 -0.70472383
0:  -0.96488476 -1.2776136  -1.579587   -1.8510151  -2.077938   -2.2890196
0:  -2.4809332  -2.60501    -2.7332935  -2.805809   -2.822503   -2.800499
0:  -3.153943   -3.2476463 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.704592 14.156336 14.551029 14.913062 15.225153 15.523489 15.826138
0:  16.057749 16.272526 16.424393 16.506012 16.543371 16.576237 16.565857
0:  16.531065 16.454294 16.357815 16.276207 15.161552 15.343033]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.8189116  6.036      6.278252   6.554874   6.857918   7.2094903
0:   7.5805616  7.918566   8.272944   8.592446   8.891727   9.14372
0:   9.4229765  9.674768   9.930674  10.163756  10.436364  10.738523
0:  11.253879  11.466646 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.386326 32.605698 31.698456 30.714434 29.810553 29.051952 28.564342
0:  28.283167 28.333992 28.512032 28.641512 28.644804 28.452152 28.098755
0:  27.66704  27.145258 26.591635 25.976892 26.047104 25.524153]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.378712  -8.4495735 -8.476639  -8.464436  -8.432848  -8.448067
0:  -8.44116   -8.485626  -8.522421  -8.531004  -8.5254345 -8.552345
0:  -8.567785  -8.5977955 -8.623745  -8.5972595 -8.54751   -8.436382
0:  -8.6486435 -8.667692 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.896224    4.1076264   4.4078054   4.7397842   5.0466456   5.2194176
0:   5.236588    5.042643    4.7122006   4.3103933   3.883913    3.394796
0:   2.869778    2.258632    1.5393109   0.8284497   0.17786789 -0.32670975
0:  -1.4162588  -1.8138266 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.839374  -5.6865716 -5.508287  -5.3021283 -5.1099944 -4.9679084
0:  -4.828153  -4.765635  -4.7149367 -4.6555343 -4.5971045 -4.5191617
0:  -4.4277043 -4.297317  -4.1614747 -4.005421  -3.8566213 -3.6484218
0:  -3.751618  -3.5265098]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.049545 25.264856 25.525105 25.775505 25.926764 26.005543 26.10698
0:  26.138943 26.257338 26.424814 26.554296 26.700905 26.871685 27.076063
0:  27.30589  27.547544 27.784534 28.003792 27.977308 28.094486]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.571686 23.94881  23.420555 22.92607  22.455652 21.979467 21.607098
0:  21.155464 20.818142 20.567423 20.299122 20.121128 20.059433 20.048536
0:  20.204523 20.399544 20.53284  20.617247 20.788086 21.062342]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.282311   -10.929849   -10.521838   -10.015687    -9.445098
0:   -8.866723    -8.196174    -7.5292864   -6.8448124   -6.1499996
0:   -5.4275885   -4.754421    -4.068942    -3.4387894   -2.8604212
0:   -2.324113    -1.8080549   -1.3371353   -0.98652744  -0.51634645]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.441238  -15.405357  -15.281845  -15.049185  -14.772509  -14.520163
0:  -14.215214  -13.986383  -13.769665  -13.605909  -13.538     -13.583702
0:  -13.724062  -13.828173  -13.939201  -13.9262295 -13.888216  -13.8184185
0:  -12.7304    -12.203894 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.142464 19.99388  19.796759 19.562216 19.35753  19.220675 19.17527
0:  19.16232  19.280376 19.432451 19.578497 19.6817   19.712683 19.635193
0:  19.459154 19.185953 18.884632 18.538656 17.053934 17.079704]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.522243  12.389862  12.357496  12.3106    12.2744255 12.313326
0:  12.395185  12.505972  12.601972  12.576439  12.378137  11.907281
0:  11.149414  10.06766    8.596969   6.8036203  4.8407764  3.0260468
0:  -0.6008582 -1.211112 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8717818 -2.6930947 -2.5096326 -2.335072  -2.1455922 -1.9959455
0:  -1.8462977 -1.762033  -1.6904712 -1.6235862 -1.5428281 -1.4672408
0:  -1.3801684 -1.2745733 -1.1790524 -1.0393128 -0.8574929 -0.6297064
0:  -0.6462302 -0.506577 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0593214  -3.7204494  -3.3501043  -3.0119915  -2.6988354  -2.4170647
0:  -2.1351442  -1.8895273  -1.6270132  -1.3666835  -1.0827751  -0.82279396
0:  -0.55018044 -0.28986788 -0.10436678  0.07809401  0.2518773   0.46372366
0:   0.981133    1.0637517 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.2153015 4.2349114 4.280843  4.367368  4.4634323 4.5505514 4.6457806
0:  4.719342  4.8126874 4.922461  5.045315  5.170261  5.334683  5.5120544
0:  5.7107196 5.9213786 6.1251884 6.318891  6.5532618 6.7516384]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.046242 23.258934 23.312893 23.223019 23.056301 22.883945 22.756844
0:  22.603474 22.501965 22.422928 22.351086 22.263153 22.222715 22.220127
0:  22.211647 22.247059 22.336655 22.420383 22.41853  22.543411]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.528799  -10.3257065 -10.101784   -9.874784   -9.63954    -9.453604
0:   -9.28109    -9.181818   -9.079287   -8.943989   -8.7537     -8.5399
0:   -8.298094   -8.047752   -7.835611   -7.6116834  -7.364905   -7.0537076
0:   -6.769769   -6.5266623]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.04280806 -0.03663731 -0.00351906  0.05512762  0.1159339   0.16102171
0:   0.20223665  0.2038827   0.23210955  0.25514603  0.2855749   0.3138275
0:   0.3614149   0.40428686  0.4544549   0.5218706   0.60279846  0.72751856
0:   0.7554145   0.853034  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.96352  22.94697  22.810204 22.454193 21.995106 21.488739 21.02847
0:  20.63294  20.454893 20.43241  20.605503 20.843063 21.186756 21.535189
0:  21.775057 21.899435 21.860373 21.677362 20.70971  20.66156 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.488922 19.248444 19.100977 18.978838 18.83514  18.600224 18.352324
0:  17.995636 17.686298 17.35684  17.06071  16.791597 16.593584 16.46456
0:  16.347084 16.311007 16.280638 16.288555 15.750944 15.65815 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1163716  -1.0235744  -0.95456696 -0.9045758  -0.91076803 -0.9949055
0:  -1.1074181  -1.2730484  -1.4206738  -1.5431342  -1.6246586  -1.6835289
0:  -1.677434   -1.6125937  -1.5014434  -1.3498478  -1.2123337  -1.0891409
0:  -1.496542   -1.6560335 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.160648 22.214886 22.288424 22.367424 22.462107 22.592148 22.843218
0:  23.079607 23.413195 23.808144 24.207144 24.626852 25.052624 25.456762
0:  25.844511 26.16207  26.428127 26.600183 26.564714 26.976604]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.60617924 -0.60581875 -0.56339455 -0.5224519  -0.46980858 -0.4454527
0:  -0.42366838 -0.42502594 -0.4002452  -0.3591795  -0.30546188 -0.2869315
0:  -0.2767067  -0.25487804 -0.25508738 -0.2150383  -0.14424324 -0.0569191
0:  -0.3313713  -0.3596735 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.4553947e-02  5.0210953e-04  1.2942934e-01  2.6786566e-01
0:   4.2248487e-01  5.4483223e-01  6.2470531e-01  6.3379526e-01
0:   6.2898493e-01  6.2048864e-01  6.4682007e-01  6.4197445e-01
0:   6.2267542e-01  5.8294249e-01  4.9957418e-01  4.6972656e-01
0:   5.4103470e-01  7.0318031e-01  9.9959803e-01  1.1439023e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.272806  12.266644  12.3048115 12.306373  12.297794  12.252344
0:  12.202082  12.0918045 11.934176  11.700743  11.366035  10.944019
0:  10.483389   9.99412    9.500364   9.02496    8.559399   8.159953
0:   7.673896   7.504282 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9045727 3.729718  3.5668519 3.4131095 3.2811513 3.1946673 3.2158043
0:  3.2955818 3.4192162 3.5573838 3.6666553 3.714454  3.7436366 3.752171
0:  3.7329288 3.7404652 3.7389843 3.735956  3.8209212 4.0175543]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [42.313675 42.296543 42.261726 42.221577 42.147438 42.049904 42.021637
0:  41.90557  41.85065  41.77585  41.60504  41.424545 41.24684  41.078255
0:  40.93472  40.768925 40.58502  40.304962 40.339626 40.069817]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.016085  7.9492545 7.875931  7.7754445 7.6785274 7.5434666 7.3968196
0:  7.215826  7.0862064 6.9908895 6.9400682 6.8673964 6.765421  6.6618786
0:  6.49898   6.370308  6.31836   6.3363004 6.3341355 6.275158 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.276995  -8.188698  -8.109751  -8.028362  -7.9575667 -7.938518
0:  -7.8868203 -7.8858867 -7.859138  -7.8260036 -7.800978  -7.8123155
0:  -7.8119044 -7.807839  -7.8016315 -7.748235  -7.6843143 -7.584271
0:  -8.090782  -7.9929366]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.60242   4.562641  4.506071  4.4199677 4.3459444 4.2427053 4.1288643
0:  4.0107384 3.9175222 3.9034019 3.9560194 4.0431156 4.1819324 4.3194
0:  4.4616227 4.613941  4.7803946 4.995105  4.2638483 4.274175 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.092405 11.038071 11.040901 11.052016 11.074413 11.071552 11.061226
0:  11.002362 10.946399 10.898861 10.827096 10.73899  10.640921 10.54527
0:  10.452606 10.394173 10.372872 10.376772 10.389047 10.22709 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.331745 38.709244 39.1233   39.5236   39.921524 40.29356  40.744602
0:  41.108448 41.483604 41.797382 41.973774 42.04567  42.064995 41.999817
0:  41.841778 41.591797 41.21303  40.738266 40.493374 40.541332]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.173748  15.119799  15.134247  15.145023  15.134232  15.129665
0:  15.123716  15.058966  15.039932  15.03471   15.062708  15.065691
0:  15.080782  15.0743885 15.031336  15.021463  15.0973    15.247236
0:  14.938799  14.868837 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7806191  -1.5116973  -1.1375575  -0.6856289  -0.16695023  0.34861708
0:   0.88467026  1.3806081   1.8511987   2.3042288   2.7852077   3.2363904
0:   3.6921039   4.1457386   4.564014    4.9686017   5.323024    5.664965
0:   5.599318    5.795704  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.4231668 6.3576155 6.2242627 5.9834404 5.6707916 5.2975044 4.9529667
0:  4.6184564 4.3707557 4.1793394 4.033082  3.899502  3.7775965 3.6688802
0:  3.5522866 3.437395  3.3109272 3.179115  2.8943095 2.8684716]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.31391  15.703201 16.145943 16.604437 17.10348  17.608303 18.162964
0:  18.64584  19.176191 19.732077 20.271097 20.825779 21.404724 21.972967
0:  22.533543 23.074484 23.639206 24.141191 24.641048 25.110632]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.626594 -12.658271 -12.634116 -12.567779 -12.495034 -12.47075
0:  -12.451411 -12.508267 -12.551267 -12.569499 -12.552316 -12.534677
0:  -12.498973 -12.4208   -12.348659 -12.243068 -12.092349 -11.895381
0:  -12.138171 -11.989638]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.617563  -7.7066627 -7.798489  -7.8599772 -7.8423133 -7.801945
0:  -7.67656   -7.5777144 -7.4569926 -7.3170886 -7.152269  -6.9989533
0:  -6.8393884 -6.616507  -6.4566827 -6.2508473 -6.0754094 -5.9166455
0:  -6.0851426 -6.2240186]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.001987 26.372562 26.608078 26.695179 26.68415  26.65543  26.737751
0:  26.811707 26.974194 27.173065 27.334427 27.489899 27.643433 27.816101
0:  28.043503 28.333527 28.675285 29.020016 29.196651 29.610828]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.4546132   1.3560753   1.2830119   1.2314048   1.24267     1.2381124
0:   1.1970329   1.0684829   0.86574125  0.6374097   0.41187477  0.17100143
0:  -0.04576731 -0.21957922 -0.41085768 -0.52535105 -0.5742998  -0.56757116
0:  -0.84599495 -0.9346585 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.3392453 4.7352476 4.2188196 3.7585685 3.3059645 2.8739204 2.4367638
0:  1.9876161 1.6063986 1.3153801 1.137116  1.1157861 1.3555255 1.7489996
0:  2.3562694 3.0372334 3.770803  4.521599  5.7505546 6.476264 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.4345951   0.24396658  0.06883335 -0.12149525 -0.30237198 -0.4699688
0:  -0.6154609  -0.7663436  -0.90131664 -1.0639925  -1.1933198  -1.3702984
0:  -1.5454254  -1.6876917  -1.8948426  -2.1020503  -2.3590693  -2.605434
0:  -2.9661212  -3.1920571 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7131639 2.8435216 3.0142894 3.216567  3.4371805 3.6168313 3.8035076
0:  3.887291  3.9228425 3.916355  3.9035468 3.8604379 3.8161178 3.8334923
0:  3.8152108 3.8784242 3.9739966 4.094471  4.127754  4.3200245]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.0594454   0.6869798   0.36694288  0.15666819  0.09246302  0.09795713
0:   0.13500452  0.15544033  0.13683558  0.09460211  0.02407074 -0.06106377
0:  -0.1219511  -0.18097878 -0.1935854  -0.14588833 -0.03062344  0.12358809
0:  -0.13696432 -0.15378428]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.23308  33.11032  32.931694 32.574486 32.10796  31.564419 30.98743
0:  30.324123 29.6422   28.889896 28.050156 27.147106 26.252533 25.420086
0:  24.673298 24.106716 23.746647 23.56445  24.021847 23.864822]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.62791   -11.087766  -10.341094   -9.4666605  -8.603517   -7.9092674
0:   -7.292371   -6.890565   -6.4510813  -6.023411   -5.54788    -4.9831076
0:   -4.4311733  -3.8015308  -3.2399054  -2.780179   -2.3755379  -2.025381
0:   -1.3502188  -1.2136173]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.068243 18.158497 18.203146 18.205585 18.143951 18.030426 17.94426
0:  17.771523 17.642153 17.518595 17.391918 17.309698 17.289333 17.29816
0:  17.302032 17.252897 17.125717 16.916708 16.37451  16.637913]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.797533 15.049782 15.322931 15.577517 15.882807 16.250387 16.648575
0:  17.004833 17.297098 17.475325 17.52501  17.414246 17.247889 17.022432
0:  16.793283 16.619646 16.525644 16.457344 16.302416 16.08954 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.96281  -10.161218 -10.369824 -10.577479 -10.727038 -10.850471
0:  -10.898757 -10.943313 -10.951644 -10.937442 -10.88853  -10.835483
0:  -10.751353 -10.576425 -10.373977 -10.066081  -9.658871  -9.262792
0:   -8.208842  -8.062198]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.9106517 7.9239836 7.9027534 7.8628674 7.7672915 7.5839653 7.411013
0:  7.1897793 6.9970655 6.8199377 6.66657   6.563241  6.504569  6.506463
0:  6.48212   6.4531555 6.370216  6.269309  5.0150294 5.0573797]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.08381   17.601368  16.646235  15.362946  13.944843  12.658926
0:  11.661055  10.996834  10.646086  10.505066  10.438884  10.302454
0:  10.119743   9.847838   9.529934   9.26094    9.0670185  8.92898
0:   8.042893   8.09699  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.512531  12.39384   12.234537  11.9986515 11.732723  11.466103
0:  11.211078  10.984722  10.784187  10.648291  10.538914  10.445208
0:  10.383848  10.315616  10.200546  10.041334   9.846499   9.65737
0:   8.0967865  8.109604 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7987442  -1.6659088  -1.5327864  -1.4177909  -1.319819   -1.2458243
0:  -1.1745934  -1.1305003  -1.1051378  -1.0679955  -1.0123248  -0.9723053
0:  -0.923954   -0.8578634  -0.8044481  -0.73879814 -0.6957803  -0.63866234
0:  -0.6802521  -0.65709305]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.56542826 -0.40425348 -0.19921732  0.04375744  0.30499268  0.5286622
0:   0.74059296  0.8646059   0.9702468   1.0302272   1.0739388   1.1181488
0:   1.1887617   1.2539201   1.319253    1.3845277   1.4528847   1.5568495
0:   1.3472142   1.464859  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.210919  -6.175368  -6.125129  -6.071901  -6.012049  -5.9756913
0:  -5.8814683 -5.7855115 -5.6208653 -5.385656  -5.0646462 -4.7242465
0:  -4.3778462 -4.0244718 -3.751103  -3.4897704 -3.2588868 -3.0597167
0:  -3.1196542 -3.0050054]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.217828  -12.929364  -12.584044  -12.124581  -11.615705  -11.137984
0:  -10.662719  -10.257894   -9.914371   -9.624597   -9.368226   -9.180778
0:   -8.940019   -8.675931   -8.368979   -8.039615   -7.7462826  -7.4659576
0:   -7.756671   -7.4840517]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.8952646  -1.131207   -1.3129916  -1.434485   -1.4522004  -1.3574319
0:  -1.168756   -0.88498306 -0.51042366 -0.09922361  0.32883692  0.6836157
0:   0.9880748   1.2072053   1.3509398   1.4385204   1.5212574   1.6084638
0:   1.6355739   1.7313337 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.241476  16.364477  16.479279  16.49732   16.473     16.421122
0:  16.339964  16.238241  16.144619  16.071503  16.007284  15.905388
0:  15.7805195 15.661999  15.544327  15.502474  15.571207  15.7415085
0:  16.10171   16.180069 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.23132   -9.028072  -8.792066  -8.5437355 -8.299861  -8.138837
0:  -8.001318  -7.9780655 -7.991184  -8.030261  -8.0607    -8.118846
0:  -8.163665  -8.151501  -8.151833  -8.086428  -7.977727  -7.8309774
0:  -8.365784  -8.204566 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.317421  14.383403  14.455465  14.493565  14.512064  14.4996605
0:  14.510708  14.47584   14.457516  14.423123  14.344097  14.21825
0:  14.088413  13.952172  13.817896  13.719275  13.655785  13.579092
0:  12.92237   12.940151 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.95799446 1.2129989  1.6444759  2.1769428  2.718352   3.1883776
0:  3.5827796  3.8571148  4.0854864  4.294467   4.5250926  4.7311053
0:  4.9525423  5.146141   5.311519   5.457465   5.5987086  5.7613463
0:  5.550577   5.509674  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.9240813 -6.9492383 -7.044412  -7.1313663 -7.275372  -7.4529195
0:  -7.66065   -7.9307084 -8.217941  -8.466145  -8.678408  -8.833038
0:  -8.876995  -8.880379  -8.824663  -8.749975  -8.680784  -8.5923195
0:  -9.197432  -9.088221 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.94117  16.047691 16.172998 16.23746  16.274494 16.29875  16.322174
0:  16.31403  16.339012 16.365883 16.380602 16.358154 16.338766 16.304184
0:  16.254301 16.215694 16.217709 16.234648 16.16438  16.24411 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.114771 10.057375 10.006886  9.976562  9.866562  9.680254  9.486902
0:   9.209208  8.958964  8.716034  8.431579  8.191849  8.057374  8.002258
0:   8.106314  8.294025  8.574847  8.89755   9.427013  9.572441]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.9503145 -12.872365  -12.7959795 -12.698512  -12.639071  -12.642428
0:  -12.614947  -12.644628  -12.648921  -12.640443  -12.644807  -12.668477
0:  -12.674855  -12.680192  -12.677337  -12.644282  -12.619065  -12.549336
0:  -12.896527  -12.797715 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.59447956 0.7372751  0.8973851  1.0958581  1.3684802  1.6511645
0:  1.9321742  2.1512637  2.3535829  2.544382   2.7573495  2.9647384
0:  3.1785765  3.4033732  3.584661   3.7779148  4.0345716  4.322674
0:  4.688994   4.585504  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.983246  8.968274  8.99334   9.022279  9.077684  9.163954  9.2564945
0:  9.3394375 9.451159  9.557222  9.65966   9.747246  9.823576  9.854881
0:  9.861502  9.844551  9.837346  9.859517  9.290649  9.459987 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.35550833  0.1920371   0.01422882 -0.17804384 -0.30840778 -0.33961582
0:  -0.18129635  0.16719961  0.6917777   1.3469291   2.050694    2.6417289
0:   3.0696824   3.2942896   3.2406218   3.0346644   2.7263784   2.3880367
0:   1.5573673   1.680026  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.3892064  -0.2591238  -0.10239553  0.01133871  0.13895321  0.21670294
0:   0.3228078   0.3932023   0.50547075  0.65669584  0.80229473  0.9146347
0:   1.0153341   1.0582924   1.0566025   1.0971332   1.1697116   1.2581863
0:   1.2016196   1.3399324 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.055654  13.245651  13.438772  13.595636  13.720203  13.824984
0:  13.935644  13.9945545 14.049415  14.071396  14.066593  14.0381775
0:  14.032314  14.026787  14.018866  13.9876995 13.957775  13.912014
0:  14.078749  14.168588 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.6966915 14.609655  14.530121  14.438232  14.289707  14.033052
0:  13.67099   13.204169  12.726154  12.247728  11.833455  11.512517
0:  11.336496  11.286169  11.366639  11.549152  11.886314  12.394224
0:  13.299812  13.546902 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.195899   -1.2840767  -1.268899   -1.1608677  -1.0223522  -0.8976898
0:  -0.74759865 -0.657568   -0.5522113  -0.47794914 -0.37076044 -0.2768917
0:  -0.12385798  0.06194925  0.25872326  0.4563222   0.63166666  0.7810235
0:   0.7303138   0.6695123 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.960598 17.564417 17.1629   16.734592 16.303402 15.90634  15.625153
0:  15.372108 15.241943 15.207903 15.159824 15.171534 15.210558 15.233158
0:  15.299183 15.317221 15.323269 15.332125 14.980581 14.805748]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.835442 22.772373 23.613636 24.35906  24.981958 25.57638  26.130787
0:  26.571056 26.999853 27.395693 27.770382 28.14311  28.575855 29.017694
0:  29.453436 29.858006 30.28792  30.717358 31.302595 32.011513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.1242485  9.241005   9.390734   9.594344   9.895367  10.275187
0:  10.720184  11.170075  11.615404  12.056458  12.50341   12.903297
0:  13.281704  13.617433  13.909007  14.174828  14.467564  14.770782
0:  14.441182  14.570006 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4609065 2.4907162 2.5796943 2.569263  2.5787687 2.5810702 2.5497098
0:  2.4975426 2.4377012 2.3462038 2.254765  2.1420856 2.0308838 2.0096326
0:  1.919076  1.8311362 1.7403994 1.6601553 1.4648461 1.4016886]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.437468   9.702348  10.03171   10.361857  10.698101  11.004408
0:  11.301481  11.537363  11.720533  11.832472  11.892916  11.878458
0:  11.865738  11.8653145 11.853518  11.862327  11.876904  11.877016
0:  11.824252  11.877201 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.59323  23.387074 23.140356 22.816574 22.489439 22.196299 22.066156
0:  21.920073 21.800064 21.624506 21.31412  20.947472 20.628736 20.411114
0:  20.296766 20.312723 20.400513 20.51657  21.383768 21.678335]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.004165  3.133871  3.284671  3.4224713 3.573832  3.677452  3.7763968
0:  3.8430154 3.94105   4.0786    4.2537    4.434312  4.6160083 4.799122
0:  4.9548645 5.1447067 5.361234  5.601758  5.376821  5.6253242]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.733946   7.7632513  7.8520365  8.021961   8.283491   8.602776
0:   8.966498   9.283269   9.573319   9.781626   9.942575  10.058733
0:  10.167608  10.231964  10.226173  10.155229  10.023635   9.858879
0:   8.628471   8.42598  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.25703   16.223295  16.179146  16.088707  15.989628  15.860659
0:  15.703829  15.520596  15.359419  15.2342415 15.150099  15.080933
0:  15.026663  14.986006  14.943905  14.925564  14.970715  15.052818
0:  15.184196  15.253878 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.226191  5.4157743 5.610985  5.8537526 6.072561  6.248968  6.4323015
0:  6.547876  6.6680174 6.770006  6.856406  6.948361  7.103493  7.267969
0:  7.4817953 7.6860003 7.913052  8.183749  8.571409  9.019606 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.244043   11.020204   10.796524   10.606131   10.3997135  10.145182
0:   9.898773    9.598087    9.286348    8.922657    8.481463    8.009781
0:   7.531354    6.9913588   6.367298    5.6449494   4.828655    4.065402
0:   1.5650444   0.97873497]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.164455  -3.1935344 -3.1140456 -2.9085326 -2.6117196 -2.3233304
0:  -2.0143266 -1.7843919 -1.6049895 -1.4847984 -1.4184251 -1.4650569
0:  -1.5823655 -1.7576413 -2.0411897 -2.3207507 -2.5824008 -2.7802205
0:  -3.3942647 -3.5025172]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.983605  -3.7692018 -3.5980144 -3.4883075 -3.3831086 -3.348885
0:  -3.288506  -3.2861247 -3.2645354 -3.2097678 -3.1339984 -3.0673184
0:  -3.0151496 -2.9385686 -2.897903  -2.7957911 -2.6456504 -2.4674602
0:  -2.7407746 -2.5945716]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.172419  6.10009   6.043144  6.018143  5.9818077 5.930745  5.8885965
0:  5.758574  5.6350455 5.572663  5.511957  5.469013  5.5170946 5.5926456
0:  5.664031  5.7576137 5.806896  5.9285583 5.8814526 6.016417 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.017856   -4.8116355  -4.3962665  -3.7918859  -2.9971642  -2.16964
0:  -1.3175173  -0.5936899  -0.12143135  0.05361462 -0.17721748 -0.87971306
0:  -1.9232488  -3.243277   -4.6858916  -5.9961343  -7.103083   -7.92259
0:  -9.04933    -9.156736  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.616909 20.843761 21.162035 21.477987 21.802387 22.130867 22.503029
0:  22.867107 23.305996 23.814083 24.337841 24.873524 25.423546 25.940037
0:  26.400879 26.739918 26.983833 27.169163 26.3348   26.526085]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.52907133 -0.50513697 -0.54299164 -0.63538694 -0.7176981  -0.7665534
0:  -0.75213814 -0.71651554 -0.6916475  -0.7371931  -0.87717485 -1.1971159
0:  -1.6030688  -1.9831128  -2.2822194  -2.346105   -2.2087607  -1.9829898
0:  -2.0008445  -2.684029  ]
0: validation loss for strategy=forecast at epoch 1 : 0.27462297677993774
0: validation loss for velocity_u : 0.03501660004258156
0: validation loss for velocity_v : 0.0684545561671257
0: validation loss for specific_humidity : 0.024004854261875153
0: validation loss for velocity_z : 0.48227742314338684
0: validation loss for temperature : 0.0783553421497345
0: validation loss for total_precip : 0.9596287608146667
0: 2 : 17:34:48 :: batch_size = 96, lr = 2e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 2, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6083, 0.6179, 0.6314, 0.6487, 0.6668, 0.6844, 0.6978, 0.7066, 0.7125, 0.7157, 0.7204, 0.7275, 0.7377, 0.7495,
0:         0.7604, 0.7695, 0.7761, 0.7832, 0.6509, 0.6538, 0.6587, 0.6675, 0.6795, 0.6945, 0.7091], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0050, -0.0382, -0.0653, -0.0854, -0.0990, -0.1091, -0.1190, -0.1319, -0.1494, -0.1705, -0.1927, -0.2136,
0:         -0.2302, -0.2438, -0.2584, -0.2752, -0.2940, -0.3088,  0.0202, -0.0190, -0.0578, -0.0914, -0.1194, -0.1412,
0:         -0.1576], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5449, -0.5038, -0.4634, -0.4102, -0.3500, -0.2871, -0.2064, -0.1200, -0.0363,  0.0636,  0.1262,  0.1869,
0:          0.2080,  0.1956,  0.1702,  0.1126,  0.0518, -0.0118, -0.5943, -0.5869, -0.5644, -0.5395, -0.5062, -0.4606,
0:         -0.4175], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1398, -0.1511, -0.1590, -0.1353, -0.1082, -0.0584, -0.0336, -0.0268, -0.0290, -0.0471,  0.0026,  0.0828,
0:          0.2241,  0.4309,  0.5858,  0.6287,  0.5485,  0.4276, -0.1568, -0.1907, -0.2155, -0.1997, -0.1816, -0.1285,
0:         -0.0856], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1481, -0.2003, -0.2555, -0.3043, -0.3489, -0.3869, -0.4185, -0.4574, -0.5017, -0.5547, -0.6068, -0.6438,
0:         -0.6572, -0.6383, -0.6014, -0.5574, -0.5269, -0.5210, -0.5346, -0.5629, -0.5886, -0.6075, -0.6145, -0.6145,
0:         -0.6232], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2468, -0.2421, -0.2409, -0.2398, -0.2315, -0.2421, -0.2468, -0.2480, -0.2480, -0.2480, -0.2480, -0.2468,
0:         -0.2468, -0.2445, -0.2457, -0.2468, -0.2480, -0.2480, -0.2480, -0.2480, -0.2480, -0.2480, -0.2480, -0.2480,
0:         -0.2480], device='cuda:0')
0: [DEBUG] Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2362,     nan,     nan,     nan,     nan, -0.1691,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1915,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2480,     nan,     nan,     nan,     nan,     nan, -0.2480,     nan, -0.2092,     nan,     nan,
0:         -0.1279,     nan,     nan,     nan,     nan, -0.1738,     nan,     nan, -0.2292,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1573,     nan,     nan, -0.2068,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2174,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2398,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2327,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2327,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1938,     nan,     nan,     nan,     nan,
0:             nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2480,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 2, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0307, 0.0415, 0.0560, 0.0696, 0.0836, 0.0977, 0.1109, 0.1214, 0.1339, 0.1434, 0.1526, 0.1579, 0.1635, 0.1697,
0:         0.1786, 0.1869, 0.1961, 0.2041, 0.0412, 0.0553, 0.0689, 0.0822, 0.0955, 0.1098, 0.1226], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0841, -0.0922, -0.1039, -0.1151, -0.1233, -0.1254, -0.1246, -0.1224, -0.1216, -0.1191, -0.1219, -0.1241,
0:         -0.1294, -0.1333, -0.1385, -0.1395, -0.1414, -0.1440, -0.0885, -0.0948, -0.1043, -0.1103, -0.1121, -0.1101,
0:         -0.1076], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2156, -0.2169, -0.2232, -0.2312, -0.2466, -0.2599, -0.2733, -0.2832, -0.2951, -0.3053, -0.3188, -0.3328,
0:         -0.3462, -0.3576, -0.3642, -0.3654, -0.3660, -0.3656, -0.1626, -0.1631, -0.1629, -0.1670, -0.1780, -0.1929,
0:         -0.2047], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4811, 0.4861, 0.4793, 0.4274, 0.3893, 0.3653, 0.3545, 0.3543, 0.3394, 0.3234, 0.3049, 0.2803, 0.2484, 0.2116,
0:         0.1952, 0.1934, 0.2214, 0.2618, 0.5306, 0.5551, 0.5518, 0.5047, 0.4839, 0.4603, 0.4266], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1685, -0.1774, -0.1825, -0.1853, -0.1841, -0.1814, -0.1783, -0.1755, -0.1711, -0.1633, -0.1535, -0.1434,
0:         -0.1365, -0.1317, -0.1293, -0.1246, -0.1189, -0.1071, -0.0890, -0.0631, -0.0318, -0.0005,  0.0265,  0.0460,
0:          0.0564], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1467, -0.1446, -0.1421, -0.1464, -0.1447, -0.1633, -0.1742, -0.1832, -0.1938, -0.1650, -0.1584, -0.1579,
0:         -0.1519, -0.1572, -0.1668, -0.1784, -0.1876, -0.1981, -0.1850, -0.1798, -0.1753, -0.1680, -0.1674, -0.1674,
0:         -0.1801], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.07219412177801132; velocity_v: 0.10472959280014038; specific_humidity: 0.03450727090239525; velocity_z: 0.4862669110298157; temperature: 0.09666033089160919; total_precip: 0.33348265290260315; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07105907797813416; velocity_v: 0.10792825371026993; specific_humidity: 0.03931134566664696; velocity_z: 0.5984954833984375; temperature: 0.08840776234865189; total_precip: 35.819034576416016; 
0: epoch: 2 [1/5 (20%)]	Loss: 18.07626 : 3.12777 :: 0.15877 (2.43 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.057367708534002304; velocity_v: 0.09067273139953613; specific_humidity: 0.0383099727332592; velocity_z: 0.5439894199371338; temperature: 0.1150115579366684; total_precip: 36.72747802734375; 
0: epoch: 2 [2/5 (40%)]	Loss: 36.72748 : 6.23586 :: 0.14908 (16.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07131972163915634; velocity_v: 0.11957819014787674; specific_humidity: 0.045514121651649475; velocity_z: 0.5752038359642029; temperature: 0.10963356494903564; total_precip: 39.8697624206543; 
0: epoch: 2 [3/5 (60%)]	Loss: 39.86976 : 6.77139 :: 0.15711 (16.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07136025279760361; velocity_v: 0.1136694848537445; specific_humidity: 0.04692598432302475; velocity_z: 0.5985982418060303; temperature: 0.1239878386259079; total_precip: 36.57199478149414; 
0: epoch: 2 [4/5 (80%)]	Loss: 36.57199 : 6.22643 :: 0.16292 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 20):
0: [4.76837158e-07 3.33786011e-06 1.28746033e-05 1.90734863e-06
0:  1.43051147e-06 1.43051147e-06 1.04904175e-05 8.10623169e-06
0:  1.43051147e-05 1.90734863e-05 3.09944153e-05 3.76701355e-05
0:  2.38418579e-05 4.14848364e-05 2.71797180e-05 4.43458557e-05
0:  4.05311548e-05 2.81333923e-05 2.00271606e-05 1.95503235e-05]
0: Target values (first 20):
0: [6.7710876e-05 7.0571899e-05 5.3405762e-05 3.3855438e-05 1.9073486e-05
0:  1.8119812e-05 4.0054321e-05 4.1961670e-05 3.5285950e-05 4.2438507e-05
0:  6.1035156e-05 8.9168549e-05 2.3603439e-04 2.8848648e-04 4.2724609e-04
0:  4.9352652e-04 5.1021576e-04 4.4584274e-04 3.0803680e-04 1.7738342e-04]
0: Prediction values (first 20):
0: [-3.9900708 -4.0224957 -3.9985232 -4.024223  -4.0583215 -4.111569
0:  -4.197985  -4.30139   -4.4048257 -4.511971  -4.511867  -4.504747
0:  -4.3733106 -4.1541343 -3.9495006 -3.7190375 -3.47761   -3.3149848
0:  -4.3739715 -4.393746 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.500, max = 0.349, mean = -0.729
0:          sample (first 20): tensor([-0.9175, -0.9202, -0.9182, -0.9203, -0.9231, -0.9275, -0.9347, -0.9432, -0.9518, -0.9606, -0.9606, -0.9600,
0:         -0.9492, -0.9310, -0.9141, -0.8951, -0.8751, -0.8617, -0.9266, -0.9282])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.214121  10.627088   9.990485   9.263296   8.527973   7.7931137
0:   7.050706   6.3004723  5.5799923  4.9030685  4.3522825  3.90419
0:   3.636091   3.5334415  3.5060732  3.5328925  3.5589955  3.5450063
0:   3.4028854  3.1253738]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.816659  -3.942378  -4.042099  -4.2347507 -4.3790703 -4.4181967
0:  -4.3442287 -4.2115717 -4.071538  -4.03757   -4.126219  -4.4186277
0:  -4.773816  -5.050929  -5.2790766 -5.2705574 -5.082717  -4.8503094
0:  -4.8982925 -4.7322226]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.40376663 -0.01097918  0.4541545   0.89045906  1.3402915   1.8109798
0:   2.2189035   2.5672965   2.8762197   3.1341126   3.4155865   3.6488523
0:   3.9592829   4.3019      4.5904226   4.8499527   5.1568055   5.3715158
0:   4.726204    4.8509235 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.685755  -12.881508  -12.927872  -12.89493   -12.885101  -12.949009
0:  -13.057713  -13.269291  -13.473644  -13.697134  -13.848734  -13.985075
0:  -14.006699  -13.920507  -13.728037  -13.541178  -13.417284  -13.335161
0:  -14.3969345 -14.430935 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [45.089306 45.468273 45.739437 45.58536  44.956486 43.878174 42.35313
0:  40.37741  38.193886 35.91623  33.654964 31.511627 29.623337 28.013916
0:  26.580154 25.374046 24.35429  23.6385   23.824783 23.969372]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.532919  -10.596863  -10.513872  -10.357185  -10.154301   -9.978306
0:   -9.847816   -9.8105545  -9.799629   -9.795658   -9.699211   -9.582151
0:   -9.358919   -9.068613   -8.809765   -8.545837   -8.2819805  -8.05962
0:   -8.667938   -8.619465 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.717797  4.4961443 4.344263  4.1806154 4.052724  3.9944453 3.9273562
0:  3.872741  3.7966766 3.691702  3.6216738 3.5093255 3.437388  3.4163032
0:  3.331156  3.2130797 3.0683181 2.8562908 1.5899487 1.2477331]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.9722867 -6.064241  -6.0430913 -5.931582  -5.8155284 -5.7825093
0:  -5.735123  -5.8158584 -5.894154  -5.9624543 -6.0249276 -6.0375686
0:  -5.954622  -5.894093  -5.770576  -5.7371593 -5.8011165 -5.8886533
0:  -5.7119365 -5.7885566]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.1074409e+00 -2.4473057e+00 -1.6807528e+00 -8.7690353e-01
0:   9.1753006e-03  9.2659712e-01  1.9357500e+00  2.9194269e+00
0:   3.8927271e+00  4.8040695e+00  5.6588426e+00  6.4203353e+00
0:   7.1338310e+00  7.8511901e+00  8.5695438e+00  9.3017426e+00
0:   1.0024408e+01  1.0763306e+01  1.1789711e+01  1.2417040e+01]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.0991    9.357684  9.622666  9.7838955 9.898523  9.926222  9.817003
0:  9.515169  9.10251   8.638844  8.256865  7.9983263 7.9344687 8.043452
0:  8.232264  8.450022  8.672381  8.816006  8.544699  8.443713 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3752141  -1.3145251  -1.1777844  -0.9938116  -0.7787404  -0.60415936
0:  -0.38345623 -0.27955675 -0.18547297 -0.13200188 -0.13271761 -0.13399982
0:  -0.09460402 -0.03037548  0.09435225  0.22673798  0.32289362  0.39643383
0:  -0.06512022  0.06352091]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.8696465  7.98183    8.07896    8.182892   8.35618    8.65152
0:   9.047737   9.460587   9.88439   10.255073  10.598501  10.906392
0:  11.245438  11.599148  11.934851  12.193895  12.399421  12.514234
0:  12.379501  12.573883 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.294023  -6.2718215 -6.1905036 -6.0750403 -5.9541297 -5.856729
0:  -5.7809157 -5.799951  -5.8797917 -6.0099998 -6.133612  -6.2662635
0:  -6.32962   -6.37382   -6.434813  -6.5110774 -6.594915  -6.672192
0:  -7.6545777 -7.72535  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.876085 16.971373 17.05196  17.130135 17.201918 17.289282 17.472311
0:  17.548393 17.629307 17.66439  17.644293 17.628134 17.64111  17.696663
0:  17.81447  17.911604 17.977417 18.040258 18.290642 18.444542]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.339359   9.573617   9.831042  10.059172  10.2833805 10.475756
0:  10.674055  10.755503  10.754183  10.619493  10.402031  10.124358
0:   9.859552   9.628411   9.400118   9.189533   8.963314   8.711735
0:   8.039263   7.866577 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.380797  -3.3384042 -3.185203  -3.01679   -2.8116431 -2.6157842
0:  -2.451549  -2.36455   -2.3525348 -2.4090686 -2.4433312 -2.5087075
0:  -2.505669  -2.464806  -2.413011  -2.3630471 -2.324347  -2.3227916
0:  -3.3379207 -3.3670964]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.154797 33.774967 34.260036 34.526234 34.714386 34.914627 35.217514
0:  35.451454 35.713398 35.92219  36.024223 35.979137 35.89369  35.785553
0:  35.70869  35.676167 35.73366  35.78706  35.745342 36.12967 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.7948666 6.855031  6.976493  7.0231276 7.0758142 7.2353907 7.3612065
0:  7.494805  7.587612  7.6013393 7.622235  7.5884805 7.589144  7.628727
0:  7.6463933 7.6399813 7.659163  7.639034  7.678714  7.7697773]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.8410177 5.748611  5.7415466 5.7328215 5.738848  5.697315  5.661018
0:  5.5339894 5.380619  5.2225866 5.1029434 5.032213  5.0612335 5.1174245
0:  5.202546  5.247389  5.1935835 5.089611  4.3968935 4.2611794]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.3763056   1.432405    1.5536251   1.585516    1.5473399   1.3840523
0:   0.96656275  0.31691837 -0.53396606 -1.5355783  -2.5722966  -3.7112527
0:  -4.7871842  -5.7251897  -6.557365   -7.127708   -7.3599553  -7.387975
0:  -8.951933   -8.884441  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3269892 2.4995842 2.7414293 2.944837  3.168829  3.4210603 3.603978
0:  3.7320511 3.7993062 3.8131676 3.850792  3.8438098 3.8966033 4.021207
0:  4.111117  4.2693524 4.5135546 4.726289  4.3405466 4.628807 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.8288436 -5.6554933 -5.4387584 -5.251508  -5.0799994 -4.9309945
0:  -4.8333488 -4.779328  -4.777146  -4.809466  -4.8257813 -4.8983846
0:  -4.9192576 -4.955108  -5.0268626 -5.0996385 -5.1222825 -5.1447387
0:  -6.1536145 -6.08855  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.66564   -10.3335495  -9.927477   -9.428471   -8.845085   -8.246271
0:   -7.635802   -7.1364064  -6.672191   -6.2437634  -5.800157   -5.3649907
0:   -4.875937   -4.3977733  -3.9652257  -3.598888   -3.2223353  -2.84026
0:   -3.404326   -2.835462 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.7180214  5.712945   5.6948533  5.584488   5.462455   5.3205514
0:  5.100592   4.8236437  4.5170064  4.190359   3.8977892  3.545727
0:  3.2159626  2.9115138  2.5589118  2.2583957  2.081863   1.9305992
0:  0.47799063 0.35579348]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.791101 22.73416  22.647297 22.552233 22.48432  22.43179  22.534492
0:  22.552385 22.583977 22.572224 22.44543  22.3046   22.118965 21.890879
0:  21.638689 21.310156 20.91335  20.515568 19.79709  19.565397]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.73975  20.903442 20.97747  20.948132 20.90587  20.961805 21.128174
0:  21.29946  21.460009 21.52296  21.468885 21.315706 21.136604 20.99025
0:  20.86744  20.787748 20.735348 20.64715  19.771338 19.45158 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.865595  -9.759819  -9.564272  -9.280621  -8.981615  -8.726791
0:  -8.432636  -8.265585  -8.126125  -8.007444  -7.9399905 -7.87841
0:  -7.751924  -7.618899  -7.4411635 -7.271662  -7.110946  -6.9049726
0:  -6.3081994 -5.9990563]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.281267   8.378619   8.631763   8.954376   9.43876   10.113395
0:  10.90727   11.681835  12.3632965 12.843535  13.096694  13.083276
0:  12.97485   12.726059  12.3393345 11.816248  11.161857  10.418621
0:   9.540744   9.18394  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.74276  13.800847 13.843819 13.821276 13.785625 13.718988 13.597733
0:  13.346994 13.026442 12.620352 12.177732 11.727266 11.322589 10.989311
0:  10.716754 10.505377 10.387831 10.335989 10.04825  10.118008]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.7580373   1.1132803   0.46028614 -0.25827074 -1.0289044  -1.8214002
0:  -2.51477    -3.2425542  -3.9123921  -4.534557   -5.1441846  -5.7328944
0:  -6.260512   -6.7325044  -7.2009807  -7.664639   -8.121632   -8.4749565
0:  -8.650539   -8.85807   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.057047 23.554728 24.00279  24.341604 24.673302 24.993128 25.445274
0:  25.825735 26.260866 26.680492 27.020235 27.293465 27.56555  27.816885
0:  28.095327 28.378971 28.705128 28.989134 29.050684 29.354984]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.8519993   1.8966126   1.9705591   2.009635    2.0442228   2.0808425
0:   2.1052818   2.0480924   1.9412904   1.7505436   1.5418258   1.2768373
0:   1.0292368   0.79350805  0.5145607   0.22210693 -0.07006979 -0.34626102
0:  -1.2480755  -1.3038602 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3126793  -3.2184906  -3.023326   -2.8507214  -2.6616402  -2.4955997
0:  -2.3784285  -2.322011   -2.276289   -2.2234526  -2.0681639  -1.9084578
0:  -1.6493073  -1.367774   -1.1581898  -0.9593644  -0.7527175  -0.60794306
0:  -1.2015467  -1.0705914 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.7556524 5.44203   5.1322136 4.8129697 4.5067945 4.2284136 3.9569745
0:  3.686287  3.451676  3.2490473 3.1025386 2.9417963 2.8420372 2.765357
0:  2.6584587 2.5398512 2.4187427 2.2750335 1.5073385 1.3695393]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.3633723  4.415864   4.455696   4.43309    4.403245   4.343837
0:  4.2118893  3.9928913  3.707988   3.3722227  3.0300796  2.643858
0:  2.2955303  1.9888177  1.6929922  1.4586673  1.3317418  1.2467012
0:  0.4538355  0.53960466]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.323347 23.02904  22.727636 22.304096 21.862848 21.393316 20.988834
0:  20.582808 20.285261 20.039389 19.842947 19.647366 19.459148 19.27925
0:  19.058653 18.809317 18.525217 18.290516 17.621971 17.565233]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.586484  -10.994054  -11.207761  -11.363254  -11.480847  -11.666144
0:  -11.87021   -12.150403  -12.415927  -12.652043  -12.887324  -13.174518
0:  -13.509084  -13.8210745 -14.175454  -14.437865  -14.560584  -14.525403
0:  -14.604165  -14.38701  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.175057  4.9654007 4.8568745 4.771612  4.7680264 4.8193913 4.975319
0:  5.1312194 5.3219233 5.5244346 5.6714215 5.755372  5.759162  5.728122
0:  5.6303577 5.5139117 5.419868  5.4238167 5.277545  5.2153316]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.485397 19.488249 19.168198 18.527903 17.90496  17.502884 17.410435
0:  17.53077  17.753708 18.041609 18.388832 18.924786 19.75222  20.942818
0:  22.233442 23.391731 24.215948 24.65237  25.27013  24.847656]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.31969  39.196037 40.156532 40.935127 41.60984  42.188328 43.014095
0:  43.718517 44.568005 45.326504 45.911755 46.346493 46.64666  46.85385
0:  46.88166  46.849586 46.631374 46.29501  47.53134  47.710308]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.6788635 5.772233  5.99261   6.255998  6.574379  6.9396586 7.2880306
0:  7.5560355 7.728744  7.8119407 7.8522034 7.829556  7.814453  7.823846
0:  7.798715  7.760378  7.6928606 7.5439863 6.9824557 6.8550534]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3553796 -1.5875459 -1.7679935 -2.029827  -2.3329473 -2.685351
0:  -3.1173692 -3.576264  -3.9927602 -4.3416057 -4.5579104 -4.75211
0:  -4.857507  -4.948066  -5.0859895 -5.2384677 -5.38187   -5.523783
0:  -6.7493567 -6.9539742]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3205113 -3.2925744 -3.2244954 -3.1730952 -3.099296  -3.0004497
0:  -2.8587728 -2.7470298 -2.6326346 -2.5669637 -2.4892058 -2.449078
0:  -2.3790793 -2.3192163 -2.3343186 -2.3926454 -2.4914107 -2.6266208
0:  -3.2021003 -3.1014209]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9674418 4.0449796 4.166666  4.348667  4.5538516 4.7102957 4.9048424
0:  4.9693427 5.018743  5.078679  5.172399  5.3672743 5.59546   5.8448825
0:  6.0743594 6.236308  6.288421  6.289117  6.71511   6.6032963]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.703858 10.01201  10.321875 10.510055 10.684418 10.858456 11.00995
0:  11.113945 11.197511 11.231308 11.241381 11.182658 11.081978 10.956162
0:  10.746343 10.484643 10.223149  9.933184  8.781372  8.663824]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.0476751 2.1943698 2.3776975 2.4975715 2.6225867 2.7657495 2.869657
0:  2.9305305 2.9480891 2.90237   2.8423533 2.7382908 2.6834507 2.6770778
0:  2.6558046 2.6456342 2.6615953 2.6181931 1.9765964 2.1455898]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.3859005 12.388967  12.4293    12.500263  12.635971  12.7821865
0:  13.01174   13.101519  13.161329  13.088343  12.938089  12.759438
0:  12.529036  12.319869  12.124991  11.891338  11.572367  11.248571
0:  11.37961   11.312134 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0849457  -4.28043    -4.3950853  -4.462023   -4.472723   -4.485036
0:  -4.4494696  -4.466036   -4.437975   -4.3638926  -4.211154   -3.9643393
0:  -3.6593761  -3.2696028  -2.7836761  -2.35952    -2.035242   -1.7324619
0:  -0.48811102  0.12726784]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1945462 -2.1469798 -2.0292916 -1.925262  -1.8391967 -1.7601523
0:  -1.7156854 -1.7145648 -1.7451453 -1.8137708 -1.8872304 -1.9986596
0:  -2.0544038 -2.0682206 -2.0695596 -2.0664973 -2.0641265 -2.0892253
0:  -2.5711408 -2.5257516]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.453559  10.720794  10.929361  11.070789  11.1872635 11.286476
0:  11.447541  11.534936  11.614941  11.642462  11.623018  11.559144
0:  11.477362  11.381943  11.261583  11.101199  10.953346  10.806984
0:  10.098613  10.106769 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4004936 -3.4516373 -3.4126654 -3.3591776 -3.2661133 -3.1261034
0:  -2.914791  -2.8197737 -2.642994  -2.573473  -2.5082293 -2.4658594
0:  -2.4140072 -2.3259754 -2.2637463 -2.2573566 -2.354146  -2.4758863
0:  -2.1744833 -2.1980972]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.9703889 -2.0041509 -1.9748297 -1.9812503 -1.9880252 -2.014008
0:  -2.09661   -2.2160873 -2.3732924 -2.5345597 -2.6579862 -2.8202224
0:  -2.9494462 -3.0767083 -3.2705684 -3.4736333 -3.6556087 -3.8409696
0:  -5.3731656 -5.5837626]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1393085 -4.161119  -4.113957  -4.094607  -4.142732  -4.266669
0:  -4.440436  -4.704407  -4.9862514 -5.3027053 -5.612779  -5.9258857
0:  -6.186851  -6.419475  -6.6529946 -6.897639  -7.180552  -7.4648414
0:  -8.33119   -8.334055 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.672543  13.844767  13.921894  13.945138  13.946232  13.985416
0:  14.162077  14.2859745 14.438492  14.562002  14.61289   14.6346855
0:  14.626928  14.626696  14.651739  14.682899  14.746421  14.825034
0:  14.989993  15.182554 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.642136  -8.509663  -8.318447  -8.147736  -7.973682  -7.8155484
0:  -7.7047935 -7.646686  -7.598964  -7.5530305 -7.4274993 -7.2936244
0:  -7.0746455 -6.8074164 -6.5507646 -6.284142  -5.9523263 -5.643396
0:  -5.8610826 -5.5914598]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.30846   -7.2534857 -7.1311684 -6.976697  -6.8270464 -6.705654
0:  -6.601145  -6.5953283 -6.6195946 -6.684965  -6.7362595 -6.7854805
0:  -6.7821636 -6.748974  -6.735099  -6.755639  -6.799368  -6.8657713
0:  -7.8615236 -7.855856 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.5086155 -2.4739532 -2.3261251 -2.1453485 -1.943119  -1.7853069
0:  -1.6694679 -1.6352711 -1.6146803 -1.5828419 -1.5243483 -1.4783478
0:  -1.3984346 -1.3099632 -1.2617931 -1.2095833 -1.152092  -1.1332064
0:  -2.290556  -2.3268347]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7932458 3.0010593 3.3142967 3.6135871 3.9577796 4.359133  4.757785
0:  5.1363444 5.419497  5.543167  5.5749583 5.435839  5.3376923 5.2343082
0:  5.0897083 4.993836  4.9153285 4.765093  2.9165077 2.9893992]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.8672242  1.906507   1.9938788  2.0177493  2.0088887  2.035973
0:  2.026301   1.9870062  1.8996258  1.7552843  1.6131244  1.4225149
0:  1.3033314  1.2393203  1.1755433  1.1091795  1.1000094  1.1023169
0:  0.38573694 0.4361019 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.26862   -10.238262  -10.108423   -9.938179   -9.703775   -9.466341
0:   -9.253429   -9.114761   -9.015743   -8.932993   -8.777407   -8.644529
0:   -8.441486   -8.189281   -8.006559   -7.807263   -7.5854487  -7.4150167
0:   -8.377071   -8.267296 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.65145   8.760494  8.802225  8.699938  8.556967  8.447381  8.369351
0:  8.3025055 8.243806  8.149726  8.0340185 7.8906584 7.7725573 7.6771007
0:  7.579074  7.4916964 7.4476104 7.373752  6.894238  7.0123997]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8206162 -3.5849004 -3.2790065 -3.0353827 -2.8063235 -2.6052508
0:  -2.4842973 -2.44062   -2.44801   -2.4953208 -2.5168605 -2.5851092
0:  -2.5946069 -2.6017241 -2.686441  -2.798448  -2.9058537 -3.064427
0:  -4.480621  -4.504477 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.293377 18.602276 18.861025 19.045486 19.217249 19.405933 19.712004
0:  19.951889 20.242794 20.530426 20.777718 21.03341  21.272152 21.535248
0:  21.838352 22.130112 22.409702 22.668526 23.97697  24.597279]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.27767   11.0519905 11.914619  12.715696  13.523941  14.378905
0:  15.353445  16.246004  17.135395  17.926346  18.563185  19.094967
0:  19.578802  19.935799  20.25221   20.471128  20.58676   20.595936
0:  20.052309  20.0918   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.309566   -0.92322874 -0.33752966  0.15963173  0.5515852   0.89526844
0:   1.1236243   1.3522763   1.6265955   1.9411893   2.4046273   2.8477488
0:   3.3960848   3.9755213   4.46546     4.919917    5.2710705   5.499077
0:   2.8520207   2.4569278 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4533715  -3.2364554  -2.8484254  -2.4157557  -1.9037719  -1.3618407
0:  -0.88900423 -0.53428936 -0.3267274  -0.28696823 -0.34891033 -0.5480323
0:  -0.76074696 -0.96364164 -1.2238536  -1.4557958  -1.6458354  -1.8396239
0:  -3.0892863  -2.987452  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.949116  -9.982042  -9.8521385 -9.624693  -9.353292  -9.103632
0:  -8.92317   -8.874838  -8.8738365 -8.9199295 -8.904486  -8.899916
0:  -8.802061  -8.683939  -8.617804  -8.537231  -8.421808  -8.30073
0:  -9.487865  -9.457788 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.5090823 -7.4364095 -7.203448  -6.94418   -6.6695104 -6.4211116
0:  -6.264793  -6.1916966 -6.179109  -6.223329  -6.2534785 -6.340074
0:  -6.395432  -6.408523  -6.457216  -6.464906  -6.431372  -6.4119964
0:  -7.5711637 -7.486481 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.381535  13.431879  13.513114  13.519416  13.511648  13.533314
0:  13.613312  13.689665  13.779547  13.865015  13.931307  13.972147
0:  14.036432  14.129114  14.22563   14.2989235 14.351732  14.33938
0:  14.568478  14.502417 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.465075 21.230642 22.161865 22.991241 23.76604  24.551094 25.215738
0:  25.722122 26.021435 26.02892  25.821632 25.38769  24.972769 24.660849
0:  24.454819 24.386002 24.466694 24.519403 23.83096  24.137768]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6495528 -2.5838017 -2.422463  -2.2648816 -2.0718818 -1.8830533
0:  -1.7361832 -1.6791549 -1.6841464 -1.7640042 -1.8379807 -1.9407325
0:  -2.0013928 -2.0388503 -2.1268935 -2.2544246 -2.4185061 -2.6146998
0:  -3.7242227 -3.77099  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.050192 24.178822 24.276676 24.241972 24.152681 24.04629  23.954697
0:  23.795084 23.61561  23.357527 22.997032 22.548409 22.07817  21.68459
0:  21.326159 21.044933 20.864952 20.709099 20.35715  20.15969 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.251665 22.4205   22.561806 22.650011 22.70064  22.732897 22.845285
0:  22.851135 22.804092 22.657394 22.375288 22.01943  21.61928  21.20354
0:  20.786705 20.35354  19.885351 19.407393 18.771933 18.585716]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.4211843   1.7700429   1.2116942   0.61300564  0.09449816 -0.33999872
0:  -0.84477234 -1.4241977  -2.0646176  -2.7868018  -3.432424   -4.046166
0:  -4.399467   -4.5503926  -4.550879   -4.459679   -4.348066   -4.317678
0:  -5.056723   -5.4164205 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0210266 -3.7820253 -3.5267553 -3.45084   -3.5101871 -3.7081375
0:  -4.0270176 -4.4781256 -4.9689474 -5.4830656 -5.9522343 -6.4138174
0:  -6.7577715 -6.9651394 -7.0909348 -7.062643  -6.899167  -6.623171
0:  -7.006693  -6.4677076]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.9630747 6.0008698 6.0355563 6.0266795 6.0071015 5.9581747 5.9459524
0:  5.827851  5.695173  5.539671  5.3212757 5.082565  4.8575473 4.6585817
0:  4.50955   4.397521  4.3028607 4.2515497 4.677313  4.855352 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.732281 10.822398 10.944842 11.022877 11.155002 11.342277 11.509536
0:  11.624605 11.710001 11.77952  11.868031 11.946522 12.056115 12.181174
0:  12.274036 12.338898 12.417527 12.41744  11.556459 11.378172]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.8503451   3.1758327   3.5706108   3.9004283   4.1821737   4.4332857
0:   4.601181    4.6928806   4.696983    4.6115904   4.466383    4.214299
0:   3.957799    3.6731956   3.2857072   2.8355765   2.373569    1.8647423
0:  -0.33984756 -0.6586561 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [49.24592  49.553852 49.429752 48.87187  48.017166 46.96859  45.92949
0:  44.655262 43.3318   41.86893  40.299236 38.777607 37.453438 36.459457
0:  35.803436 35.366177 34.977848 34.500023 34.676903 34.318546]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.7317414 -6.70775   -6.509578  -6.2414947 -5.9103656 -5.563313
0:  -5.25876   -5.03245   -4.8778834 -4.7911973 -4.6886916 -4.6400123
0:  -4.5351415 -4.3964257 -4.3123164 -4.2364745 -4.1614985 -4.1236796
0:  -5.368234  -5.3238273]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.008968 17.11244  17.191916 17.270824 17.332603 17.46188  17.770338
0:  17.997662 18.31168  18.620848 18.877956 19.168907 19.48634  19.872765
0:  20.327904 20.770683 21.210077 21.654861 23.095415 23.456308]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.6484993 3.764732  3.8839004 3.9466412 4.039118  4.1299314 4.2046747
0:  4.221506  4.2275105 4.2153654 4.2557793 4.3067827 4.4236946 4.6086082
0:  4.785125  4.94557   5.109949  5.2388225 4.8395386 5.0449324]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.5974455 7.7424903 7.8635793 7.911954  7.9525213 8.0433445 8.150868
0:  8.240392  8.326488  8.384622  8.444104  8.484949  8.570185  8.697561
0:  8.831052  8.96588   9.117806  9.232053  9.087336  9.257476 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.752962   -8.844852   -8.840935   -8.8961525  -9.028254   -9.34127
0:   -9.868559  -10.575855  -11.320757  -12.028799  -12.458502  -12.760363
0:  -12.789951  -12.56205   -12.245298  -11.811142  -11.3129635 -10.900372
0:  -12.437494  -12.486063 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.915984  6.7237515 6.6210594 6.5481815 6.5076003 6.49744   6.573801
0:  6.564487  6.505439  6.3720207 6.125544  5.820067  5.4840994 5.12776
0:  4.8454227 4.570943  4.3104753 4.1362376 4.097622  4.1133957]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.404281  10.473347  10.491449  10.403494  10.27146   10.176747
0:  10.136116  10.058304   9.969133   9.812292   9.556438   9.220202
0:   8.882648   8.572566   8.28785    8.024653   7.79008    7.5367527
0:   7.290153   7.360844 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.951982  8.643737  8.312109  7.933635  7.6894765 7.641966  7.7446384
0:  7.943685  8.17334   8.370102  8.47722   8.389858  8.119834  7.6441975
0:  7.019579  6.380263  5.9142275 5.631621  5.6885867 6.8381963]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.306918  4.1640763 4.03493   3.895144  3.7412937 3.5693102 3.4229221
0:  3.2273717 3.0612946 2.910867  2.7858508 2.6808665 2.5971775 2.5373054
0:  2.472856  2.3824542 2.2771876 2.1681652 1.6931362 1.5406199]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.994195  12.747048  12.526649  12.296777  12.049105  11.827721
0:  11.705719  11.501756  11.297438  11.015444  10.629377  10.176468
0:   9.75468    9.377738   9.004086   8.667196   8.32959    8.040681
0:   7.2212033  6.978889 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.371035 22.757195 23.001768 23.105936 23.163177 23.21723  23.401125
0:  23.540487 23.741335 23.904802 24.00385  24.05636  24.104565 24.09417
0:  24.060125 24.015072 23.961828 23.835106 23.946419 24.177702]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.483955 24.477676 24.434612 24.405388 24.470062 24.580128 24.992126
0:  25.282684 25.643242 25.946575 26.093254 26.191662 26.211384 26.230091
0:  26.26162  26.271587 26.211235 26.071983 27.45649  27.408707]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.16498   -10.212568  -10.125986   -9.9330845  -9.690994   -9.505547
0:   -9.297401   -9.2209     -9.199741   -9.211891   -9.264519   -9.310154
0:   -9.287539   -9.226059   -9.152016   -9.043982   -8.9457     -8.889753
0:  -10.170622  -10.172127 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5661826  1.5683675  1.5223875  1.3939633  1.2817202  1.2186832
0:  1.1786313  1.1605759  1.1723781  1.1889424  1.2601233  1.3109312
0:  1.3976464  1.484407   1.488039   1.4486623  1.4402509  1.4019451
0:  0.12172794 0.05181694]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.30516624 0.62872314 1.0320525  1.3871937  1.7174859  1.9978051
0:  2.179885   2.2334905  2.1597648  1.9860897  1.7663407  1.5087962
0:  1.2974501  1.1358938  1.0204625  0.97512865 1.0232334  1.090663
0:  0.64202356 0.8729873 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.697937 18.051548 18.282019 18.40134  18.527536 18.673958 18.998886
0:  19.29647  19.598408 19.870293 20.017857 20.084194 20.025902 19.889053
0:  19.658783 19.396013 19.114433 18.854412 17.787422 17.671406]
0: validation loss for strategy=forecast at epoch 2 : 6.2431182861328125
0: validation loss for velocity_u : 0.03383911773562431
0: validation loss for velocity_v : 0.05437736213207245
0: validation loss for specific_humidity : 0.02509433776140213
0: validation loss for velocity_z : 0.4637589752674103
0: validation loss for temperature : 0.09500115364789963
0: validation loss for total_precip : 36.7866325378418
0: 3 : 17:38:49 :: batch_size = 96, lr = 1.9512195121951222e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 3, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7039, -0.7184, -0.7328, -0.7476, -0.7638, -0.7819, -0.8025, -0.8255, -0.8502, -0.8752, -0.8996, -0.9222,
0:         -0.9428, -0.9616, -0.9796, -0.9974, -1.0159, -1.0352, -0.7741, -0.7920, -0.8098, -0.8275, -0.8452, -0.8637,
0:         -0.8836], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3221, -0.3309, -0.3402, -0.3484, -0.3545, -0.3581, -0.3599, -0.3607, -0.3618, -0.3644, -0.3692, -0.3756,
0:         -0.3838, -0.3923, -0.4005, -0.4072, -0.4110, -0.4117, -0.4177, -0.4311, -0.4432, -0.4529, -0.4596, -0.4633,
0:         -0.4648], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0334, -0.0786, -0.1186, -0.1610, -0.2033, -0.2405, -0.2847, -0.3287, -0.3725, -0.3889, -0.4051, -0.4321,
0:         -0.4410, -0.4462, -0.4515, -0.4533, -0.4533, -0.4532, -0.1540, -0.1962, -0.2321, -0.2639, -0.2976, -0.3412,
0:         -0.3781], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2910,  0.2688,  0.2299,  0.1822,  0.1356,  0.0968,  0.0735,  0.0690,  0.0835,  0.1079,  0.1301,  0.1345,
0:          0.1156,  0.0735,  0.0158, -0.0419, -0.0841, -0.1041,  0.1600,  0.1334,  0.0968,  0.0590,  0.0213, -0.0120,
0:         -0.0431], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.6955, 1.6971, 1.6987, 1.6997, 1.7016, 1.7037, 1.7056, 1.7070, 1.7078, 1.7066, 1.7052, 1.7020, 1.6990, 1.6955,
0:         1.6911, 1.6863, 1.6804, 1.6730, 1.6638, 1.6521, 1.6374, 1.6204, 1.6008, 1.5797, 1.5593], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621,
0:         -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621, -0.2621,
0:         -0.2609], device='cuda:0')
0: [DEBUG] Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.6616,     nan,     nan,     nan,     nan,
0:             nan,  0.0860,     nan,     nan,     nan,  0.3065,     nan,     nan,     nan, -0.1855,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2458,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2621,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.1336,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0985,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan, -0.2493,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2574,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2539,     nan, -0.2551,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2435,
0:         -0.2365,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0930,  0.1093,     nan,
0:             nan,  0.0013,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0802,     nan,     nan, -0.2435,     nan, -0.2528,     nan, -0.2470,     nan,     nan,     nan, -0.2458,
0:             nan, -0.2563,     nan, -0.2586,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2597,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2516,     nan, -0.2563, -0.2551,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 3, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5021, 0.5162, 0.5257, 0.5243, 0.5206, 0.5221, 0.5251, 0.5271, 0.5262, 0.5230, 0.5189, 0.5120, 0.5141, 0.5210,
0:         0.5344, 0.5500, 0.5698, 0.5869, 0.4660, 0.4823, 0.4917, 0.4927, 0.4867, 0.4860, 0.4906], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2703,  0.2153,  0.1486,  0.0711, -0.0080, -0.0873, -0.1638, -0.2323, -0.2982, -0.3554, -0.4101, -0.4583,
0:         -0.5053, -0.5519, -0.5941, -0.6247, -0.6313, -0.6085,  0.2219,  0.1769,  0.1226,  0.0595, -0.0061, -0.0660,
0:         -0.1282], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1452,  0.1335,  0.1177,  0.1017,  0.0779,  0.0568,  0.0290,  0.0076, -0.0139, -0.0290, -0.0404, -0.0495,
0:         -0.0577, -0.0618, -0.0685, -0.0775, -0.0887, -0.1054,  0.0919,  0.0812,  0.0725,  0.0606,  0.0435,  0.0210,
0:          0.0003], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1988, 0.3295, 0.4586, 0.4914, 0.5047, 0.5547, 0.6105, 0.6304, 0.6926, 0.7942, 0.8687, 0.9695, 1.0157, 0.9981,
0:         1.0445, 1.2202, 1.5215, 1.7131, 0.3004, 0.3929, 0.5046, 0.4936, 0.4781, 0.5223, 0.5623], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.3572, 0.3703, 0.3869, 0.4015, 0.4189, 0.4407, 0.4685, 0.5010, 0.5335, 0.5634, 0.5919, 0.6195, 0.6499, 0.6830,
0:         0.7159, 0.7479, 0.7785, 0.8067, 0.8329, 0.8562, 0.8745, 0.8876, 0.8972, 0.9065, 0.9192], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([7.0153, 7.1256, 6.9291, 6.7161, 6.2612, 5.5851, 4.9448, 4.2334, 3.5549, 6.2297, 6.1702, 5.9455, 5.6288, 5.1514,
0:         4.5471, 4.0420, 3.5712, 2.9827, 4.7842, 4.5538, 4.2227, 3.8227, 3.4339, 3.0928, 2.7252], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.0628109946846962; velocity_v: 0.1055164784193039; specific_humidity: 0.04287950322031975; velocity_z: 0.6053509712219238; temperature: 0.10258501768112183; total_precip: 38.16616439819336; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06256099045276642; velocity_v: 0.10158388316631317; specific_humidity: 0.04707734286785126; velocity_z: 0.6067934632301331; temperature: 0.11482427269220352; total_precip: 38.719974517822266; 
0: epoch: 3 [1/5 (20%)]	Loss: 38.44307 : 6.53513 :: 0.15013 (2.56 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05474662408232689; velocity_v: 0.087972491979599; specific_humidity: 0.037230756133794785; velocity_z: 0.4481157958507538; temperature: 0.08534769713878632; total_precip: 34.04988098144531; 
0: epoch: 3 [2/5 (40%)]	Loss: 34.04988 : 5.76885 :: 0.14605 (16.54 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.060474563390016556; velocity_v: 0.09999186545610428; specific_humidity: 0.039520252496004105; velocity_z: 0.4762793481349945; temperature: 0.10851847380399704; total_precip: 34.170658111572266; 
0: epoch: 3 [3/5 (60%)]	Loss: 34.17066 : 5.79990 :: 0.15051 (16.43 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.058051638305187225; velocity_v: 0.10100623965263367; specific_humidity: 0.042824793606996536; velocity_z: 0.5612171292304993; temperature: 0.10757240653038025; total_precip: 36.57309341430664; 
0: epoch: 3 [4/5 (80%)]	Loss: 36.57309 : 6.21425 :: 0.14935 (16.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 20):
0: [5.2452087e-06 3.8146973e-06 3.8146973e-06 4.7683716e-07 0.0000000e+00
0:  0.0000000e+00 0.0000000e+00 5.2452087e-06 1.4305115e-05 2.9087067e-05
0:  9.9658966e-05 9.1075897e-05 1.2826920e-04 1.3065338e-04 9.5844269e-05
0:  5.2452087e-05 3.3855438e-05 3.5285950e-05 1.3351440e-05 3.8146973e-06]
0: Target values (first 20):
0: [3.8146973e-06 2.3841858e-06 9.5367432e-07 9.5367432e-07 9.5367432e-07
0:  2.3841858e-06 5.2452087e-06 5.7220459e-06 6.6757202e-06 7.6293945e-06
0:  1.0967255e-05 2.3841858e-05 3.7193298e-05 5.3405762e-05 9.1552734e-05
0:  8.5353851e-05 8.3923340e-05 7.7724457e-05 4.9591061e-05 4.1484833e-05]
0: Prediction values (first 20):
0: [17.823956  17.866558  17.824772  17.701147  17.566883  17.427422
0:  17.511961  17.515066  17.516468  17.496458  17.307089  17.0916
0:  16.740866  16.341421  15.997452  15.656553  15.3621235 15.125483
0:  16.297564  16.278738 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -0.755, max = 3.818, mean = 1.156
0:          sample (first 20): tensor([0.9740, 0.9777, 0.9741, 0.9635, 0.9520, 0.9401, 0.9473, 0.9476, 0.9477, 0.9460, 0.9298, 0.9114, 0.8813, 0.8471,
0:         0.8177, 0.7885, 0.7633, 0.7430, 0.9889, 0.9981])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.836046  13.02033   13.212979  13.36455   13.489126  13.539007
0:  13.7333355 13.821047  13.947642  14.10392   14.24184   14.465363
0:  14.657156  14.863874  15.074474  15.201168  15.194986  15.067175
0:  15.409355  15.3041725]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.806683 15.913582 16.054266 16.14326  16.21842  16.322456 16.428865
0:  16.511095 16.566996 16.592768 16.59078  16.563456 16.63009  16.717161
0:  16.837133 16.927256 17.071264 17.249317 17.316393 17.463846]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.872726 19.602285 19.28909  18.839533 18.314768 17.690485 17.160576
0:  16.539482 15.927169 15.344239 14.712172 14.160183 13.648263 13.233213
0:  12.921072 12.660184 12.413677 12.142443 11.972293 11.246115]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.551183  -10.735829  -10.7765    -10.792569  -10.790063  -10.747579
0:  -10.7777605 -10.791112  -10.823642  -10.894776  -10.912101  -11.004209
0:  -10.926013  -10.71621   -10.414187   -9.97424    -9.5062685  -9.074461
0:   -9.174954   -8.867481 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.319489 13.061556 12.859303 12.665432 12.457088 12.206342 12.047598
0:  11.806377 11.563431 11.315804 10.997854 10.702534 10.443737 10.200971
0:  10.022669  9.86328   9.714975  9.59099  10.266532 10.254617]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.021137 20.127386 20.209253 20.109745 19.998035 20.010529 20.175797
0:  20.510904 20.912699 21.281029 21.602684 21.723612 21.867268 21.974133
0:  22.007568 21.992277 21.871851 21.61548  20.64281  20.57421 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.8590064 -2.6233916 -2.2557158 -1.9228044 -1.6349702 -1.4549279
0:  -1.3765588 -1.3386378 -1.3524442 -1.3907657 -1.4031291 -1.507463
0:  -1.5039887 -1.4509139 -1.3552108 -1.1636062 -0.9710145 -0.8198786
0:  -2.1231875 -1.9807448]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.729843 26.124683 26.618305 27.195724 28.018402 29.125515 30.564754
0:  32.083183 33.662354 35.14317  36.33234  37.12576  37.54149  37.47511
0:  37.058907 36.264526 35.32225  34.304848 34.470478 34.41933 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.59066  20.4705   20.25959  19.977507 19.698975 19.345987 19.152918
0:  18.787647 18.388657 17.968943 17.46816  17.060478 16.613699 16.211256
0:  15.810347 15.387053 14.891037 14.369761 15.01163  14.890982]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.331366 11.231309 11.337927 11.418948 11.566904 11.683601 11.799715
0:  11.91426  11.973226 12.035528 12.042937 11.968153 11.882698 11.786627
0:  11.649231 11.536779 11.408321 11.207998  9.443661  9.307476]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6539464  -1.5746684  -1.4152322  -1.3081813  -1.2247243  -1.1875339
0:  -1.2161098  -1.2489638  -1.2963576  -1.342711   -1.34658    -1.3876023
0:  -1.3383431  -1.1955314  -1.0057392  -0.725997   -0.38102436 -0.08273458
0:  -0.9748845  -0.7331538 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.097951  9.241673  9.412648  9.52961   9.605791  9.676885  9.811956
0:   9.950947 10.095011 10.273724 10.428653 10.544793 10.663261 10.753124
0:  10.826147 10.877025 10.915218 10.873605 10.445721 10.419366]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9598975 -4.722221  -4.379211  -4.0921884 -3.8714852 -3.7198567
0:  -3.737327  -3.7683158 -3.8961425 -4.0707994 -4.235911  -4.522245
0:  -4.700744  -4.781591  -4.7855487 -4.6382775 -4.4160743 -4.2465014
0:  -5.8689227 -5.5918355]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.281742  13.203265  13.10417   12.907373  12.664673  12.366875
0:  12.095711  11.719788  11.355146  10.978815  10.633595  10.333511
0:  10.112141  10.009267   9.965675   9.961916   9.931858   9.880429
0:   9.5928135  9.694231 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.1347275 10.257537  10.41494   10.548049  10.623428  10.632273
0:  10.704855  10.702181  10.719718  10.741242  10.705212  10.665339
0:  10.617667  10.5892935 10.580544  10.545072  10.500061  10.441562
0:  10.307002  10.369705 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 2.5563493  2.6526818  2.8765817  3.1005743  3.3182993  3.5387144
0:   3.701361   3.8353012  3.9067986  3.9768376  4.0621705  3.983816
0:   3.7665527  3.2508378  2.2975764  1.060638  -0.3451743 -1.6399531
0:  -4.8508544 -5.089373 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.590688  5.6956763 5.88157   6.0137    6.1464825 6.238126  6.3235865
0:  6.387947  6.435032  6.448792  6.4370875 6.364385  6.31242   6.3024354
0:  6.3143873 6.3690567 6.449028  6.479347  5.987311  6.065648 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.370237  9.656768  9.804409  9.717475  9.450901  9.038434  8.507773
0:  7.922047  7.3228946 6.739597  6.204327  5.7188783 5.330514  5.042842
0:  4.837212  4.697684  4.58047   4.458499  3.8268683 3.9102023]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.55971575  0.7621212   1.0874724   1.3954735   1.6472917   1.8105059
0:   1.8757887   1.8671179   1.7816334   1.6465907   1.4855781   1.2403998
0:   1.0617642   0.9606252   0.92955446  0.99348545  1.1070414   1.1718946
0:  -0.02116299  0.10984278]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.009423  11.148052  11.299581  11.373351  11.399769  11.379391
0:  11.480463  11.518212  11.611113  11.751662  11.825003  11.92947
0:  12.016142  12.105412  12.1965885 12.269282  12.299799  12.313139
0:  12.766933  12.949485 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.108992  -6.178674  -6.0563655 -5.885636  -5.7337327 -5.661959
0:  -5.660873  -5.732352  -5.8922324 -6.0557055 -6.212744  -6.4123473
0:  -6.5597844 -6.644053  -6.6679635 -6.6314425 -6.6140943 -6.6517987
0:  -7.686894  -7.769572 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3373628  -2.780684   -2.0944037  -1.3188128  -0.4761653   0.35440588
0:   1.2896795   2.115007    2.9596133   3.7623932   4.548181    5.2938104
0:   6.0404687   6.7432437   7.404523    8.025524    8.623287    9.195986
0:   9.101129    9.476728  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.3268633  -0.15257692  0.00672817  0.06776237  0.10994291  0.10381079
0:   0.12137556  0.08165121  0.08039331  0.14396429  0.32518148  0.65733147
0:   1.1190419   1.6677194   2.1280024   2.4273682   2.486591    2.3138568
0:  -0.8532162  -1.5383549 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-27.507668 -26.988758 -26.284313 -25.481297 -24.619156 -23.783976
0:  -23.007692 -22.319124 -21.756021 -21.346094 -21.000399 -20.891388
0:  -20.718847 -20.486433 -20.242981 -19.890293 -19.51575  -19.183853
0:  -22.597452 -22.32837 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.552382 18.437666 18.34613  18.057127 17.66631  17.228605 16.91297
0:  16.606594 16.413717 16.354404 16.334248 16.44009  16.636988 16.910025
0:  17.251879 17.546526 17.810131 18.040174 18.673012 18.987158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.3405743   1.0836654   0.93981934  0.7890372   0.6789813   0.5662675
0:   0.37816477  0.16637897 -0.14153671 -0.5061116  -0.8658786  -1.294929
0:  -1.6077619  -1.8083138  -1.9915681  -2.1205163  -2.2954316  -2.5845456
0:  -5.179485   -5.426335  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.0754266  7.1862836  7.496191   7.8652363  8.320697   8.725792
0:   9.081842   9.376265   9.61557    9.831672  10.035504  10.14214
0:  10.250929  10.342606  10.409349  10.503913  10.573369  10.590337
0:  10.170634  10.30754  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.8679705 4.1345577 4.4772787 4.7134113 4.8816557 5.0167828 5.074432
0:  5.1323986 5.110447  4.9954605 4.8199625 4.4801035 4.2012367 4.0092163
0:  3.8816822 3.8680494 3.8890636 3.772766  1.778862  1.4946404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.1523    10.392866  10.621347  10.748487  10.807699  10.819488
0:  10.949631  11.020751  11.12601   11.240458  11.30801   11.371771
0:  11.42701   11.447638  11.493216  11.49731   11.481268  11.4651985
0:  11.476296  11.643747 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.0999246 -3.9926276 -3.7886767 -3.6263175 -3.5161767 -3.4513583
0:  -3.4896226 -3.5423741 -3.6651893 -3.8410287 -3.999125  -4.2221513
0:  -4.3359447 -4.3465495 -4.2997193 -4.159987  -4.0273204 -3.9697042
0:  -5.7482657 -5.682363 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.114344 28.364103 28.505646 28.511223 28.481096 28.385866 28.49652
0:  28.445433 28.40808  28.33563  28.07279  27.822672 27.476053 27.184889
0:  26.94176  26.667917 26.35526  25.955154 26.378532 26.021736]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.9645886 5.8814178 5.968348  6.0887914 6.200616  6.253366  6.199526
0:  6.0668344 5.8557663 5.622818  5.3618402 5.0469236 4.791279  4.614975
0:  4.5168176 4.527567  4.6268835 4.686245  3.5256748 3.4430268]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.5999241 1.0310168 1.4732037 1.7922578 2.0400538 2.26688   2.4494438
0:  2.6017613 2.7077513 2.7174835 2.6932638 2.5727978 2.5230248 2.507005
0:  2.522944  2.610806  2.7408464 2.8178208 1.8160954 2.1980007]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.197319  10.305752  10.41761   10.504912  10.5932045 10.654753
0:  10.852958  10.961295  11.116603  11.277362  11.387817  11.507086
0:  11.629999  11.81658   12.047989  12.286352  12.536701  12.777153
0:  13.098436  13.274412 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.545792 11.721409 12.018469 12.374735 12.784027 13.20237  13.769222
0:  14.24869  14.745199 15.285421 15.783976 16.318869 16.912745 17.488005
0:  18.13653  18.724148 19.223045 19.602173 20.795359 21.693249]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.5389557 0.6940079 0.9436393 1.1856971 1.4146533 1.6327109 1.8703744
0:  2.1043303 2.31583   2.4864683 2.6214595 2.6514125 2.690031  2.7211728
0:  2.7372837 2.7550914 2.7389755 2.645575  1.3945627 1.3392248]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.404047 22.986132 23.433437 23.673983 23.893925 24.022324 24.420904
0:  24.697634 25.136944 25.529268 25.73994  26.014679 26.08033  26.118368
0:  26.159504 26.063221 25.7487   25.17234  23.29144  22.936064]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.0945177 -5.931075  -5.612727  -5.305743  -5.0575533 -4.962186
0:  -5.0058184 -5.1595697 -5.36804   -5.6006136 -5.777068  -5.943246
0:  -5.993864  -5.940098  -5.823531  -5.6136427 -5.3754625 -5.1650934
0:  -6.0971966 -5.9402494]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.723771  -4.570225  -4.2884655 -4.0133047 -3.799961  -3.661189
0:  -3.6450157 -3.6651106 -3.762827  -3.8955736 -4.0312433 -4.2734394
0:  -4.4174356 -4.5070333 -4.5366516 -4.4499855 -4.2965136 -4.1847076
0:  -5.747123  -5.661474 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.064541  -5.073704  -4.90526   -4.7833414 -4.7811565 -4.9831266
0:  -5.3021994 -5.6661615 -5.9906616 -6.177986  -6.2192583 -6.16852
0:  -5.977132  -5.7204437 -5.3958015 -5.0418825 -4.723522  -4.4520826
0:  -4.40279   -4.214448 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.856711  7.1594725 7.484536  7.684471  7.844314  7.970083  8.096355
0:  8.227515  8.352247  8.472838  8.583595  8.653173  8.76184   8.928893
0:  9.109094  9.324604  9.575724  9.772506  9.512212  9.831097 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.489777  14.682848  14.832548  14.972401  15.1454735 15.264421
0:  15.539417  15.67606   15.801899  15.929569  16.004831  16.18753
0:  16.33887   16.521696  16.714142  16.788338  16.701647  16.539145
0:  16.658718  16.733004 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.957285  4.0748034 4.2887225 4.48572   4.711183  4.8912983 5.073234
0:  5.221101  5.323053  5.4077234 5.48419   5.489104  5.5562086 5.6191607
0:  5.7293253 5.874329  6.0393567 6.172396  5.2689013 5.3637857]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.017858   9.169783   9.298756   9.425384   9.6381035  9.973295
0:  10.47793   11.039056  11.5927305 12.061861  12.420856  12.591036
0:  12.674978  12.655384  12.505267  12.249159  11.940731  11.5334835
0:  10.179318  10.098524 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -6.066766   -5.8501596  -5.4853907  -5.1961956  -5.1106806  -5.3015594
0:   -5.7428017  -6.3810015  -7.0871115  -7.8025475  -8.448959   -9.11598
0:   -9.622775   -9.959424  -10.17291   -10.233855  -10.258914  -10.340249
0:  -11.89857   -12.0449   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.503281  12.778303  13.0387535 13.193184  13.292938  13.360484
0:  13.492813  13.577471  13.685122  13.799671  13.909188  14.027971
0:  14.226221  14.510809  14.876397  15.277436  15.681965  15.986617
0:  16.109253  16.21786  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.816836  -7.515393  -7.0557446 -6.599727  -6.184014  -5.8275332
0:  -5.594688  -5.4025006 -5.261928  -5.137403  -4.973473  -4.882355
0:  -4.693812  -4.4604955 -4.2139754 -3.8744845 -3.50559   -3.1999736
0:  -4.399999  -3.9175906]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4042015  -1.2107525  -0.97875786 -0.82899904 -0.6732602  -0.4545331
0:  -0.2044859   0.11210728  0.38763952  0.554533    0.6806841   0.64526176
0:   0.70526266  0.8975568   1.1408854   1.543448    1.9690652   2.2590313
0:   1.1500654   2.0188909 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [37.112385 36.966793 36.647953 36.212814 35.802986 35.368954 35.218857
0:  34.933277 34.702103 34.44057  34.06533  33.80213  33.526787 33.372223
0:  33.155807 32.83654  32.305782 31.61867  32.83402  32.791557]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.717062 24.940691 25.228765 25.404337 25.604284 25.833118 26.261131
0:  26.584766 26.978157 27.34654  27.61903  27.901756 28.233997 28.657955
0:  29.202675 29.723722 30.277563 30.727232 32.22156  32.58281 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.1632714  -1.831676   -1.3973508  -0.99266243 -0.6405916  -0.3701043
0:  -0.1704216  -0.0462532   0.0091815   0.01943207  0.02368116 -0.02875042
0:   0.01047277  0.11375284  0.2571149   0.47095633  0.6920228   0.8291664
0:  -0.07787609  0.19732285]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.627007 27.522888 27.476326 27.485607 27.591099 27.790276 28.329361
0:  28.786804 29.369148 30.036003 30.63356  31.410213 32.299404 33.290462
0:  34.39926  35.38605  36.201366 36.76469  37.462006 38.671574]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.936007 17.838478 17.770555 17.72735  17.767675 17.821108 18.107521
0:  18.224903 18.392311 18.520142 18.532932 18.613674 18.661613 18.721651
0:  18.74599  18.746578 18.681957 18.539036 19.475994 19.46085 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.904687    7.012933    7.142345    7.03045     6.6808195   6.140751
0:   5.4144607   4.676096    3.9057949   3.133844    2.3903584   1.5572314
0:   0.8205428   0.14171886 -0.46504164 -0.8975177  -1.2289271  -1.4438167
0:  -2.7842202  -2.5680604 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [38.2216   39.560467 40.628773 41.36642  41.79313  41.962223 42.234364
0:  42.277935 42.292557 42.231625 42.02924  41.785404 41.542793 41.39752
0:  41.36719  41.37051  41.398335 41.37894  43.078957 43.12439 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.589789  -2.2986207 -1.9913478 -1.8234649 -1.7759624 -1.7866931
0:  -1.9200664 -1.9901805 -2.0653338 -2.1149812 -2.0973477 -2.1962943
0:  -2.1750474 -2.0829244 -1.9296756 -1.6449008 -1.2932839 -1.0023985
0:  -2.7247615 -2.4411273]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.9045439  4.0878086  4.5692205  5.104066   5.6937633  6.1544094
0:   6.3949475  6.4281726  6.2044063  5.830017   5.3574877  4.687321
0:   4.008263   3.2865     2.5484629  1.8761454  1.2758918  0.6549158
0:  -2.9870973 -3.3828626]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.949934  10.946113  10.9706135 10.914709  10.794497  10.657688
0:  10.538895  10.44995   10.37542   10.326246  10.265186  10.110917
0:   9.932387   9.719801   9.507103   9.348305   9.298338   9.337053
0:   8.769163   9.039413 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.527689 12.823509 12.957312 12.910303 12.729683 12.55389  12.583731
0:  12.668697 12.860048 13.056476 13.188086 13.241634 13.29025  13.295504
0:  13.270983 13.198078 13.066445 12.880175 12.329879 12.281311]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.51468  -10.342559 -10.005481  -9.681797  -9.435973  -9.300521
0:   -9.395383  -9.561833  -9.84147  -10.152541 -10.35604  -10.676963
0:  -10.842988 -10.888293 -10.837319 -10.559346 -10.168809  -9.747171
0:  -12.099343 -11.887481]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4158545 1.5908418 1.7238903 1.7788296 1.7715878 1.7238379 1.690526
0:  1.603096  1.5045509 1.3907914 1.2668433 1.1459675 1.1101203 1.1955957
0:  1.4017563 1.706285  2.078298  2.4158466 2.9243245 3.3049598]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.51128    8.718264   8.94928    9.193586   9.470536   9.756809
0:  10.208453  10.621265  11.044439  11.417276  11.658949  11.818415
0:  11.864048  11.835511  11.7553005 11.620588  11.473053  11.358019
0:  11.049463  11.1969795]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.052498 32.39102  32.63172  32.779053 32.909122 33.019855 33.40534
0:  33.679245 34.00912  34.31071  34.537056 34.84666  35.158813 35.563755
0:  35.974434 36.276215 36.38592  36.247074 37.53109  37.89408 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.12808418  0.22657728  0.36131716  0.37872314  0.28442097  0.1502552
0:   0.01086044 -0.0995903  -0.14890337 -0.19861984 -0.22639513 -0.32066774
0:  -0.3465619  -0.27583933 -0.11529064  0.16188335  0.49151754  0.7867212
0:   0.01306677  0.17516327]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.4141483 6.747565  7.1116204 7.542685  7.9426265 8.212004  8.545701
0:  8.666479  8.727377  8.649189  8.370175  8.0761175 7.7026787 7.3853884
0:  7.125051  6.8545675 6.546822  6.2437267 6.2798347 6.216281 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2001872 1.6041889 2.15061   2.7012157 3.285759  3.8556564 4.3737774
0:  4.827504  5.188552  5.4434867 5.6311083 5.6891527 5.794325  5.967833
0:  6.180388  6.470263  6.788232  7.0046244 5.7638025 5.8413897]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.7881927  6.0607834  6.3530536  6.6066504  6.8393736  7.0564227
0:   7.3537107  7.6221433  7.9001026  8.160904   8.382814   8.585297
0:   8.801019   9.07481    9.392565   9.691321   9.996114  10.250568
0:  10.40038   10.723007 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.276173 28.229774 28.003967 27.510723 26.903437 26.259954 25.633387
0:  25.011114 24.387518 23.808428 23.230116 22.644487 22.130974 21.67667
0:  21.221401 20.791533 20.40566  20.000292 19.567562 19.515224]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4103055 3.1632042 2.9240286 2.601149  2.2671967 2.0036955 1.7517695
0:  1.6722937 1.7145119 1.8723307 2.2687588 2.7468395 3.5150228 4.453171
0:  5.453701  6.4389863 7.306756  7.878871  6.8462987 7.500044 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.0322833 1.5076308 2.1308913 2.7087064 3.20702   3.653604  3.9320922
0:  4.1475363 4.2562532 4.244046  4.208506  4.0492544 4.0011435 4.0277987
0:  4.098997  4.249725  4.4242463 4.4738865 2.6978166 2.7825477]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.16484356 -0.11008978  0.06339979  0.18244123  0.21529055  0.19563484
0:   0.05611038 -0.07197905 -0.25006962 -0.4451685  -0.6058574  -0.8667302
0:  -0.9760299  -0.94514656 -0.7884755  -0.4605651  -0.04226398  0.3128724
0:  -1.1462193  -0.81721926]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.1328373 -6.0362377 -5.8327017 -5.6931925 -5.658223  -5.696412
0:  -5.8241262 -5.9286118 -6.0315866 -6.125057  -6.15837   -6.259367
0:  -6.1962156 -5.99061   -5.686425  -5.2775536 -4.8936644 -4.6683435
0:  -5.932538  -5.8552313]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.77094316 -0.28491592  0.3294382   0.9273491   1.4361916   1.7887135
0:   2.029716    2.1138444   2.1452003   2.2198129   2.3512325   2.5150042
0:   2.74925     2.9664874   3.2456667   3.5159268   3.6536849   3.7047436
0:   4.0358515   4.4795036 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.616541  -7.901621  -7.936124  -7.795025  -7.5717525 -7.31133
0:  -7.0692053 -6.820904  -6.5500383 -6.2469234 -5.856492  -5.526931
0:  -5.0726366 -4.5217457 -3.9376793 -3.3163052 -2.7303295 -2.2631845
0:  -2.6624155 -2.0334082]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.416901  10.398628  10.461614  10.443436  10.411556  10.355768
0:  10.281074  10.164777  10.019704   9.841296   9.637526   9.357771
0:   9.087748   8.868357   8.693644   8.572413   8.479759   8.327885
0:   7.3993406  7.299663 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.249063    1.3607206   1.5801082   1.7812834   1.9237432   1.9839101
0:   1.9727173   1.9176927   1.7888165   1.5932264   1.356832    1.0202684
0:   0.76815987  0.62822866  0.5903444   0.70094776  0.86437607  0.95636225
0:  -0.03236008  0.19498491]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.521452  -10.600374  -10.667709  -10.787451  -10.979212  -11.326546
0:  -11.685465  -12.145769  -12.5990925 -12.983341  -13.348713  -13.659317
0:  -13.884768  -13.97407   -14.002136  -13.888336  -13.710312  -13.491407
0:  -13.922106  -13.356634 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.6847954 6.0487733 6.5195694 6.9773326 7.3638783 7.649608  7.890889
0:  8.011794  7.9867187 7.834575  7.520365  7.0658135 6.5459833 6.0101395
0:  5.491432  4.990258  4.4977074 4.0086646 2.5185246 2.311952 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.668396  3.0097163 3.4266145 3.7990599 4.124178  4.3560567 4.5504537
0:  4.649279  4.69032   4.659302  4.579255  4.453032  4.3753386 4.4001393
0:  4.5079107 4.734596  5.039572  5.335745  5.232089  5.605791 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.898565   -9.882353   -9.712936   -9.554186   -9.43199    -9.357822
0:   -9.393435   -9.431269   -9.515539   -9.625893   -9.690989   -9.874161
0:   -9.97897   -10.021725  -10.043015   -9.952581   -9.7912655  -9.620674
0:  -11.718906  -11.810404 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.0886   16.477825 16.997877 17.517426 18.007013 18.389057 18.636658
0:  18.657837 18.52557  18.261566 17.903992 17.456188 16.925503 16.32164
0:  15.6365   14.920029 14.274256 13.749954 13.532469 13.671448]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.603397 8.713346 8.829159 8.900532 8.964979 8.962066 8.994955 8.948631
0:  8.889124 8.802688 8.7008   8.61212  8.553896 8.552253 8.600294 8.664705
0:  8.703927 8.720106 8.532861 8.676724]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.554874  11.754389  11.899084  12.01606   12.10395   12.148362
0:  12.343293  12.469949  12.722256  13.032602  13.4024105 13.853957
0:  14.385342  14.977812  15.60836   16.184958  16.688519  17.184607
0:  17.638828  17.974895 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.867325 17.78696  17.719795 17.569155 17.413652 17.22253  17.181
0:  17.074059 17.01294  16.953823 16.86578  16.813473 16.768906 16.766386
0:  16.74756  16.679415 16.47932  16.18848  15.918963 15.605736]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.5480933 -7.4625697 -7.2144685 -6.9180503 -6.5982137 -6.342504
0:  -6.160073  -6.039164  -5.960695  -5.880632  -5.7710567 -5.7386403
0:  -5.630031  -5.484072  -5.3128896 -5.068642  -4.8014283 -4.6050563
0:  -5.597079  -5.414037 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.480682  9.3333435 9.249359  9.240954  9.288599  9.345989  9.528212
0:  9.567312  9.517475  9.313947  8.930784  8.535267  8.154747  7.8961997
0:  7.747664  7.5916505 7.3163085 6.98685   5.71334   5.463596 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.54096  29.424274 29.18632  28.826519 28.40752  27.95188  27.715897
0:  27.394653 27.12644  26.973783 26.804794 26.828123 26.922567 27.159025
0:  27.560116 27.91651  28.20889  28.375862 30.558832 30.913206]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.8545656 6.862886  6.985821  7.092999  7.214324  7.328126  7.3291154
0:  7.2902145 7.1104765 6.8066797 6.4596496 5.9951015 5.623824  5.353943
0:  5.1779175 5.0984826 5.0518565 4.8966103 3.0154915 2.6846752]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.326297  8.517843  8.774294  9.040873  9.318436  9.539086  9.890914
0:  10.140347 10.409697 10.691876 10.949952 11.253786 11.55113  11.858961
0:  12.168148 12.425198 12.627167 12.808975 13.06522  13.346028]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.605108  4.708626  4.8718433 5.112986  5.411353  5.722132  6.0912967
0:  6.3300014 6.451918  6.3852105 6.1599474 5.7896285 5.3469315 4.9664407
0:  4.615525  4.379198  4.248411  4.149544  1.6671824 1.2155409]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.060707 17.710354 17.356014 16.944607 16.525473 16.063793 15.810632
0:  15.468105 15.159616 14.932477 14.651283 14.453451 14.262324 14.112158
0:  14.005283 13.889066 13.711522 13.484343 13.817986 13.699332]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.136164  9.154472  9.120977  9.040188  8.878344  8.629379  8.535682
0:  8.329293  8.133525  7.99259   7.757666  7.5882792 7.312826  7.030984
0:  6.76599   6.44509   6.0574465 5.685733  6.2047105 6.184481 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.7968273 1.7443604 1.8224139 1.9648576 2.1463032 2.2534084 2.3750043
0:  2.39676   2.433097  2.4956465 2.5773592 2.6931882 2.9091203 3.1950784
0:  3.587848  4.054388  4.5915475 5.149277  5.6827507 6.275721 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.5341539 1.7837334 2.122232  2.3902416 2.6140893 2.8063326 2.8744283
0:  2.9345326 2.9025123 2.7974029 2.6731899 2.4228716 2.274571  2.2309842
0:  2.281736  2.5098014 2.8452199 3.118451  1.8528113 2.1080828]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.158074  8.257152  8.445129  8.605928  8.721488  8.799546  8.807018
0:  8.701388  8.4892435 8.149318  7.751371  7.3397713 7.0916934 7.021351
0:  7.1321206 7.358124  7.587292  7.729099  7.181366  6.980361 ]
0: validation loss for strategy=forecast at epoch 3 : 0.2565103769302368
0: validation loss for velocity_u : 0.03992990031838417
0: validation loss for velocity_v : 0.06754674017429352
0: validation loss for specific_humidity : 0.026894832029938698
0: validation loss for velocity_z : 0.3981892764568329
0: validation loss for temperature : 0.08588439971208572
0: validation loss for total_precip : 0.9206165671348572
0: 4 : 17:42:52 :: batch_size = 96, lr = 1.903628792385485e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 4, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.3615, -1.3801, -1.3967, -1.4121, -1.4271, -1.4420, -1.4556, -1.4638, -1.4647, -1.4573, -1.4418, -1.4216,
0:         -1.3997, -1.3775, -1.3547, -1.3308, -1.3082, -1.2887, -1.3370, -1.3526, -1.3676, -1.3824, -1.3969, -1.4126,
0:         -1.4263], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3970, -0.4158, -0.4322, -0.4454, -0.4568, -0.4665, -0.4740, -0.4804, -0.4878, -0.4971, -0.5067, -0.5160,
0:         -0.5197, -0.5174, -0.5077, -0.4903, -0.4686, -0.4417, -0.4048, -0.4291, -0.4491, -0.4657, -0.4808, -0.4940,
0:         -0.5065], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2975, 1.2548, 1.2060, 1.1648, 1.1393, 1.1192, 1.1041, 1.0918, 1.0757, 1.0659, 1.0549, 1.0463, 1.0439, 1.0489,
0:         1.0594, 1.0736, 1.0910, 1.1064, 1.3009, 1.2584, 1.2059, 1.1627, 1.1289, 1.1074, 1.0909], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1850, 0.1286, 0.0914, 0.0631, 0.0767, 0.0959, 0.1162, 0.1647, 0.1749, 0.2076, 0.2132, 0.2008, 0.2132, 0.1884,
0:         0.2121, 0.1794, 0.1749, 0.1884, 0.0823, 0.0835, 0.0801, 0.0801, 0.1207, 0.1388, 0.1726], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.2519, -1.2152, -1.1508, -1.0776, -1.0083, -0.9489, -0.8964, -0.8441, -0.7934, -0.7468, -0.6949, -0.6479,
0:         -0.5943, -0.5265, -0.4445, -0.3530, -0.2559, -0.1617, -0.0771, -0.0015,  0.0692,  0.1341,  0.1955,  0.2511,
0:          0.2930], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2380, -0.2404, -0.2357, -0.2380, -0.2404, -0.2380, -0.2404, -0.2404, -0.2404, -0.2404, -0.2404, -0.2333,
0:         -0.2333, -0.2333, -0.2310, -0.2357, -0.2380, -0.2404, -0.2404, -0.2404, -0.2357, -0.2263, -0.2263, -0.2310,
0:         -0.2380], device='cuda:0')
0: [DEBUG] Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.2682,  0.4021,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1899,     nan,     nan,     nan,     nan,     nan,     nan, -0.0019,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2333,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2404,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2380,     nan, -0.1464,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1981,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0900,     nan,     nan,     nan,     nan,     nan, -0.1370,     nan, -0.1464,
0:         -0.1605, -0.1323,     nan,     nan,     nan, -0.1723,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2404,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2122,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1770,     nan,     nan,     nan,
0:             nan, -0.1088,     nan,     nan,     nan,     nan,     nan,     nan, -0.1605,     nan, -0.1112,     nan,
0:             nan, -0.0971,     nan,     nan,     nan,     nan,     nan,     nan, -0.2310,     nan,     nan,     nan,
0:             nan, -0.0994,     nan, -0.2333,     nan,     nan,     nan,     nan, -0.2145,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 4, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.4115, -1.4204, -1.4149, -1.4097, -1.4119, -1.4232, -1.4485, -1.4748, -1.5005, -1.5216, -1.5352, -1.5567,
0:         -1.5707, -1.5783, -1.5787, -1.5642, -1.5391, -1.5144, -1.3736, -1.3852, -1.3888, -1.3890, -1.3984, -1.4171,
0:         -1.4451], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4211, -0.4058, -0.3852, -0.3660, -0.3484, -0.3322, -0.3115, -0.2817, -0.2476, -0.2119, -0.1789, -0.1528,
0:         -0.1374, -0.1325, -0.1367, -0.1347, -0.1234, -0.0986, -0.3724, -0.3480, -0.3196, -0.2964, -0.2764, -0.2608,
0:         -0.2494], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2729, 1.2836, 1.2671, 1.2395, 1.1892, 1.1397, 1.1059, 1.1007, 1.1176, 1.1615, 1.2121, 1.2695, 1.3142, 1.3490,
0:         1.3734, 1.3895, 1.3905, 1.3825, 1.3971, 1.4093, 1.4074, 1.3785, 1.3378, 1.2973, 1.2788], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3030, -0.1948, -0.0417,  0.0407,  0.0572,  0.0434,  0.0540,  0.1320,  0.2759,  0.4847,  0.6582,  0.7033,
0:          0.5906,  0.3563,  0.1958,  0.0920,  0.0361,  0.1426, -0.3667, -0.2627, -0.0796,  0.0116,  0.0871,  0.1709,
0:          0.2416], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.1877, -1.2023, -1.1671, -1.0908, -0.9914, -0.8935, -0.8319, -0.8304, -0.9043, -1.0492, -1.2452, -1.4503,
0:         -1.6280, -1.7564, -1.8344, -1.8719, -1.8878, -1.8852, -1.8615, -1.7979, -1.6781, -1.5016, -1.2863, -1.0601,
0:         -0.8666], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.8533, 0.9057, 1.0690, 1.2808, 1.5393, 1.7639, 1.9495, 2.0689, 2.0352, 0.4640, 0.5140, 0.6017, 0.7133, 0.8403,
0:         1.0030, 1.1840, 1.3452, 1.4188, 0.1633, 0.1598, 0.2244, 0.2494, 0.2519, 0.3305, 0.4460], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.05805353447794914; velocity_v: 0.09320001304149628; specific_humidity: 0.04263024777173996; velocity_z: 0.5256601572036743; temperature: 0.11206059902906418; total_precip: 1.2051457166671753; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08129257708787918; velocity_v: 0.13716007769107819; specific_humidity: 0.055433351546525955; velocity_z: 0.6079750061035156; temperature: 0.16098804771900177; total_precip: 0.9902859330177307; 
0: epoch: 4 [1/5 (20%)]	Loss: 1.09772 : 0.30976 :: 0.16836 (3.03 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08243978768587112; velocity_v: 0.1292089819908142; specific_humidity: 0.04971525818109512; velocity_z: 0.5610582232475281; temperature: 0.10409875214099884; total_precip: 0.8065134882926941; 
0: epoch: 4 [2/5 (40%)]	Loss: 0.80651 : 0.26035 :: 0.16901 (16.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08204055577516556; velocity_v: 0.11994040757417679; specific_humidity: 0.07343652099370956; velocity_z: 0.6855974197387695; temperature: 0.13000938296318054; total_precip: 1.3094937801361084; 
0: epoch: 4 [3/5 (60%)]	Loss: 1.30949 : 0.37073 :: 0.17343 (16.05 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08657553791999817; velocity_v: 0.11628428846597672; specific_humidity: 0.0754639208316803; velocity_z: 0.4521491527557373; temperature: 0.09956230968236923; total_precip: 0.6007456183433533; 
0: epoch: 4 [4/5 (80%)]	Loss: 0.60075 : 0.21015 :: 0.17884 (16.06 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 20):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
0: Target values (first 20):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
0: Prediction values (first 20):
0: [13.6182    13.675089  13.803901  13.883173  13.947624  13.8817215
0:  14.031763  14.025909  13.956627  13.932447  13.767939  13.728228
0:  13.656214  13.7348    13.924828  14.103008  14.231891  14.24984
0:  14.872916  14.74669  ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -0.783, max = 2.516, mean = 0.845
0:          sample (first 20): tensor([0.5540, 0.5587, 0.5695, 0.5761, 0.5815, 0.5760, 0.5886, 0.5881, 0.5823, 0.5803, 0.5665, 0.5632, 0.5572, 0.5637,
0:         0.5796, 0.5945, 0.6053, 0.6068, 0.5557, 0.5631])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.714108 18.972303 19.055435 18.919899 18.624405 18.218203 17.9211
0:  17.602983 17.213156 16.856514 16.48881  16.150347 16.016783 16.046864
0:  16.161724 16.269196 16.343662 16.2751   17.201302 17.386454]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.5325007 3.8354914 4.394839  4.8199515 5.0280914 4.9591703 4.8316255
0:  4.7075076 4.587322  4.511644  4.3004556 3.9118555 3.4261692 3.0228868
0:  2.8246768 2.894134  3.0839868 3.2553372 1.0664763 1.3154688]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -3.9924545  -3.6730824  -3.1474133  -2.7843075  -2.7440863  -3.0330553
0:   -3.453823   -3.6837673  -3.9708967  -4.235586   -4.8632107  -6.081791
0:   -7.526474   -8.917617   -9.875997  -10.035792   -9.651679   -8.924138
0:   -9.093452   -7.6723185]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.397871 23.574923 23.595552 23.350882 23.066992 22.703115 22.702812
0:  22.685741 22.717134 22.869848 22.954689 23.247387 23.612158 24.093163
0:  24.577095 24.88855  25.093767 25.173986 26.682621 26.818386]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.410236 15.62151  15.943914 16.158405 16.338346 16.474012 16.77581
0:  17.122326 17.507593 17.983746 18.38064  18.754692 19.136433 19.55209
0:  20.034803 20.49436  20.913265 21.207577 20.66387  20.978636]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.92717886 1.6904788  2.5494604  3.1560302  3.447184   3.5118506
0:  3.5217204  3.7340744  3.9027638  4.0018187  3.8732157  3.2574282
0:  2.6945004  2.2861438  2.2847247  2.859054   3.6131365  4.292691
0:  1.1472378  1.7678099 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.3018427  5.813343   6.485176   6.9727507  7.180217   7.1029987
0:   6.7805467  6.4101524  5.8678765  5.1723394  4.2737584  3.0956626
0:   2.0565977  1.2960172  1.0031023  1.2540832  1.7792454  2.3351483
0:  -0.0098629  0.2714963]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.6112056 6.2910357 7.0511675 7.584433  7.81908   7.8235044 7.7574816
0:  7.769036  7.6806526 7.4662924 7.040064  6.2412696 5.551998  4.987557
0:  4.7734714 4.9855676 5.341632  5.606151  2.4186099 2.5936966]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.564137 34.218357 33.76545  33.142185 32.646175 32.128727 32.121216
0:  31.989582 31.763426 31.552845 31.110796 30.996017 30.847334 30.804283
0:  30.676504 30.319849 29.83065  29.180748 30.95599  30.815952]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.03536   4.9017086 4.763758  4.553591  4.288904  3.9116662 3.7969575
0:  3.6697276 3.6670587 3.8120587 4.0153055 4.363768  4.830488  5.5118203
0:  6.269193  7.0459065 7.779714  8.328939  9.53313   9.79141  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.2846193 3.5150003 3.9661725 4.3192453 4.556928  4.627621  4.7254133
0:  4.808213  4.8466234 4.831865  4.6686363 4.32493   3.9573727 3.794697
0:  3.8769977 4.277894  4.796752  5.287134  4.32036   4.512662 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.3824   40.05106  40.317383 40.158073 40.022953 39.920876 40.34535
0:  40.630394 40.891357 41.177456 41.333477 41.72521  42.006653 42.259323
0:  42.249737 41.82835  41.175694 40.352436 40.596836 41.175888]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.335296    3.5697474   3.9808185   4.3510914   4.6143193   4.699768
0:   4.7857795   4.819215    4.7157197   4.5209103   4.0818987   3.4580953
0:   2.8302073   2.3331995   2.076085    2.1117692   2.2011566   2.1983838
0:  -0.49284887 -0.6420264 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.628489 25.01699  24.442219 23.727154 23.041069 22.22546  21.739405
0:  21.18666  20.673841 20.270685 19.750107 19.448977 19.056631 18.737534
0:  18.368956 17.95598  17.495037 16.99134  19.0357   18.639158]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.7785   36.848858 36.664    36.20888  35.721485 35.2218   35.115826
0:  34.92712  34.819717 34.711025 34.478848 34.338787 34.29031  34.340355
0:  34.417385 34.40458  34.27256  34.04642  35.256325 35.86559 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.82011986 1.4637408  2.2919521  3.0449765  3.714518   4.2596583
0:  4.8479886  5.3980255  5.86292    6.1887956  6.2724414  6.0979247
0:  5.857211   5.6801424  5.6279306  5.746444   5.913789   6.032648
0:  4.4014244  4.9272985 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.005466  -7.5709853 -6.910706  -6.3793054 -6.0381436 -5.891687
0:  -5.7531266 -5.4338794 -5.145038  -4.920219  -4.970423  -5.5086465
0:  -6.117246  -6.62824   -6.8815236 -6.6571    -6.2618575 -5.865663
0:  -8.382605  -7.897594 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.6424513 7.1315646 7.7111993 8.132774  8.264773  8.16139   7.961982
0:  7.9335475 7.8447943 7.687241  7.3108687 6.404485  5.5373654 4.800623
0:  4.4792223 4.7886467 5.3353753 5.822136  1.632597  1.9273715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.09058523  0.51139545  1.1240916   1.586132    1.8474336   1.9104166
0:   1.8941016   2.0630307   2.178547    2.2791948   2.184093    1.7090263
0:   1.244031    0.9178176   0.89341545  1.3108249   1.8273554   2.2343965
0:  -0.6195216  -0.25161982]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.236775 -12.280566 -12.081399 -11.927924 -11.904994 -12.055863
0:  -12.110112 -12.027343 -11.867444 -11.641791 -11.63251  -11.875358
0:  -12.301902 -12.671748 -12.968139 -13.032401 -13.110963 -13.231166
0:  -17.397984 -17.97065 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.6700907 5.804761  6.0688906 6.221234  6.203435  6.0538483 5.9443355
0:  5.831028  5.6855884 5.5462265 5.329321  4.940669  4.668496  4.500158
0:  4.524313  4.7464747 5.030413  5.2197313 3.624382  3.7381594]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.918694 15.744715 15.554197 15.148813 14.632519 13.95846  13.49658
0:  12.990463 12.48366  12.142323 11.716287 11.448961 11.068326 10.747165
0:  10.474001 10.21038   9.973939  9.692605  9.916225  9.75144 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.124296 12.811714 13.964591 15.393704 17.033733 18.616776 20.238216
0:  21.528126 22.525534 23.294544 23.893963 24.431793 25.05909  25.64051
0:  26.101398 26.35189  26.398525 26.173172 26.126244 26.068924]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.313507 16.611528 16.831448 16.816696 16.694977 16.469849 16.457174
0:  16.443752 16.446457 16.565868 16.620003 16.76009  16.870235 17.104996
0:  17.445429 17.82061  18.19096  18.418493 19.410683 19.746513]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.752249  12.539663  12.426732  12.250626  12.064958  11.787171
0:  11.79476   11.701073  11.629322  11.587282  11.402891  11.260128
0:  11.048965  10.915338  10.775058  10.605539  10.381064  10.0718975
0:   9.579665   9.428615 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.882357  11.064805  11.227531  11.240307  11.224335  11.129686
0:  11.251899  11.361691  11.44182   11.574981  11.56789   11.551071
0:  11.438812  11.379942  11.3494835 11.355837  11.393297  11.366291
0:  11.399929  11.483948 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.9644198 3.459184  4.044304  4.405034  4.4712453 4.274967  4.004404
0:  3.8876574 3.7595723 3.6261625 3.3259482 2.6566763 2.039527  1.5746198
0:  1.4658875 1.8495564 2.3942714 2.8793468 0.3734498 0.7682347]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.2373214 2.8356805 3.5302396 4.055148  4.325987  4.283633  4.1835136
0:  4.020238  3.7660995 3.5487845 3.2437267 2.8672628 2.4461336 2.1226325
0:  1.8869467 1.8519807 1.9155478 2.0293365 1.8370032 2.2540727]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.312927 18.73931  18.205488 17.63564  17.141277 16.68259  16.66442
0:  16.56901  16.468323 16.354965 16.031664 15.858488 15.669409 15.517546
0:  15.31776  14.966469 14.431494 13.826717 13.791245 13.460613]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.890675  15.362003  14.840904  14.105746  13.229393  12.243332
0:  11.462021  10.726132  10.022385   9.521284   8.9927     8.531108
0:   8.061732   7.688903   7.3825126  7.1036167  6.845427   6.54943
0:   4.8703747  4.580821 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.048656   -0.5453315  -0.10116625  0.03547668 -0.15901804 -0.5623126
0:  -1.0258489  -1.1531568  -1.1511474  -1.0703616  -1.0768876  -1.5792794
0:  -2.0761704  -2.4777694  -2.613789   -2.2072158  -1.6532836  -1.0933766
0:  -3.7582288  -3.3220944 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.852294 11.35342  11.816343 12.023311 12.096586 12.093974 12.14447
0:  12.279392 12.385475 12.491425 12.504921 12.385545 12.352512 12.485359
0:  12.800572 13.229357 13.716579 14.020565 13.143875 13.597953]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.2833953  -3.6053872  -2.8133483  -2.2048616  -1.8821292  -1.7733688
0:  -1.805253   -1.6455293  -1.5535564  -1.5396023  -1.7143645  -2.3673358
0:  -2.8950105  -3.172711   -2.9939523  -2.2110276  -1.2911296  -0.48182917
0:  -2.84788    -2.1905322 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.274151   8.798758   9.51497   10.083672  10.435335  10.581232
0:  10.621225  10.727066  10.752047  10.6918745 10.454275   9.868279
0:   9.264193   8.75569    8.515645   8.655542   9.000892   9.348803
0:   6.19943    6.438828 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.313526 35.321045 35.0259   34.527916 34.00621  33.477005 33.299965
0:  32.95135  32.576706 32.195587 31.69265  31.608395 31.74421  32.260456
0:  32.89398  33.314663 33.527367 33.481567 36.91093  37.343315]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.528202   -9.973864   -9.187759   -8.497622   -8.102114   -8.003433
0:   -8.146307   -8.157064   -8.230606   -8.278828   -8.418398   -8.997337
0:   -9.457182   -9.728128   -9.604212   -8.939791   -8.209421   -7.5191064
0:  -10.670681  -10.281994 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.494627 31.730007 31.843786 31.753416 31.576458 31.276472 31.314838
0:  31.187471 31.075716 30.993917 30.772228 30.766054 30.798885 30.996769
0:  31.341146 31.566599 31.73759  31.725136 33.251633 33.411366]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.470041   9.886431  10.47319   10.93949   11.144999  11.061428
0:  10.77288   10.513158  10.0938425  9.586636   8.897594   7.8275366
0:   6.8110127  5.912176   5.3202624  5.1967497  5.19866    5.2171173
0:   1.3404164  1.3686113]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.678618 12.067076 12.54726  12.855387 13.055534 13.109079 13.239664
0:  13.341962 13.436532 13.578264 13.594917 13.548182 13.421093 13.323961
0:  13.328032 13.42608  13.560636 13.658712 12.665563 12.712473]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.47151  11.803712 12.232813 12.553377 12.765379 12.836094 12.960951
0:  13.055941 13.093082 13.204794 13.212134 13.180971 13.12208  13.207251
0:  13.486375 13.880119 14.252281 14.470638 14.391588 14.565474]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.512457  -10.939571  -10.123928   -9.440939   -8.938934   -8.656012
0:   -8.455627   -8.086285   -7.732844   -7.3883452  -7.210507   -7.36581
0:   -7.47745    -7.4111323  -7.0857463  -6.3765826  -5.6733775  -5.0887923
0:   -7.993158   -7.4709983]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.744637  11.951609  12.0127325 11.763987  11.242849  10.576389
0:  10.042752   9.580967   9.189798   8.828927   8.383341   7.854267
0:   7.322504   6.910728   6.655065   6.5100493  6.4379234  6.364789
0:   5.227397   5.4722743]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.573806  4.8317485 5.165989  5.4015546 5.4958653 5.47079   5.5042048
0:  5.551196  5.6264176 5.6792655 5.621114  5.4492397 5.3093586 5.259267
0:  5.3909388 5.6782107 6.0061827 6.2369876 5.104952  5.34587  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.591518   -1.8728986  -0.91274023 -0.08479166  0.5022044   0.84495544
0:   1.0360866   1.3429174   1.5110545   1.5924935   1.4335871   0.78887653
0:   0.19516706 -0.22895622 -0.25351715  0.27093506  0.960186    1.5486965
0:  -2.091001   -1.6806397 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.2384396 -6.481043  -5.6143913 -4.9414415 -4.6424093 -4.599863
0:  -4.6731873 -4.5006876 -4.343753  -4.271723  -4.4198923 -5.184209
0:  -5.818351  -6.2255135 -6.1084747 -5.259412  -4.271925  -3.4199724
0:  -6.57524   -6.0270643]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.0971966  5.642478   6.268934   6.709743   6.9522924  7.060559
0:   7.25706    7.5971985  7.9544897  8.276713   8.468708   8.360658
0:   8.347502   8.429461   8.765413   9.419814  10.156252  10.72178
0:   8.974096   9.567345 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8470798 -3.6947598 -3.303739  -2.9341836 -2.7109704 -2.6835485
0:  -2.5877624 -2.4887118 -2.333487  -2.076778  -1.9587874 -1.9195132
0:  -1.991673  -2.0154529 -1.9462209 -1.7549129 -1.530891  -1.3367257
0:  -2.5602355 -2.4130454]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.6388    -4.364158  -3.9664435 -3.7127414 -3.690393  -3.9082923
0:  -4.223145  -4.441829  -4.6317096 -4.795791  -4.9828963 -5.370711
0:  -5.6196103 -5.6563697 -5.440207  -4.8551564 -4.2306514 -3.7331872
0:  -6.1550536 -6.0473666]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.583637 23.573757 23.498886 23.16008  22.797573 22.290989 22.127354
0:  21.894073 21.651814 21.52183  21.2557   21.268787 21.065777 20.865925
0:  20.486118 19.840975 19.02573  18.120693 20.487186 20.145275]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.18708  13.320564 13.527225 13.627849 13.699791 13.673956 13.842932
0:  13.993359 14.144346 14.369749 14.510763 14.667149 14.761715 14.900814
0:  15.081314 15.276886 15.456003 15.582872 15.282143 15.216488]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.548645 28.480875 28.283138 27.827225 27.414257 26.97183  26.924555
0:  26.822515 26.688646 26.771269 26.788597 27.160194 27.446106 27.832968
0:  28.148357 28.258953 28.253325 28.133373 30.931757 31.413433]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.044244   9.193254   9.492235   9.762894  10.066664  10.250964
0:  10.808004  11.271904  11.801355  12.4697075 13.0483265 13.8052845
0:  14.516568  15.300386  16.121483  16.711445  17.129944  17.319254
0:  17.305216  17.277052 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.164658   -9.800741   -9.257343   -8.68766    -8.296881   -8.078613
0:   -8.028408   -7.763396   -7.570482   -7.412084   -7.383961   -7.8625703
0:   -8.194231   -8.268707   -7.9421105  -7.008262   -6.0408616  -5.2650485
0:   -8.254213   -7.7852225]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.817799 23.821938 23.861862 23.667152 23.356197 22.892685 22.670052
0:  22.429054 22.233288 22.16475  22.00703  21.981163 21.849422 21.757343
0:  21.68039  21.56929  21.45964  21.312546 22.45123  22.363247]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.523487 20.774593 20.88869  20.71406  20.3624   19.908918 19.60882
0:  19.336445 19.093403 18.950838 18.754343 18.562199 18.387371 18.259209
0:  18.197727 18.117325 17.99257  17.770645 16.306137 16.187883]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7778397 3.197818  3.804814  4.332476  4.7314005 4.9395347 5.1999717
0:  5.4662724 5.7332854 6.0885415 6.2892404 6.3152437 6.220437  6.191927
0:  6.3306413 6.675824  7.0599284 7.3330116 6.2190127 6.311619 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.143584   -5.348738   -4.379167   -3.5389247  -3.0286927  -2.7810092
0:  -2.6904082  -2.388019   -2.101686   -1.8515058  -1.803637   -2.3419623
0:  -2.807548   -3.129232   -3.0318017  -2.2606034  -1.3588777  -0.52209663
0:  -4.0467315  -3.645     ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.310265 18.8522   19.135265 19.162807 19.039795 18.817493 18.833515
0:  18.787832 18.698666 18.638824 18.445927 18.314367 18.259146 18.375174
0:  18.567055 18.714226 18.868114 18.850395 20.185904 20.77202 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.1363735  8.123793   8.166213   8.154123   8.073724   7.8692183
0:   7.876551   7.7885137  7.74859    7.6779704  7.600374   7.560811
0:   7.727514   8.079376   8.490543   8.837336   9.038069   9.070738
0:   9.711737  10.030147 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.4455924 7.5364547 7.787702  7.969718  8.047726  8.032644  8.160449
0:  8.410785  8.720073  8.997927  9.128323  9.010452  8.901536  8.880426
0:  9.026968  9.360511  9.637244  9.749092  8.041386  8.266832 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.3283691  -1.0579958  -0.5886426  -0.19435596  0.10045147  0.24273586
0:   0.45557404  0.65417576  0.86101437  1.1047583   1.2449875   1.2888002
0:   1.3011284   1.4429007   1.7428193   2.1675467   2.5796652   2.8947563
0:   2.2947664   2.6884165 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.090588 -17.612534 -16.872883 -16.398357 -16.312328 -16.545372
0:  -16.900124 -16.894077 -16.73796  -16.409752 -16.25753  -16.597958
0:  -16.996925 -17.275402 -17.245155 -16.75793  -16.24133  -15.748618
0:  -20.399403 -19.950966]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.055145 -15.591092 -14.852767 -14.192765 -13.776282 -13.644057
0:  -13.668492 -13.552606 -13.487138 -13.413157 -13.564582 -14.140407
0:  -14.681722 -14.983932 -14.899652 -14.260886 -13.537595 -12.856334
0:  -16.684864 -16.457298]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.4677315  7.865369   8.422592   9.011045   9.606258  10.125945
0:  10.716992  11.218074  11.534216  11.656499  11.522284  11.127459
0:  10.75751   10.474164  10.351721  10.441086  10.599845  10.652416
0:   8.4280405  8.514275 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.3834043 -6.712855  -5.8147426 -5.0889583 -4.6745677 -4.6062894
0:  -4.714891  -4.736584  -4.7713523 -4.7815967 -4.911162  -5.3549237
0:  -5.6620154 -5.6728797 -5.2443433 -4.2185025 -3.0772977 -1.9543042
0:  -3.2723384 -2.6019864]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.384064 35.250813 34.937057 34.195244 33.477436 32.76154  32.617184
0:  32.49655  32.393112 32.312603 32.014847 31.975513 31.859997 31.943762
0:  32.051544 31.9433   31.646988 31.084661 31.77832  31.70143 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.98350763 -0.7931919  -0.28553915  0.16443872  0.50232077  0.62339115
0:   0.6561856   0.7522116   0.7577281   0.7937026   0.6651416   0.30463934
0:  -0.08159733 -0.3504014  -0.41015005 -0.1977334   0.02172136  0.13920832
0:  -3.1424413  -3.0336556 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.777636  12.091011  12.454514  12.650182  12.6764145 12.520797
0:  12.396866  12.262209  12.095697  11.943901  11.671309  11.273636
0:  10.913456  10.618628  10.494395  10.547453  10.68385   10.725996
0:   8.834034   8.832397 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.853324  6.1339583 6.458485  6.7114735 6.811431  6.733467  6.7796946
0:  6.7378516 6.6992025 6.6873136 6.5735297 6.439988  6.332489  6.399891
0:  6.6282053 6.970749  7.3056755 7.438842  7.4920316 7.6776342]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.5506015 0.6689882 1.0012369 1.2902594 1.599123  1.7496047 2.1278472
0:  2.3836732 2.5698051 2.8159816 2.9182825 3.1172094 3.2700992 3.6256862
0:  4.176339  4.811463  5.4496717 5.9847465 6.8300867 7.1756487]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -1.1353078   -0.701509    -0.1580205   -0.19089556  -0.8006363
0:   -1.8806129   -3.0219607   -3.9356074   -4.581697    -4.870396
0:   -5.187652    -5.6236424   -6.348578    -6.9988446   -7.719259
0:   -8.389317    -9.13139     -9.881367   -10.607256    -9.712788  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.436592  6.2500896 6.1972713 6.1547184 6.1217966 5.9516973 5.9509616
0:  5.774993  5.634854  5.519777  5.3743033 5.364394  5.5219374 5.8733163
0:  6.307909  6.669023  6.946793  7.0247927 9.168567  9.102643 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.999609 20.789898 20.462336 19.901512 19.17757  18.265461 17.553493
0:  16.794538 16.156918 15.655714 15.176411 14.791022 14.479406 14.309305
0:  14.242901 14.220354 14.205166 14.046222 14.231594 14.396891]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.5000315 4.8105288 5.092758  5.125162  4.9638414 4.7013693 4.7436037
0:  4.844498  5.023475  5.2402496 5.254003  5.0743833 4.768551  4.558148
0:  4.4264193 4.437518  4.402302  4.3119736 4.7529593 4.41273  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.342028 23.532677 23.702698 23.626179 23.483974 23.288054 23.394304
0:  23.518105 23.703054 23.919008 24.041477 24.218052 24.415386 24.722164
0:  25.064358 25.363503 25.59773  25.684937 26.40685  26.694275]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.5484309  1.1568079  2.0058203  2.674172   3.060721   3.1970837
0:  3.2339509  3.4102979  3.4871914  3.5677083  3.5047038  3.0822139
0:  2.7967541  2.6807756  2.8458867  3.397091   3.9487467  4.3400793
0:  0.13453674 0.41626358]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.22242   4.308776  4.659187  5.03054   5.3873453 5.6148047 5.960231
0:  6.290473  6.6116185 6.972539  7.2205777 7.382434  7.5216684 7.6775107
0:  7.8913627 8.179846  8.434529  8.567001  6.9399204 6.9738607]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.188326  14.047303  13.952486  13.72389   13.37553   12.89267
0:  12.5752735 12.242317  11.970415  11.785372  11.608873  11.505785
0:  11.509368  11.623234  11.821444  12.006933  12.090754  12.025606
0:  11.575598  11.624258 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.6343522  -0.32855225  0.10516453  0.42466927  0.531641    0.415123
0:   0.22787428  0.12544823  0.01633692 -0.09746742 -0.31172657 -0.7658515
0:  -1.1398058  -1.3393116  -1.2460108  -0.76531553 -0.18540287  0.2971282
0:  -2.1487894  -1.975235  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.668724 15.882074 16.21805  16.390419 16.470554 16.415724 16.495546
0:  16.592588 16.7238   16.912127 16.979801 16.931429 16.79489  16.640545
0:  16.552223 16.599514 16.648094 16.612446 14.114897 14.225594]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.4370642 -2.541915  -2.3898106 -2.2587194 -2.1995535 -2.2838306
0:  -2.351409  -2.4050798 -2.5240335 -2.6587162 -2.920249  -3.3532963
0:  -3.738822  -3.9422598 -3.9417372 -3.6861773 -3.367138  -3.148632
0:  -5.4608526 -5.414755 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6293874  -2.1485686  -1.495811   -0.95908785 -0.5148001  -0.20547104
0:   0.15756083  0.54449654  0.9345031   1.3069329   1.5295906   1.5783787
0:   1.6193767   1.7745676   2.1190014   2.6727014   3.2668822   3.7428555
0:   2.0008326   2.5208693 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [33.691975 33.948936 33.928158 33.613865 33.237614 32.833557 32.801815
0:  32.668972 32.528774 32.46828  32.293465 32.397976 32.508705 32.82495
0:  33.14976  33.266533 33.32958  33.219013 36.084385 36.5919  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [35.463943 35.191864 34.742565 34.14694  33.458042 32.70628  32.220432
0:  31.612272 31.052284 30.493587 29.881674 29.44969  29.213259 29.152163
0:  29.173689 29.149239 29.00899  28.753744 30.668427 30.39998 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.219946  -7.703586  -6.942014  -6.37203   -6.1043572 -6.1441255
0:  -6.419     -6.518496  -6.665858  -6.746445  -6.9180446 -7.502685
0:  -8.013174  -8.356348  -8.366322  -7.783005  -7.0676894 -6.312035
0:  -9.286129  -8.892292 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.197716 31.492342 31.645332 31.446762 31.045868 30.540762 30.439312
0:  30.325872 30.291294 30.40043  30.353867 30.476826 30.642876 31.179663
0:  32.091312 33.081524 34.03975  34.6099   36.51722  36.928837]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.9830651   0.19626331  1.4176731   2.4061403   3.1349983   3.6354473
0:   4.0957384   4.6372604   4.9950285   4.9809585   4.446933    3.2506695
0:   1.9304857   0.8220186   0.19310331  0.29999733  0.77920437  1.338563
0:  -0.633965    0.06675863]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.983238  -10.287678   -9.4053135  -8.668186   -8.178299   -7.92316
0:   -7.823975   -7.5419326  -7.340711   -7.1640496  -7.147718   -7.595959
0:   -7.9482465  -8.11607    -7.916557   -7.162227   -6.335036   -5.5527787
0:   -8.050812   -7.4271646]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.822893    7.126701    7.7245836   8.073222    8.021997    7.5515857
0:   6.8224893   6.0375566   5.1283183   4.3641806   3.4427626   2.3845353
0:   1.2388592   0.35529757 -0.14363623 -0.22591019 -0.07690382  0.08995104
0:  -0.07027435  0.36371088]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.2165546 5.40953   5.685345  5.813812  5.821811  5.7119627 5.644817
0:  5.639137  5.696057  5.779969  5.8456545 5.8373985 5.8929815 6.083057
0:  6.441335  6.950884  7.5151005 7.9891195 7.1920238 7.771986 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7613966 4.0450716 4.4845066 4.795183  4.930974  4.9207034 4.9412026
0:  5.0110974 5.085475  5.183215  5.1231117 4.8517733 4.560416  4.321889
0:  4.26982   4.443882  4.7396584 4.9888315 3.3337193 3.5765278]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.794392   -8.741747   -8.506714   -8.404547   -8.512907   -8.790735
0:   -9.048914   -9.081772   -8.990736   -8.842987   -8.887453   -9.2754135
0:   -9.858908  -10.462809  -10.983103  -11.41029   -11.866758  -12.420252
0:  -18.194553  -19.083874 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.4877524 3.6876764 3.9971247 4.157773  4.156487  4.002874  3.895982
0:  3.9171474 4.003421  4.1653204 4.2901707 4.2690206 4.3304477 4.497657
0:  4.874569  5.5318794 6.2393885 6.871203  5.294612  5.7085037]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.411036  -10.837856  -10.064758   -9.429998   -9.090231   -9.034744
0:   -9.2022915  -9.160139   -9.135944   -9.064148   -9.022835   -9.392126
0:   -9.60677    -9.581629   -9.199696   -8.358902   -7.5049405  -6.760694
0:   -9.918425   -9.351    ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.234992  -6.000244  -5.5959263 -5.2282467 -4.9915667 -4.9057856
0:  -4.7408085 -4.6537714 -4.51847   -4.401821  -4.41889   -4.577196
0:  -4.6646347 -4.647152  -4.448127  -4.063504  -3.658821  -3.3389225
0:  -4.491124  -4.2406263]
0: validation loss for strategy=forecast at epoch 4 : 0.2962902784347534
0: validation loss for velocity_u : 0.062166452407836914
0: validation loss for velocity_v : 0.09087678790092468
0: validation loss for specific_humidity : 0.08496773988008499
0: validation loss for velocity_z : 0.530590295791626
0: validation loss for temperature : 0.0891396701335907
0: validation loss for total_precip : 0.9200007319450378
0: 5 : 17:46:44 :: batch_size = 96, lr = 1.857198821839498e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 5, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5398, -0.5424, -0.5467, -0.5531, -0.5615, -0.5716, -0.5833, -0.5961, -0.6098, -0.6236, -0.6376, -0.6511,
0:         -0.6639, -0.6758, -0.6865, -0.6956, -0.7035, -0.7098, -0.4821, -0.4812, -0.4810, -0.4818, -0.4836, -0.4864,
0:         -0.4905], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0326, -1.0414, -1.0519, -1.0634, -1.0759, -1.0888, -1.1016, -1.1141, -1.1260, -1.1369, -1.1467, -1.1555,
0:         -1.1626, -1.1680, -1.1718, -1.1732, -1.1724, -1.1688, -1.0481, -1.0454, -1.0443, -1.0452, -1.0473, -1.0510,
0:         -1.0560], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6063, -0.6075, -0.6088, -0.6088, -0.6088, -0.6087, -0.6088, -0.6083, -0.6079, -0.6075, -0.6071, -0.6074,
0:         -0.6075, -0.6077, -0.6078, -0.6085, -0.6092, -0.6099, -0.6018, -0.6032, -0.6047, -0.6059, -0.6066, -0.6074,
0:         -0.6080], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4038, -0.3912, -0.3727, -0.3462, -0.3094, -0.2610, -0.2034, -0.1400, -0.0732, -0.0053,  0.0592,  0.1214,
0:          0.1779,  0.2251,  0.2620,  0.2838,  0.2885,  0.2723, -0.3969, -0.4027, -0.4050, -0.3992, -0.3866, -0.3647,
0:         -0.3359], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.0332,  0.0464,  0.0589,  0.0700,  0.0796,  0.0874,  0.0932,  0.0962,  0.0971,  0.0946,  0.0895,  0.0816,
0:          0.0710,  0.0578,  0.0428,  0.0266,  0.0095, -0.0077, -0.0237, -0.0386, -0.0512, -0.0612, -0.0682, -0.0725,
0:         -0.0735], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2066, -0.2042, -0.2005, -0.1993, -0.1969, -0.1956, -0.1932, -0.1908, -0.1871, -0.2139, -0.2115, -0.2090,
0:         -0.2066, -0.2054, -0.2042, -0.2017, -0.2005, -0.1981, -0.2103, -0.2090, -0.2078, -0.2066, -0.2054, -0.2042,
0:         -0.2017], device='cuda:0')
0: [DEBUG] Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2383,     nan,     nan, -0.2322,     nan,     nan,     nan,     nan,
0:             nan, -0.2407,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2420,     nan,     nan,
0:             nan, -0.2383,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2407,     nan,     nan,     nan, -0.2371,     nan,     nan,     nan,     nan, -0.2395,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2395,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2310,     nan,     nan,     nan,     nan, -0.2407,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2286,
0:             nan, -0.2261,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2359,     nan,     nan,     nan, -0.2346,     nan,     nan, -0.2420,     nan,
0:         -0.2407,     nan,     nan,     nan,     nan, -0.2407,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2359, -0.2334,     nan,     nan, -0.2420,     nan, -0.2407, -0.2395,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2322, -0.2298,     nan,     nan,     nan, -0.2383,     nan, -0.2346,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2432,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 5, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.6303, -1.6110, -1.5674, -1.5282, -1.5009, -1.4925, -1.4905, -1.4811, -1.4721, -1.4604, -1.4582, -1.4758,
0:         -1.4968, -1.5106, -1.5085, -1.4887, -1.4634, -1.4439, -1.5211, -1.5206, -1.4966, -1.4676, -1.4482, -1.4384,
0:         -1.4323], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7762, -0.7842, -0.8032, -0.8255, -0.8447, -0.8520, -0.8535, -0.8334, -0.8005, -0.7863, -0.7873, -0.8054,
0:         -0.8272, -0.8333, -0.8021, -0.7500, -0.6818, -0.6033, -0.7972, -0.7875, -0.8007, -0.8164, -0.8377, -0.8661,
0:         -0.8749], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6431, -0.6442, -0.6480, -0.6472, -0.6453, -0.6407, -0.6409, -0.6398, -0.6440, -0.6510, -0.6568, -0.6622,
0:         -0.6622, -0.6614, -0.6633, -0.6648, -0.6766, -0.6821, -0.6557, -0.6526, -0.6516, -0.6472, -0.6454, -0.6416,
0:         -0.6378], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3393, 0.3333, 0.3526, 0.3864, 0.3930, 0.4164, 0.4418, 0.4296, 0.4551, 0.4810, 0.4645, 0.5262, 0.6062, 0.5621,
0:         0.4377, 0.4065, 0.4425, 0.3519, 0.2983, 0.2614, 0.2510, 0.2779, 0.3316, 0.3974, 0.4403], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1892, -0.1868, -0.1909, -0.2067, -0.2203, -0.2290, -0.2270, -0.2223, -0.2197, -0.2225, -0.2312, -0.2416,
0:         -0.2483, -0.2483, -0.2459, -0.2411, -0.2376, -0.2325, -0.2226, -0.2082, -0.1933, -0.1815, -0.1722, -0.1647,
0:         -0.1563], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2430, -0.2450, -0.2434, -0.2464, -0.2552, -0.2590, -0.2576, -0.2548, -0.2505, -0.2411, -0.2419, -0.2501,
0:         -0.2489, -0.2547, -0.2577, -0.2559, -0.2551, -0.2513, -0.2424, -0.2468, -0.2430, -0.2539, -0.2567, -0.2545,
0:         -0.2541], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.08924631774425507; velocity_v: 0.1367807537317276; specific_humidity: 0.09524882584810257; velocity_z: 0.5461351275444031; temperature: 0.10566221177577972; total_precip: 1.0815083980560303; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11076600849628448; velocity_v: 0.14746206998825073; specific_humidity: 0.11122302711009979; velocity_z: 0.583414614200592; temperature: 0.13058733940124512; total_precip: 1.124900460243225; 
0: epoch: 5 [1/5 (20%)]	Loss: 1.10320 : 0.32489 :: 0.18407 (2.54 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.12421339750289917; velocity_v: 0.15712539851665497; specific_humidity: 0.16066886484622955; velocity_z: 0.5552173852920532; temperature: 0.13710786402225494; total_precip: 1.0653235912322998; 
0: epoch: 5 [2/5 (40%)]	Loss: 1.06532 : 0.33388 :: 0.18920 (16.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.12148815393447876; velocity_v: 0.1627485603094101; specific_humidity: 0.1287151724100113; velocity_z: 0.5312841534614563; temperature: 0.10963880270719528; total_precip: 0.7421257495880127; 
0: epoch: 5 [3/5 (60%)]	Loss: 0.74213 : 0.26875 :: 0.18966 (16.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13436219096183777; velocity_v: 0.14237500727176666; specific_humidity: 0.18195068836212158; velocity_z: 0.5499839782714844; temperature: 0.13789115846157074; total_precip: 1.2210890054702759; 
0: epoch: 5 [4/5 (80%)]	Loss: 1.22109 : 0.36264 :: 0.18396 (16.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 20):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
0: Target values (first 20):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
0: Prediction values (first 20):
0: [ 5.7592926  6.100234   6.690484   7.1991744  7.570535   7.7197933
0:   7.90581    8.123406   8.334469   8.572952   8.624012   8.4709215
0:   8.272508   8.238697   8.491583   9.035722   9.629412  10.0529785
0:   8.182475   8.380186 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -2.592, max = 2.897, mean = 0.169
0:          sample (first 20): tensor([-0.1569, -0.1269, -0.0749, -0.0301,  0.0026,  0.0158,  0.0322,  0.0513,  0.0699,  0.0909,  0.0954,  0.0819,
0:          0.0645,  0.0615,  0.0838,  0.1317,  0.1840,  0.2213, -0.1074, -0.0932])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.060997 -11.18923  -10.111105  -9.149961  -8.654921  -8.611202
0:   -8.826168  -8.864616  -8.983362  -9.150667  -9.540413 -10.540655
0:  -11.377161 -11.869631 -11.780302 -10.929926 -10.074039  -9.347748
0:  -12.240231 -11.663544]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.581571  -11.899457  -10.847003   -9.977473   -9.433564   -9.285796
0:   -9.208165   -8.80603    -8.362262   -7.8581133  -7.684429   -8.03146
0:   -8.521297   -8.8653555  -8.813235   -8.2297     -7.595751   -7.023077
0:  -10.981263  -10.461371 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.52076  29.939272 29.265306 28.342197 27.543262 26.783924 26.678085
0:  26.619637 26.631392 26.679688 26.503202 26.501127 26.39197  26.413471
0:  26.336655 26.037931 25.573538 24.908869 25.833063 25.897732]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.21839   7.0356283 6.898542  6.6030207 6.208384  5.77004   5.6496954
0:  5.7024508 5.9151783 6.27656   6.484786  6.4894066 6.3235087 6.1692457
0:  6.0496798 5.995951  5.8987536 5.6889787 4.8956566 4.608206 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.85162  22.395605 22.87099  23.078074 23.230095 23.288103 23.760098
0:  24.20334  24.638792 25.209457 25.634706 26.297333 26.941362 27.770685
0:  28.616627 29.339762 29.913523 30.264273 33.252773 33.824417]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.98841  23.079245 23.200388 23.025412 22.631802 22.005901 21.570898
0:  21.100773 20.571482 20.165857 19.581347 19.006367 18.23819  17.53594
0:  16.862944 16.235258 15.674284 15.165976 14.244055 14.122309]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [15.963926 16.524803 17.28829  17.990566 18.57968  19.04291  19.479332
0:  20.102654 20.567926 21.005432 21.199131 20.98756  20.763914 20.678518
0:  21.006199 21.738443 22.644423 23.370943 21.858679 22.052034]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.653    22.886929 23.019451 23.013405 22.866753 22.555433 22.658758
0:  22.703281 22.838966 23.106762 23.326061 23.697407 24.133308 24.666973
0:  25.231567 25.749939 26.190678 26.52523  30.036926 30.619884]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.699181  8.717092  8.801549  8.65992   8.322622  7.810114  7.3367715
0:  6.7735443 6.025286  5.2167234 4.200385  3.1372094 2.2727704 1.9114928
0:  2.0886784 2.7450328 3.626441  4.3995733 6.0084577 6.4462886]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.648032  5.955704  6.4981503 6.869446  6.990473  6.8395    6.7527447
0:  6.7692957 6.84511   7.0001006 6.9462223 6.6119237 6.190509  5.850448
0:  5.7811847 6.125523  6.621068  7.0559397 4.9311156 5.224374 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.6839857  8.1797495  8.761933   9.099573   9.1909     9.083664
0:   9.104904   9.229826   9.414663   9.597254   9.586063   9.352239
0:   9.14983    9.077685   9.266614   9.742568  10.267715  10.631165
0:   9.189321   9.66296  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4484506 2.9650385 3.590055  3.8587916 3.8729367 3.7062733 3.762509
0:  4.085659  4.4680743 4.8651924 4.9031343 4.4690323 3.8209658 3.22039
0:  2.9150927 3.0886285 3.511903  3.9863849 1.8292298 2.1782784]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.685593  13.959314  14.1455765 13.992487  13.772324  13.4685545
0:  13.507126  13.52454   13.503633  13.627085  13.632633  13.833387
0:  13.929789  14.17931   14.41375   14.515642  14.563282  14.539343
0:  15.696882  16.15984  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.8180108  2.2765453  2.941919   3.2545836  3.311746   3.1771865
0:   3.2469373  3.6699755  4.257285   4.8934546  5.1240125  4.8123074
0:   4.084608   3.3036718  2.6922476  2.482719   2.4596598  2.5223198
0:  -1.525866  -0.8177829]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.344416  -7.5395513 -6.548392  -5.795712  -5.3903813 -5.320392
0:  -5.307487  -5.053492  -4.799682  -4.5282664 -4.543245  -5.0011077
0:  -5.495753  -5.782433  -5.6840873 -5.001579  -4.2238307 -3.5125995
0:  -6.466273  -5.611935 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.01876688  0.5117631   1.1406546   1.5876684   1.7784214   1.725174
0:   1.7512741   1.881124    1.9788032   2.0535917   1.8920813   1.4523187
0:   1.0403199   0.8572793   1.0135684   1.5505824   2.0913992   2.4891658
0:  -0.4417796  -0.17201376]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.9730983  8.73514    9.458327   9.845291   9.864892   9.696273
0:   9.701401  10.056067  10.490797  10.868694  10.924198  10.339661
0:   9.775449   9.304111   9.266799   9.839711  10.630525  11.289859
0:   7.850732   8.484719 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.816119   -11.360181   -10.597459    -9.725862    -8.949687
0:   -8.450586    -7.817315    -7.217767    -6.5380106   -5.7268324
0:   -4.9831133   -4.239642    -3.5100536   -2.5886116   -1.4934082
0:   -0.25663328   0.8590894    1.723165     1.0636878    1.5613112 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.55419636 1.3614974  2.2928367  2.9848824  3.2611222  3.2249694
0:  3.2626038  3.6340353  4.100747   4.4967766  4.5067167  3.7926486
0:  3.040224   2.5091991  2.5558963  3.4316025  4.490124   5.3594103
0:  2.3306217  2.886065  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 3.6531205  4.3428698  5.177489   5.822625   6.3597603  6.7292585
0:   7.229328   7.761893   8.212376   8.704323   9.01362    9.231539
0:   9.495016  10.00182   10.849216  11.889809  12.932243  13.757153
0:  14.440194  14.727375 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.64759  28.881666 29.067549 28.997643 28.78854  28.313803 28.095587
0:  27.765429 27.416214 27.184849 26.86632  26.706673 26.635286 26.679096
0:  26.785938 26.895792 26.974625 27.034489 27.458767 27.747395]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6785383  -1.7993135  -0.67981577  0.09353924  0.40578175  0.27724886
0:   0.11599207  0.11889172  0.2961588   0.626843    0.83708096  0.74774027
0:   0.605885    0.64235544  0.98721933  1.7412872   2.5369983   3.143855
0:   0.9241862   1.9964356 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.9094934 -3.986165  -3.005167  -2.3492527 -2.0562325 -2.0196114
0:  -1.9466686 -1.6127834 -1.3451538 -1.182827  -1.383914  -2.1009512
0:  -2.7620277 -3.131175  -3.0522385 -2.3992987 -1.6977787 -1.1547146
0:  -4.4018946 -3.5946455]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4467282 2.8962865 3.5352669 4.0223637 4.2554245 4.235878  4.2647266
0:  4.3973656 4.568769  4.774063  4.7738647 4.481856  4.1527014 4.0201216
0:  4.2260237 4.871401  5.6469393 6.2729187 4.2930903 4.964671 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.5531163  -3.0108838  -2.270813   -1.7646046  -1.5937204  -1.6845875
0:  -1.7464342  -1.4868984  -1.1698489  -0.8487663  -0.863287   -1.4724326
0:  -2.131826   -2.5753722  -2.4906592  -1.6298432  -0.5642562   0.41930437
0:  -3.1833692  -2.7925048 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [24.715513 24.64369  24.48883  24.080835 23.631752 23.040594 22.769663
0:  22.414774 22.078245 21.79599  21.409388 21.206207 21.068417 21.111366
0:  21.114752 21.04383  20.830572 20.43769  20.658808 20.689623]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.33049107e-01 -1.04393959e-02 -4.49820518e-01 -1.02097845e+00
0:  -1.53276873e+00 -2.04353857e+00 -2.13909769e+00 -2.05823088e+00
0:  -1.77640009e+00 -1.05686140e+00 -1.70309544e-01  1.05460787e+00
0:   2.48281431e+00  4.25251961e+00  6.13498783e+00  7.87404203e+00
0:   9.35843849e+00  1.03359995e+01  1.27029753e+01  1.36158247e+01]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-22.395657 -22.248672 -21.641487 -21.024143 -20.48067  -20.128366
0:  -19.749277 -19.194582 -18.668653 -18.216984 -18.22757  -18.668201
0:  -19.297462 -19.734882 -19.731005 -19.357384 -18.89742  -18.602253
0:  -23.61449  -23.620003]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.1379886 -6.269366  -5.219461  -4.336745  -3.8370662 -3.659327
0:  -3.6435227 -3.4361243 -3.3010602 -3.265184  -3.5297856 -4.371575
0:  -5.1255727 -5.5455966 -5.36962   -4.426417  -3.3609328 -2.423612
0:  -5.91361   -5.277551 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.718443 25.37845  25.138737 24.55438  23.775898 22.753925 22.042479
0:  21.354225 20.796741 20.434639 19.938505 19.576904 19.105928 18.758219
0:  18.517885 18.375868 18.253345 18.080578 18.665222 18.734404]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.775131 11.841639 11.945327 11.733402 11.489498 11.101976 11.085475
0:  11.104141 11.129819 11.33649  11.369757 11.617338 11.682816 11.872633
0:  12.039443 12.151724 12.201017 12.120467 15.037849 15.32162 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.8073778  -3.0520124  -2.1438904  -1.509922   -1.2697592  -1.3532166
0:  -1.370636   -1.1297784  -0.86978817 -0.629961   -0.7472477  -1.4885464
0:  -2.3537416  -3.1008186  -3.5003138  -3.23098    -2.7785444  -2.35534
0:  -6.666602   -6.2492743 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.351234  8.79471   9.342641  9.630381  9.756857  9.696678  9.770649
0:   9.989935 10.178949 10.401554 10.487467 10.324778 10.310694 10.476946
0:  10.98351  11.830021 12.835483 13.653706 13.217028 14.037855]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.893144  4.1340656 4.467552  4.7659883 4.9476576 4.935481  5.02015
0:  5.033361  4.9789543 4.8823605 4.6326265 4.3068924 4.0377893 3.9716105
0:  4.1126966 4.406947  4.6862884 4.764619  3.4458807 3.808265 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.6068864 6.97407   7.4801817 7.80524   7.905652  7.7809415 7.7562504
0:  7.8035398 7.8747916 7.935383  7.790275  7.3868403 6.9863086 6.702711
0:  6.6833267 6.9746723 7.371706  7.6906586 5.5858345 5.8961496]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.600296  9.96584  10.531532 10.83373  10.938072 10.845025 10.940016
0:  11.105291 11.170117 11.315207 11.192991 10.926861 10.53187  10.32712
0:  10.372923 10.620516 10.897133 10.963928 10.602353 10.747747]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.8691025  7.19145    7.5816097  7.888415   8.187049   8.320585
0:   8.800285   9.178571   9.533573  10.011044  10.347498  10.844764
0:  11.288906  11.932128  12.611076  13.2302    13.792722  14.139451
0:  14.693661  14.86253  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.774355  -12.3905945 -11.755753  -11.257315  -10.996201  -10.927616
0:  -10.784285  -10.290485   -9.740454   -9.152544   -8.925627   -9.267975
0:   -9.727846  -10.051418   -9.977785   -9.331051   -8.628153   -7.985492
0:  -11.756477  -11.452261 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.299867   -4.429689   -3.4135985  -2.6530232  -2.2936645  -2.244574
0:  -2.234964   -1.9516339  -1.6564698  -1.3880982  -1.4230161  -2.0405583
0:  -2.6043172  -2.9143767  -2.7082887  -1.7851062  -0.7456789   0.15884304
0:  -2.3041425  -1.591249  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.08725  27.262947 27.16703  26.549572 25.462622 24.036543 22.922068
0:  22.030916 21.454365 21.23769  21.106043 21.025982 21.042322 21.28567
0:  21.611853 21.98327  22.257746 22.255981 21.117018 21.298956]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.056614  -7.6408706 -6.9118295 -6.2341266 -5.722628  -5.434049
0:  -5.0464263 -4.457992  -3.881701  -3.370359  -3.3527322 -3.7944045
0:  -4.348212  -4.6082664 -4.4764276 -3.8319545 -3.3020492 -2.865912
0:  -6.839209  -6.431247 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.853561 12.92777  13.025049 12.97196  12.845806 12.586689 12.599779
0:  12.536388 12.503233 12.508564 12.379831 12.285524 12.203609 12.25741
0:  12.357337 12.540615 12.704015 12.770269 12.677063 12.826083]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3701282  -2.8326297  -1.972733   -1.3407073  -0.99242306 -0.88291216
0:  -0.6400347  -0.08473921  0.6678915   1.4904957   1.990931    1.991252
0:   1.66745     1.3489833   1.2122283   1.4874015   1.8950024   2.3085423
0:  -0.79208755  0.04775095]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.767531   9.282527   9.703352   9.929329   9.921627   9.704141
0:   9.642492   9.525769   9.339316   9.206652   9.084224   8.9962845
0:   9.203459   9.687496  10.350359  11.088131  11.715638  11.982182
0:  12.3353815 12.567966 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.532452   9.687574  10.0005865 10.235617  10.39316   10.421833
0:  10.670841  10.909751  11.147755  11.438032  11.579395  11.657465
0:  11.64356   11.711194  11.840015  11.991398  12.064457  11.943986
0:  10.149776  10.052855 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.355457 20.338768 20.305172 20.016205 19.617794 19.056473 19.011097
0:  18.95933  19.029005 19.206371 19.131622 19.129078 18.981339 18.936966
0:  18.975586 19.00777  19.027029 18.942194 20.079786 19.961773]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.9092016  8.388465   8.865465   9.103854   9.229005   9.319298
0:   9.598253  10.201719  10.813299  11.517472  11.968981  12.055951
0:  12.1249275 12.365925  12.942811  13.851488  14.910147  15.804002
0:  14.265289  14.856293 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.142096 17.31383  17.616877 17.807508 17.944744 18.001623 18.300138
0:  18.591167 18.881996 19.242043 19.487688 19.772795 20.072498 20.498686
0:  21.035236 21.58482  22.093061 22.418415 23.254602 23.806765]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.1770196   1.5930977   2.0188828   2.150622    1.94701     1.5161166
0:   1.1926985   1.1464763   1.1605606   1.1834102   0.9333782   0.27823973
0:  -0.3375759  -0.74678516 -0.7365298  -0.1487627   0.5569935   1.2250566
0:  -1.7620745  -1.2968931 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.235558   -9.540733   -8.608191   -7.8120656  -7.4420733  -7.489391
0:   -7.728132   -7.732966   -7.714994   -7.643726   -7.7791457  -8.479633
0:   -9.091335   -9.430187   -9.231844   -8.307989   -7.290028   -6.341495
0:   -9.499365   -8.987793 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.082072  -3.3994575 -2.6147828 -2.1252117 -2.0618234 -2.3057895
0:  -2.5233436 -2.4268603 -2.2899747 -2.1828842 -2.381146  -3.2339835
0:  -4.0004563 -4.5004287 -4.3931165 -3.4757504 -2.392695  -1.4670172
0:  -4.939365  -4.3782153]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.567682   -3.9094853  -3.1712804  -2.6689286  -2.5721917  -2.7549415
0:  -2.9387794  -2.7922683  -2.5702882  -2.3414369  -2.3634248  -2.9945502
0:  -3.5810132  -3.9434867  -3.7820573  -2.887465   -1.8491426  -0.92505693
0:  -1.7929854  -0.71828604]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0439038  -0.407866    0.39018822  0.98770857  1.3465185   1.5584445
0:   2.0061488   2.6962183   3.4212933   4.0723114   4.319249    3.9929674
0:   3.5059588   3.091382    3.010231    3.4657824   4.0618734   4.608473
0:   1.6380534   2.1123304 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.86067  35.549774 36.055508 36.153866 36.06441  35.756706 35.723396
0:  35.591244 35.46331  35.52846  35.559265 35.84813  36.23136  36.798332
0:  37.385643 37.71351  37.844448 37.685863 39.758705 40.48402 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5816298 -5.671811  -5.474307  -5.300112  -5.2079782 -5.2806597
0:  -5.192334  -5.097127  -4.996416  -4.8587656 -4.9817896 -5.1815205
0:  -5.514511  -5.5961237 -5.4184475 -5.0295906 -4.6180506 -4.3742876
0:  -5.5482244 -5.577571 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.3705688 -2.9155068 -2.2890868 -1.9058146 -1.8998284 -2.2665133
0:  -2.7545047 -3.0930114 -3.400025  -3.6630177 -4.1247582 -4.988011
0:  -5.8487544 -6.4575057 -6.617307  -6.0929666 -5.2785497 -4.451073
0:  -6.5869145 -5.9119554]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.509519   -5.91536    -4.9659324  -4.111263   -3.3399835  -2.7030034
0:  -1.956325   -1.0611567  -0.17655039  0.6265316   1.0714417   1.1230879
0:   0.98365927  0.85954905  0.8925109   1.1736555   1.5192041   1.7270494
0:  -2.8732057  -2.7565742 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.848312 26.62764  26.268467 25.672443 25.160995 24.617775 24.5915
0:  24.481869 24.399624 24.318401 24.03106  23.995857 23.957642 24.054905
0:  24.125565 24.136116 24.15503  24.10614  26.743008 26.870625]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.462245  9.200783  9.958057 10.523065 10.77014  10.855644 11.010019
0:  11.405468 11.811102 12.134489 12.211894 11.720873 11.317864 11.090953
0:  11.34335  12.260706 13.442177 14.506201 11.885187 12.534637]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.0167546  -0.82733727 -0.4832549  -0.2041645  -0.00536203  0.11235094
0:   0.33361435  0.6777649   0.9392438   1.0766168   0.94994354  0.5349698
0:   0.2261877   0.25673103  0.70457315  1.57932     2.525199    3.237362
0:   1.3002429   1.5932007 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.55281  22.807482 22.861351 22.555546 22.01211  21.334148 20.764563
0:  20.217995 19.586084 18.984627 18.296875 17.586447 17.020504 16.601973
0:  16.325022 16.061327 15.694244 15.184095 13.272486 13.326309]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.843283 15.175453 15.428745 15.428331 15.329205 15.09783  15.172771
0:  15.208548 15.194408 15.294416 15.251478 15.326284 15.361265 15.543562
0:  15.743357 15.914255 16.08582  16.130209 17.140696 17.59354 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.264658  15.002834  15.733172  16.160952  16.295918  16.241385
0:  16.255335  16.355988  16.37828   16.322311  16.091217  15.555453
0:  15.207507  15.0109005 15.102764  15.460194  15.860705  16.052063
0:  12.894686  13.290081 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.309922  9.265175  9.245818  8.999817  8.5644245 7.92335   7.486857
0:  7.068294  6.7258706 6.553129  6.3621554 6.2591853 6.182546  6.318474
0:  6.6320543 7.0594964 7.49869   7.7572556 8.08206   8.197456 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.359816   -2.993441   -2.3928866  -1.9413552  -1.663455   -1.6336699
0:  -1.5456758  -1.3879647  -1.214181   -1.0330563  -1.0706425  -1.3376794
0:  -1.6446195  -1.7678447  -1.6138034  -1.0903182  -0.52216864 -0.10793877
0:  -2.324729   -2.0759053 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.645392  9.825402  9.9832325 9.934518  9.6935425 9.235106  8.957079
0:  8.726276  8.584932  8.57201   8.507286  8.388973  8.302975  8.37639
0:  8.626332  9.050253  9.483911  9.754328  9.252111  9.438179 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.9673467   4.956125    5.1229467   5.0956187   4.794997    4.2018657
0:   3.6622307   3.15663     2.631324    2.1363206   1.4002051   0.41521454
0:  -0.68611956 -1.6848211  -2.4893236  -2.968855   -3.2820597  -3.5295582
0:  -7.0344     -7.3923464 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.5943356 -2.1475086 -2.3776512 -2.735961  -3.2074556 -3.5581436
0:  -3.5117521 -3.0916872 -2.5197134 -1.8181839 -1.5077381 -1.6315956
0:  -2.1347208 -2.697649  -3.090187  -3.3379307 -3.5062246 -3.7755709
0:  -7.070076  -7.037801 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.178438   -9.55406    -8.697882   -8.02936    -7.695125   -7.6703973
0:   -7.695329   -7.4674973  -7.2247443  -6.971927   -6.994639   -7.6305394
0:   -8.219095   -8.590649   -8.451927   -7.618962   -6.69589    -5.8760123
0:   -8.323927   -7.702019 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.893524 19.74117  20.360537 20.52973  20.245502 19.623405 19.019054
0:  18.58056  18.16194  17.79072  17.307497 16.380276 15.530951 14.697647
0:  14.081199 13.934719 14.06748  14.275093 10.528836 10.973508]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.100086   -9.678713   -8.8992     -8.337816   -7.9070745  -7.6113505
0:   -7.113155   -6.427982   -5.713302   -5.001497   -4.75604    -4.9584193
0:   -5.4492764  -5.708038   -5.643349   -5.194794   -4.75466    -4.5121865
0:   -7.1869264  -6.787296 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.5718656 7.158431  7.9197907 8.500761  8.770882  8.765108  8.723634
0:  8.96246   9.126486  9.293435  9.122583  8.448612  7.752985  7.3064294
0:  7.4042397 8.180927  9.08782   9.850694  7.29003   7.703621 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.020084 22.25355  22.247309 21.694328 21.117834 20.41616  20.172268
0:  19.946434 19.648705 19.547249 19.293016 19.424679 19.416475 19.553387
0:  19.486576 19.133738 18.63762  18.013369 21.294333 21.715206]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7883673 3.4644866 4.1818333 4.62389   4.6902    4.5164604 4.4141226
0:  4.646082  5.003088  5.35035   5.4516225 4.9792595 4.549424  4.3062544
0:  4.5138073 5.3659735 6.3603044 7.1462007 2.2659287 2.5418706]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.6003184 4.858433  5.230722  5.47101   5.567324  5.4767947 5.503336
0:  5.54752   5.6175494 5.722154  5.6692123 5.526229  5.3466654 5.2932563
0:  5.413464  5.713728  6.0491967 6.280115  4.781754  5.021649 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.3423   34.63059  34.84823  34.73529  34.724617 34.67313  35.196728
0:  35.66551  36.068832 36.427795 36.401585 36.488255 36.2698   35.973507
0:  35.47646  34.676296 33.726788 32.667618 32.184635 32.186577]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.41286   -5.850462  -5.0653505 -4.4625826 -4.0939493 -3.9874048
0:  -3.9096012 -3.6298704 -3.3388472 -3.0301638 -2.9577475 -3.3155766
0:  -3.7010775 -3.9276152 -3.8226252 -3.1989899 -2.4733295 -1.8238573
0:  -5.1351156 -4.623045 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4057484 1.6847267 2.2158074 2.624898  2.8094878 2.7663012 2.8639216
0:  3.1109538 3.441029  3.825061  3.9350896 3.6787822 3.2819102 2.9944615
0:  2.9557505 3.317053  3.768937  4.0647078 1.72051   1.9935274]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.7263126 -1.852426  -0.7454238  0.2256136  0.9435706  1.3830514
0:   1.8148141  2.350806   2.7586086  3.0313997  2.911594   2.293687
0:   1.668551   1.2742796  1.2823191  1.8081937  2.399509   2.8590248
0:  -0.8999324 -0.2836995]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.436821 27.241692 26.874115 26.329979 25.727345 24.963123 24.53624
0:  23.91769  23.321861 22.872257 22.492428 22.512611 22.829796 23.462887
0:  24.121246 24.60316  24.903133 24.837543 28.63787  28.54472 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.929436 -17.557007 -17.581371 -17.412388 -17.166267 -16.925896
0:  -16.435589 -15.701096 -14.770758 -13.684652 -12.668867 -11.927536
0:  -11.231632 -10.437889  -9.456436  -8.388205  -7.452002  -6.922486
0:  -11.758024 -11.520714]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.3646    -4.8193283 -4.113456  -3.6453505 -3.4517584 -3.5186672
0:  -3.4897137 -3.222128  -2.8134842 -2.368329  -2.1759624 -2.414484
0:  -2.8231382 -3.1166725 -3.2015266 -2.7946262 -2.243637  -1.70154
0:  -4.600772  -4.1244774]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.1293793 -1.1969676 -1.2034316 -1.3695498 -1.6673894 -2.0844102
0:  -2.3434615 -2.4718595 -2.5818334 -2.655336  -2.9040189 -3.325315
0:  -3.7708344 -3.9581037 -3.8764243 -3.4507346 -2.9045396 -2.4739652
0:  -3.3654141 -2.9070153]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.7588    14.368956  14.92543   15.170671  15.210842  15.163149
0:  15.297764  15.586523  15.859333  16.059628  16.03038   15.7151375
0:  15.41548   15.130718  15.010605  14.993492  14.952839  14.774757
0:  11.506514  11.69215  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.5049186  6.5801964  6.8256736  6.989931   7.0844574  7.0154157
0:   7.2517624  7.514063   7.875803   8.319516   8.544302   8.596363
0:   8.532525   8.708439   9.197237  10.12012   11.183405  12.0767
0:  10.682539   9.956639 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.1255126 -3.5016494 -2.7885003 -2.3085513 -2.1855245 -2.3492818
0:  -2.526547  -2.432261  -2.2918773 -2.2033105 -2.474751  -3.3066964
0:  -4.1387095 -4.619663  -4.529897  -3.6446595 -2.564159  -1.5033784
0:  -3.8971534 -3.2295203]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.010712 24.898552 24.717571 24.216572 23.707218 23.026394 22.77684
0:  22.461504 22.190294 22.050682 21.731558 21.639402 21.387493 21.208366
0:  20.999218 20.76196  20.579891 20.372734 22.683262 22.686466]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.387356 18.35647  18.371218 18.286848 18.215658 18.023684 18.224617
0:  18.328789 18.470156 18.685705 18.735184 18.884932 18.937763 19.034653
0:  19.160793 19.24376  19.311344 19.31582  19.851624 19.651886]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.652946  11.861717  12.11909   12.129159  11.884975  11.439606
0:  11.0567875 10.762135  10.513795  10.371222  10.153922   9.828895
0:   9.538272   9.3827915  9.453168   9.730198  10.054248  10.275208
0:   8.900462   9.116911 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.66016  17.992683 18.165792 18.007256 17.634773 17.101212 16.689606
0:  16.2987   15.839516 15.419226 14.856472 14.209133 13.540117 12.982601
0:  12.502934 12.049414 11.62581  11.148821 10.229406 10.323168]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [9.065752 9.251075 9.522365 9.564617 9.433652 9.096991 8.969422 8.85655
0:  8.82896  8.948306 8.920987 8.871238 8.626335 8.434089 8.350811 8.373939
0:  8.427946 8.400072 8.341761 8.490542]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7733779  -0.9656391   0.04562616  0.83381844  1.2032747   1.1942363
0:   1.0620236   1.1157427   1.0842643   0.98231363  0.57339334 -0.43701124
0:  -1.345777   -1.9653983  -1.9903579  -1.258779   -0.35855627  0.44504833
0:  -3.131576   -2.527495  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.598407  5.9335613 6.3828588 6.632811  6.7401805 6.6577234 6.617305
0:  6.688889  6.7535954 6.8441257 6.797993  6.528915  6.3008037 6.310532
0:  6.6983967 7.4083915 8.255211  8.950756  7.3094645 7.9677734]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.4943957 6.609429  6.91296   7.031531  7.034411  6.882099  6.9565735
0:  7.095472  7.2349153 7.5136576 7.6075497 7.613238  7.4600644 7.4453387
0:  7.6129675 7.983462  8.473864  8.895259  9.63212   9.979339 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.24535  21.894276 22.45721  22.665226 22.57882  22.274555 22.225971
0:  22.204922 22.22039  22.367767 22.36491  22.31208  22.205399 22.265427
0:  22.523815 22.938385 23.421242 23.75858  24.566605 25.153648]
0: validation loss for strategy=forecast at epoch 5 : 0.2975572645664215
0: validation loss for velocity_u : 0.11368498206138611
0: validation loss for velocity_v : 0.1179804801940918
0: validation loss for specific_humidity : 0.16119077801704407
0: validation loss for velocity_z : 0.5975967049598694
0: validation loss for temperature : 0.091094970703125
0: validation loss for total_precip : 0.7037959694862366
0: 6 : 17:50:43 :: batch_size = 96, lr = 1.8119012895995104e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 6, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1204, 1.0893, 1.0430, 0.9986, 0.9752, 0.9742, 0.9842, 0.9942, 0.9945, 0.9767, 0.9401, 0.8949, 0.8618, 0.8614,
0:         0.8807, 0.8816, 0.8489, 0.7878, 1.0790, 1.0227, 0.9680, 0.9424, 0.9485, 0.9621, 0.9594], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5059, 0.6127, 0.7246, 0.7928, 0.7774, 0.6874, 0.5658, 0.4591, 0.3917, 0.3582, 0.3738, 0.4801, 0.6882, 0.9612,
0:         1.2238, 1.4011, 1.4771, 1.4927, 0.5442, 0.6529, 0.7268, 0.7266, 0.6551, 0.5570, 0.4713], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2576, -0.1959, -0.1403, -0.1583, -0.2325, -0.2828, -0.3432, -0.3864, -0.3881, -0.3724, -0.3442, -0.2727,
0:         -0.1724, -0.0064,  0.3226,  0.5865,  0.8442,  1.0142, -0.1556, -0.0936, -0.1366, -0.2124, -0.2936, -0.3659,
0:         -0.4253], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 2.2592,  1.0104, -0.9279, -2.8396, -3.8541, -3.5712, -1.7589,  0.3518,  1.3972,  1.8304,  2.3586,  3.1256,
0:          3.5123,  2.6924,  1.5144,  0.6811, -0.4704, -1.3588,  1.4900, -0.1499, -2.3645, -3.7303, -3.2971, -1.7942,
0:         -0.1565], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.1510, 0.2009, 0.2395, 0.2708, 0.2899, 0.3088, 0.3318, 0.3566, 0.3847, 0.3965, 0.3952, 0.4125, 0.4557, 0.5380,
0:         0.6649, 0.7999, 0.9162, 0.9851, 0.9795, 0.9304, 0.8830, 0.8574, 0.8621, 0.8854, 0.9348], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2207, -0.2184, -0.2343, -0.2366, -0.2366, -0.2025, -0.2003, -0.2161, -0.2025, -0.2343, -0.2320, -0.2343,
0:         -0.2366, -0.2366, -0.2366, -0.2343, -0.2320, -0.2048, -0.2320, -0.2320, -0.2320, -0.2366, -0.2366, -0.2366,
0:         -0.2366], device='cuda:0')
0: [DEBUG] Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,
0:             nan, -0.2366,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan, -0.2320,     nan, -0.2366,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366, -0.2366,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1526,     nan,
0:             nan,     nan,     nan, -0.2366,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2366, -0.1980,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2343, -0.2354,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan,     nan,
0:             nan, -0.1367,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2366,     nan,     nan,     nan, -0.2366,     nan,     nan,     nan, -0.2366,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2354,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2161,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1594,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,     nan, -0.1299,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 6, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.4126, 0.4146, 0.4128, 0.3997, 0.3841, 0.3667, 0.3780, 0.3973, 0.4194, 0.4404, 0.4451, 0.4352, 0.4187, 0.4133,
0:         0.4200, 0.4383, 0.4587, 0.4683, 0.4154, 0.4040, 0.3936, 0.3697, 0.3365, 0.3172, 0.3176], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7285, 0.7467, 0.7409, 0.6994, 0.6348, 0.5714, 0.5133, 0.4734, 0.4361, 0.3937, 0.3461, 0.2981, 0.2324, 0.1534,
0:         0.0722, 0.0250, 0.0164, 0.0375, 0.7174, 0.7688, 0.7842, 0.7501, 0.6783, 0.5924, 0.5190], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5151, -0.5119, -0.5140, -0.5077, -0.5076, -0.5064, -0.5102, -0.5129, -0.5167, -0.5186, -0.5199, -0.5210,
0:         -0.5191, -0.5195, -0.5235, -0.5242, -0.5254, -0.5209, -0.5227, -0.5194, -0.5166, -0.5110, -0.5091, -0.5099,
0:         -0.5115], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0983,  0.1285,  0.3884,  0.4567,  0.4593,  0.5450,  0.5399,  0.4328,  0.3361,  0.2548,  0.1837,  0.2089,
0:          0.3092,  0.3177,  0.3757,  0.5301,  0.6410,  0.7307,  0.1325,  0.2420,  0.3810,  0.4543,  0.5412,  0.7028,
0:          0.7563], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4402, -0.4812, -0.5126, -0.5274, -0.5191, -0.4848, -0.4405, -0.3982, -0.3683, -0.3632, -0.3860, -0.4236,
0:         -0.4632, -0.4985, -0.5247, -0.5535, -0.5836, -0.6140, -0.6397, -0.6550, -0.6535, -0.6403, -0.6236, -0.6090,
0:         -0.6034], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1902, -0.1915, -0.1903, -0.1978, -0.2092, -0.2120, -0.2159, -0.2083, -0.2059, -0.1909, -0.1900, -0.2004,
0:         -0.1999, -0.2087, -0.2116, -0.2110, -0.2123, -0.2075, -0.1943, -0.2015, -0.1975, -0.2098, -0.2118, -0.2099,
0:         -0.2084], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.12469828873872757; velocity_v: 0.15532410144805908; specific_humidity: 0.18431387841701508; velocity_z: 0.5721875429153442; temperature: 0.11970608681440353; total_precip: 0.7782500386238098; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13559451699256897; velocity_v: 0.15248917043209076; specific_humidity: 0.1930747628211975; velocity_z: 0.5637972950935364; temperature: 0.12649647891521454; total_precip: 1.0546163320541382; 
0: epoch: 6 [1/5 (20%)]	Loss: 0.91643 : 0.31440 :: 0.19542 (2.67 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15629227459430695; velocity_v: 0.1935601532459259; specific_humidity: 0.19461548328399658; velocity_z: 0.490854412317276; temperature: 0.13204199075698853; total_precip: 0.9026397466659546; 
0: epoch: 6 [2/5 (40%)]	Loss: 0.90264 : 0.31136 :: 0.19814 (16.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18664640188217163; velocity_v: 0.20742295682430267; specific_humidity: 0.22462888062000275; velocity_z: 0.5678954720497131; temperature: 0.14566545188426971; total_precip: 1.1478345394134521; 
0: epoch: 6 [3/5 (60%)]	Loss: 1.14783 : 0.37861 :: 0.19853 (16.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18912798166275024; velocity_v: 0.1763744205236435; specific_humidity: 0.20771874487400055; velocity_z: 0.663868248462677; temperature: 0.13623714447021484; total_precip: 1.0873523950576782; 
0: epoch: 6 [4/5 (80%)]	Loss: 1.08735 : 0.37471 :: 0.20598 (16.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 20):
0: [4.7683716e-07 4.7683716e-07 9.5367432e-07 9.5367432e-07 9.5367432e-07
0:  9.5367432e-07 9.5367432e-07 9.5367432e-07 9.5367432e-07 9.5367432e-07
0:  9.5367432e-07 9.5367432e-07 9.5367432e-07 9.5367432e-07 1.4305115e-06
0:  1.4305115e-06 1.4305115e-06 1.4305115e-06 1.4305115e-06 1.4305115e-06]
0: Target values (first 20):
0: [1.4305115e-05 1.4781952e-05 1.4781952e-05 1.5258789e-05 1.5258789e-05
0:  1.5735626e-05 1.5735626e-05 1.5735626e-05 1.6212463e-05 1.6212463e-05
0:  1.6212463e-05 1.5735626e-05 1.5258789e-05 1.4781952e-05 1.4305115e-05
0:  1.3828278e-05 1.3351440e-05 1.2874603e-05 1.2397766e-05 1.1920929e-05]
0: Prediction values (first 20):
0: [-3.3489432  -2.9917073  -2.359037   -1.8845086  -1.6211286  -1.5858064
0:  -1.4134107  -1.0093489  -0.5547147  -0.08349752  0.01406717 -0.3301115
0:  -0.8514371  -1.2075887  -1.2216616  -0.69693136 -0.02914715  0.5889807
0:  -2.905703   -2.6480527 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.112, max = 2.358, mean = 0.006
0:          sample (first 20): tensor([-0.8521, -0.8218, -0.7683, -0.7282, -0.7059, -0.7029, -0.6883, -0.6541, -0.6157, -0.5758, -0.5675, -0.5967,
0:         -0.6408, -0.6709, -0.6721, -0.6277, -0.5712, -0.5189, -0.8266, -0.8163])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [30.38875  30.227306 29.884592 29.140985 28.50945  27.833982 27.708395
0:  27.439566 27.046282 26.743563 26.2621   26.26767  26.28574  26.551163
0:  26.713112 26.571442 26.34218  26.019232 27.50082  27.98996 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.025476 25.018866 24.990746 24.659292 24.306417 23.807951 23.768377
0:  23.56199  23.394796 23.30887  23.05523  23.093876 23.167765 23.399418
0:  23.630339 23.79078  24.011686 24.21133  26.83577  27.204796]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.081463 22.168922 22.104893 21.791788 21.499603 21.043118 21.077484
0:  20.986143 20.886364 20.91968  20.740509 20.885597 21.036877 21.28689
0:  21.546213 21.589073 21.616575 21.541176 23.450262 23.711802]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.014907   -4.2994213  -3.3451643  -2.6266785  -2.3191223  -2.3245707
0:  -2.269247   -1.9124012  -1.4680719  -1.0399084  -0.9592309  -1.521431
0:  -2.106226   -2.4801483  -2.3528404  -1.514473   -0.5694976   0.25432348
0:  -3.3134623  -2.6231108 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.2229295 -3.6667352 -2.9987512 -2.6187806 -2.6367326 -2.9322448
0:  -3.162397  -3.0270505 -2.810413  -2.5905337 -2.7210746 -3.4846654
0:  -4.271485  -4.817866  -4.776427  -3.934362  -2.8487334 -1.8296528
0:  -3.1444173 -2.5007405]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.285159   5.5573554  5.972643   6.1192946  5.861233   5.2315135
0:  4.666999   4.18437    3.806363   3.5628443  3.2238193  2.7193127
0:  2.2092414  1.9177227  1.9139714  2.2232976  2.5931563  2.854411
0:  0.34695053 0.49832106]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.6133823 7.854981  8.113962  8.269943  8.296817  8.172799  8.283272
0:  8.403601  8.523948  8.6480875 8.604162  8.458587  8.315356  8.260435
0:  8.34285   8.551703  8.727314  8.792249  7.1602864 7.082896 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.2228246  6.4259644  6.80132    7.10771    7.401612   7.584614
0:   8.061975   8.483217   8.826739   9.228975   9.397513   9.584539
0:   9.741625  10.069173  10.528734  10.976569  11.412167  11.662823
0:  12.146019  12.582485 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.9205701 4.8235188 5.8751373 6.709862  7.119792  7.192608  7.262414
0:  7.578278  7.8769894 8.022499  7.712694  6.680258  5.6259875 4.822524
0:  4.6448975 5.28755   6.139059  6.8454885 2.474056  2.6944463]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.2138333 -6.4676976 -5.5635943 -4.763079  -4.398684  -4.3476768
0:  -4.4017015 -4.148957  -3.9099278 -3.7705092 -4.0068936 -5.002936
0:  -5.967461  -6.5947127 -6.545398  -5.5829196 -4.4913335 -3.5567422
0:  -6.703152  -6.146746 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.145613  -7.800551  -7.0824637 -6.5063224 -6.0720935 -5.8481145
0:  -5.446551  -4.7922044 -4.0634027 -3.2641854 -2.8110518 -2.837191
0:  -3.0531402 -3.069528  -2.705669  -1.9157681 -1.0241675 -0.3352418
0:  -2.4321065 -1.8381124]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.911526   -6.1665587  -5.3065066  -4.7111897  -4.413814   -4.3949847
0:  -4.269195   -3.946426   -3.527884   -3.1236348  -3.0008893  -3.2747436
0:  -3.557241   -3.5344605  -3.1102595  -2.1833787  -1.1839614  -0.31879663
0:  -2.6424117  -2.065083  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.622452  14.894659  14.923931  14.402244  13.801596  13.135071
0:  12.959879  12.845875  12.681725  12.679365  12.458059  12.545328
0:  12.459895  12.576176  12.555858  12.298195  11.9015255 11.373173
0:  14.159794  14.610498 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.941586 24.227203 24.347244 24.052011 23.581741 22.971397 22.7277
0:  22.435051 22.025606 21.636026 21.006424 20.469215 20.046917 19.90747
0:  20.03189  20.316462 20.79818  21.322214 25.044199 26.338385]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.7806752  4.637171   5.5662727  6.167014   6.228862   5.882288
0:  5.6084805  5.687951   5.84349    5.9158196  5.4803166  4.240885
0:  2.9288402  1.9075847  1.6329274  2.366311   3.4046347  4.3316956
0:  0.41893196 0.956511  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.315115   7.1559124  8.011774   8.547537   8.636233   8.48825
0:   8.439406   8.771421   9.144699   9.432454   9.300463   8.475249
0:   7.6608224  7.0899396  7.1549444  8.040237   9.195124  10.250659
0:   7.4459314  8.220512 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.232512  -5.2524467 -4.1788535 -3.429749  -3.2099519 -3.3574739
0:  -3.417418  -3.104488  -2.7251372 -2.4497952 -2.6964107 -3.8189921
0:  -4.954143  -5.7309537 -5.7196765 -4.6435914 -3.3384566 -2.2080207
0:  -5.376019  -4.6349854]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.767056  6.461775  7.2766604 7.869009  8.12108   8.072456  8.088754
0:  8.207734  8.343826  8.460618  8.285173  7.7688465 7.198629  6.7459292
0:  6.6176405 6.8464556 7.156213  7.322168  5.6591654 6.039788 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0284195  -2.3073635  -2.4153433  -2.744275   -3.2142878  -3.7243643
0:  -3.9368324  -3.8234072  -3.580783   -3.2120848  -3.1096659  -3.2880125
0:  -3.5565863  -3.5299368  -3.1015263  -2.2875738  -1.3476076  -0.61712694
0:  -1.426465   -1.3845415 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -8.529549   -7.9912868  -7.0200286  -6.291144   -5.863754   -5.6857657
0:   -5.419994   -4.8517375  -4.2989974  -3.8251004  -3.8638291  -4.4608436
0:   -5.3012004  -6.005715   -6.283561   -6.208882   -6.052482   -6.02266
0:  -11.06315   -10.68297  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.8970113  -5.1942773  -4.1815724  -3.391399   -2.8857684  -2.5640178
0:  -1.9921107  -0.9147053   0.36903858  1.6785436   2.518744    2.580768
0:   2.3535733   2.1660404   2.3274713   3.0928776   3.9675496   4.6823373
0:  -1.0844264  -0.7174387 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.314547 16.612291 16.903313 17.020626 17.07871  17.029322 17.322363
0:  17.504211 17.642767 17.812967 17.793377 17.889639 18.0037   18.389189
0:  18.929983 19.514498 20.01518  20.229956 21.108511 21.299997]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.838907  -16.18733   -15.150415  -14.469898  -14.094841  -13.804764
0:  -13.135775  -11.869389  -10.376617   -8.870701   -8.068348   -8.086716
0:   -8.593613   -9.032219   -9.036224   -8.496685   -7.839722   -7.2851987
0:  -11.9560795 -11.3613205]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.210247  4.6223965 5.085001  5.2660675 5.0830193 4.612251  4.3272176
0:  4.2188067 4.233014  4.330405  4.239545  3.843568  3.4948275 3.2965467
0:  3.4378567 4.021457  4.6868515 5.31524   4.1022115 4.8717566]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.791101  2.3496647 3.0584748 3.5115814 3.6976976 3.678519  3.8096523
0:  4.1878586 4.676615  5.258927  5.55911   5.4911594 5.28694   5.1406136
0:  5.3224344 5.8985963 6.5980816 7.2109704 5.2578263 5.788682 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.811401  12.213586  12.503952  12.621038  12.573592  12.416275
0:  12.533453  12.705288  12.920628  12.972137  12.787716  12.387173
0:  12.1845665 12.050201  12.032075  12.082506  11.844439  11.525344
0:   9.690204   9.946125 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [7.8177657 8.230761  8.787119  9.158201  9.291016  9.173578  9.155676
0:  9.199761  9.268076  9.39195   9.325244  9.0764265 8.797073  8.636862
0:  8.728657  9.103863  9.573176  9.92131   8.078853  8.462662 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.7963538  -1.040134   -0.01527452  0.78948927  1.2361093   1.3211837
0:   1.4172864   1.7623873   2.1401067   2.5059943   2.4656353   1.8368702
0:   1.1016617   0.59357214  0.6160965   1.4072466   2.4252973   3.4263768
0:   0.22767639  0.849576  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.548834   -9.843855   -8.920706   -8.242592   -7.929769   -7.933289
0:   -7.8960786  -7.50293    -7.040993   -6.5787206  -6.481007   -7.0418153
0:   -7.6613154  -8.063944   -7.939511   -7.0857387  -6.110192   -5.21452
0:   -7.6983457  -7.0018   ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.666475  17.873697  18.184946  18.17937   17.870008  17.292276
0:  16.797655  16.368689  15.988527  15.756548  15.499447  15.202616
0:  15.0155735 15.060261  15.276073  15.636894  15.961311  16.119036
0:  14.85893   15.215395 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.706501 23.844019 23.743631 23.235958 22.662437 21.925472 21.7285
0:  21.405386 21.024101 20.769836 20.30246  20.154633 19.9458   19.967487
0:  19.920269 19.681215 19.336636 18.794672 21.393476 21.70292 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.755126 10.728218 10.880432 10.92118  10.875148 10.680956 10.777183
0:  10.92235  11.102573 11.421946 11.609511 11.746704 11.876204 12.179842
0:  12.703072 13.44678  14.268385 14.932284 15.942993 16.39716 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.843061 26.520817 26.00344  24.96458  23.937445 22.881609 22.486202
0:  22.31697  22.195158 22.40317  22.49893  22.999065 23.45918  24.141062
0:  24.769682 25.179077 25.314821 25.073412 28.639719 29.277927]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.3602152  1.2528682  2.2301874  2.9645529  3.233241   3.1908064
0:  3.1749854  3.500918   3.8013196  3.9463613  3.5878832  2.4355488
0:  1.2699385  0.4592986  0.38933086 1.2424932  2.2976146  3.1831827
0:  0.7051587  1.4945283 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [26.00743  26.243267 26.377127 26.16421  25.829834 25.28947  25.242006
0:  25.205141 25.253729 25.575865 25.791708 26.28309  26.713734 27.282738
0:  27.811987 28.248655 28.580065 28.71936  31.500479 31.844458]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.8275466  3.7080598  4.599493   5.1597457  5.215645   4.9290915
0:  4.665412   4.7076373  4.739165   4.6234245  4.0594196  2.7639446
0:  1.5129204  0.60360384 0.443676   1.2389035  2.339995   3.3596427
0:  1.0397425  1.6894825 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [34.915535 35.26164  35.341812 35.07868  34.69289  34.143475 33.830765
0:  33.468052 33.046543 32.886585 32.766586 33.044296 33.574623 34.327545
0:  35.020264 35.305252 35.305004 34.877483 37.89011  38.224792]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.682127 22.753963 22.825598 22.635708 22.254736 21.70761  21.515007
0:  21.39258  21.350254 21.417788 21.287693 21.088337 20.942263 21.055965
0:  21.49833  22.128006 22.885565 23.378767 24.818832 25.155102]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 6.47321    6.8895326  7.531278   8.030052   8.327115   8.316564
0:   8.397408   8.445477   8.503008   8.706756   8.800626   8.845088
0:   8.927175   9.150286   9.53013   10.005887  10.44817   10.768739
0:  10.186335  10.317044 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [17.61048  18.131025 18.531483 18.387379 17.849493 16.836685 15.950556
0:  15.09366  14.2934   13.753731 13.205193 12.749689 12.33774  12.194724
0:  12.199426 12.356152 12.521811 12.596464 11.630676 11.629256]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.460009 21.202778 20.987919 20.575308 20.130535 19.420155 19.027632
0:  18.508858 17.949694 17.495913 16.827482 16.387335 15.867268 15.508661
0:  15.241085 15.002699 14.861741 14.774872 17.153278 16.900814]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.38008  21.250542 21.099228 20.82962  20.78511  20.642845 21.09967
0:  21.318455 21.484917 21.61178  21.461676 21.687988 21.832493 22.163769
0:  22.45729  22.59423  22.722923 22.74588  25.594578 25.953766]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.997448  13.452446  13.836247  13.954609  13.849985  13.600166
0:  13.516844  13.490849  13.45541   13.463905  13.334617  13.111992
0:  12.990244  13.044582  13.3258915 13.748004  14.173697  14.4168415
0:  13.975586  14.46364  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.609894 20.855831 21.11582  21.062504 20.788074 20.375046 20.10319
0:  19.93491  19.757774 19.6852   19.469452 19.15155  18.80893  18.55716
0:  18.432081 18.351871 18.236752 17.997135 17.211464 17.344696]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.8741555 -7.325992  -6.4638705 -5.632562  -5.1826034 -5.1430383
0:  -5.1939816 -5.073714  -4.963944  -5.0009694 -5.4684033 -6.6816955
0:  -7.9008527 -8.724064  -8.871353  -7.9771047 -6.798015  -5.6771407
0:  -8.739349  -8.439522 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.590342 19.160778 19.521729 19.499336 19.123072 18.523993 18.093922
0:  17.780205 17.415476 17.199732 16.897066 16.58386  16.513435 16.746695
0:  17.331577 18.104548 18.919085 19.517927 20.685707 21.549688]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.890274 20.604069 21.35648  21.75731  21.888163 21.746416 21.757317
0:  21.797987 21.84724  21.961288 21.86568  21.592928 21.296804 21.02819
0:  20.860285 20.75715  20.607094 20.37639  17.967615 18.266468]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.839478 22.11338  22.3694   22.410507 22.291138 21.959154 21.846483
0:  21.718235 21.592861 21.610298 21.513712 21.531406 21.57279  21.796986
0:  22.167145 22.586636 23.02512  23.322685 24.991264 25.390553]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.39409  28.27513  28.014013 27.595118 27.23059  26.710777 26.638317
0:  26.426352 26.186983 25.933073 25.522982 25.321232 25.124556 25.02656
0:  24.86217  24.584639 24.246758 23.867231 24.45266  24.168453]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.248837   0.8026886  1.8495297  2.4992676  2.6620502  2.5422854
0:   2.644453   3.071842   3.532709   3.8557842  3.7382622  2.9506927
0:   2.2557588  1.9159942  2.2235894  3.249296   4.3917103  5.2288947
0:   1.7493215  2.5700479]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.9648094  8.658228   9.473729  10.009965  10.232937  10.210923
0:  10.240299  10.433404  10.612181  10.796788  10.687831  10.249759
0:   9.798703   9.553385   9.737076  10.299268  10.969794  11.487314
0:   9.661534  10.290591 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.220564   6.1665816  7.110602   7.7088947  7.8226366  7.65106
0:   7.636651   8.054516   8.584737   8.998381   8.949189   8.098108
0:   7.252275   6.7151713  6.904197   8.04185    9.451223  10.610376
0:   7.489837   8.404846 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.843176  -11.174618  -10.259821   -9.580303   -9.16658    -8.977737
0:   -8.616737   -7.8097997  -6.94525    -6.1034694  -5.7885766  -6.2341456
0:   -6.8829417  -7.348388   -7.342442   -6.513635   -5.570188   -4.684526
0:   -8.914728   -8.41259  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.5642128   1.7114573   1.858582    1.7829275   1.3324413   0.68062735
0:   0.1131444  -0.14146376 -0.43830204 -0.83981323 -1.5754576  -2.9628267
0:  -4.1396666  -4.904643   -4.8787622  -3.7980433  -2.2964153  -0.7811556
0:  -2.91578    -2.2161283 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.980244 12.120157 12.215957 12.10252  11.88636  11.630925 11.708279
0:  11.956007 12.295551 12.738242 13.089821 13.385036 13.796222 14.376408
0:  15.154562 16.016373 16.890324 17.519188 17.689302 18.034798]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.5838318 1.1470623 1.9540067 2.6150868 3.1211958 3.4754536 4.0372458
0:  4.737719  5.4960785 6.2703176 6.7986784 7.0325975 7.1839604 7.365157
0:  7.717782  8.240948  8.771359  9.117395  6.521562  6.807747 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.539225  10.635155  10.849619  10.933895  10.838239  10.546444
0:  10.334112  10.136302   9.926873   9.7470665  9.438835   9.038984
0:   8.647573   8.4184265  8.386367   8.497646   8.591451   8.513938
0:   6.457511   6.555002 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.052441  10.763755  10.510409   9.967901   9.279372   8.319852
0:   7.6840353  7.0767007  6.5634713  6.2251825  5.6964717  5.2599688
0:   4.6464834  4.2492914  4.0375957  4.0590916  4.252863   4.471725
0:   5.6234927  5.5880075]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 0.09954786  1.0078487   1.9923935   2.6659014   2.9055321   2.8215652
0:   2.9252014   3.4101088   3.9735107   4.46091     4.4271283   3.546086
0:   2.4826124   1.5644321   1.2150049   1.8312469   2.8735073   3.971375
0:  -0.1713171   0.45060778]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.1769595  -7.0077014  -6.6242785  -6.3631644  -6.158134   -5.95629
0:  -5.5189834  -4.667085   -3.7321324  -2.8364396  -2.3190398  -2.3946471
0:  -2.5674968  -2.6542401  -2.3842425  -1.6445975  -0.8255129  -0.12229824
0:  -3.6112819  -3.3029075 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [29.712982 30.02267  30.1311   29.869205 29.440765 28.805359 28.443436
0:  27.978794 27.510754 27.238026 26.977142 27.106964 27.43686  28.025425
0:  28.636349 29.029083 29.26812  29.275448 31.98988  32.955605]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.24905443  0.23525953  0.9644904   1.4887857   1.6304011   1.4196224
0:   1.2649751   1.4127111   1.6400056   1.8415847   1.6657114   0.7734046
0:  -0.15816307 -0.75126123 -0.6650996   0.40855455  1.8233662   3.1087255
0:   0.36648798  0.8718457 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6759753  -2.8230433  -1.7710681  -0.9329829  -0.5579982  -0.59097385
0:  -0.6697693  -0.52215767 -0.38344955 -0.30196762 -0.58255816 -1.5200019
0:  -2.399919   -2.9748826  -2.8977323  -1.9912839  -0.8885484   0.08664036
0:  -2.927009   -2.2129502 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.6876945e+00 -1.9775171e+00 -1.2036233e+00 -7.3069429e-01
0:  -7.2728014e-01 -1.0370727e+00 -1.1488748e+00 -8.6705303e-01
0:  -4.1196871e-01  2.5424957e-03 -7.9579353e-03 -7.8318739e-01
0:  -1.4980383e+00 -1.8473606e+00 -1.4722266e+00 -1.4015722e-01
0:   1.3797531e+00  2.6256657e+00 -6.3659430e-01  2.7577829e-01]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.686428  11.195121  11.658207  11.931475  12.105481  12.138157
0:  12.416381  12.658818  12.901655  13.205265  13.417765  13.674292
0:  14.011694  14.5651245 15.212509  15.846876  16.371716  16.69125
0:  17.784548  18.206932 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.6353545 7.148522  7.668758  7.966748  8.062294  7.942612  7.9122257
0:  7.8983083 7.7929134 7.700095  7.452478  7.068866  6.85219   6.924179
0:  7.2485476 7.800596  8.306076  8.598694  6.8914824 7.341344 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.4006844 2.2610998 3.2692838 4.0459356 4.383087  4.44167   4.560846
0:  5.031996  5.525345  5.881275  5.7882924 4.8912554 4.046504  3.5112662
0:  3.7101734 4.8535156 6.2542777 7.4758263 4.248216  5.036253 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.324636 21.534653 21.5797   21.396694 21.104803 20.70744  20.625385
0:  20.553959 20.519526 20.607876 20.67843  20.867908 21.233551 21.814444
0:  22.52167  23.20397  23.88455  24.448683 26.60881  27.08248 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.750946 22.68255  23.537306 24.125216 24.584585 24.90467  25.650618
0:  26.4917   27.381348 28.314308 28.949198 29.33692  29.382206 29.136477
0:  28.541359 27.520933 26.232853 24.712149 21.365704 20.691236]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.436668  -6.616467  -5.6485324 -4.8605685 -4.5536437 -4.6075044
0:  -4.7154155 -4.514072  -4.354358  -4.2993693 -4.6627507 -5.842216
0:  -6.8962407 -7.5899215 -7.517938  -6.466821  -5.3184915 -4.3345017
0:  -7.415726  -6.8329873]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.465534  3.9226549 4.5942802 5.132149  5.4553456 5.5091877 5.6638284
0:  5.885092  6.104942  6.3550653 6.3639374 6.1079392 5.793563  5.595064
0:  5.6401887 5.9922748 6.3474464 6.5596676 3.521024  3.7738786]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.6852236  6.2840576  7.0168185  7.504716   7.711007   7.7224236
0:   7.947707   8.495004   9.169462   9.902073  10.394421  10.472394
0:  10.565439  10.833669  11.53805   12.8179455 14.328501  15.72677
0:  15.039227  16.019777 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.999159   8.980864  10.171019  11.189066  12.04232   12.62982
0:  13.392977  14.043205  14.484759  14.868982  14.955503  14.857349
0:  14.816961  15.0998955 15.817284  16.930677  18.25447   19.436249
0:  21.885752  23.100754 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.723743  13.287398  13.678528  13.773327  13.6158695 13.196413
0:  13.031057  12.803892  12.526486  12.29709   11.870543  11.39245
0:  11.01483   10.842967  10.834303  10.904567  10.820436  10.5184
0:  10.391498  10.732348 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.511581  6.831613  7.343204  7.6481743 7.7415648 7.6069613 7.6085134
0:  7.7762256 7.963786  8.187038  8.147548  7.805366  7.330091  7.0401692
0:  7.04149   7.3659477 7.7540655 7.9983253 4.642265  4.927269 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.0254807  -1.1902223  -0.25863504  0.4944358   0.7114911   0.5454216
0:   0.28233385  0.2979102   0.29148436  0.16522169 -0.37916183 -1.6961017
0:  -2.9115338  -3.719236   -3.6821828  -2.6309419  -1.3772969  -0.28826952
0:  -3.5286984  -3.0279603 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.956738  -9.578082  -8.7434435 -7.9584494 -7.330843  -6.937503
0:  -6.465873  -5.8420715 -5.1947327 -4.563076  -4.320878  -4.5318356
0:  -4.8754044 -4.9668226 -4.6498594 -3.8111715 -2.8548632 -2.0673475
0:  -4.829323  -4.1941566]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.538799  11.3016815 11.936189  12.17891   12.101643  11.929491
0:  11.942129  12.168606  12.408819  12.593361  12.580416  12.161553
0:  11.890535  11.8381    12.144966  12.900768  13.822241  14.778524
0:  13.30418   13.963982 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [13.278694  13.390146  13.592508  13.617471  13.488243  13.169159
0:  13.083883  13.016561  12.976107  13.0558815 12.997105  12.925732
0:  12.797934  12.823692  12.984068  13.2726    13.603985  13.784067
0:  12.793019  12.817322 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.669188 21.845411 22.025215 21.96256  21.825283 21.516006 21.72237
0:  21.904547 22.161118 22.432423 22.420395 22.440239 22.361584 22.370186
0:  22.393053 22.39019  22.381615 22.255825 22.719189 22.93018 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 8.508946   8.597639   8.567517   8.368786   8.092626   7.7430086
0:   7.760538   7.8253684  7.864942   7.998524   7.9790673  7.9860864
0:   8.04692    8.337057   8.741278   9.168451   9.464745   9.500763
0:  10.269366  10.654367 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.645984  14.995342  15.279771  15.326457  15.222547  14.970892
0:  14.937456  14.881794  14.784752  14.772959  14.6465645 14.571013
0:  14.560701  14.804232  15.186052  15.622255  16.058685  16.319939
0:  16.004168  16.37996  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.34326   -13.372276  -11.961596  -11.002966  -10.503271  -10.426025
0:  -10.33242    -9.8232     -9.203508   -8.548616   -8.425491   -8.900978
0:   -9.72052   -10.311676  -10.390068   -9.895664   -9.271839   -8.66991
0:  -15.442511  -14.7576275]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.095251   -6.9700093  -5.741299   -4.794253   -4.4160676  -4.462791
0:  -4.4990487  -4.1601796  -3.735436   -3.359775   -3.3872533  -4.1840587
0:  -4.928266   -5.3078923  -4.9335876  -3.6098552  -2.1581006  -0.89592123
0:  -2.606378   -1.5601506 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 7.330036  8.396079  9.596056 10.535721 11.158657 11.545984 12.132061
0:  12.878439 13.64106  14.350868 14.812197 14.802895 14.902828 15.096655
0:  15.566823 16.36669  17.14022  17.614017 13.300652 13.435741]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.176546  -11.218756  -10.099401   -9.103962   -8.594521   -8.4598465
0:   -8.419291   -8.110512   -7.79584    -7.6055484  -7.777923   -8.737938
0:   -9.576618  -10.011162   -9.694807   -8.458933   -7.196422   -6.2006087
0:   -8.839907   -8.163565 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.116085   -2.050232   -0.8246527   0.13181114  0.62809753  0.6810794
0:   0.6570616   0.82270336  0.9667616   1.058404    0.81769705  0.03254604
0:  -0.699049   -1.1303444  -0.97626305 -0.09488964  0.9306598   1.8570662
0:  -0.96764183 -0.28737354]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-2.5586114  -2.0018353  -1.2152419  -0.7380085  -0.57506704 -0.62373257
0:  -0.46111727  0.0580802   0.70388794  1.3940964   1.5808787   1.2122731
0:   0.5643959   0.02980661 -0.1101141   0.27095318  0.84386253  1.434063
0:  -2.4184203  -1.9790149 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.3410468 4.157502  4.9371142 5.3233166 5.3868885 5.311956  5.456563
0:  5.867256  6.258997  6.588523  6.616399  6.2206903 5.969649  5.956605
0:  6.4399214 7.4425516 8.585767  9.533203  7.40273   8.165151 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.3718767 2.5105233 2.8252435 3.0216155 3.0481014 2.871203  2.7862892
0:  2.7439995 2.6643662 2.510643  2.1300173 1.5454421 1.0637121 0.9051275
0:  1.1484556 1.8393235 2.6487107 3.4206438 1.3008785 1.4803872]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [8.994356  8.840168  8.807188  8.645232  8.396345  7.949555  7.7059646
0:  7.422066  7.090859  6.8641667 6.428364  6.01757   5.4414864 5.0389423
0:  4.773593  4.6130443 4.5067225 4.351527  3.5384555 3.1973672]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.1120048  -4.367184   -3.4883218  -2.8192992  -2.594294   -2.641933
0:  -2.6725373  -2.312592   -1.8577418  -1.4664111  -1.4699159  -2.2915435
0:  -3.1335511  -3.7236228  -3.6364474  -2.5844808  -1.2673144  -0.05356216
0:  -3.1938243  -2.5985436 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [11.278596  11.289665  11.460391  11.475212  11.359716  11.045401
0:  10.89907   10.7571945 10.592224  10.478878  10.190056   9.835497
0:   9.493569   9.3196335  9.366878   9.586152   9.825093   9.958483
0:   8.8441305  9.01615  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.62872124 1.5944314  2.5505743  3.1738174  3.3378863  3.2593827
0:  3.351477   3.7892954  4.314926   4.717531   4.684093   3.9329402
0:  3.2274022  2.807754   3.0540848  4.1212397  5.452269   6.6434884
0:  3.3022296  4.0593996 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.912013  11.129145  11.361434  11.3642645 11.252605  10.998737
0:  11.048523  11.133684  11.231592  11.420108  11.449479  11.486717
0:  11.429583  11.524918  11.758411  12.123728  12.530714  12.836744
0:  12.947678  13.287803 ]
0: validation loss for strategy=forecast at epoch 6 : 0.4293774366378784
0: validation loss for velocity_u : 0.13853952288627625
0: validation loss for velocity_v : 0.1367073655128479
0: validation loss for specific_humidity : 0.22027404606342316
0: validation loss for velocity_z : 0.6967107653617859
0: validation loss for temperature : 0.10263508558273315
0: validation loss for total_precip : 1.281397819519043
0: 7 : 17:54:46 :: batch_size = 96, lr = 1.7677085752190346e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 7, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0321, -1.0136, -0.9881, -0.9825, -0.9918, -1.0349, -1.0625, -1.0525, -1.0520, -1.0459, -1.0262, -1.0056,
0:         -0.9817, -0.9470, -0.9151, -0.8977, -0.8876, -0.8853, -1.0797, -1.0619, -1.0362, -1.0426, -1.0501, -1.0702,
0:         -1.0869], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0616,  0.0355,  0.0372,  0.0597,  0.0931,  0.1869,  0.2472,  0.2602,  0.2819,  0.2739,  0.2566,  0.2205,
0:          0.1707,  0.1196,  0.0764,  0.0547,  0.0136, -0.0039,  0.1580,  0.1219,  0.0973,  0.1187,  0.1481,  0.2147,
0:          0.2572], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.0734,  1.5087,  1.7676,  1.5215,  0.9844,  0.0209, -0.3140, -0.3603, -0.3934, -0.4060, -0.4032, -0.3840,
0:          0.0210,  0.2826,  0.6063,  0.7653,  1.1281,  1.3348,  0.1181,  0.4032,  0.9643,  0.5129,  0.3987, -0.1999,
0:         -0.3573], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0802, -0.0059, -0.1538,  0.0493,  0.0935, -0.1317, -0.1649, -0.1251, -0.1008, -0.1604, -0.1958,  0.0493,
0:          0.2172,  0.2172,  0.2260,  0.3099,  0.4755,  0.2878,  0.0295,  0.0537,  0.0361,  0.0847, -0.0500, -0.2819,
0:         -0.2223], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 1.8168e-01,  1.2258e-03,  8.6041e-02,  2.5332e-01,  3.2526e-01,  7.0987e-01,  8.6697e-01,  7.0872e-01,
0:          5.7553e-01,  2.3907e-01,  2.9814e-02, -9.5641e-02, -2.9337e-01, -5.5876e-01, -7.9333e-01, -9.3180e-01,
0:         -1.2056e+00, -1.3500e+00, -1.2451e+00, -1.0050e+00, -8.1544e-01, -6.9932e-01, -5.8776e-01, -5.1702e-01,
0:         -4.1634e-01], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0852,  0.0747,  0.6142,  0.8409,  0.9674,  1.1538,  1.1687,  0.9478,  0.7787, -0.0978, -0.0070,  0.1460,
0:          0.7212,  0.9570,  1.2642,  1.2308,  0.9732,  0.8271, -0.0886,  0.0713,  0.3704,  0.6948,  0.9939,  1.1400,
0:          1.1319], device='cuda:0')
0: [DEBUG] Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0760,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.7109,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.6810,     nan,     nan,
0:             nan,  0.1748,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.5222,     nan,
0:             nan,     nan,     nan,     nan,  0.5728,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0990,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0024,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.0966,     nan,     nan,     nan,  0.1483,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.4359,     nan,
0:             nan, -0.0449,     nan,     nan,     nan,     nan,  0.8696,     nan,  0.0471,     nan,     nan,     nan,
0:             nan,  0.8685,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0104,     nan,     nan,
0:             nan,     nan,     nan,  0.1345,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1322,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0679,     nan, -0.0449,     nan,
0:             nan,     nan,     nan,     nan, -0.1473,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 7, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1044, -1.0315, -0.9311, -0.8268, -0.7589, -0.7328, -0.7289, -0.7154, -0.7207, -0.7309, -0.7715, -0.8759,
0:         -0.9577, -1.0164, -1.0024, -0.8969, -0.7775, -0.6776, -0.9993, -0.9818, -0.9314, -0.8551, -0.8358, -0.8153,
0:         -0.8149], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3944, 0.3996, 0.3830, 0.3582, 0.3311, 0.3151, 0.3112, 0.3248, 0.3379, 0.3370, 0.3241, 0.3008, 0.2565, 0.1943,
0:         0.1276, 0.1040, 0.1311, 0.1928, 0.3545, 0.3796, 0.3753, 0.3465, 0.3092, 0.2763, 0.2635], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0032, 1.0895, 1.2025, 1.3190, 1.4190, 1.4669, 1.5105, 1.5393, 1.5801, 1.6415, 1.7162, 1.7503, 1.7678, 1.7355,
0:         1.6279, 1.4925, 1.3359, 1.1860, 0.9439, 1.0420, 1.1775, 1.3056, 1.4126, 1.4726, 1.5090], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5622, -0.1291,  0.3862,  0.3381,  0.2894,  0.4497,  0.2651,  0.1989,  0.3052,  0.1215,  0.0225,  0.1215,
0:          0.1025,  0.0358,  0.0277,  0.1202,  0.1671, -0.0032, -0.3588, -0.2047,  0.0583, -0.0561, -0.0646,  0.1983,
0:          0.1209], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0704, -0.1357, -0.1989, -0.2566, -0.3124, -0.3676, -0.4296, -0.4892, -0.5345, -0.5534, -0.5396, -0.4917,
0:         -0.4221, -0.3577, -0.3148, -0.3117, -0.3464, -0.3995, -0.4538, -0.5005, -0.5259, -0.5261, -0.4926, -0.4247,
0:         -0.3470], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.2800, 0.2247, 0.2083, 0.1397, 0.1030, 0.0891, 0.0780, 0.0940, 0.1061, 0.3670, 0.3049, 0.2401, 0.1693, 0.1336,
0:         0.0981, 0.1042, 0.0963, 0.1066, 0.4761, 0.3949, 0.3294, 0.2479, 0.1787, 0.1642, 0.1376], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.1734798103570938; velocity_v: 0.2594955563545227; specific_humidity: 0.22533537447452545; velocity_z: 0.5575633645057678; temperature: 0.1501195877790451; total_precip: 0.9210986495018005; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18619461357593536; velocity_v: 0.1717277467250824; specific_humidity: 0.204869344830513; velocity_z: 0.5362452864646912; temperature: 0.12651211023330688; total_precip: 0.728061854839325; 
0: epoch: 7 [1/5 (20%)]	Loss: 0.82458 : 0.31899 :: 0.20415 (2.65 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15633271634578705; velocity_v: 0.19183336198329926; specific_humidity: 0.2401229739189148; velocity_z: 0.5799272656440735; temperature: 0.12834618985652924; total_precip: 0.6874091029167175; 
0: epoch: 7 [2/5 (40%)]	Loss: 0.68741 : 0.29651 :: 0.19958 (16.03 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17312626540660858; velocity_v: 0.2112608700990677; specific_humidity: 0.2824207544326782; velocity_z: 0.6171663403511047; temperature: 0.15807472169399261; total_precip: 1.0507023334503174; 
0: epoch: 7 [3/5 (60%)]	Loss: 1.05070 : 0.37995 :: 0.20033 (16.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.198561891913414; velocity_v: 0.23031651973724365; specific_humidity: 0.2794828712940216; velocity_z: 0.520139753818512; temperature: 0.14970232546329498; total_precip: 1.103425145149231; 
0: epoch: 7 [4/5 (80%)]	Loss: 1.10343 : 0.37841 :: 0.20521 (16.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: 
0: [DEBUG] Denormalized values for total_precip:
0: Source values (first 20):
0: [1.4305115e-06 2.8610229e-06 4.2915344e-06 7.6293945e-06 1.3828278e-05
0:  1.8596649e-05 2.3841858e-05 2.8610229e-05 3.1948090e-05 3.4332275e-05
0:  3.4809113e-05 3.3378601e-05 2.7656555e-05 2.2411346e-05 1.7642975e-05
0:  1.1444092e-05 9.0599060e-06 7.6293945e-06 6.1988831e-06 6.1988831e-06]
0: Target values (first 20):
0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
0: Prediction values (first 20):
0: [ 9.751326  10.265255  10.949712  11.448061  11.654579  11.5560665
0:  11.639118  11.822645  12.114725  12.467106  12.562761  12.356676
0:  12.037804  11.762139  11.744719  12.115565  12.652569  13.131655
0:  11.268049  11.645256 ]
0: [DEBUG] Validation prediction values for 'velocity_u' with shape: torch.Size([180, 972])
0:          min = -1.473, max = 1.590, mean = 0.042
0:          sample (first 20): tensor([0.1947, 0.2400, 0.3003, 0.3442, 0.3623, 0.3537, 0.3610, 0.3771, 0.4029, 0.4339, 0.4423, 0.4242, 0.3961, 0.3718,
0:         0.3703, 0.4029, 0.4502, 0.4924, 0.2597, 0.2995])
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.800458 -13.158732 -12.240781 -11.53881  -11.15961  -11.046382
0:  -10.858882 -10.271529  -9.694671  -9.167344  -9.190218  -9.889501
0:  -10.730544 -11.252629 -11.124017 -10.219991  -9.159713  -8.176427
0:  -12.171548 -11.817941]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [28.648048 28.941334 28.912163 28.203667 27.408161 26.523102 26.258537
0:  26.063862 25.7519   25.650616 25.311226 25.345943 25.277748 25.394024
0:  25.303215 24.818886 24.144363 23.30294  24.51369  25.128212]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.472481  5.116557  5.8258357 6.1942854 6.074891  5.5592318 5.118147
0:  4.8156776 4.597522  4.4289675 4.03831   3.2676473 2.4949841 1.9504099
0:  1.8528023 2.3652039 3.1264567 3.8401918 1.9742556 2.4553456]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.985615 21.305897 21.527683 21.501326 21.222855 20.705868 20.355038
0:  20.021896 19.63052  19.372892 19.099304 18.868427 18.929497 19.230675
0:  19.7636   20.390614 21.006432 21.394701 22.225407 22.64339 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.058197  4.812326  5.6776547 6.225071  6.394818  6.251533  6.2234235
0:  6.442869  6.626585  6.784626  6.5291705 5.740384  4.9444466 4.378426
0:  4.343821  4.9179015 5.66687   6.2957816 3.2038472 3.8511639]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [43.998074 44.522556 44.579624 44.23146  43.76247  43.194954 43.144775
0:  43.0323   42.817688 42.74279  42.510315 42.706375 42.99333  43.55761
0:  44.162613 44.406067 44.470432 44.285515 46.98194  47.311764]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.8846936  5.695223   6.7449083  7.6347256  8.380713   9.046219
0:   9.96991   11.098629  12.231652  13.405815  14.253235  14.78903
0:  15.264845  15.8888    16.825592  18.052174  19.26776   20.239454
0:  20.104599  20.795164 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [25.880573 26.069496 25.941967 25.20747  24.485497 23.612492 23.45787
0:  23.361229 23.213932 23.239487 22.943611 23.033215 22.902817 22.978645
0:  22.843422 22.418678 21.861065 21.1628   25.420193 25.857956]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.925775 -13.600648 -12.071273 -10.834947 -10.223617 -10.067689
0:  -10.0176    -9.682595  -9.418927  -9.386185  -9.836644 -11.129586
0:  -12.206744 -12.661979 -12.15668  -10.599248  -8.991444  -7.684762
0:  -10.101705  -9.051692]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.532093 19.892065 19.0457   18.311962 17.939358 17.794619 18.463701
0:  19.109688 20.072989 21.613958 23.751545 26.551636 29.300987 31.186554
0:  31.42389  29.777224 26.939754 23.836233 21.569933 21.251268]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.940945 20.692947 20.416988 19.838392 19.177847 18.322018 17.90694
0:  17.49704  17.200459 17.09107  16.78483  16.578585 16.168488 15.781614
0:  15.330803 14.925356 14.592282 14.333111 15.539915 15.525328]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.124478  -6.2933345 -5.1736684 -4.34965   -4.084206  -4.377849
0:  -4.7695355 -5.024506  -5.30862   -5.6446366 -6.40066   -7.723228
0:  -8.9885025 -9.736208  -9.649534  -8.614359  -7.3045783 -6.0855575
0:  -8.272472  -7.425158 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [36.170177 36.635822 36.739243 36.21051  35.72114  35.281513 35.704903
0:  36.28685  36.919933 37.636215 38.075863 38.77772  39.34125  39.997684
0:  40.499832 40.502804 40.174007 39.496475 40.045467 40.69958 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-18.38742   -17.680172  -16.685936  -16.080475  -15.71972   -15.258939
0:  -14.333995  -12.725202  -10.963038   -9.20382    -8.192765   -7.960143
0:   -8.171071   -8.29047    -7.9119616  -7.145567   -6.237278   -5.411151
0:  -10.287354   -9.661075 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6878476e+00 -2.8364615e+00 -1.8858781e+00 -1.1996012e+00
0:  -1.0787816e+00 -1.3559909e+00 -1.5318804e+00 -1.3087049e+00
0:  -9.3226862e-01 -6.5715504e-01 -9.0188646e-01 -2.0364118e+00
0:  -3.2297053e+00 -4.0238900e+00 -3.9509883e+00 -2.7386479e+00
0:  -1.2608805e+00  2.0885468e-04 -2.1323986e+00 -1.2597980e+00]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.720192 33.061825 33.10438  32.44139  31.67926  30.835592 30.493748
0:  30.1265   29.634787 29.37742  29.004692 29.132027 29.285864 29.686602
0:  29.92028  29.74176  29.380495 28.886213 29.506935 30.395214]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [14.744317 15.136317 15.528164 15.693987 15.649466 15.394699 15.373793
0:  15.375039 15.375059 15.447243 15.319969 15.077001 14.843574 14.716564
0:  14.784449 14.988052 15.25017  15.35938  14.958093 15.280843]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.706834  3.2909076 4.121355  4.704624  4.94401   4.8418884 4.829276
0:  5.120967  5.394144  5.6564226 5.4266386 4.5647416 3.5958092 2.8508487
0:  2.7062485 3.2979014 4.0926113 4.752988  0.7906265 1.0765176]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-5.5821934  -4.7018137  -3.6257267  -2.742899   -2.2431936  -2.1065998
0:  -1.9339571  -1.497263   -1.07162    -0.7765293  -0.9361534  -1.7583733
0:  -2.549203   -2.964962   -2.697834   -1.6341553  -0.47235203  0.5126958
0:  -1.5365496  -0.7338505 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ -9.63932    -8.656868   -7.516723   -6.555632   -6.199223   -6.29791
0:   -6.517531   -6.485456   -6.447415   -6.551924   -7.0914817  -8.423759
0:   -9.641283  -10.377239  -10.24605    -9.110153   -7.901502   -6.957193
0:   -9.747128   -9.047714 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.157596   -9.729474   -9.033931   -8.474596   -8.195783   -8.231613
0:   -8.2215     -8.078865   -7.9564714  -7.9022856  -8.150139   -8.860509
0:   -9.507289   -9.774001   -9.507692   -8.524824   -7.3864775  -6.3743305
0:   -7.860404   -7.3225346]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.132523 32.300556 32.11634  31.402298 30.721333 30.062405 30.129757
0:  30.140038 30.082466 30.091957 29.88335  30.126213 30.437477 31.0774
0:  31.692913 31.901508 31.867207 31.532642 31.622353 32.02973 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.70426   10.517784  10.521895  10.425299  10.324526  10.049038
0:  10.1670885 10.314895  10.512839  10.806531  10.845424  10.854097
0:  10.685371  10.637352  10.6499405 10.748217  10.79863   10.700831
0:  10.81381   10.824417 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.697642  -6.032882  -5.2107797 -4.6850686 -4.6596837 -4.956387
0:  -5.1646485 -4.866492  -4.4547706 -3.995665  -3.9912553 -4.8606935
0:  -5.779225  -6.491237  -6.4659524 -5.4908595 -4.295814  -3.1432405
0:  -6.6036716 -6.073972 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.4793925 2.942356  3.5607977 4.0180836 4.19013   4.096677  4.1764936
0:  4.341937  4.5627794 4.807333  4.816863  4.5307736 4.164645  3.93236
0:  3.9575202 4.3074365 4.7123756 4.97876   3.100663  3.6223886]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.375361  9.588906 10.005688 10.321749 10.430961 10.274519 10.29022
0:  10.249711 10.194973 10.249792 10.136123  9.977753  9.833883  9.912735
0:  10.273225 10.888376 11.557793 12.132465 13.536754 14.073052]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [4.770487  5.2625914 5.900857  6.22975   6.325116  6.276014  6.4586325
0:  6.8801465 7.348243  7.753364  7.7233453 7.25956   6.5784264 6.097481
0:  5.9271655 6.2171044 6.6407905 6.9958987 3.1940145 3.7751803]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-8.362753  -7.346022  -6.182942  -5.3908315 -5.142176  -5.393295
0:  -5.6826673 -5.68584   -5.647799  -5.573069  -5.811999  -6.5995526
0:  -7.3151927 -7.6292853 -7.30926   -6.2226787 -5.0471015 -4.0834947
0:  -7.0504537 -6.2517705]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.133205   5.545153   5.976924   6.260092   6.4662237  6.555859
0:   6.988553   7.413345   7.8455825  8.28865    8.521899   8.769035
0:   8.98579    9.473836  10.166913  11.008665  11.924609  12.7711525
0:  15.2018795 15.672224 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.77975655 -0.24321079  0.47088957  0.9242711   0.9991274   0.80955124
0:   0.7573185   1.021831    1.3296337   1.5761709   1.3889289   0.53196573
0:  -0.40955496 -1.1427135  -1.3729839  -0.8321266  -0.03563404  0.6792283
0:  -3.3966327  -3.1924777 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [18.048107 18.304283 18.511555 18.25393  17.890656 17.417763 17.489904
0:  17.676922 17.92868  18.399488 18.648556 19.015423 19.13673  19.312395
0:  19.462978 19.485065 19.396582 19.145658 20.2738   20.566715]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.757329 10.564732 11.470551 12.036966 12.178568 11.987635 11.970997
0:  12.223848 12.548941 12.850644 12.823691 12.243781 11.703728 11.360113
0:  11.528572 12.352867 13.38649  14.212973 11.213732 11.775812]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-11.429423  -10.6257925  -9.535619   -8.649542   -8.228851   -8.2652645
0:   -8.328201   -8.066518   -7.8666277  -7.6994877  -7.9950294  -9.08786
0:  -10.138295  -10.797788  -10.734431   -9.764003   -8.728277   -7.850809
0:  -11.32598   -10.789277 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.812962   -3.8639684  -2.7722878  -1.9548774  -1.7234006  -1.9038606
0:  -2.0339751  -1.7609582  -1.402575   -1.1597962  -1.4314723  -2.6563334
0:  -3.837336   -4.5864244  -4.4358573  -3.1133971  -1.5641961  -0.22894812
0:  -3.39473    -2.5494494 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.6425557   0.20851612  1.158474    1.8568392   2.0468292   1.8641086
0:   1.7316704   1.9121027   2.0796957   2.0679564   1.4666791  -0.04456568
0:  -1.5941057  -2.807579   -3.1780033  -2.42838    -1.3350029  -0.285182
0:  -3.1141748  -2.3992825 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.69344  23.790884 23.817867 23.615171 23.153238 22.422531 21.778467
0:  21.084929 20.309526 19.709192 19.129063 18.620298 18.332771 18.237328
0:  18.28441  18.383606 18.466705 18.403324 17.838554 17.92082 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [6.879216  7.474538  8.122362  8.565713  8.64695   8.491713  8.497164
0:  8.814927  9.155773  9.427831  9.2992735 8.511818  7.721796  7.141952
0:  7.129108  7.911804  8.967728  9.898327  7.1468115 7.769775 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.851095  11.436504  12.062362  12.446409  12.571089  12.476109
0:  12.550576  12.674549  12.758692  12.856401  12.767437  12.533276
0:  12.411625  12.528074  12.9412365 13.610348  14.360375  14.973145
0:  14.953784  15.645948 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.711698  3.5243087 4.4447145 5.089436  5.2754636 5.15634   5.1419063
0:  5.5364943 6.0002728 6.355024  6.1746874 5.200394  4.148967  3.3831782
0:  3.3464954 4.235752  5.399313  6.462192  3.0121205 3.7575395]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.236324   -9.3262005  -8.232651   -7.2575116  -6.8464513  -6.8171105
0:   -6.8690324  -6.6222825  -6.3917165  -6.3050714  -6.667686   -7.8893943
0:   -9.009243   -9.664999   -9.44281    -8.221563   -6.91707    -5.9087033
0:   -8.813902   -8.173819 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.567981    4.891956    5.366591    5.5793896   5.437398    4.985858
0:   4.613386    4.4557366   4.2691383   4.06944     3.5190444   2.550508
0:   1.6296611   0.97687435  0.8274002   1.258853    1.7819042   2.1247907
0:  -1.4333725  -1.1165028 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [19.761099 20.105942 20.37051  20.279495 19.965723 19.502274 19.241419
0:  19.055317 18.853601 18.740435 18.47071  18.121468 17.816273 17.720829
0:  17.854557 18.123865 18.462542 18.63387  18.050762 18.417898]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-32.536926 -32.208183 -31.311546 -30.60854  -30.111736 -29.753185
0:  -29.17249  -28.112415 -26.959667 -25.829086 -25.504707 -25.894573
0:  -26.629128 -27.038467 -26.776306 -25.91116  -25.035263 -24.332418
0:  -26.672783 -26.169186]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.91138  17.152792 17.373558 17.484844 17.460674 17.26498  17.38356
0:  17.504648 17.61453  17.802538 17.846613 17.87771  17.953987 18.233149
0:  18.673277 19.178988 19.673082 19.961231 20.867867 21.13442 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [21.954655 22.453451 22.936636 23.108599 23.13665  22.974651 23.167988
0:  23.413296 23.657549 24.09174  24.363752 24.672785 24.92364  25.30161
0:  25.780876 26.270275 26.73627  26.982136 27.073244 27.555067]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.725857  10.697574  10.730238  10.383568   9.851807   9.209547
0:   8.8487625  8.579538   8.189712   7.9548626  7.463048   7.003643
0:   6.4423876  6.210564   6.242929   6.4995203  6.903752   7.2788544
0:   9.590067   9.999614 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.9784365  -5.952777   -4.7754526  -3.793995   -3.3734188  -3.3315248
0:  -3.2695608  -2.8353558  -2.347869   -2.009976   -2.1878119  -3.2607508
0:  -4.31112    -4.914198   -4.661497   -3.3603287  -1.9721646  -0.90078735
0:  -3.2731276  -2.418291  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [32.135284 32.14375  31.850864 31.106777 30.2425   29.265701 28.853184
0:  28.380775 27.789318 27.370583 26.710339 26.456974 26.348991 26.549427
0:  26.76064  26.691921 26.414509 25.877647 26.69765  27.148197]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 4.5436883  5.101629   5.81738    6.323525   6.640149   6.758429
0:   7.130339   7.597464   8.004387   8.442938   8.649592   8.661536
0:   8.802897   9.259996  10.134351  11.254473  12.398106  13.184383
0:  13.107068  13.310829 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-10.142722   -9.47932    -8.420199   -7.673622   -7.335205   -7.3036547
0:   -7.253129   -6.7111616  -6.122385   -5.548747   -5.5585346  -6.35771
0:   -7.406417   -8.277477   -8.563891   -8.156632   -7.56316    -6.9982595
0:  -11.922589  -11.645358 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.911165  13.0064535 13.182719  13.232147  13.136129  12.814428
0:  12.746138  12.665974  12.613612  12.689175  12.636406  12.566555
0:  12.495394  12.5205555 12.666298  12.924529  13.19681   13.31019
0:  12.751576  12.86866  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.4875903 -2.7839503 -1.9720368 -1.3025708 -1.1617332 -1.3613648
0:  -1.5667663 -1.426125  -1.2545915 -1.2296958 -1.6944871 -3.0679364
0:  -4.385218  -5.287621  -5.291119  -4.209883  -2.973329  -1.977139
0:  -5.385216  -5.058519 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [0.67645407 1.1098409  1.5725336  1.8826766  2.0494552  2.0852818
0:  2.4288216  2.7982025  3.213572   3.6097198  3.7938905  3.7938282
0:  3.7536018  3.830702   4.043185   4.467084   4.9220057  5.284896
0:  3.6509366  4.0853157 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [1.2740092 1.9074678 2.7798572 3.220205  3.3273582 3.2858448 3.5739226
0:  4.2295866 4.952647  5.722348  5.9522986 5.6415453 5.0425196 4.6604958
0:  4.7271385 5.298649  5.955497  6.421796  4.810906  5.2760363]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-14.501389  -13.910933  -13.015545  -12.364445  -12.011875  -11.806108
0:  -11.429592  -10.420549   -9.217379   -7.9572005  -7.2900186  -7.5353103
0:   -8.155384   -8.716202   -8.795015   -8.226591   -7.4866405  -6.752457
0:  -11.732689  -11.26582  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-16.184795 -16.12342  -15.66664  -15.297391 -15.360034 -15.852305
0:  -16.187832 -16.26558  -16.100883 -15.872544 -16.209118 -17.125668
0:  -18.502956 -19.774532 -20.669048 -20.885036 -20.834457 -20.654968
0:  -24.884884 -25.027744]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.4324079  -0.44712114  0.6075349   1.3698659   1.6081133   1.4909444
0:   1.5108008   1.9281216   2.4732895   2.9455128   2.9332519   2.0572338
0:   1.1008263   0.41444874  0.4849491   1.565578    2.9302897   4.114949
0:   1.4296684   2.2423425 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.7626987 6.174773  6.726866  7.0624123 7.1281686 6.909066  6.862264
0:  6.9075246 7.0159535 7.1340394 6.9925985 6.6177354 6.200221  5.9816475
0:  6.066062  6.5267477 7.03539   7.369381  5.6092525 6.060838 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-6.18301    -6.241894   -5.973283   -5.695343   -5.4772024  -5.4986362
0:  -5.125965   -4.8052835  -4.3220406  -3.6867008  -3.2761211  -2.841961
0:  -2.568593   -2.2176704  -1.7864151  -1.3058043  -0.83257246 -0.48692274
0:   0.6027093   0.810009  ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-13.083953  -12.410764  -11.572175  -10.9191675 -10.685233  -10.718205
0:  -10.723605  -10.375712  -10.078719   -9.936206  -10.323165  -11.510739
0:  -12.58599   -13.202285  -12.958632  -11.787444  -10.455766   -9.250259
0:  -13.231544  -12.696208 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 5.5721807  6.6689677  7.8408284  8.628088   8.856561   8.722731
0:   8.722755   9.194691   9.763639  10.212512  10.065496   9.155626
0:   8.096556   7.365891   7.354827   8.228126   9.302246  10.236135
0:   7.7979393  8.733686 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [39.787277 40.08766  39.99801  39.41097  38.68882  37.852825 37.51077
0:  37.076946 36.55605  36.149677 35.61497  35.50164  35.57401  35.89954
0:  36.248737 36.37714  36.431763 36.34407  39.337463 39.939693]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.753576   6.1846685  6.515453   6.315022   5.567258   4.434679
0:  3.5076978  2.960446   2.679558   2.5430367  2.1706877  1.4415083
0:  0.649971   0.18718576 0.27981806 1.1403613  2.2941723  3.4238763
0:  1.2392569  1.9458647 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [23.745522 24.1329   24.506248 24.67092  24.635378 24.38775  24.278208
0:  24.127136 23.920088 23.830498 23.66316  23.541119 23.577974 23.778612
0:  24.119648 24.507418 24.891676 25.12029  24.983032 25.305023]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [5.918564  6.3936815 7.008399  7.405572  7.5905313 7.5678596 7.6636405
0:  7.881019  8.017625  8.09567   7.869417  7.332527  6.765608  6.3495364
0:  6.2753806 6.593154  7.070117  7.488488  4.616805  4.9398794]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-15.054511  -14.709527  -14.005527  -13.411588  -12.961284  -12.722466
0:  -12.336333  -11.715301  -11.114146  -10.567895  -10.474723  -10.9285555
0:  -11.4275675 -11.606219  -11.289235  -10.407247   -9.5556     -8.953447
0:  -13.466968  -13.329747 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.620989  3.758356  4.216443  4.606776  4.8190317 4.731135  4.7488585
0:  4.773793  4.823681  4.9229198 4.790635  4.4815693 4.01948   3.7529428
0:  3.7173326 3.9517796 4.231808  4.3612676 2.095738  2.1045918]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.6575456  -2.5961099  -1.4811053  -0.6434164  -0.50309896 -0.84476614
0:  -1.199162   -1.2023473  -1.1400323  -1.2083659  -1.7843642  -3.3208585
0:  -4.783159   -5.76183    -5.718968   -4.4414964  -2.9384599  -1.7002015
0:  -5.257961   -4.4396987 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.7337527 -6.710794  -5.519778  -4.6034055 -4.0166583 -3.6764417
0:  -3.2205548 -2.451489  -1.7387376 -1.237598  -1.2098193 -1.8829522
0:  -2.555489  -2.9018579 -2.609282  -1.6233249 -0.5072341  0.385561
0:  -4.007733  -3.0274987]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-9.250491  -8.536445  -7.6965466 -7.1924114 -7.0827074 -7.218192
0:  -7.131401  -6.5381784 -5.852004  -5.1988077 -5.0976114 -5.8229876
0:  -6.7203445 -7.4297123 -7.5925236 -6.9138775 -6.0736847 -5.264143
0:  -8.855878  -8.221752 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-4.812401   -4.005209   -2.9996567  -2.3477821  -2.184142   -2.385065
0:  -2.4041414  -1.9668837  -1.3576689  -0.71617603 -0.59640074 -1.2667522
0:  -2.1261225  -2.8166304  -2.9557056  -2.243248   -1.3356514  -0.43149567
0:  -3.9453506  -3.3135872 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [2.7947545 3.6524317 4.6278563 5.2899876 5.5992904 5.6155667 5.7019763
0:  6.00041   6.2828794 6.503843  6.3834295 5.8142924 5.2745137 5.0176287
0:  5.2591696 6.0535426 6.997396  7.791517  5.267231  6.1645103]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.113266 16.973717 17.576641 17.749144 17.75089  17.653103 17.943993
0:  18.308994 18.650047 19.134993 19.436478 19.867874 20.374596 21.17913
0:  22.11433  22.991074 23.788876 24.340603 25.794743 26.71381 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.97651  27.961063 27.460241 26.530252 25.581278 24.52133  24.173449
0:  23.69882  23.184984 22.79874  22.334448 22.408268 22.804676 23.831709
0:  24.943378 25.883646 26.61864  27.023558 31.649967 32.069893]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-3.2234     -2.8003874  -2.1739225  -1.7731037  -1.5604525  -1.5396547
0:  -1.3810225  -1.0339708  -0.65268755 -0.26001835 -0.19269562 -0.41471767
0:  -0.7798567  -0.9108305  -0.74163055 -0.27794504  0.21253777  0.5528002
0:  -1.8480635  -1.3278513 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.308151 22.779829 23.447092 24.063057 24.63049  25.071255 25.802923
0:  26.513802 27.274729 28.163809 28.983788 29.852404 30.747972 31.605406
0:  32.356373 32.787853 32.94365  32.806858 31.2475   31.217636]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.185821  -6.492039  -5.660072  -5.0018334 -4.8223233 -5.0023284
0:  -5.1979213 -5.0880637 -4.954946  -4.9125333 -5.283897  -6.4228854
0:  -7.5003324 -8.182283  -8.098644  -7.0439677 -5.8448987 -4.848499
0:  -8.287172  -7.801706 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.287785   -0.05538988  1.346303    2.3860583   2.8706226   2.9457006
0:   3.0994217   3.6073964   4.095976    4.4432907   4.2135105   3.132447
0:   2.0717812   1.4153924   1.5784278   2.762239    4.1607933   5.357602
0:   2.5489998   3.5571644 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.642488 21.26777  21.887201 22.248924 22.464012 22.465633 22.748169
0:  22.974506 23.234211 23.594717 23.833559 24.198984 24.64265  25.280891
0:  26.049858 26.777227 27.52767  28.20333  30.967022 31.65252 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [31.66578  32.57827  33.2203   33.448666 33.266464 32.81464  32.460052
0:  32.3253   32.156784 32.10926  31.966034 31.60715  31.415953 31.340467
0:  31.480457 31.792515 32.112587 32.123314 28.987352 29.453821]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-7.133102  -6.5897603 -5.9091687 -5.3853    -5.046202  -4.868193
0:  -4.492765  -3.9342341 -3.450152  -3.059483  -3.0912719 -3.5570111
0:  -4.0748267 -4.216204  -3.8263764 -2.9434562 -2.0794415 -1.5218902
0:  -4.5192447 -3.9352336]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [10.425391  10.896595  11.406259  11.618655  11.463556  10.951424
0:  10.578829  10.254861   9.906877   9.596888   9.061525   8.283288
0:   7.5128484  6.9307804  6.6868033  6.865158   7.19154    7.46465
0:   5.291586   5.614502 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [16.823427  16.899355  16.988491  16.833136  16.453827  15.805384
0:  15.359521  14.905339  14.424729  14.0861225 13.607511  13.184536
0:  12.741421  12.481209  12.427417  12.520449  12.6528225 12.751568
0:  12.487528  12.611095 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-0.42701054  0.2642498   1.1509852   1.7807813   2.1797237   2.4047542
0:   2.8765492   3.60951     4.429819    5.1367307   5.3465505   5.0196004
0:   4.404599    3.8863525   3.610819    3.7907639   4.08994     4.357768
0:   0.35834837  0.92851067]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-12.740758  -11.7216215 -10.498435   -9.449857   -8.937191   -8.897992
0:   -8.912184   -8.656431   -8.415983   -8.269741   -8.563613   -9.622234
0:  -10.60197   -11.123719  -10.85617    -9.694286   -8.5444975  -7.687603
0:  -10.429452   -9.576258 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [12.754555 13.050346 13.415708 13.556364 13.580801 13.420361 13.597859
0:  13.799401 14.029024 14.41882  14.618683 14.885624 15.029043 15.312398
0:  15.697704 16.115278 16.536802 16.797615 18.136555 18.58891 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [20.127264 20.293373 20.352432 20.03908  19.540796 18.867167 18.554655
0:  18.302967 18.054115 17.968458 17.766344 17.703005 17.590752 17.68064
0:  17.868647 18.10383  18.310648 18.341343 18.63453  18.694544]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [3.1523192 3.4277067 3.8957653 4.2425694 4.3382816 4.1362896 4.1675076
0:  4.2092233 4.4019737 4.591413  4.570288  4.316755  4.1717668 4.2042027
0:  4.525497  5.1938    5.875353  6.3644633 5.4236608 5.795349 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [27.983942 27.871786 27.673662 27.11016  26.637657 26.266613 26.581957
0:  27.009832 27.463661 28.04666  28.413288 29.055384 29.667368 30.419106
0:  31.101423 31.403906 31.570244 31.428787 32.309    32.65116 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 9.383074   9.830812  10.3852    10.810232  11.078567  11.165748
0:  11.435509  11.76413   12.135981  12.55271   12.812058  12.908798
0:  13.055178  13.385609  13.946621  14.683569  15.404871  15.902237
0:  15.196562  15.7168865]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-1.6674795  -1.6018305  -1.3495607  -1.3346734  -1.4670019  -1.8703938
0:  -1.8206825  -1.7212305  -1.5230861  -1.1231956  -1.0207033  -0.88400126
0:  -1.016737   -0.92576456 -0.7083335  -0.29640532  0.14877748  0.47138357
0:   1.6350913   1.8260336 ]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [-19.3421   -18.74591  -17.717606 -16.895054 -16.416979 -16.272993
0:  -16.0842   -15.469692 -14.726307 -13.927515 -13.631521 -14.000406
0:  -14.637655 -15.115948 -15.106621 -14.497349 -13.782843 -13.087826
0:  -17.469923 -16.980507]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [22.34064  22.824558 23.178942 23.136698 22.763004 22.150124 21.61867
0:  21.168688 20.706459 20.328812 19.909554 19.405895 19.10561  18.961384
0:  19.083473 19.378223 19.767056 20.092592 19.99225  20.892643]
0: Created sparse mask for total_precip with 10.0% data retained
0: Prediction values (first 20):
0: [ 1.4237142   2.2428813   3.1398847   3.8171124   4.131051    4.177038
0:   4.2492857   4.618998    4.8869143   4.9356127   4.472215    3.2537658
0:   2.0185113   1.1412816   0.99843025  1.7551985   2.7866533   3.6844864
0:  -0.90330696 -0.4695239 ]
0: Created sparse mask for total_precip with 10.0% data retained
