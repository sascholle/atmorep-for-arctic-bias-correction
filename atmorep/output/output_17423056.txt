0: Wandb run: atmorep-horob4h4-17423056
0: l40369:2258724:2258724 [0] NCCL INFO Bootstrap : Using ib0:10.128.9.108<0>
0: l40369:2258724:2258724 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
0: l40369:2258724:2258724 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
0: l40369:2258724:2258724 [0] NCCL INFO NET/Plugin: Using internal network plugin.
0: l40369:2258724:2258724 [0] NCCL INFO cudaDriverVersion 12050
0: NCCL version 2.21.5+cuda12.4
0: l40369:2258724:2258966 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.9.108<0>
0: l40369:2258724:2258966 [0] NCCL INFO Using non-device net plugin version 0
0: l40369:2258724:2258966 [0] NCCL INFO Using network IB
0: l40369:2258724:2258966 [0] NCCL INFO ncclCommInitRank comm 0x55555f2619d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0xc704dccd28a2cea8 - Init START
0: l40369:2258724:2258966 [0] NCCL INFO Setting affinity for GPU 0 to 0f0000,00000000,00000000,00000000,000f0000,00000000
0: l40369:2258724:2258966 [0] NCCL INFO comm 0x55555f2619d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 00/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 01/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 02/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 03/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 04/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 05/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 06/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 07/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 08/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 09/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 10/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 11/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 12/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 13/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 14/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 15/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 16/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 17/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 18/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 19/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 20/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 21/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 22/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 23/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 24/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 25/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 26/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 27/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 28/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 29/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 30/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Channel 31/32 :    0
0: l40369:2258724:2258966 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l40369:2258724:2258966 [0] NCCL INFO P2P Chunksize set to 131072
0: l40369:2258724:2258966 [0] NCCL INFO Connected all rings
0: l40369:2258724:2258966 [0] NCCL INFO Connected all trees
0: l40369:2258724:2258966 [0] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l40369:2258724:2258966 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
0: l40369:2258724:2258966 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
0: l40369:2258724:2258966 [0] NCCL INFO ncclCommInitRank comm 0x55555f2619d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 3000 commId 0xc704dccd28a2cea8 - Init COMPLETE
0: with_ddp : True
0: num_accs_per_task : 4
0: par_rank : 0
0: par_size : 1
0: fields : [['velocity_u', [1, 1024, ['velocity_v', 'temperature'], 0, ['j8dwr5qj', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_v', [1, 1024, ['velocity_u', 'temperature'], 1, ['0tlnm5up', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['specific_humidity', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 2, ['v63l01zu', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['velocity_z', [1, 1024, ['velocity_u', 'velocity_v', 'temperature'], 3, ['9l1errbo', -2]], [96, 105, 114, 123, 137], [12, 3, 6], [3, 18, 18], [0.1, 0, 0, 0]], ['temperature', [1, 1024, ['velocity_u', 'velocity_v', 'specific_humidity'], 3, ['7ojls62c', -2]], [96, 105, 114, 123, 137], [12, 2, 4], [3, 27, 27], [0.1, 0, 0, 0], 'Local'], ['total_precip', [1, 1024, ['velocity_u', 'velocity_v', 'velocity_z', 'specific_humidity'], 0], [0], [12, 6, 12], [3, 9, 9], [1, 0, 0, 0]]]
0: fields_prediction : [['velocity_u', 0.0], ['velocity_v', 0.0], ['specific_humidity', 0.0], ['velocity_z', 0.0], ['temperature', 0.0], ['total_precip', 1.0]]
0: fields_targets : ['total_precip']
0: years_train : [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
0: years_val : [2021]
0: month : None
0: geo_range_sampling : [[70.0, 90.0], [0.0, 360.0]]
0: time_sampling : 1
0: torch_seed : 5521420987310380410
0: batch_size_validation : 1
0: batch_size : 96
0: num_epochs : 50
0: num_samples_per_epoch : 480
0: num_samples_validate : 96
0: num_loader_workers : 5
0: size_token_info : 8
0: size_token_info_net : 16
0: grad_checkpointing : True
0: with_cls : False
0: with_mixed_precision : True
0: with_layernorm : True
0: coupling_num_heads_per_field : 1
0: dropout_rate : 0.05
0: with_qk_lnorm : True
0: encoder_num_layers : 6
0: encoder_num_heads : 16
0: encoder_num_mlp_layers : 2
0: encoder_att_type : dense
0: decoder_num_layers : 6
0: decoder_num_heads : 16
0: decoder_num_mlp_layers : 2
0: decoder_self_att : False
0: decoder_cross_att_ratio : 0.5
0: decoder_cross_att_rate : 1.0
0: decoder_att_type : dense
0: net_tail_num_nets : 16
0: net_tail_num_layers : 0
0: losses : ['mse_ensemble']
0: optimizer_zero : False
0: lr_start : 1e-05
0: lr_max : 2e-05
0: lr_min : 1e-05
0: weight_decay : 0.025
0: lr_decay_rate : 1.025
0: lr_start_epochs : 3
0: model_log_frequency : 256
0: BERT_strategy : forecast
0: forecast_num_tokens : 2
0: BERT_fields_synced : False
0: BERT_mr_max : 2
0: log_test_num_ranks : 0
0: save_grads : False
0: profile : False
0: test_initial : False
0: attention : False
0: rng_seed : None
0: with_wandb : True
0: slurm_job_id : 17423056
0: wandb_id : horob4h4
0: file_path : /work/ab1412/atmorep/data/era5_y2010_2020_res25.zarr
0: n_size : [36, 13.5, 27.0]
0: model_id : wc5e2i3t
0: sparse_target : True
0: sparse_target_field : total_precip
0: sparse_target_sparsity : 0.9
0: years_test : [2021]
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: Available keys in the Zarr dataset: ['data', 'data_sfc', 'lats', 'lons', 'time']
0: self.ds['data'] : (105192, 7, 5, 721, 1440) :: (105192,)
0: self.lats : (721,)
0: self.lons : (1440,)
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 1
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 2
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 3
0: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'] 0
0: Loaded model id = wc5e2i3t at epoch = 0.
0: Loaded run 'wc5e2i3t' at epoch 0.
0: l40369:2258724:2258995 [1] NCCL INFO Using non-device net plugin version 0
0: l40369:2258724:2258995 [1] NCCL INFO Using network IB
0: l40369:2258724:2258995 [1] NCCL INFO ncclCommInitRank comm 0x555570d10410 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0x365b64a7bcd9d546 - Init START
0: l40369:2258724:2258995 [1] NCCL INFO Setting affinity for GPU 1 to 0f0000,00000000,00000000,00000000,000f0000
0: l40369:2258724:2258995 [1] NCCL INFO comm 0x555570d10410 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 00/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 01/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 02/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 03/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 04/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 05/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 06/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 07/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 08/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 09/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 10/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 11/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 12/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 13/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 14/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 15/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 16/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 17/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 18/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 19/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 20/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 21/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 22/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 23/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 24/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 25/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 26/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 27/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 28/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 29/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 30/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Channel 31/32 :    0
0: l40369:2258724:2258995 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l40369:2258724:2258995 [1] NCCL INFO P2P Chunksize set to 131072
0: l40369:2258724:2258995 [1] NCCL INFO Connected all rings
0: l40369:2258724:2258995 [1] NCCL INFO Connected all trees
0: l40369:2258724:2258995 [1] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l40369:2258724:2258995 [1] NCCL INFO ncclCommInitRank comm 0x555570d10410 rank 0 nranks 1 cudaDev 1 nvmlDev 1 busId 44000 commId 0x365b64a7bcd9d546 - Init COMPLETE
0: l40369:2258724:2259000 [2] NCCL INFO Using non-device net plugin version 0
0: l40369:2258724:2259000 [2] NCCL INFO Using network IB
0: l40369:2258724:2259000 [2] NCCL INFO ncclCommInitRank comm 0x55557ec1ff60 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0xbaf273bf953f66e4 - Init START
0: l40369:2258724:2259000 [2] NCCL INFO Setting affinity for GPU 2 to 1f0000,00000000,00000000,00000000,001f0000,00000000,00000000,00000000
0: l40369:2258724:2259000 [2] NCCL INFO comm 0x55557ec1ff60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 00/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 01/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 02/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 03/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 04/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 05/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 06/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 07/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 08/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 09/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 10/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 11/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 12/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 13/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 14/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 15/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 16/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 17/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 18/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 19/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 20/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 21/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 22/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 23/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 24/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 25/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 26/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 27/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 28/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 29/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 30/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Channel 31/32 :    0
0: l40369:2258724:2259000 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l40369:2258724:2259000 [2] NCCL INFO P2P Chunksize set to 131072
0: l40369:2258724:2259000 [2] NCCL INFO Connected all rings
0: l40369:2258724:2259000 [2] NCCL INFO Connected all trees
0: l40369:2258724:2259000 [2] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l40369:2258724:2259000 [2] NCCL INFO ncclCommInitRank comm 0x55557ec1ff60 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 84000 commId 0xbaf273bf953f66e4 - Init COMPLETE
0: l40369:2258724:2259005 [3] NCCL INFO Using non-device net plugin version 0
0: l40369:2258724:2259005 [3] NCCL INFO Using network IB
0: l40369:2258724:2259005 [3] NCCL INFO ncclCommInitRank comm 0x5555877f0560 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0x8e1e31bbcdef8858 - Init START
0: l40369:2258724:2259005 [3] NCCL INFO Setting affinity for GPU 3 to 0f0000,00000000,00000000,00000000,000f0000,00000000,00000000
0: l40369:2258724:2259005 [3] NCCL INFO comm 0x5555877f0560 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 00/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 01/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 02/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 03/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 04/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 05/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 06/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 07/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 08/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 09/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 10/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 11/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 12/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 13/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 14/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 15/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 16/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 17/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 18/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 19/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 20/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 21/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 22/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 23/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 24/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 25/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 26/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 27/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 28/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 29/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 30/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Channel 31/32 :    0
0: l40369:2258724:2259005 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
0: l40369:2258724:2259005 [3] NCCL INFO P2P Chunksize set to 131072
0: l40369:2258724:2259005 [3] NCCL INFO Connected all rings
0: l40369:2258724:2259005 [3] NCCL INFO Connected all trees
0: l40369:2258724:2259005 [3] NCCL INFO 32 coll channels, 32 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
0: l40369:2258724:2259005 [3] NCCL INFO ncclCommInitRank comm 0x5555877f0560 rank 0 nranks 1 cudaDev 3 nvmlDev 3 busId c4000 commId 0x8e1e31bbcdef8858 - Init COMPLETE
0: Number of trainable parameters: 741,136,272
0: 1 : 19:29:15 :: batch_size = 96, lr = 1.5000000000000002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 1, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1953, 1.1584, 1.1216, 1.0856, 1.0523, 1.0229, 0.9936, 0.9626, 0.9291, 0.8958, 0.8662, 0.8421, 0.8272, 0.8235,
0:         0.8287, 0.8354, 0.8329, 0.8120, 1.2908, 1.2662, 1.2383, 1.2068, 1.1718, 1.1333, 1.0910], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3925, -0.4048, -0.4157, -0.4270, -0.4415, -0.4596, -0.4801, -0.5037, -0.5304, -0.5610, -0.5926, -0.6203,
0:         -0.6380, -0.6363, -0.6158, -0.5875, -0.5686, -0.5690, -0.3976, -0.4134, -0.4265, -0.4366, -0.4473, -0.4608,
0:         -0.4768], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4002, -0.3929, -0.3981, -0.4070, -0.4192, -0.4334, -0.4364, -0.4366, -0.4282, -0.4146, -0.3996, -0.3828,
0:         -0.3598, -0.3359, -0.3128, -0.3116, -0.3260, -0.3635, -0.4362, -0.4187, -0.4019, -0.4026, -0.4027, -0.4121,
0:         -0.4233], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.4159,  0.4594,  0.4638,  0.4716,  0.4950,  0.5574,  0.5863,  0.5874,  0.5652,  0.5139,  0.5006,  0.3658,
0:          0.0863, -0.1698, -0.2589, -0.0696,  0.1999,  0.2667,  0.2812,  0.4193,  0.4839,  0.5095,  0.5663,  0.6053,
0:          0.5952], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.7703, -0.7578, -0.7494, -0.7513, -0.7562, -0.7580, -0.7503, -0.7216, -0.6687, -0.5863, -0.4816, -0.3734,
0:         -0.2848, -0.2376, -0.2198, -0.2019, -0.1525, -0.0763, -0.0189, -0.0070, -0.0226, -0.0359, -0.0494, -0.0823,
0:         -0.1301], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2044, -0.2437, -0.2342, -0.1615, -0.1556, -0.2270, -0.2497, -0.2497, -0.2485, -0.1973, -0.2163, -0.2342,
0:         -0.2104, -0.1961, -0.2139, -0.2378, -0.2497, -0.2497, -0.1865, -0.1830, -0.1568, -0.2163, -0.2259, -0.2318,
0:         -0.2473], device='cuda:0')
0: [DEBUG] Epoch 1, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan, -0.2211,     nan, -0.2508,     nan, -0.2508, -0.2463,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0361,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2508,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0566,     nan,     nan,     nan,     nan,     nan, -0.1183,     nan,     nan,     nan, -0.0635, -0.0978,
0:             nan,     nan,     nan,     nan, -0.1960,     nan,     nan,     nan, -0.1663,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2508,     nan,     nan,     nan,     nan,     nan, -0.0932,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.0005,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1572,     nan, -0.0349,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0153,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0165,     nan,     nan, -0.0555,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1286,     nan,     nan,     nan,     nan,     nan, -0.1720,
0:             nan,     nan, -0.0555,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0007,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 1, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6297, 0.6388, 0.6464, 0.6549, 0.6561, 0.6563, 0.6494, 0.6376, 0.6252, 0.6110, 0.5952, 0.5759, 0.5631, 0.5513,
0:         0.5429, 0.5324, 0.5235, 0.5128, 0.6471, 0.6593, 0.6692, 0.6792, 0.6846, 0.6887, 0.6877], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0271,  0.0348,  0.0392,  0.0362,  0.0319,  0.0351,  0.0387,  0.0456,  0.0543,  0.0590,  0.0568,  0.0553,
0:          0.0433,  0.0347,  0.0182,  0.0023, -0.0123, -0.0172,  0.0306,  0.0404,  0.0437,  0.0420,  0.0410,  0.0389,
0:          0.0422], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2861, -0.2992, -0.3190, -0.3359, -0.3566, -0.3778, -0.3992, -0.4160, -0.4310, -0.4459, -0.4600, -0.4700,
0:         -0.4795, -0.4909, -0.4998, -0.5083, -0.5217, -0.5326, -0.2375, -0.2552, -0.2719, -0.2918, -0.3150, -0.3367,
0:         -0.3558], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1926,  0.1701,  0.1167,  0.0427, -0.0181, -0.0580, -0.0809, -0.0567,  0.0143,  0.0751,  0.1300,  0.2282,
0:          0.3132,  0.3497,  0.3618,  0.3352,  0.2642,  0.2141,  0.2332,  0.2027,  0.1556,  0.0790,  0.0191, -0.0369,
0:         -0.0897], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.0153, 0.0443, 0.0755, 0.1003, 0.1171, 0.1266, 0.1312, 0.1357, 0.1458, 0.1630, 0.1884, 0.2204, 0.2565, 0.2941,
0:         0.3298, 0.3605, 0.3828, 0.3955, 0.3986, 0.3945, 0.3860, 0.3769, 0.3700, 0.3658, 0.3627], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1280, -0.1316, -0.1216, -0.1215, -0.1190, -0.1441, -0.1586, -0.1805, -0.1932, -0.1336, -0.1211, -0.1130,
0:         -0.1079, -0.0911, -0.1108, -0.1199, -0.1374, -0.1653, -0.1278, -0.1210, -0.1074, -0.0871, -0.0712, -0.0650,
0:         -0.0822], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.07424317300319672; velocity_v: 0.10829931497573853; specific_humidity: 0.04632551223039627; velocity_z: 0.4771150052547455; temperature: 0.1506887525320053; total_precip: 0.7251657247543335; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05082732439041138; velocity_v: 0.08194039016962051; specific_humidity: 0.034350186586380005; velocity_z: 0.4886176288127899; temperature: 0.09581154584884644; total_precip: 0.6385015249252319; 
0: epoch: 1 [1/5 (20%)]	Loss: 0.68183 : 0.22010 :: 0.14161 (2.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.059913963079452515; velocity_v: 0.09922614693641663; specific_humidity: 0.04435793310403824; velocity_z: 0.6173813343048096; temperature: 0.13869528472423553; total_precip: 0.8015366196632385; 
0: epoch: 1 [2/5 (40%)]	Loss: 0.80154 : 0.26680 :: 0.14615 (15.57 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.04504907876253128; velocity_v: 0.07959029078483582; specific_humidity: 0.03343646228313446; velocity_z: 0.43805164098739624; temperature: 0.09616478532552719; total_precip: 0.6925267577171326; 
0: epoch: 1 [3/5 (60%)]	Loss: 0.69253 : 0.20726 :: 0.13230 (15.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05010198801755905; velocity_v: 0.08165087550878525; specific_humidity: 0.037002190947532654; velocity_z: 0.44371044635772705; temperature: 0.09317012131214142; total_precip: 0.6218096017837524; 
0: epoch: 1 [4/5 (80%)]	Loss: 0.62181 : 0.19686 :: 0.13648 (15.54 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 1 : 0.20836511254310608
0: validation loss for velocity_u : 0.029791569337248802
0: validation loss for velocity_v : 0.05450773611664772
0: validation loss for specific_humidity : 0.024451756849884987
0: validation loss for velocity_z : 0.4252943992614746
0: validation loss for temperature : 0.07130184769630432
0: validation loss for total_precip : 0.6448435187339783
0: 2 : 19:33:21 :: batch_size = 96, lr = 2e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 2, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8050, 0.7991, 0.8283, 0.9052, 1.0184, 1.1568, 1.3121, 1.4659, 1.5959, 1.6939, 1.7668, 1.8211, 1.8598, 1.8943,
0:         1.9388, 1.9970, 2.0632, 2.1354, 1.0896, 1.0656, 1.0804, 1.1652, 1.3035, 1.4537, 1.5903], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.7941, 1.7768, 1.7229, 1.6891, 1.7048, 1.7580, 1.8229, 1.8879, 1.9532, 2.0198, 2.0868, 2.1516, 2.2067, 2.2432,
0:         2.2593, 2.2553, 2.2294, 2.1840, 1.5152, 1.5045, 1.4751, 1.4668, 1.5068, 1.5853, 1.6725], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3179, -0.2535, -0.2364, -0.2215, -0.2150, -0.2173, -0.2197, -0.2345, -0.2527, -0.2617, -0.2396, -0.2075,
0:         -0.1486, -0.1118, -0.0548, -0.0163,  0.0096,  0.0206, -0.4264, -0.3710, -0.3477, -0.3370, -0.3583, -0.3633,
0:         -0.3857], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-7.4266, -8.0671, -8.3001, -8.2675, -8.0491, -7.4502, -6.3177, -4.8757, -3.6115, -2.9417, -2.9012, -3.0295,
0:         -2.8055, -2.2629, -1.8542, -1.7360, -1.6888, -1.6426, -7.0709, -7.0528, -6.4900, -5.7504, -5.0434, -4.3973,
0:         -3.8355], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.2544, -1.2183, -1.1608, -1.1126, -1.0775, -1.0485, -1.0253, -1.0043, -0.9770, -0.9432, -0.9109, -0.8780,
0:         -0.8346, -0.7831, -0.7355, -0.6980, -0.6693, -0.6484, -0.6332, -0.6199, -0.6100, -0.6090, -0.6170, -0.6258,
0:         -0.6275], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([1.1374, 1.2066, 1.6152, 2.1862, 2.5590, 2.2818, 2.1002, 0.8602, 0.3083, 2.1146, 2.4443, 2.6975, 2.3893, 1.5507,
0:         1.2927, 0.8339, 0.8052, 0.3680, 2.8624, 2.9938, 1.9855, 1.6773, 0.9104, 0.7097, 0.3871], device='cuda:0')
0: [DEBUG] Epoch 2, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,  1.5626,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0107,     nan,  0.1267,     nan,     nan,     nan,
0:             nan,     nan, -0.1349,     nan,     nan,     nan, -0.0143,     nan,     nan,     nan, -0.1887, -0.1684,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2042,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,     nan,     nan, -0.2412,
0:         -0.2484,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2460,     nan,     nan,
0:             nan,     nan,     nan,  0.1255,     nan,     nan,     nan,     nan,     nan,     nan, -0.1301,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1230, -0.2460,     nan,
0:             nan,  0.0514,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484, -0.2484, -0.2484,
0:             nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,
0:             nan,     nan, -0.2484,     nan, -0.2484,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.5257,     nan,     nan,     nan,     nan,     nan,     nan, -0.1935,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,     nan,  0.4779,     nan,  0.3608,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2484,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 2, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4772, 1.4880, 1.5044, 1.5253, 1.5526, 1.5751, 1.5986, 1.6011, 1.5856, 1.5613, 1.5239, 1.4956, 1.4856, 1.4926,
0:         1.5191, 1.5569, 1.6080, 1.6607, 1.5851, 1.6171, 1.6385, 1.6578, 1.6742, 1.6938, 1.7085], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0907, 0.1181, 0.1847, 0.2804, 0.4205, 0.5909, 0.7903, 1.0294, 1.2980, 1.5912, 1.8782, 2.1491, 2.3712, 2.5568,
0:         2.6870, 2.7807, 2.8525, 2.8977, 0.0128, 0.0375, 0.1100, 0.2120, 0.3534, 0.5283, 0.7393], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5203, -0.5241, -0.5348, -0.5427, -0.5626, -0.5776, -0.5979, -0.6101, -0.6171, -0.6189, -0.6157, -0.5987,
0:         -0.5806, -0.5517, -0.5085, -0.4596, -0.3969, -0.3308, -0.5468, -0.5521, -0.5558, -0.5685, -0.5821, -0.6004,
0:         -0.6177], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 1.4305,  1.5906,  1.7470,  1.8416,  1.8135,  1.8044,  1.9779,  2.2368,  2.4280,  2.2711,  1.6240,  0.6951,
0:         -0.4158, -1.5120, -2.1909, -2.3944, -2.3693, -2.2522,  1.8523,  2.1678,  2.4553,  2.5990,  2.5213,  2.3791,
0:          2.3490], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.1880, -1.2036, -1.2342, -1.2676, -1.2827, -1.2633, -1.2015, -1.1085, -1.0016, -0.9054, -0.8405, -0.8148,
0:         -0.8234, -0.8495, -0.8755, -0.8820, -0.8603, -0.8131, -0.7469, -0.6735, -0.6010, -0.5335, -0.4732, -0.4234,
0:         -0.3820], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([ 0.0968,  0.1684,  0.2754,  0.4147,  0.5211,  0.6438,  0.7251,  0.7680,  0.7870, -0.0117,  0.0494,  0.1206,
0:          0.2127,  0.3018,  0.3924,  0.4631,  0.5163,  0.5486, -0.1022, -0.0706, -0.0110,  0.0256,  0.0871,  0.1333,
0:          0.1736], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.04789060354232788; velocity_v: 0.07790206372737885; specific_humidity: 0.036315008997917175; velocity_z: 0.40052562952041626; temperature: 0.09776461869478226; total_precip: 0.5056360960006714; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05899578705430031; velocity_v: 0.09121476858854294; specific_humidity: 0.03776934742927551; velocity_z: 0.5280659198760986; temperature: 0.12186898291110992; total_precip: 32.955169677734375; 
0: epoch: 2 [1/5 (20%)]	Loss: 16.73040 : 2.88757 :: 0.15097 (2.39 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07495489716529846; velocity_v: 0.11349991708993912; specific_humidity: 0.04647969827055931; velocity_z: 0.4990345537662506; temperature: 0.12547914683818817; total_precip: 35.91257095336914; 
0: epoch: 2 [2/5 (40%)]	Loss: 35.91257 : 6.10057 :: 0.17039 (15.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07922389358282089; velocity_v: 0.10581188648939133; specific_humidity: 0.05157805606722832; velocity_z: 0.5745089650154114; temperature: 0.13729669153690338; total_precip: 39.462520599365234; 
0: epoch: 2 [3/5 (60%)]	Loss: 39.46252 : 6.70545 :: 0.17702 (15.37 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05459524691104889; velocity_v: 0.08362958580255508; specific_humidity: 0.04034963622689247; velocity_z: 0.4910036027431488; temperature: 0.09846718609333038; total_precip: 36.07367706298828; 
0: epoch: 2 [4/5 (80%)]	Loss: 36.07368 : 6.11502 :: 0.14903 (15.44 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 2 : 6.808798313140869
0: validation loss for velocity_u : 0.03969893977046013
0: validation loss for velocity_v : 0.062232133001089096
0: validation loss for specific_humidity : 0.029796568676829338
0: validation loss for velocity_z : 0.5264014601707458
0: validation loss for temperature : 0.08632858842611313
0: validation loss for total_precip : 40.10832595825195
0: 3 : 19:37:20 :: batch_size = 96, lr = 1.9512195121951222e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 3, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0469, 1.0521, 1.0614, 1.0676, 1.0621, 1.0508, 1.0436, 1.0436, 1.0569, 1.0752, 1.0812, 1.0814, 1.0872, 1.0908,
0:         1.0830, 1.0776, 1.0773, 1.0614, 1.1431, 1.1403, 1.1449, 1.1511, 1.1549, 1.1567, 1.1534], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3785, 0.3604, 0.3451, 0.3328, 0.3239, 0.3106, 0.2903, 0.2672, 0.2420, 0.2178, 0.1994, 0.1785, 0.1516, 0.1274,
0:         0.1084, 0.0928, 0.0748, 0.0475, 0.4284, 0.3959, 0.3729, 0.3600, 0.3510, 0.3395, 0.3250], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6004, -0.5988, -0.5960, -0.5895, -0.5820, -0.5713, -0.5620, -0.5522, -0.5452, -0.5378, -0.5285, -0.5200,
0:         -0.5119, -0.5034, -0.4971, -0.4942, -0.4931, -0.4611, -0.5976, -0.5980, -0.5979, -0.5967, -0.5934, -0.5884,
0:         -0.5818], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0283, -0.3170, -0.4764, -0.5561, -0.5561, -0.5406, -0.3547, -0.0293,  0.4666,  0.9425,  0.9978,  0.9890,
0:          0.7831,  0.3714,  0.2585,  0.0947,  0.1522,  0.2740,  0.3448, -0.1245, -0.5251, -0.6956, -0.6446, -0.6690,
0:         -0.5406], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.6855, 1.6587, 1.6060, 1.5471, 1.4792, 1.4056, 1.3379, 1.2827, 1.2471, 1.2401, 1.2651, 1.2967, 1.3133, 1.3201,
0:         1.3049, 1.2791, 1.2646, 1.2580, 1.2692, 1.3135, 1.3946, 1.4858, 1.5294, 1.5115, 1.4397], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513,
0:         -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513,
0:         -0.2513], device='cuda:0')
0: [DEBUG] Epoch 3, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2513,     nan,     nan, -0.2454,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2466,     nan, -0.2489,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,
0:         -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2513,     nan, -0.2513,     nan, -0.2513,     nan,     nan,
0:         -0.2513, -0.2513,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489,     nan,
0:             nan,     nan, -0.2513,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan, -0.2513,
0:             nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2513,     nan, -0.2513,     nan,     nan, -0.2513,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513, -0.2513,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 3, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2445, 0.2839, 0.3237, 0.3584, 0.3973, 0.4358, 0.4770, 0.5148, 0.5542, 0.5897, 0.6225, 0.6485, 0.6682, 0.6886,
0:         0.7106, 0.7354, 0.7673, 0.8017, 0.2392, 0.2845, 0.3266, 0.3675, 0.4089, 0.4475, 0.4885], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5371, 0.5523, 0.5802, 0.6170, 0.6572, 0.7013, 0.7351, 0.7660, 0.7916, 0.8080, 0.8126, 0.8025, 0.7802, 0.7518,
0:         0.7222, 0.6995, 0.6751, 0.6532, 0.5617, 0.5875, 0.6219, 0.6671, 0.7129, 0.7544, 0.7845], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1145, -0.1255, -0.1477, -0.1714, -0.2025, -0.2306, -0.2566, -0.2769, -0.2944, -0.3054, -0.3162, -0.3244,
0:         -0.3378, -0.3524, -0.3696, -0.3895, -0.4096, -0.4265, -0.1703, -0.1829, -0.2026, -0.2252, -0.2482, -0.2683,
0:         -0.2851], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.9200,  0.9607,  0.9047,  0.9154,  0.9057,  0.7210,  0.5022,  0.4665,  0.7698,  1.3087,  1.6866,  1.6019,
0:          1.0589,  0.3204, -0.0511,  0.0957,  0.3154,  0.4415,  0.8021,  0.8283,  0.7529,  0.7852,  0.8380,  0.6623,
0:          0.4108], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.2223, 0.2110, 0.2063, 0.2073, 0.2117, 0.2132, 0.2032, 0.1857, 0.1687, 0.1612, 0.1693, 0.1849, 0.1986, 0.1989,
0:         0.1838, 0.1596, 0.1397, 0.1285, 0.1248, 0.1259, 0.1246, 0.1208, 0.1127, 0.0980, 0.0727], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([1.9675, 1.5658, 1.0807, 1.0340, 1.1023, 1.4130, 2.0687, 2.6204, 3.0644, 1.6370, 1.1222, 0.7352, 0.7276, 0.9290,
0:         1.5924, 2.4462, 3.3436, 3.9965, 1.2871, 0.7298, 0.4248, 0.4226, 0.9051, 1.8907, 2.9930], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.06512920558452606; velocity_v: 0.09741397202014923; specific_humidity: 0.035746634006500244; velocity_z: 0.5404724478721619; temperature: 0.09834955632686615; total_precip: 37.67987060546875; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05916944146156311; velocity_v: 0.09228770434856415; specific_humidity: 0.03491613268852234; velocity_z: 0.5041990280151367; temperature: 0.08234463632106781; total_precip: 36.00918197631836; 
0: epoch: 3 [1/5 (20%)]	Loss: 36.84453 : 6.24964 :: 0.15136 (2.78 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08860258013010025; velocity_v: 0.13638094067573547; specific_humidity: 0.0486210435628891; velocity_z: 0.6485506892204285; temperature: 0.12849678099155426; total_precip: 38.280391693115234; 
0: epoch: 3 [2/5 (40%)]	Loss: 38.28039 : 6.52593 :: 0.17826 (15.37 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.06397683918476105; velocity_v: 0.10060890763998032; specific_humidity: 0.04576792195439339; velocity_z: 0.5116337537765503; temperature: 0.1161857470870018; total_precip: 36.829803466796875; 
0: epoch: 3 [3/5 (60%)]	Loss: 36.82980 : 6.25108 :: 0.15573 (15.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.05546361953020096; velocity_v: 0.07776401937007904; specific_humidity: 0.03829529136419296; velocity_z: 0.46474286913871765; temperature: 0.08553615212440491; total_precip: 36.12582015991211; 
0: epoch: 3 [4/5 (80%)]	Loss: 36.12582 : 6.11627 :: 0.14605 (15.03 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 3 : 0.28872156143188477
0: validation loss for velocity_u : 0.03698660805821419
0: validation loss for velocity_v : 0.06492237001657486
0: validation loss for specific_humidity : 0.024471541866660118
0: validation loss for velocity_z : 0.4686570465564728
0: validation loss for temperature : 0.06737194210290909
0: validation loss for total_precip : 1.0699199438095093
0: 4 : 19:41:11 :: batch_size = 96, lr = 1.903628792385485e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 4, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1602,  0.1512,  0.1452,  0.1199,  0.0722,  0.0184, -0.0233, -0.0450, -0.0524, -0.0652, -0.1019, -0.1610,
0:         -0.2190, -0.2499, -0.2435, -0.2108, -0.1744, -0.1541,  0.1568,  0.1489,  0.1491,  0.1310,  0.0871,  0.0304,
0:         -0.0207], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9523, 0.9799, 0.9782, 0.9587, 0.9366, 0.9170, 0.8945, 0.8607, 0.8110, 0.7496, 0.6855, 0.6213, 0.5500, 0.4637,
0:         0.3628, 0.2521, 0.1378, 0.0284, 0.9939, 0.9827, 0.9406, 0.8911, 0.8535, 0.8314, 0.8166], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.7062,  1.5748,  1.2517,  0.9619,  0.6526,  0.5994,  0.6046,  0.6246,  0.6008,  0.5470,  0.3768,  0.1930,
0:         -0.0299, -0.2015, -0.3447, -0.4105, -0.4599, -0.4819,  1.7920,  1.6599,  1.3786,  1.0635,  0.7428,  0.6715,
0:          0.6002], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.5941e-01,  4.0171e-01,  5.7476e-01,  3.3516e-01,  1.8651e-01,  2.0648e-01,  8.4460e-02, -1.6402e-01,
0:         -1.9064e-01,  1.5544e-04,  1.0443e-01,  1.0443e-01,  2.3088e-01,  4.8158e-01,  6.9456e-01,  8.3877e-01,
0:          9.3416e-01,  9.3195e-01, -9.4938e-01, -2.3279e-01,  4.5939e-01,  5.7254e-01,  3.1963e-01,  1.9982e-01,
0:          1.7986e-01], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.7792, 1.7748, 1.7591, 1.7284, 1.6878, 1.6481, 1.6223, 1.6163, 1.6267, 1.6506, 1.6897, 1.7390, 1.7837, 1.8127,
0:         1.8262, 1.8254, 1.8082, 1.7767, 1.7367, 1.6893, 1.6315, 1.5671, 1.5091, 1.4690, 1.4494], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2371, -0.2394, -0.2394, -0.2394, -0.2360, -0.2360, -0.2268, -0.2234, -0.2222, -0.2371, -0.2394, -0.2383,
0:         -0.2371, -0.2348, -0.2348, -0.2360, -0.2394, -0.2371, -0.2371, -0.2394, -0.2383, -0.2371, -0.2371, -0.2360,
0:         -0.2371], device='cuda:0')
0: [DEBUG] Epoch 4, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,
0:         -0.2394,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan, -0.2406,     nan,     nan,
0:             nan, -0.2417,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,
0:         -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2417,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,
0:             nan,     nan, -0.2417, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417, -0.2417,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 4, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7960, 0.7885, 0.7814, 0.7621, 0.7393, 0.7187, 0.7001, 0.6864, 0.6728, 0.6576, 0.6391, 0.6065, 0.5758, 0.5449,
0:         0.5133, 0.4901, 0.4709, 0.4525, 0.7106, 0.7071, 0.6952, 0.6746, 0.6380, 0.6097, 0.5862], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.5106,  0.5040,  0.4882,  0.4642,  0.4298,  0.3899,  0.3406,  0.2840,  0.2171,  0.1458,  0.0728,  0.0011,
0:         -0.0689, -0.1393, -0.2062, -0.2635, -0.2972, -0.3055,  0.4979,  0.5058,  0.5021,  0.4891,  0.4641,  0.4292,
0:          0.3805], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0107, 1.0216, 1.0146, 1.0014, 0.9639, 0.9248, 0.8775, 0.8426, 0.8002, 0.7635, 0.7345, 0.7061, 0.6726, 0.6504,
0:         0.6408, 0.6413, 0.6582, 0.6912, 1.0215, 1.0522, 1.0722, 1.0678, 1.0365, 0.9841, 0.9332], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6269, 0.6053, 0.5324, 0.4645, 0.4058, 0.3446, 0.2805, 0.2196, 0.2080, 0.2396, 0.2541, 0.2820, 0.2944, 0.2696,
0:         0.2609, 0.3220, 0.4197, 0.4440, 0.5940, 0.5551, 0.5267, 0.4717, 0.4087, 0.3458, 0.2673], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.3668, 1.4049, 1.4390, 1.4685, 1.4995, 1.5323, 1.5660, 1.5922, 1.6120, 1.6268, 1.6413, 1.6564, 1.6684, 1.6741,
0:         1.6708, 1.6653, 1.6689, 1.6882, 1.7197, 1.7502, 1.7610, 1.7466, 1.7159, 1.6857, 1.6744], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2283, -0.2270, -0.2305, -0.2310, -0.2379, -0.2414, -0.2367, -0.2367, -0.2371, -0.2285, -0.2282, -0.2330,
0:         -0.2346, -0.2372, -0.2353, -0.2372, -0.2412, -0.2364, -0.2329, -0.2347, -0.2296, -0.2320, -0.2311, -0.2347,
0:         -0.2294], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.07617569714784622; velocity_v: 0.12079627066850662; specific_humidity: 0.05050986632704735; velocity_z: 0.5964577198028564; temperature: 0.12849950790405273; total_precip: 1.1841709613800049; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08237701654434204; velocity_v: 0.12786415219306946; specific_humidity: 0.05193169787526131; velocity_z: 0.5537563562393188; temperature: 0.13242366909980774; total_precip: 0.9126547574996948; 
0: epoch: 4 [1/5 (20%)]	Loss: 1.04841 : 0.30317 :: 0.17729 (2.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08142521977424622; velocity_v: 0.14278805255889893; specific_humidity: 0.04684294015169144; velocity_z: 0.4936594069004059; temperature: 0.09541766345500946; total_precip: 0.8988481760025024; 
0: epoch: 4 [2/5 (40%)]	Loss: 0.89885 : 0.26472 :: 0.16977 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.07349143177270889; velocity_v: 0.11100193113088608; specific_humidity: 0.05190997198224068; velocity_z: 0.5024695992469788; temperature: 0.10437923669815063; total_precip: 0.8124890327453613; 
0: epoch: 4 [3/5 (60%)]	Loss: 0.81249 : 0.24776 :: 0.16886 (15.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.08212538808584213; velocity_v: 0.13139230012893677; specific_humidity: 0.061714548617601395; velocity_z: 0.5692868828773499; temperature: 0.11075329035520554; total_precip: 1.265142798423767; 
0: epoch: 4 [4/5 (80%)]	Loss: 1.26514 : 0.34049 :: 0.17738 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 4 : 0.30306297540664673
0: validation loss for velocity_u : 0.05284737050533295
0: validation loss for velocity_v : 0.08393020182847977
0: validation loss for specific_humidity : 0.05721062421798706
0: validation loss for velocity_z : 0.4863170385360718
0: validation loss for temperature : 0.08659344166517258
0: validation loss for total_precip : 1.051479458808899
0: 5 : 19:45:08 :: batch_size = 96, lr = 1.857198821839498e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 5, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7530, -0.7735, -0.8002, -0.8359, -0.8778, -0.9252, -0.9752, -1.0135, -1.0312, -1.0202, -0.9899, -0.9498,
0:         -0.9038, -0.8562, -0.8043, -0.7531, -0.7034, -0.6589, -0.7348, -0.7569, -0.7766, -0.7976, -0.8252, -0.8602,
0:         -0.9006], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1808, 0.1906, 0.2042, 0.2146, 0.2171, 0.2121, 0.2044, 0.1992, 0.1965, 0.1881, 0.1721, 0.1454, 0.1102, 0.0741,
0:         0.0439, 0.0284, 0.0330, 0.0570, 0.1644, 0.1717, 0.1800, 0.1869, 0.1869, 0.1819, 0.1771], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.1205, 2.1537, 2.1369, 2.1293, 2.1393, 2.1589, 2.1991, 2.2297, 2.3444, 2.4465, 2.5331, 2.5717, 2.5553, 2.5025,
0:         2.4579, 2.3999, 2.3671, 2.3603, 2.1646, 2.2318, 2.2233, 2.2048, 2.1925, 2.1948, 2.1641], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6605, 0.6366, 0.6322, 0.6409, 0.6910, 0.7171, 0.6409, 0.5387, 0.4168, 0.2885, 0.2450, 0.2363, 0.2450, 0.3255,
0:         0.2994, 0.2885, 0.2624, 0.1775, 0.5887, 0.5234, 0.5365, 0.6279, 0.6627, 0.6953, 0.6148], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.7327, -0.8142, -0.8101, -0.7602, -0.7227, -0.6977, -0.7029, -0.6963, -0.6817, -0.6262, -0.5085, -0.3318,
0:         -0.1531, -0.0140,  0.1184,  0.2306,  0.3494,  0.4431,  0.4801,  0.4541,  0.3505,  0.1861, -0.0474, -0.2635,
0:         -0.4213], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481,
0:         -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481, -0.2481,
0:         -0.2481], device='cuda:0')
0: [DEBUG] Epoch 5, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2481,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2481,     nan, -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2481,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan,
0:             nan,     nan, -0.2481,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,
0:         -0.2481,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2481,     nan,     nan,     nan,     nan, -0.2481,     nan, -0.2481,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 5, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9066, -0.8667, -0.8086, -0.7582, -0.7220, -0.7004, -0.6711, -0.6369, -0.5911, -0.5363, -0.4889, -0.4597,
0:         -0.4410, -0.4218, -0.3990, -0.3608, -0.3169, -0.2794, -0.8254, -0.7867, -0.7309, -0.6878, -0.6599, -0.6408,
0:         -0.6177], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2333, -0.2349, -0.2490, -0.2729, -0.2996, -0.3273, -0.3513, -0.3640, -0.3722, -0.3841, -0.3907, -0.3965,
0:         -0.4046, -0.4174, -0.4180, -0.3880, -0.3218, -0.2274, -0.2407, -0.2118, -0.2079, -0.2178, -0.2405, -0.2736,
0:         -0.2972], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5537, 0.5364, 0.4916, 0.4711, 0.4487, 0.4508, 0.4572, 0.4598, 0.4506, 0.4069, 0.3539, 0.2930, 0.2418, 0.2198,
0:         0.2275, 0.2731, 0.3590, 0.4657, 0.5129, 0.4749, 0.4368, 0.4031, 0.3768, 0.3618, 0.3686], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3406, 0.4728, 0.5259, 0.5365, 0.4812, 0.4670, 0.5141, 0.5659, 0.6510, 0.7508, 0.8133, 0.8352, 0.7913, 0.7245,
0:         0.6661, 0.6571, 0.6817, 0.6178, 0.4216, 0.4525, 0.4345, 0.4170, 0.3669, 0.3721, 0.4189], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.9532, 0.9858, 1.0182, 1.0447, 1.0705, 1.0915, 1.1072, 1.1246, 1.1660, 1.2408, 1.3473, 1.4667, 1.5780, 1.6568,
0:         1.6930, 1.6894, 1.6631, 1.6268, 1.5966, 1.5749, 1.5591, 1.5427, 1.5170, 1.4875, 1.4485], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2319, -0.2340, -0.2358, -0.2448, -0.2514, -0.2560, -0.2503, -0.2416, -0.2396, -0.2316, -0.2328, -0.2450,
0:         -0.2450, -0.2533, -0.2505, -0.2480, -0.2528, -0.2396, -0.2381, -0.2424, -0.2413, -0.2467, -0.2456, -0.2537,
0:         -0.2439], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.09327907115221024; velocity_v: 0.15265825390815735; specific_humidity: 0.0757104754447937; velocity_z: 0.5626779794692993; temperature: 0.11968642473220825; total_precip: 0.8861218094825745; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.09512995928525925; velocity_v: 0.1450357288122177; specific_humidity: 0.08200719952583313; velocity_z: 0.6199602484703064; temperature: 0.10404698550701141; total_precip: 0.9897316694259644; 
0: epoch: 5 [1/5 (20%)]	Loss: 0.93793 : 0.29647 :: 0.18415 (2.47 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.09347444027662277; velocity_v: 0.12668642401695251; specific_humidity: 0.10292452573776245; velocity_z: 0.5524994134902954; temperature: 0.14859884977340698; total_precip: 0.6901499629020691; 
0: epoch: 5 [2/5 (40%)]	Loss: 0.69015 : 0.25323 :: 0.18766 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.093925341963768; velocity_v: 0.15510329604148865; specific_humidity: 0.09808778762817383; velocity_z: 0.5669702291488647; temperature: 0.12664417922496796; total_precip: 0.9596340656280518; 
0: epoch: 5 [3/5 (60%)]	Loss: 0.95963 : 0.30204 :: 0.18588 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.10306631773710251; velocity_v: 0.15092156827449799; specific_humidity: 0.13017919659614563; velocity_z: 0.6022600531578064; temperature: 0.14596915245056152; total_precip: 0.8877905011177063; 
0: epoch: 5 [4/5 (80%)]	Loss: 0.88779 : 0.30306 :: 0.19083 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 5 : 0.2947995364665985
0: validation loss for velocity_u : 0.07156949490308762
0: validation loss for velocity_v : 0.09518170356750488
0: validation loss for specific_humidity : 0.09530609846115112
0: validation loss for velocity_z : 0.5059127807617188
0: validation loss for temperature : 0.08409157395362854
0: validation loss for total_precip : 0.916735827922821
0: 6 : 19:49:10 :: batch_size = 96, lr = 1.8119012895995104e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 6, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6901, 0.6917, 0.6933, 0.6950, 0.6968, 0.6984, 0.7001, 0.7016, 0.7029, 0.7040, 0.7049, 0.7057, 0.7063, 0.7068,
0:         0.7073, 0.7076, 0.7080, 0.7081, 0.6779, 0.6794, 0.6809, 0.6822, 0.6835, 0.6846, 0.6856], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8971, -0.8996, -0.9023, -0.9053, -0.9085, -0.9119, -0.9155, -0.9191, -0.9229, -0.9267, -0.9305, -0.9343,
0:         -0.9381, -0.9419, -0.9459, -0.9499, -0.9542, -0.9584, -0.8381, -0.8414, -0.8452, -0.8495, -0.8539, -0.8588,
0:         -0.8636], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5701, -0.5743, -0.5783, -0.5802, -0.5802, -0.5795, -0.5802, -0.5830, -0.5856, -0.5883, -0.5901, -0.5912,
0:         -0.5925, -0.5937, -0.5924, -0.5906, -0.5887, -0.5885, -0.6137, -0.6139, -0.6140, -0.6140, -0.6120, -0.6106,
0:         -0.6098], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0288, -0.0331, -0.0439, -0.0612, -0.0807, -0.0969, -0.1077, -0.1109, -0.1066, -0.0947, -0.0796, -0.0655,
0:         -0.0547, -0.0504, -0.0526, -0.0601, -0.0709, -0.0807, -0.0126, -0.0212, -0.0331, -0.0493, -0.0634, -0.0742,
0:         -0.0785], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.0767, 0.0778, 0.0786, 0.0794, 0.0801, 0.0804, 0.0810, 0.0812, 0.0817, 0.0821, 0.0824, 0.0830, 0.0835, 0.0842,
0:         0.0851, 0.0861, 0.0875, 0.0887, 0.0907, 0.0923, 0.0942, 0.0964, 0.0984, 0.1009, 0.1032], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2325, -0.2314, -0.2314, -0.2314, -0.2314, -0.2314, -0.2314, -0.2314, -0.2314, -0.2269, -0.2269, -0.2269,
0:         -0.2269, -0.2258, -0.2269, -0.2269, -0.2269, -0.2280, -0.2190, -0.2190, -0.2190, -0.2179, -0.2179, -0.2168,
0:         -0.2179], device='cuda:0')
0: [DEBUG] Epoch 6, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2033,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2213,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1605, -0.1639,     nan, -0.1707,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2078,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2112,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2022,     nan,     nan, -0.2067,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1740,     nan,     nan, -0.2044,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2067,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1774,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1695,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2089,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2067,     nan,
0:         -0.2134, -0.2123, -0.2100,     nan, -0.2055,     nan,     nan,     nan,     nan,     nan,     nan, -0.2213,
0:         -0.2213,     nan,     nan,     nan,     nan,     nan,     nan, -0.2247, -0.2168,     nan, -0.2010,     nan,
0:             nan,     nan, -0.1853,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2134])
0: [DEBUG] Epoch 6, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1281, 1.1401, 1.1657, 1.1854, 1.1943, 1.1785, 1.1503, 1.1045, 1.0488, 1.0135, 0.9865, 0.9799, 0.9856, 1.0040,
0:         1.0234, 1.0363, 1.0479, 1.0446, 1.2002, 1.1937, 1.2064, 1.2159, 1.2120, 1.1923, 1.1622], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6384, -0.6117, -0.6091, -0.6110, -0.6272, -0.6332, -0.6467, -0.6515, -0.6613, -0.6668, -0.6639, -0.6516,
0:         -0.6447, -0.6628, -0.6804, -0.6681, -0.6053, -0.4982, -0.6665, -0.6155, -0.6023, -0.6212, -0.6524, -0.6870,
0:         -0.7067], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6376, -0.6356, -0.6365, -0.6335, -0.6303, -0.6235, -0.6211, -0.6155, -0.6182, -0.6205, -0.6272, -0.6313,
0:         -0.6328, -0.6321, -0.6359, -0.6396, -0.6476, -0.6458, -0.6486, -0.6436, -0.6437, -0.6384, -0.6368, -0.6349,
0:         -0.6285], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0185, -0.0261, -0.0432, -0.0939, -0.1561, -0.1628, -0.1463, -0.1362, -0.0821, -0.0837, -0.1351, -0.1025,
0:         -0.0922, -0.1743, -0.2882, -0.2803, -0.1467, -0.1624, -0.1085, -0.1499, -0.1856, -0.2175, -0.2165, -0.1751,
0:         -0.1444], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.8594, -0.8947, -0.9222, -0.9427, -0.9522, -0.9582, -0.9662, -0.9841, -1.0085, -1.0349, -1.0615, -1.0821,
0:         -1.1025, -1.1197, -1.1373, -1.1512, -1.1621, -1.1680, -1.1721, -1.1731, -1.1748, -1.1779, -1.1828, -1.1945,
0:         -1.2167], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1793, -0.1792, -0.1803, -0.1945, -0.2000, -0.2086, -0.2048, -0.1974, -0.1968, -0.1758, -0.1797, -0.1878,
0:         -0.1926, -0.2023, -0.2063, -0.2021, -0.2069, -0.1978, -0.1842, -0.1862, -0.1887, -0.2000, -0.1974, -0.2070,
0:         -0.1987], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1123507097363472; velocity_v: 0.14810322225093842; specific_humidity: 0.13134033977985382; velocity_z: 0.5836992263793945; temperature: 0.1210198625922203; total_precip: 1.4254382848739624; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.10593241453170776; velocity_v: 0.13525308668613434; specific_humidity: 0.14404509961605072; velocity_z: 0.6020928621292114; temperature: 0.14441780745983124; total_precip: 1.0421693325042725; 
0: epoch: 6 [1/5 (20%)]	Loss: 1.23380 : 0.35805 :: 0.19588 (2.55 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.12301094084978104; velocity_v: 0.18851792812347412; specific_humidity: 0.13690252602100372; velocity_z: 0.6244461536407471; temperature: 0.17721456289291382; total_precip: 0.8151607513427734; 
0: epoch: 6 [2/5 (40%)]	Loss: 0.81516 : 0.30856 :: 0.19714 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11228322237730026; velocity_v: 0.17323891818523407; specific_humidity: 0.14183752238750458; velocity_z: 0.6310897469520569; temperature: 0.1420912891626358; total_precip: 1.2996848821640015; 
0: epoch: 6 [3/5 (60%)]	Loss: 1.29968 : 0.38241 :: 0.19355 (15.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1308673769235611; velocity_v: 0.17099811136722565; specific_humidity: 0.17044250667095184; velocity_z: 0.6955000162124634; temperature: 0.16872969269752502; total_precip: 1.0097551345825195; 
0: epoch: 6 [4/5 (80%)]	Loss: 1.00976 : 0.35503 :: 0.20290 (15.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 6 : 0.2796339988708496
0: validation loss for velocity_u : 0.08631373196840286
0: validation loss for velocity_v : 0.1278935968875885
0: validation loss for specific_humidity : 0.12565670907497406
0: validation loss for velocity_z : 0.5299884080886841
0: validation loss for temperature : 0.10082855820655823
0: validation loss for total_precip : 0.7071225047111511
0: 7 : 19:53:04 :: batch_size = 96, lr = 1.7677085752190346e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 7, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2294, 0.2292, 0.2292, 0.2292, 0.2291, 0.2289, 0.2289, 0.2288, 0.2286, 0.2283, 0.2281, 0.2280, 0.2277, 0.2274,
0:         0.2270, 0.2267, 0.2264, 0.2261, 0.1661, 0.1661, 0.1662, 0.1662, 0.1662, 0.1662, 0.1662], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0844, -0.0906, -0.0970, -0.1032, -0.1094, -0.1157, -0.1221, -0.1283, -0.1345, -0.1409, -0.1472, -0.1534,
0:         -0.1598, -0.1660, -0.1722, -0.1787, -0.1849, -0.1913, -0.0683, -0.0749, -0.0814, -0.0880, -0.0944, -0.1009,
0:         -0.1075], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6124, -0.6122, -0.6121, -0.6118, -0.6116, -0.6115, -0.6113, -0.6112, -0.6110, -0.6109, -0.6107, -0.6106,
0:         -0.6104, -0.6103, -0.6103, -0.6102, -0.6101, -0.6100, -0.6090, -0.6089, -0.6089, -0.6088, -0.6087, -0.6087,
0:         -0.6086], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0231, 0.0231, 0.0231, 0.0231, 0.0231, 0.0231, 0.0253, 0.0253, 0.0253, 0.0253, 0.0253, 0.0253, 0.0253, 0.0253,
0:         0.0274, 0.0274, 0.0274, 0.0274, 0.1124, 0.1124, 0.1145, 0.1145, 0.1145, 0.1145, 0.1145], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4136, -0.4136, -0.4138, -0.4141, -0.4141, -0.4143, -0.4143, -0.4146, -0.4148, -0.4148, -0.4150, -0.4150,
0:         -0.4153, -0.4155, -0.4155, -0.4158, -0.4158, -0.4160, -0.4163, -0.4163, -0.4165, -0.4165, -0.4168, -0.4170,
0:         -0.4171], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368,
0:         -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368, -0.2368,
0:         -0.2368], device='cuda:0')
0: [DEBUG] Epoch 7, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2368,     nan,
0:         -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2368,     nan, -0.2368,     nan,     nan,     nan,     nan, -0.2368,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2368,     nan, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2368,     nan,
0:             nan,     nan,     nan, -0.2368, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2368,     nan,     nan, -0.2368, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2368,     nan,     nan,     nan, -0.2368,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2368,
0:             nan,     nan,     nan,     nan, -0.2368,     nan,     nan,     nan,     nan,     nan, -0.2368,     nan,
0:             nan,     nan, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2368,     nan,
0:         -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2368, -0.2368,     nan, -0.2368,
0:             nan,     nan, -0.2368,     nan, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2368,     nan,     nan,     nan,     nan,     nan,
0:         -0.2368,     nan,     nan,     nan, -0.2368,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2368,     nan])
0: [DEBUG] Epoch 7, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0584,  0.0012,  0.0602,  0.0858,  0.0817,  0.0676,  0.0799,  0.1258,  0.1733,  0.2185,  0.2210,  0.1795,
0:          0.1285,  0.0909,  0.0945,  0.1422,  0.2012,  0.2562, -0.0330,  0.0226,  0.0772,  0.0918,  0.0654,  0.0393,
0:          0.0320], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9988, 0.9802, 0.9245, 0.8495, 0.7817, 0.7531, 0.7517, 0.7627, 0.7721, 0.7693, 0.7490, 0.7301, 0.7167, 0.6993,
0:         0.6886, 0.6797, 0.6929, 0.7169, 0.9677, 0.9522, 0.9173, 0.8590, 0.8139, 0.8134, 0.8385], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7371, -0.7332, -0.7247, -0.7094, -0.6938, -0.6816, -0.6802, -0.6825, -0.6888, -0.7005, -0.7067, -0.7094,
0:         -0.7031, -0.6949, -0.6980, -0.7000, -0.7164, -0.7248, -0.7365, -0.7220, -0.7068, -0.6883, -0.6762, -0.6702,
0:         -0.6684], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2965, -0.3029, -0.2126, -0.2262, -0.3139, -0.3350, -0.3492, -0.3575, -0.3537, -0.3894, -0.3582, -0.2661,
0:         -0.2710, -0.3827, -0.5101, -0.5034, -0.3879, -0.3910, -0.2978, -0.3730, -0.3552, -0.3882, -0.4135, -0.3834,
0:         -0.3781], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.4506, -0.4808, -0.4998, -0.5145, -0.5212, -0.5226, -0.5154, -0.5058, -0.4971, -0.4949, -0.5047, -0.5172,
0:         -0.5237, -0.5155, -0.4988, -0.4809, -0.4753, -0.4786, -0.4906, -0.5037, -0.5115, -0.5086, -0.4936, -0.4730,
0:         -0.4555], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1762, -0.1781, -0.1759, -0.1902, -0.1955, -0.2005, -0.1984, -0.1921, -0.1917, -0.1732, -0.1741, -0.1841,
0:         -0.1839, -0.1899, -0.1927, -0.1918, -0.1973, -0.1883, -0.1840, -0.1821, -0.1782, -0.1859, -0.1834, -0.1924,
0:         -0.1842], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1285039335489273; velocity_v: 0.1539238691329956; specific_humidity: 0.14279481768608093; velocity_z: 0.5931777954101562; temperature: 0.1299668401479721; total_precip: 1.012805461883545; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.11711689829826355; velocity_v: 0.14651042222976685; specific_humidity: 0.13670024275779724; velocity_z: 0.5323054194450378; temperature: 0.12997163832187653; total_precip: 0.825973391532898; 
0: epoch: 7 [1/5 (20%)]	Loss: 0.91939 : 0.30396 :: 0.19812 (2.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13294507563114166; velocity_v: 0.17369361221790314; specific_humidity: 0.1687781661748886; velocity_z: 0.6497950553894043; temperature: 0.15191413462162018; total_precip: 1.0611542463302612; 
0: epoch: 7 [2/5 (40%)]	Loss: 1.06115 : 0.35346 :: 0.20575 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1337776631116867; velocity_v: 0.16341473162174225; specific_humidity: 0.1931816041469574; velocity_z: 0.7212318181991577; temperature: 0.17977078258991241; total_precip: 1.457554817199707; 
0: epoch: 7 [3/5 (60%)]	Loss: 1.45755 : 0.43752 :: 0.20499 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13436107337474823; velocity_v: 0.15903396904468536; specific_humidity: 0.19398392736911774; velocity_z: 0.4956214427947998; temperature: 0.1647794097661972; total_precip: 0.7858654260635376; 
0: epoch: 7 [4/5 (80%)]	Loss: 0.78587 : 0.28598 :: 0.20506 (15.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 7 : 0.2984592914581299
0: validation loss for velocity_u : 0.09009344130754471
0: validation loss for velocity_v : 0.10777616500854492
0: validation loss for specific_humidity : 0.1340649276971817
0: validation loss for velocity_z : 0.4991256296634674
0: validation loss for temperature : 0.08957764506340027
0: validation loss for total_precip : 0.8701187968254089
0: 8 : 19:57:00 :: batch_size = 96, lr = 1.7245937319210094e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 8, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0792, -1.0138, -0.9704, -0.8825, -0.8009, -0.7611, -0.7749, -0.8519, -0.8779, -0.8833, -0.9225, -0.9477,
0:         -0.9593, -0.9303, -0.8799, -0.8418, -0.8118, -0.8112, -1.1043, -1.0584, -1.0159, -0.9106, -0.8623, -0.8625,
0:         -0.8280], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3793, -0.4454, -0.5274, -0.5977, -0.5931, -0.5283, -0.4687, -0.4476, -0.4691, -0.4824, -0.4876, -0.5487,
0:         -0.6342, -0.6734, -0.6868, -0.7055, -0.7042, -0.6847, -0.4080, -0.4783, -0.5563, -0.6338, -0.6712, -0.6464,
0:         -0.5502], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 2.0396,  1.9902,  1.9838,  1.9122,  1.6197,  1.0142, -0.0095, -0.5309, -0.4982, -0.3148, -0.0178,  0.2583,
0:          0.4474,  0.5002,  0.4938,  0.4683,  0.4077,  0.3521,  2.0597,  2.0855,  2.1215,  2.1331,  2.0221,  1.5313,
0:          0.3194], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1403, -0.7752, -0.5031, -0.2596,  0.0302, -0.2065, -0.0693,  0.0479, -0.1579,  0.0612, -0.0251, -0.0118,
0:          0.1763, -0.1955, -0.3083, -0.2928, -0.2751, -0.1202, -1.0894, -1.0651, -1.1713, -0.7664,  0.5392,  0.6985,
0:          0.3046], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.2728, 0.2778, 0.2043, 0.1508, 0.2196, 0.4903, 0.9354, 1.0539, 0.9276, 0.6294, 0.3779, 0.2428, 0.1518, 0.1041,
0:         0.1424, 0.2308, 0.3053, 0.3584, 0.3850, 0.3818, 0.3860, 0.3594, 0.2866, 0.2111, 0.1577], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438,
0:         -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438,
0:         -0.2438], device='cuda:0')
0: [DEBUG] Epoch 8, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan, -0.2438,     nan,     nan,     nan,
0:         -0.2438,     nan,     nan,     nan, -0.2416,     nan,     nan, -0.2438,     nan,     nan,     nan, -0.2438,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2416,     nan,     nan,
0:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,
0:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan, -0.2438, -0.2438,     nan,
0:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2416,     nan,     nan, -0.2438,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2438,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,
0:         -0.2438,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2438,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan, -0.2438, -0.2438,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 8, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2186, -1.0664, -0.9225, -0.8661, -0.8848, -0.9148, -0.8842, -0.7679, -0.6167, -0.4755, -0.4151, -0.4485,
0:         -0.5434, -0.6035, -0.6077, -0.5374, -0.4528, -0.3951, -1.1416, -0.9858, -0.8409, -0.8037, -0.8505, -0.8972,
0:         -0.8851], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0979, -0.0835, -0.0978, -0.1168, -0.1248, -0.1269, -0.1582, -0.2017, -0.2528, -0.3113, -0.3454, -0.3556,
0:         -0.3501, -0.3474, -0.3345, -0.2839, -0.2013, -0.1000, -0.1747, -0.1151, -0.0907, -0.0734, -0.0774, -0.1072,
0:         -0.1520], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 2.2983,  2.3051,  2.2033,  1.9675,  1.5716,  1.0460,  0.4809, -0.0304, -0.3791, -0.5236, -0.4888, -0.3617,
0:         -0.2072, -0.0557,  0.0635,  0.2197,  0.4242,  0.6719,  2.3255,  2.3543,  2.2959,  2.1005,  1.7365,  1.2127,
0:          0.6126], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5515, -0.3962, -0.2629, -0.1234, -0.0804, -0.4797, -0.5450, -0.0182,  0.0015, -0.0060,  0.1357, -0.1581,
0:         -0.1139,  0.0030, -0.2788, -0.3315, -0.2009, -0.1206, -0.6532, -0.4723, -0.3967, -0.3376, -0.2069, -0.9487,
0:         -1.3568], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.9489, -1.0452, -0.9905, -0.8193, -0.6011, -0.4107, -0.2752, -0.1839, -0.1229, -0.1151, -0.1821, -0.3006,
0:         -0.4092, -0.4560, -0.4360, -0.4047, -0.4286, -0.5228, -0.6489, -0.7499, -0.7785, -0.7409, -0.6735, -0.6205,
0:         -0.6308], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1253, -0.1279, -0.1273, -0.1497, -0.1530, -0.1575, -0.1583, -0.1496, -0.1533, -0.1220, -0.1267, -0.1377,
0:         -0.1388, -0.1478, -0.1510, -0.1509, -0.1521, -0.1450, -0.1304, -0.1318, -0.1307, -0.1451, -0.1423, -0.1459,
0:         -0.1362], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.13700531423091888; velocity_v: 0.16732370853424072; specific_humidity: 0.1848415583372116; velocity_z: 0.49997615814208984; temperature: 0.14430280029773712; total_precip: 0.6342326402664185; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14403420686721802; velocity_v: 0.19520460069179535; specific_humidity: 0.17755189538002014; velocity_z: 0.6322727203369141; temperature: 0.1603213995695114; total_precip: 1.0096368789672852; 
0: epoch: 8 [1/5 (20%)]	Loss: 0.82193 : 0.30332 :: 0.20832 (2.54 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13204285502433777; velocity_v: 0.16184057295322418; specific_humidity: 0.15855306386947632; velocity_z: 0.6289905309677124; temperature: 0.11932817846536636; total_precip: 0.9035595655441284; 
0: epoch: 8 [2/5 (40%)]	Loss: 0.90356 : 0.31606 :: 0.20389 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.12942419946193695; velocity_v: 0.16094961762428284; specific_humidity: 0.1672900766134262; velocity_z: 0.4636342227458954; temperature: 0.13172225654125214; total_precip: 0.4330347180366516; 
0: epoch: 8 [3/5 (60%)]	Loss: 0.43303 : 0.21223 :: 0.21069 (15.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15064196288585663; velocity_v: 0.18199540674686432; specific_humidity: 0.17499306797981262; velocity_z: 0.5734049081802368; temperature: 0.14498461782932281; total_precip: 0.8978235721588135; 
0: epoch: 8 [4/5 (80%)]	Loss: 0.89782 : 0.31766 :: 0.21276 (15.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 8 : 0.32350409030914307
0: validation loss for velocity_u : 0.10990729182958603
0: validation loss for velocity_v : 0.10375430434942245
0: validation loss for specific_humidity : 0.16623274981975555
0: validation loss for velocity_z : 0.5149565935134888
0: validation loss for temperature : 0.11512914299964905
0: validation loss for total_precip : 0.9310447573661804
0: 9 : 20:00:59 :: batch_size = 96, lr = 1.6825304701668386e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 9, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.0618, 1.9724, 1.9022, 1.8489, 1.8069, 1.7920, 1.7949, 1.7866, 1.7689, 1.7528, 1.7185, 1.6503, 1.5731, 1.5125,
0:         1.4634, 1.4223, 1.4042, 1.4171, 2.1619, 2.0005, 1.8861, 1.8539, 1.8671, 1.8846, 1.8781], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.5449, 1.6200, 1.7165, 1.7898, 1.8143, 1.8011, 1.7745, 1.7520, 1.7367, 1.7208, 1.6972, 1.6652, 1.6338, 1.6216,
0:         1.6389, 1.6747, 1.7089, 1.7278, 1.6135, 1.6554, 1.7047, 1.7369, 1.7407, 1.7244, 1.7019], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6954, -0.6946, -0.6943, -0.6941, -0.6943, -0.6923, -0.6919, -0.6921, -0.6923, -0.6895, -0.6598, -0.6186,
0:         -0.5295, -0.4719, -0.4397, -0.4526, -0.4603, -0.4611, -0.6931, -0.6922, -0.6919, -0.6926, -0.6931, -0.6944,
0:         -0.6958], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.5665, -2.6701, -4.6902, -4.0340, -2.1500, -0.6768, -0.0273, -0.1054, -0.3956, -0.3777, -0.5987, -1.8018,
0:         -3.4179, -4.5362, -5.1791, -5.4447, -4.9291, -3.7014,  2.3299, -2.3643, -6.5206, -6.2170, -2.9045, -0.1858,
0:          0.4616], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.1240, 0.1762, 0.2026, 0.2079, 0.2085, 0.2191, 0.2404, 0.2662, 0.2913, 0.3096, 0.3137, 0.3018, 0.2813, 0.2587,
0:         0.2335, 0.2028, 0.1677, 0.1349, 0.1134, 0.1098, 0.1242, 0.1518, 0.1858, 0.2174, 0.2401], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1785, -0.2210, -0.2304, -0.2399, -0.2422, -0.2493, -0.2493, -0.2493, -0.2493, -0.1454, -0.1926, -0.2399,
0:         -0.2469, -0.2493, -0.2493, -0.2493, -0.2493, -0.2493, -0.0557, -0.1737, -0.2399, -0.2469, -0.2469, -0.2446,
0:         -0.2469], device='cuda:0')
0: [DEBUG] Epoch 9, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2481,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2493,     nan,     nan,     nan,     nan, -0.2469,     nan, -0.2493,     nan, -0.2493,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan, -0.2493,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2493,     nan,     nan,     nan, -0.2493,     nan, -0.2493,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2481,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,
0:         -0.2493,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan, -0.2493,     nan,
0:             nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2481,     nan,     nan,     nan,     nan,
0:         -0.2493,     nan,     nan])
0: [DEBUG] Epoch 9, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4595, 1.3935, 1.3356, 1.3030, 1.2924, 1.2808, 1.2919, 1.2782, 1.2464, 1.2151, 1.1734, 1.1577, 1.1560, 1.1801,
0:         1.2157, 1.2553, 1.3022, 1.3252, 1.4075, 1.3248, 1.2668, 1.2387, 1.2302, 1.2355, 1.2548], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0190,  0.0214,  0.0474,  0.0728,  0.1168,  0.2029,  0.3156,  0.4505,  0.5893,  0.7192,  0.8367,  0.9400,
0:          1.0238,  1.0751,  1.1131,  1.1551,  1.2157,  1.2985,  0.0041,  0.0499,  0.0836,  0.1158,  0.1826,  0.2918,
0:          0.4419], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7737, -0.7749, -0.7728, -0.7687, -0.7620, -0.7598, -0.7626, -0.7651, -0.7694, -0.7724, -0.7722, -0.7690,
0:         -0.7632, -0.7600, -0.7596, -0.7620, -0.7688, -0.7729, -0.7743, -0.7731, -0.7683, -0.7584, -0.7518, -0.7489,
0:         -0.7483], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1106, -0.3867, -0.3105, -0.0044,  0.0604,  0.2624,  0.5325,  0.2193, -0.4596, -0.5805, -0.2679, -0.0913,
0:          0.3095,  1.2513,  1.9610,  1.9155,  1.7715,  1.7517,  0.3372, -1.0320, -1.2854, -0.2701,  0.4687,  0.5713,
0:          0.4535], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.4297, -1.4356, -1.4229, -1.4053, -1.3793, -1.3657, -1.3549, -1.3483, -1.3407, -1.3363, -1.3472, -1.3693,
0:         -1.3919, -1.3867, -1.3528, -1.3028, -1.2647, -1.2471, -1.2463, -1.2421, -1.2188, -1.1693, -1.1036, -1.0512,
0:         -1.0267], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1323, -0.1298, -0.1303, -0.1425, -0.1418, -0.1429, -0.1357, -0.1266, -0.1248, -0.1310, -0.1313, -0.1436,
0:         -0.1414, -0.1449, -0.1422, -0.1356, -0.1376, -0.1317, -0.1405, -0.1404, -0.1357, -0.1484, -0.1430, -0.1475,
0:         -0.1398], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1602209061384201; velocity_v: 0.18405361473560333; specific_humidity: 0.19602878391742706; velocity_z: 0.6455626487731934; temperature: 0.15089678764343262; total_precip: 1.2340105772018433; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1391395628452301; velocity_v: 0.19169475138187408; specific_humidity: 0.18216700851917267; velocity_z: 0.5837268233299255; temperature: 0.15795551240444183; total_precip: 0.9036489129066467; 
0: epoch: 9 [1/5 (20%)]	Loss: 1.06883 : 0.35726 :: 0.21065 (2.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15779368579387665; velocity_v: 0.15525466203689575; specific_humidity: 0.1964871883392334; velocity_z: 0.6814874410629272; temperature: 0.16229850053787231; total_precip: 0.7973857522010803; 
0: epoch: 9 [2/5 (40%)]	Loss: 0.79739 : 0.32149 :: 0.21631 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13857905566692352; velocity_v: 0.15905915200710297; specific_humidity: 0.15832382440567017; velocity_z: 0.6203757524490356; temperature: 0.1442810297012329; total_precip: 0.9681907296180725; 
0: epoch: 9 [3/5 (60%)]	Loss: 0.96819 : 0.32779 :: 0.21345 (15.21 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15652614831924438; velocity_v: 0.16536110639572144; specific_humidity: 0.19220882654190063; velocity_z: 0.5973702073097229; temperature: 0.13702483475208282; total_precip: 0.9200143814086914; 
0: epoch: 9 [4/5 (80%)]	Loss: 0.92001 : 0.32504 :: 0.21535 (15.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 9 : 0.3186178505420685
0: validation loss for velocity_u : 0.0947742834687233
0: validation loss for velocity_v : 0.1189267709851265
0: validation loss for specific_humidity : 0.15045949816703796
0: validation loss for velocity_z : 0.7237718105316162
0: validation loss for temperature : 0.09473550319671631
0: validation loss for total_precip : 0.7290392518043518
0: 10 : 20:04:56 :: batch_size = 96, lr = 1.6414931416261842e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 10, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3337, -0.3094, -0.2865, -0.2659, -0.2471, -0.2309, -0.2156, -0.1994, -0.1855, -0.1720, -0.1567, -0.1380,
0:         -0.1145, -0.0883, -0.0600, -0.0318, -0.0057,  0.0187, -0.2928, -0.2706, -0.2481, -0.2267, -0.2076, -0.1913,
0:         -0.1747], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9462, 0.9716, 0.9940, 1.0139, 1.0326, 1.0499, 1.0656, 1.0812, 1.0953, 1.1027, 1.1050, 1.1061, 1.1105, 1.1192,
0:         1.1317, 1.1476, 1.1606, 1.1699, 0.9569, 0.9838, 1.0079, 1.0300, 1.0491, 1.0662, 1.0812], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7502, 0.7916, 0.8164, 0.8368, 0.8312, 0.8193, 0.7980, 0.7622, 0.7147, 0.6554, 0.5984, 0.5244, 0.4631, 0.4050,
0:         0.3549, 0.3132, 0.2776, 0.2441, 0.8000, 0.8260, 0.8482, 0.8488, 0.8375, 0.8136, 0.7867], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5889, 0.5935, 0.5433, 0.4795, 0.4339, 0.3792, 0.2538, 0.2379, 0.2744, 0.2310, 0.3291, 0.4294, 0.5479, 0.7052,
0:         0.7850, 0.8807, 0.9400, 1.0152, 0.5570, 0.5866, 0.5616, 0.5251, 0.5023, 0.4020, 0.2789], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([2.3176, 2.3321, 2.3419, 2.3521, 2.3549, 2.3600, 2.3643, 2.3601, 2.3496, 2.3287, 2.3019, 2.2935, 2.3144, 2.3552,
0:         2.4116, 2.4646, 2.5014, 2.5329, 2.5567, 2.5753, 2.5948, 2.6080, 2.6051, 2.5881, 2.5566], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1239, -0.1264, -0.1594, -0.1924, -0.1875, -0.2290, -0.2095, -0.2241, -0.2156, -0.1691, -0.1569, -0.1862,
0:         -0.2217, -0.2388, -0.1960, -0.1410, -0.2082, -0.1630, -0.1569, -0.1948, -0.2510, -0.2486, -0.1838, -0.0897,
0:         -0.2156], device='cuda:0')
0: [DEBUG] Epoch 10, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1276,     nan,
0:             nan, -0.1044,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0787,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0261,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1007,     nan,     nan,     nan,     nan,     nan,  0.1205,     nan,     nan,     nan,     nan,
0:             nan,  0.2403,     nan,  0.2880,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1141,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0909,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0399,     nan,     nan,     nan,     nan,
0:         -0.1545,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2807,     nan,
0:             nan, -0.0457,     nan,     nan,  0.0558,     nan,     nan, -0.1129,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1422, -0.0640, -0.0347,     nan, -0.0762, -0.0530,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0545,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0322,     nan,     nan,     nan, -0.1948,
0:             nan, -0.2437,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1740,     nan,     nan, -0.1386,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1178,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 10, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4290, -0.3725, -0.3079, -0.2617, -0.2394, -0.2334, -0.1983, -0.1538, -0.1061, -0.0694, -0.0743, -0.1281,
0:         -0.1980, -0.2406, -0.2452, -0.1946, -0.1310, -0.0823, -0.3657, -0.3143, -0.2614, -0.2367, -0.2490, -0.2553,
0:         -0.2480], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1740, 0.2397, 0.2764, 0.2781, 0.2641, 0.2558, 0.2534, 0.2613, 0.2707, 0.2763, 0.2764, 0.2775, 0.2815, 0.2642,
0:         0.2507, 0.2557, 0.2930, 0.3554, 0.1028, 0.1778, 0.2268, 0.2334, 0.2273, 0.2248, 0.2365], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1156, -0.1132, -0.1129, -0.1007, -0.0951, -0.0932, -0.0959, -0.1078, -0.1234, -0.1381, -0.1476, -0.1529,
0:         -0.1454, -0.1306, -0.1205, -0.1021, -0.0910, -0.0724, -0.1133, -0.1049, -0.0983, -0.0833, -0.0791, -0.0809,
0:         -0.0767], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2657, 0.3060, 0.4403, 0.4254, 0.3384, 0.2883, 0.2149, 0.2050, 0.1848, 0.1199, 0.1174, 0.1349, 0.1409, 0.1714,
0:         0.1041, 0.1301, 0.3370, 0.3122, 0.1891, 0.1318, 0.2451, 0.2441, 0.2441, 0.2658, 0.2097], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.3439, 0.3681, 0.4048, 0.4775, 0.5841, 0.6981, 0.7928, 0.8581, 0.9112, 0.9691, 1.0428, 1.1342, 1.2164, 1.2773,
0:         1.2996, 1.2963, 1.3082, 1.3885, 1.5498, 1.7573, 1.9331, 2.0295, 2.0537, 2.0660, 2.1302], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.3863, 0.3800, 0.3998, 0.3263, 0.3041, 0.2769, 0.2754, 0.2820, 0.2991, 0.4126, 0.3672, 0.3260, 0.2993, 0.2710,
0:         0.2429, 0.2611, 0.2685, 0.3080, 0.3985, 0.3364, 0.3113, 0.2329, 0.2106, 0.2234, 0.2383], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.1633247435092926; velocity_v: 0.16931912302970886; specific_humidity: 0.18396201729774475; velocity_z: 0.6548307538032532; temperature: 0.14005374908447266; total_precip: 0.9698789119720459; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16681058704853058; velocity_v: 0.2028103768825531; specific_humidity: 0.17425508797168732; velocity_z: 0.6492112874984741; temperature: 0.13731977343559265; total_precip: 1.0479152202606201; 
0: epoch: 10 [1/5 (20%)]	Loss: 1.00890 : 0.35062 :: 0.22262 (2.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17481236159801483; velocity_v: 0.19437186419963837; specific_humidity: 0.1790008395910263; velocity_z: 0.601017951965332; temperature: 0.18610528111457825; total_precip: 1.2572165727615356; 
0: epoch: 10 [2/5 (40%)]	Loss: 1.25722 : 0.39492 :: 0.21244 (15.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16523374617099762; velocity_v: 0.17076556384563446; specific_humidity: 0.18179836869239807; velocity_z: 0.5922096371650696; temperature: 0.13445349037647247; total_precip: 0.659905731678009; 
0: epoch: 10 [3/5 (60%)]	Loss: 0.65991 : 0.28066 :: 0.21569 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1477404534816742; velocity_v: 0.1785779595375061; specific_humidity: 0.21742022037506104; velocity_z: 0.7029295563697815; temperature: 0.1629105806350708; total_precip: 1.1012014150619507; 
0: epoch: 10 [4/5 (80%)]	Loss: 1.10120 : 0.37876 :: 0.21282 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 10 : 0.37411653995513916
0: validation loss for velocity_u : 0.08719729632139206
0: validation loss for velocity_v : 0.10482894629240036
0: validation loss for specific_humidity : 0.20051980018615723
0: validation loss for velocity_z : 0.6081405282020569
0: validation loss for temperature : 0.12520833313465118
0: validation loss for total_precip : 1.1188045740127563
0: 11 : 20:09:36 :: batch_size = 96, lr = 1.601456723537741e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 11, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.6172, -1.6051, -1.5944, -1.5849, -1.5766, -1.5686, -1.5609, -1.5532, -1.5449, -1.5358, -1.5258, -1.5144,
0:         -1.5018, -1.4876, -1.4715, -1.4533, -1.4331, -1.4110, -1.6302, -1.6145, -1.6006, -1.5884, -1.5777, -1.5684,
0:         -1.5599], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.6762, 2.6935, 2.7108, 2.7279, 2.7446, 2.7609, 2.7761, 2.7899, 2.8020, 2.8118, 2.8185, 2.8218, 2.8208, 2.8152,
0:         2.8041, 2.7872, 2.7640, 2.7344, 2.7054, 2.7165, 2.7277, 2.7396, 2.7524, 2.7659, 2.7799], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6654, -0.6651, -0.6648, -0.6610, -0.6540, -0.6469, -0.6412, -0.6382, -0.6271, -0.6119, -0.5949, -0.5779,
0:         -0.5561, -0.5297, -0.5032, -0.4785, -0.4579, -0.4301, -0.6651, -0.6665, -0.6679, -0.6686, -0.6663, -0.6633,
0:         -0.6603], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3778, -0.3582, -0.3365, -0.3170, -0.3083, -0.3104, -0.3213, -0.3365, -0.3495, -0.3561, -0.3495, -0.3343,
0:         -0.3083, -0.2822, -0.2561, -0.2387, -0.2279, -0.2279, -0.3343, -0.3256, -0.3104, -0.2887, -0.2648, -0.2452,
0:         -0.2322], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0692, -0.0291,  0.0131,  0.0569,  0.1018,  0.1467,  0.1913,  0.2345,  0.2763,  0.3163,  0.3548,  0.3907,
0:          0.4238,  0.4534,  0.4790,  0.5005,  0.5168,  0.5293,  0.5381,  0.5440,  0.5476,  0.5500,  0.5521,  0.5548,
0:          0.5598], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2334, -0.2346, -0.2346, -0.2357, -0.2369, -0.2380, -0.2380, -0.2380, -0.2380, -0.2369, -0.2369, -0.2369,
0:         -0.2380, -0.2380, -0.2380, -0.2380, -0.2380, -0.2380, -0.2277, -0.2288, -0.2334, -0.2334, -0.2346, -0.2346,
0:         -0.2357], device='cuda:0')
0: [DEBUG] Epoch 11, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2392,     nan, -0.2392,     nan,     nan,     nan, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2380,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2380,
0:             nan,     nan,     nan, -0.2392,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan, -0.2392,
0:             nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan, -0.2392,     nan,     nan, -0.2392,
0:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan, -0.2392, -0.2392,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,
0:             nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2392,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2392,     nan])
0: [DEBUG] Epoch 11, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.3804, -1.3305, -1.2856, -1.2709, -1.2836, -1.2998, -1.2742, -1.2128, -1.1429, -1.0928, -1.0866, -1.1351,
0:         -1.1924, -1.2041, -1.1696, -1.0751, -0.9849, -0.9092, -1.2572, -1.2096, -1.1695, -1.1720, -1.2287, -1.2654,
0:         -1.2654], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.9949, 1.9770, 1.9424, 1.9090, 1.8870, 1.9002, 1.9204, 1.9357, 1.9319, 1.9088, 1.8678, 1.8237, 1.7913, 1.7606,
0:         1.7533, 1.7466, 1.7504, 1.7339, 2.0375, 1.9875, 1.9427, 1.9075, 1.9146, 1.9554, 2.0129], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6115, -0.6084, -0.6015, -0.5889, -0.5764, -0.5608, -0.5542, -0.5495, -0.5498, -0.5490, -0.5461, -0.5375,
0:         -0.5214, -0.5071, -0.5006, -0.4937, -0.4976, -0.4926, -0.6274, -0.6154, -0.6022, -0.5870, -0.5757, -0.5680,
0:         -0.5592], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3146, 0.4127, 0.6283, 0.5865, 0.3909, 0.3240, 0.2676, 0.2124, 0.2137, 0.1598, 0.0999, 0.1480, 0.1438, 0.0761,
0:         0.0733, 0.0802, 0.0748, 0.0089, 0.3116, 0.2826, 0.4000, 0.3636, 0.2558, 0.2628, 0.2641], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.0303, -0.9934, -0.9579, -0.9498, -0.9463, -0.9366, -0.9004, -0.8442, -0.7899, -0.7558, -0.7481, -0.7534,
0:         -0.7449, -0.6903, -0.6071, -0.5152, -0.4557, -0.4387, -0.4556, -0.4792, -0.4716, -0.4169, -0.3287, -0.2558,
0:         -0.2279], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1872, -0.1812, -0.1775, -0.1742, -0.1649, -0.1694, -0.1593, -0.1659, -0.1632, -0.1924, -0.1856, -0.1873,
0:         -0.1726, -0.1719, -0.1650, -0.1634, -0.1653, -0.1710, -0.2069, -0.1983, -0.1783, -0.1802, -0.1696, -0.1728,
0:         -0.1673], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1598309725522995; velocity_v: 0.17849479615688324; specific_humidity: 0.1990506947040558; velocity_z: 0.6178116202354431; temperature: 0.14865048229694366; total_precip: 0.7763083577156067; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14361071586608887; velocity_v: 0.21017232537269592; specific_humidity: 0.1647748351097107; velocity_z: 0.5987538695335388; temperature: 0.15502169728279114; total_precip: 0.6952440142631531; 
0: epoch: 11 [1/5 (20%)]	Loss: 0.73578 : 0.30043 :: 0.21003 (2.58 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16943766176700592; velocity_v: 0.19311018288135529; specific_humidity: 0.18032033741474152; velocity_z: 0.5542759895324707; temperature: 0.15755483508110046; total_precip: 0.872194230556488; 
0: epoch: 11 [2/5 (40%)]	Loss: 0.87219 : 0.31735 :: 0.21845 (15.21 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15159042179584503; velocity_v: 0.16920223832130432; specific_humidity: 0.17698542773723602; velocity_z: 0.6263484954833984; temperature: 0.14751778542995453; total_precip: 0.9595585465431213; 
0: epoch: 11 [3/5 (60%)]	Loss: 0.95956 : 0.33556 :: 0.20788 (15.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1371290534734726; velocity_v: 0.15098488330841064; specific_humidity: 0.23635688424110413; velocity_z: 0.619763195514679; temperature: 0.16603457927703857; total_precip: 0.9627846479415894; 
0: epoch: 11 [4/5 (80%)]	Loss: 0.96278 : 0.34152 :: 0.20688 (15.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 11 : 0.28530949354171753
0: validation loss for velocity_u : 0.11406736820936203
0: validation loss for velocity_v : 0.11878000944852829
0: validation loss for specific_humidity : 0.1686587929725647
0: validation loss for velocity_z : 0.5841108560562134
0: validation loss for temperature : 0.12083490937948227
0: validation loss for total_precip : 0.6054046750068665
0: 12 : 20:13:30 :: batch_size = 96, lr = 1.5623968034514547e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 12, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.6047, -1.6012, -1.5800, -1.5501, -1.5326, -1.5299, -1.5185, -1.4824, -1.4376, -1.4085, -1.3970, -1.3864,
0:         -1.3688, -1.3508, -1.3395, -1.3332, -1.3254, -1.3144, -1.6618, -1.6587, -1.6453, -1.6214, -1.6005, -1.5940,
0:         -1.5905], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2355, -0.2572, -0.2658, -0.2654, -0.2627, -0.2627, -0.2647, -0.2647, -0.2627, -0.2635, -0.2696, -0.2783,
0:         -0.2877, -0.2988, -0.3124, -0.3276, -0.3441, -0.3619, -0.1104, -0.1461, -0.1760, -0.1964, -0.2104, -0.2236,
0:         -0.2365], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6860, -0.6916, -0.6913, -0.6907, -0.6905, -0.6908, -0.6831, -0.6765, -0.6669, -0.6619, -0.6638, -0.6602,
0:         -0.6593, -0.6544, -0.6524, -0.6521, -0.6555, -0.6554, -0.6801, -0.6858, -0.6888, -0.6916, -0.6913, -0.6929,
0:         -0.6944], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 1.2582,  1.1407,  0.2987,  0.1616,  0.8516,  1.1592,  0.6673,  0.2031,  0.2964,  0.5890,  0.6650,  0.5176,
0:          0.3310,  0.3275,  0.5579,  0.7249,  0.5936,  0.4174,  0.0948,  0.3482, -0.1033, -0.5733, -0.0998,  0.8574,
0:          1.1177], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.7948, 0.7991, 0.7954, 0.7968, 0.8110, 0.8280, 0.8257, 0.7990, 0.7736, 0.7698, 0.7784, 0.7804, 0.7722, 0.7651,
0:         0.7639, 0.7637, 0.7619, 0.7659, 0.7812, 0.8008, 0.8113, 0.8119, 0.8099, 0.8108, 0.8120], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2493, -0.2493, -0.2493, -0.2481, -0.2481, -0.2481, -0.2493, -0.2493, -0.2493, -0.2493, -0.2493, -0.2493,
0:         -0.2493, -0.2493, -0.2481, -0.2481, -0.2493, -0.2493, -0.2493, -0.2493, -0.2493, -0.2493, -0.2493, -0.2493,
0:         -0.2493], device='cuda:0')
0: [DEBUG] Epoch 12, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan, -0.2493,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2493,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,
0:             nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2493, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,
0:             nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493, -0.2493,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan, -0.2493,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493, -0.2493,     nan,     nan,
0:             nan,     nan,     nan, -0.2493,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 12, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3621, -0.2514, -0.1678, -0.1464, -0.1742, -0.2055, -0.1749, -0.0881,  0.0142,  0.0995,  0.1073,  0.0479,
0:         -0.0506, -0.1241, -0.1477, -0.1116, -0.0609, -0.0320, -0.3651, -0.2671, -0.1870, -0.1838, -0.2400, -0.2789,
0:         -0.2634], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7790, -0.7114, -0.6820, -0.6942, -0.7421, -0.7958, -0.8639, -0.9021, -0.9227, -0.9384, -0.9325, -0.9262,
0:         -0.9299, -0.9578, -0.9875, -0.9667, -0.8783, -0.7325, -0.8684, -0.7621, -0.7149, -0.7212, -0.7717, -0.8540,
0:         -0.9171], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7580, -0.7449, -0.7294, -0.7135, -0.6965, -0.6857, -0.6853, -0.6848, -0.6894, -0.6958, -0.6968, -0.6931,
0:         -0.6838, -0.6701, -0.6678, -0.6703, -0.6881, -0.7007, -0.7441, -0.7238, -0.7078, -0.6894, -0.6807, -0.6782,
0:         -0.6781], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.7431,  0.7723,  0.7873,  0.6068,  0.6124,  0.7339,  0.5465,  0.2668,  0.1571,  0.1473,  0.2097,  0.2148,
0:          0.1286,  0.1366,  0.0146, -0.0793,  0.0570, -0.1125,  0.7029,  0.5658,  0.5530,  0.3587,  0.3909,  0.6183,
0:          0.5453], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4071, 0.3716, 0.3450, 0.3252, 0.3272, 0.3482, 0.3799, 0.4122, 0.4350, 0.4431, 0.4321, 0.4120, 0.3996, 0.4069,
0:         0.4303, 0.4587, 0.4743, 0.4709, 0.4547, 0.4367, 0.4325, 0.4475, 0.4774, 0.5004, 0.4967], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2302, -0.2217, -0.2214, -0.2089, -0.1999, -0.2032, -0.1941, -0.2009, -0.1977, -0.2369, -0.2304, -0.2298,
0:         -0.2150, -0.2080, -0.1974, -0.2004, -0.1997, -0.2065, -0.2519, -0.2444, -0.2213, -0.2229, -0.2050, -0.2101,
0:         -0.2046], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.16808339953422546; velocity_v: 0.19752603769302368; specific_humidity: 0.19340191781520844; velocity_z: 0.5769394040107727; temperature: 0.14460080862045288; total_precip: 0.9741147756576538; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16835308074951172; velocity_v: 0.207023486495018; specific_humidity: 0.18730628490447998; velocity_z: 0.7052887082099915; temperature: 0.30137574672698975; total_precip: 0.6775893568992615; 
0: epoch: 12 [1/5 (20%)]	Loss: 0.82585 : 0.33700 :: 0.21711 (2.72 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13583174347877502; velocity_v: 0.14494572579860687; specific_humidity: 0.17695607244968414; velocity_z: 0.46336132287979126; temperature: 0.12831883132457733; total_precip: 0.5458190441131592; 
0: epoch: 12 [2/5 (40%)]	Loss: 0.54582 : 0.23152 :: 0.20935 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14645715057849884; velocity_v: 0.1673513948917389; specific_humidity: 0.19215534627437592; velocity_z: 0.5965939164161682; temperature: 0.16377918422222137; total_precip: 0.6308404207229614; 
0: epoch: 12 [3/5 (60%)]	Loss: 0.63084 : 0.27863 :: 0.20864 (15.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16904747486114502; velocity_v: 0.18588022887706757; specific_humidity: 0.19702143967151642; velocity_z: 0.5529879331588745; temperature: 0.200025737285614; total_precip: 0.6181876063346863; 
0: epoch: 12 [4/5 (80%)]	Loss: 0.61819 : 0.28202 :: 0.21009 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 12 : 0.31096041202545166
0: validation loss for velocity_u : 0.11546516418457031
0: validation loss for velocity_v : 0.1217593252658844
0: validation loss for specific_humidity : 0.1724976897239685
0: validation loss for velocity_z : 0.5949953198432922
0: validation loss for temperature : 0.10502957552671432
0: validation loss for total_precip : 0.756014883518219
0: 13 : 20:17:23 :: batch_size = 96, lr = 1.5242895643428828e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 13, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.4490, -1.4113, -1.3724, -1.3332, -1.2933, -1.2530, -1.2147, -1.1793, -1.1461, -1.1146, -1.0861, -1.0601,
0:         -1.0359, -1.0137, -0.9938, -0.9753, -0.9573, -0.9407, -1.5691, -1.5334, -1.4969, -1.4595, -1.4195, -1.3775,
0:         -1.3367], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3159, -0.3367, -0.3604, -0.3874, -0.4173, -0.4501, -0.4875, -0.5287, -0.5715, -0.6170, -0.6656, -0.7157,
0:         -0.7672, -0.8227, -0.8819, -0.9447, -1.0124, -1.0866, -0.2444, -0.2619, -0.2829, -0.3078, -0.3352, -0.3652,
0:         -0.4001], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6337, -0.6330, -0.6327, -0.6329, -0.6334, -0.6350, -0.6367, -0.6390, -0.6416, -0.6443, -0.6471, -0.6494,
0:         -0.6518, -0.6534, -0.6548, -0.6555, -0.6558, -0.6559, -0.6547, -0.6530, -0.6513, -0.6498, -0.6491, -0.6485,
0:         -0.6485], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5471, 0.5634, 0.5318, 0.4795, 0.4446, 0.4195, 0.4032, 0.4086, 0.4141, 0.4054, 0.4152, 0.4544, 0.4806, 0.4882,
0:         0.5111, 0.5460, 0.5776, 0.6266, 0.6332, 0.6321, 0.6146, 0.5689, 0.5024, 0.4282, 0.3737], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2469, -0.2511, -0.2559, -0.2607, -0.2641, -0.2667, -0.2696, -0.2716, -0.2725, -0.2729, -0.2735, -0.2740,
0:         -0.2749, -0.2772, -0.2813, -0.2871, -0.2952, -0.3045, -0.3139, -0.3240, -0.3339, -0.3416, -0.3464, -0.3494,
0:         -0.3502], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1114, -0.0926, -0.0645, -0.0177,  0.0421,  0.0444, -0.0528, -0.1910, -0.1219, -0.0341,  0.0198, -0.0423,
0:         -0.1910, -0.1277, -0.1196, -0.1734, -0.1769, -0.1746, -0.0282, -0.0844, -0.0739, -0.0434, -0.1453, -0.2191,
0:         -0.2132], device='cuda:0')
0: [DEBUG] Epoch 13, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2004,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2273,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2437,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2039,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1441,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2402, -0.2367,
0:             nan,     nan,     nan, -0.2331,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1910,     nan,     nan,     nan, -0.2449,     nan,     nan, -0.1945,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2144,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0868,     nan,     nan, -0.0903, -0.1371, -0.2449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan, -0.2449,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2437,     nan,     nan,     nan,
0:         -0.2320,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1547,     nan,     nan,     nan,     nan, -0.1875,     nan,     nan, -0.2449,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 13, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1192, 0.2025, 0.3012, 0.3759, 0.4140, 0.4299, 0.4717, 0.5297, 0.6006, 0.6725, 0.7073, 0.7164, 0.7052, 0.7174,
0:         0.7680, 0.8457, 0.9255, 0.9652, 0.0868, 0.1460, 0.2312, 0.2824, 0.3040, 0.3035, 0.3248], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1496, -0.1275, -0.1322, -0.1640, -0.2130, -0.2631, -0.3260, -0.3719, -0.4159, -0.4631, -0.4875, -0.4951,
0:         -0.4982, -0.5236, -0.5508, -0.5435, -0.4927, -0.4114, -0.1410, -0.0799, -0.0650, -0.0815, -0.1313, -0.2027,
0:         -0.2613], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6250, -0.6184, -0.6168, -0.6043, -0.5982, -0.5920, -0.5990, -0.6068, -0.6185, -0.6365, -0.6492, -0.6570,
0:         -0.6583, -0.6576, -0.6672, -0.6733, -0.6860, -0.6871, -0.6690, -0.6606, -0.6535, -0.6374, -0.6306, -0.6282,
0:         -0.6297], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5086, 0.6020, 0.7320, 0.6819, 0.5217, 0.5761, 0.5995, 0.4594, 0.4807, 0.5023, 0.4605, 0.5483, 0.6054, 0.6240,
0:         0.5849, 0.5998, 0.7410, 0.6367, 0.4864, 0.4145, 0.4135, 0.4416, 0.4150, 0.4689, 0.5269], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0709,  0.0501,  0.0418,  0.0386,  0.0429,  0.0434,  0.0408,  0.0315,  0.0215,  0.0135,  0.0036, -0.0071,
0:         -0.0149, -0.0169, -0.0170, -0.0188, -0.0246, -0.0313, -0.0375, -0.0417, -0.0511, -0.0624, -0.0727, -0.0783,
0:         -0.0786], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2131, -0.2013, -0.2003, -0.1893, -0.1756, -0.1800, -0.1701, -0.1782, -0.1780, -0.2238, -0.2131, -0.2108,
0:         -0.1959, -0.1894, -0.1757, -0.1811, -0.1782, -0.1881, -0.2376, -0.2307, -0.2073, -0.2046, -0.1883, -0.1932,
0:         -0.1865], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15771669149398804; velocity_v: 0.17075222730636597; specific_humidity: 0.18693391978740692; velocity_z: 0.5468783378601074; temperature: 0.19114461541175842; total_precip: 1.108089566230774; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16779693961143494; velocity_v: 0.18272006511688232; specific_humidity: 0.19340410828590393; velocity_z: 0.5354709625244141; temperature: 0.14284028112888336; total_precip: 0.7581115961074829; 
0: epoch: 13 [1/5 (20%)]	Loss: 0.93310 : 0.32491 :: 0.21097 (2.69 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.19883987307548523; velocity_v: 0.2493591606616974; specific_humidity: 0.1703144758939743; velocity_z: 0.6997820138931274; temperature: 0.16792234778404236; total_precip: 0.7976107001304626; 
0: epoch: 13 [2/5 (40%)]	Loss: 0.79761 : 0.34205 :: 0.22180 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.164210706949234; velocity_v: 0.18530884385108948; specific_humidity: 0.14914309978485107; velocity_z: 0.48979610204696655; temperature: 0.12503916025161743; total_precip: 0.44906580448150635; 
0: epoch: 13 [3/5 (60%)]	Loss: 0.44907 : 0.22483 :: 0.21454 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18302087485790253; velocity_v: 0.21580064296722412; specific_humidity: 0.1541982740163803; velocity_z: 0.614189088344574; temperature: 0.1318225860595703; total_precip: 0.5357254147529602; 
0: epoch: 13 [4/5 (80%)]	Loss: 0.53573 : 0.26853 :: 0.22136 (15.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 13 : 0.26787546277046204
0: validation loss for velocity_u : 0.11702584475278854
0: validation loss for velocity_v : 0.1084633395075798
0: validation loss for specific_humidity : 0.16974122822284698
0: validation loss for velocity_z : 0.5426843762397766
0: validation loss for temperature : 0.11253353208303452
0: validation loss for total_precip : 0.556804358959198
0: 14 : 20:21:18 :: batch_size = 96, lr = 1.4871117700906175e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 14, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4577, -0.4547, -0.4520, -0.4502, -0.4495, -0.4499, -0.4502, -0.4502, -0.4500, -0.4500, -0.4502, -0.4504,
0:         -0.4497, -0.4474, -0.4435, -0.4387, -0.4345, -0.4317, -0.4279, -0.4254, -0.4237, -0.4244, -0.4269, -0.4295,
0:         -0.4305], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8620, 0.8806, 0.8991, 0.9175, 0.9348, 0.9492, 0.9584, 0.9609, 0.9563, 0.9463, 0.9325, 0.9175, 0.9029, 0.8889,
0:         0.8751, 0.8620, 0.8495, 0.8390, 0.8530, 0.8720, 0.8916, 0.9131, 0.9365, 0.9607, 0.9822], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3994, -0.4046, -0.4086, -0.4077, -0.4077, -0.4181, -0.4187, -0.4135, -0.3982, -0.3908, -0.3823, -0.3716,
0:         -0.3639, -0.3588, -0.3562, -0.3434, -0.3262, -0.3097, -0.3992, -0.4045, -0.4102, -0.4172, -0.4278, -0.4426,
0:         -0.4517], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1311, -0.1647, -0.2082, -0.2538, -0.2875, -0.3016, -0.2951, -0.2734, -0.2430, -0.2126, -0.1854, -0.1658,
0:         -0.1550, -0.1582, -0.1756, -0.2071, -0.2495, -0.2951, -0.1778, -0.2245, -0.2886, -0.3538, -0.4081, -0.4418,
0:         -0.4592], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.5620, -1.5606, -1.5621, -1.5650, -1.5670, -1.5657, -1.5606, -1.5537, -1.5463, -1.5405, -1.5355, -1.5312,
0:         -1.5260, -1.5194, -1.5116, -1.5018, -1.4912, -1.4794, -1.4664, -1.4532, -1.4400, -1.4296, -1.4222, -1.4207,
0:         -1.4250], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.0805, -0.0437, -0.1173, -0.1564, -0.1794, -0.2208, -0.2323, -0.2300, -0.2300,  0.2691,  0.1426,  0.0299,
0:         -0.0506, -0.0943, -0.1564, -0.2001, -0.2001, -0.2001,  0.2392,  0.1909,  0.1058,  0.0092, -0.0713, -0.1472,
0:         -0.1472], device='cuda:0')
0: [DEBUG] Epoch 14, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan, -0.2323,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1541,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2208,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1748,     nan,     nan, -0.1564,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1610,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1725,     nan,     nan,     nan,     nan,     nan, -0.2093,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2346,     nan,     nan,     nan, -0.1564,     nan,     nan, -0.2001,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1886,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2093,     nan,     nan, -0.2047, -0.2024,     nan,     nan,     nan, -0.2162,     nan,     nan,
0:             nan,     nan, -0.2093,     nan,     nan, -0.2208,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1575,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1863,     nan,     nan,     nan,     nan, -0.1552,     nan,     nan,     nan,     nan,
0:         -0.1989,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2392,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 14, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0511, 0.1265, 0.1994, 0.2424, 0.2546, 0.2422, 0.2562, 0.2783, 0.2963, 0.3067, 0.2770, 0.2016, 0.1199, 0.0777,
0:         0.0775, 0.1430, 0.2099, 0.2648, 0.0922, 0.1704, 0.2431, 0.2797, 0.2594, 0.2393, 0.2345], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.3612, 1.3409, 1.3002, 1.2646, 1.2411, 1.2356, 1.2422, 1.2474, 1.2259, 1.1815, 1.1204, 1.0610, 1.0198, 0.9908,
0:         0.9812, 1.0076, 1.0551, 1.1103, 1.3617, 1.3375, 1.2956, 1.2638, 1.2548, 1.2677, 1.2953], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6828, -0.6824, -0.6773, -0.6681, -0.6562, -0.6470, -0.6486, -0.6538, -0.6647, -0.6734, -0.6785, -0.6773,
0:         -0.6674, -0.6569, -0.6546, -0.6473, -0.6503, -0.6436, -0.6851, -0.6765, -0.6689, -0.6555, -0.6472, -0.6436,
0:         -0.6436], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2499, 0.4316, 0.6025, 0.5613, 0.3940, 0.3033, 0.2323, 0.1732, 0.1663, 0.1405, 0.1564, 0.2344, 0.2406, 0.2391,
0:         0.2553, 0.3312, 0.4336, 0.3063, 0.3236, 0.3105, 0.3817, 0.3950, 0.3558, 0.3404, 0.3136], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.5224, -1.5570, -1.5588, -1.5701, -1.5811, -1.6092, -1.6356, -1.6560, -1.6668, -1.6793, -1.7054, -1.7485,
0:         -1.7877, -1.7957, -1.7708, -1.7241, -1.6910, -1.6928, -1.7298, -1.7887, -1.8329, -1.8414, -1.8098, -1.7602,
0:         -1.7226], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2277, -0.2190, -0.2243, -0.2095, -0.1963, -0.1994, -0.1851, -0.1914, -0.1860, -0.2329, -0.2268, -0.2308,
0:         -0.2159, -0.2073, -0.1946, -0.1954, -0.1899, -0.1952, -0.2443, -0.2401, -0.2184, -0.2183, -0.2012, -0.2074,
0:         -0.2008], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1683676838874817; velocity_v: 0.18224114179611206; specific_humidity: 0.2128504365682602; velocity_z: 0.6018369793891907; temperature: 0.15739218890666962; total_precip: 1.139541745185852; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15363551676273346; velocity_v: 0.17134346067905426; specific_humidity: 0.18356242775917053; velocity_z: 0.5984904766082764; temperature: 0.14369550347328186; total_precip: 0.7269109487533569; 
0: epoch: 14 [1/5 (20%)]	Loss: 0.93323 : 0.33251 :: 0.21500 (2.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.157907634973526; velocity_v: 0.1733754575252533; specific_humidity: 0.18012620508670807; velocity_z: 0.4611837565898895; temperature: 0.1784915179014206; total_precip: 0.5157451033592224; 
0: epoch: 14 [2/5 (40%)]	Loss: 0.51575 : 0.24066 :: 0.21467 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16214919090270996; velocity_v: 0.2052644044160843; specific_humidity: 0.1841839849948883; velocity_z: 0.6493359804153442; temperature: 0.13684524595737457; total_precip: 0.8133068084716797; 
0: epoch: 14 [3/5 (60%)]	Loss: 0.81331 : 0.32226 :: 0.21270 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1715467870235443; velocity_v: 0.19007055461406708; specific_humidity: 0.1654791533946991; velocity_z: 0.6499413251876831; temperature: 0.13701847195625305; total_precip: 0.7303919196128845; 
0: epoch: 14 [4/5 (80%)]	Loss: 0.73039 : 0.30406 :: 0.21708 (15.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 14 : 0.2686018943786621
0: validation loss for velocity_u : 0.11860257387161255
0: validation loss for velocity_v : 0.1251218318939209
0: validation loss for specific_humidity : 0.1578352302312851
0: validation loss for velocity_z : 0.4949885308742523
0: validation loss for temperature : 0.15203852951526642
0: validation loss for total_precip : 0.5630249977111816
0: 15 : 20:25:23 :: batch_size = 96, lr = 1.4508407513079195e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 15, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8503, -0.8537, -0.8570, -0.8604, -0.8637, -0.8670, -0.8702, -0.8736, -0.8768, -0.8801, -0.8835, -0.8867,
0:         -0.8899, -0.8932, -0.8964, -0.8995, -0.9027, -0.9059, -0.7786, -0.7823, -0.7861, -0.7898, -0.7935, -0.7972,
0:         -0.8009], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9881, -0.9854, -0.9827, -0.9800, -0.9776, -0.9749, -0.9720, -0.9694, -0.9667, -0.9640, -0.9611, -0.9583,
0:         -0.9556, -0.9527, -0.9498, -0.9470, -0.9441, -0.9412, -1.0285, -1.0254, -1.0224, -1.0193, -1.0160, -1.0127,
0:         -1.0096], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6553, -0.6554, -0.6554, -0.6554, -0.6555, -0.6555, -0.6555, -0.6556, -0.6556, -0.6556, -0.6556, -0.6556,
0:         -0.6557, -0.6557, -0.6557, -0.6557, -0.6557, -0.6558, -0.6684, -0.6684, -0.6685, -0.6685, -0.6686, -0.6686,
0:         -0.6687], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0896, 0.0907, 0.0907, 0.0907, 0.0907, 0.0907, 0.0918, 0.0918, 0.0918, 0.0918, 0.0918, 0.0929, 0.0929, 0.0929,
0:         0.0929, 0.0929, 0.0940, 0.0940, 0.0677, 0.0677, 0.0687, 0.0687, 0.0687, 0.0687, 0.0687], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4308, -0.4308, -0.4309, -0.4310, -0.4310, -0.4311, -0.4311, -0.4311, -0.4312, -0.4312, -0.4313, -0.4314,
0:         -0.4314, -0.4315, -0.4315, -0.4316, -0.4316, -0.4317, -0.4317, -0.4317, -0.4318, -0.4318, -0.4319, -0.4319,
0:         -0.4320], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1814, -0.1814, -0.1814, -0.1814, -0.1814, -0.1814, -0.1814, -0.1814, -0.1826, -0.1732, -0.1732, -0.1744,
0:         -0.1744, -0.1744, -0.1756, -0.1756, -0.1756, -0.1756, -0.1545, -0.1545, -0.1545, -0.1557, -0.1557, -0.1557,
0:         -0.1569], device='cuda:0')
0: [DEBUG] Epoch 15, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.0261,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0576,     nan,
0:         -0.0553, -0.0553,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1055,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1335,     nan,     nan,     nan,     nan,     nan, -0.1592,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1744,     nan,     nan,
0:             nan,     nan, -0.1674,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1814,     nan,     nan,     nan, -0.1779,     nan,     nan,     nan,     nan,
0:          0.1047,     nan,     nan,     nan,     nan,     nan,     nan,  0.0639,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0027,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0342,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0611,
0:             nan,     nan,     nan,     nan, -0.1066,     nan,     nan,     nan, -0.0961,     nan,     nan,     nan,
0:         -0.1440,     nan,     nan,     nan,     nan,     nan,     nan, -0.1300,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1569,     nan,  0.1188,     nan,     nan,  0.1164,     nan,
0:          0.1188,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1141,
0:             nan,     nan,     nan,     nan,  0.0884,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.0020,     nan,     nan,     nan,  0.0066,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0681,     nan, -0.0658,     nan,     nan,     nan,     nan,     nan, -0.1148,     nan, -0.1125,
0:             nan, -0.1078,     nan])
0: [DEBUG] Epoch 15, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0792, -1.0240, -0.9675, -0.9489, -0.9822, -1.0316, -1.0359, -0.9830, -0.9045, -0.8351, -0.8251, -0.8927,
0:         -0.9837, -1.0413, -1.0406, -0.9700, -0.8940, -0.8329, -0.9545, -0.8977, -0.8528, -0.8643, -0.9469, -1.0147,
0:         -1.0356], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9264, -0.9009, -0.9344, -0.9829, -1.0556, -1.1400, -1.2506, -1.3361, -1.3916, -1.3971, -1.3385, -1.2617,
0:         -1.2046, -1.2125, -1.2696, -1.3055, -1.2627, -1.1478, -1.0708, -0.9998, -1.0008, -1.0341, -1.0984, -1.1882,
0:         -1.2775], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7145, -0.7131, -0.7084, -0.6958, -0.6807, -0.6667, -0.6651, -0.6625, -0.6634, -0.6693, -0.6732, -0.6740,
0:         -0.6674, -0.6611, -0.6641, -0.6666, -0.6836, -0.6956, -0.7111, -0.7001, -0.6893, -0.6715, -0.6625, -0.6575,
0:         -0.6551], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1989, -0.1536, -0.0201, -0.0798, -0.2235, -0.2195, -0.2054, -0.2450, -0.2548, -0.2946, -0.3296, -0.3067,
0:         -0.2895, -0.2855, -0.3858, -0.3773, -0.1852, -0.2425, -0.1806, -0.2656, -0.2202, -0.2612, -0.3166, -0.2285,
0:         -0.1599], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.8811, -0.9059, -0.8954, -0.8851, -0.8820, -0.9103, -0.9389, -0.9546, -0.9444, -0.9288, -0.9322, -0.9592,
0:         -0.9765, -0.9508, -0.8766, -0.7921, -0.7474, -0.7788, -0.8715, -0.9872, -1.0656, -1.0667, -0.9986, -0.9112,
0:         -0.8457], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2146, -0.2094, -0.2146, -0.1985, -0.1879, -0.1894, -0.1743, -0.1838, -0.1811, -0.2279, -0.2210, -0.2214,
0:         -0.2080, -0.1992, -0.1858, -0.1885, -0.1851, -0.1900, -0.2434, -0.2384, -0.2178, -0.2135, -0.1955, -0.2023,
0:         -0.1966], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.17490240931510925; velocity_v: 0.21910879015922546; specific_humidity: 0.17832940816879272; velocity_z: 0.6214110255241394; temperature: 0.17603524029254913; total_precip: 0.5003703832626343; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15284830331802368; velocity_v: 0.16540510952472687; specific_humidity: 0.17792333662509918; velocity_z: 0.4502430558204651; temperature: 0.14966978132724762; total_precip: 0.48732271790504456; 
0: epoch: 15 [1/5 (20%)]	Loss: 0.49385 : 0.25067 :: 0.20989 (2.56 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.20335449278354645; velocity_v: 0.23163896799087524; specific_humidity: 0.17442083358764648; velocity_z: 0.6776975393295288; temperature: 0.1736161857843399; total_precip: 1.0904897451400757; 
0: epoch: 15 [2/5 (40%)]	Loss: 1.09049 : 0.38675 :: 0.21832 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14340272545814514; velocity_v: 0.16775955259799957; specific_humidity: 0.18064181506633759; velocity_z: 0.5715013146400452; temperature: 0.15232358872890472; total_precip: 0.722052276134491; 
0: epoch: 15 [3/5 (60%)]	Loss: 0.72205 : 0.28572 :: 0.21232 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16273775696754456; velocity_v: 0.18464533984661102; specific_humidity: 0.207085520029068; velocity_z: 0.676375687122345; temperature: 0.13904233276844025; total_precip: 0.8920249938964844; 
0: epoch: 15 [4/5 (80%)]	Loss: 0.89202 : 0.33882 :: 0.21270 (15.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 15 : 0.28271743655204773
0: validation loss for velocity_u : 0.10644418001174927
0: validation loss for velocity_v : 0.11283957958221436
0: validation loss for specific_humidity : 0.17282409965991974
0: validation loss for velocity_z : 0.5298737287521362
0: validation loss for temperature : 0.12121081352233887
0: validation loss for total_precip : 0.6531124711036682
0: 16 : 20:29:24 :: batch_size = 96, lr = 1.4154543915199217e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 16, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8250, 0.8393, 0.8562, 0.8737, 0.8931, 0.9185, 0.9444, 0.9713, 1.0018, 1.0322, 1.0842, 1.1401, 1.1281, 1.0745,
0:         1.0548, 1.0540, 1.0422, 1.0240, 0.8385, 0.8480, 0.8611, 0.8783, 0.8962, 0.9206, 0.9466], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3109, 0.3101, 0.3140, 0.3174, 0.3184, 0.3216, 0.3249, 0.3227, 0.3136, 0.2964, 0.2723, 0.2466, 0.2255, 0.2197,
0:         0.2373, 0.2739, 0.3243, 0.3900, 0.3206, 0.3164, 0.3162, 0.3204, 0.3239, 0.3271, 0.3310], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5642, -0.5673, -0.5686, -0.5732, -0.5770, -0.5820, -0.5879, -0.5936, -0.5997, -0.5993, -0.5849, -0.5504,
0:         -0.5604, -0.5812, -0.5975, -0.6073, -0.6144, -0.6200, -0.5907, -0.5945, -0.5982, -0.6034, -0.6082, -0.6136,
0:         -0.6193], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 2.7092e-01,  3.9557e-01,  1.7879e-01,  6.4985e-02,  1.9180e-01,  2.5250e-01,  4.1399e-01,  6.5895e-01,
0:          9.5051e-01,  1.1532e+00,  1.0817e+00,  7.0664e-01, -2.7427e-01, -1.1349e+00, -9.9071e-01, -5.0947e-01,
0:         -2.9053e-01, -3.1654e-01, -1.0518e-01,  3.0301e-02,  1.4043e-02, -1.6913e-01, -1.1314e-03,  6.4985e-02,
0:          2.1022e-01], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.6063, 0.6500, 0.6904, 0.7161, 0.7383, 0.7644, 0.7944, 0.8368, 0.8837, 0.9968, 1.2251, 1.4128, 1.4081, 1.2720,
0:         1.1584, 1.1034, 1.0796, 1.0899, 1.1298, 1.1879, 1.2391, 1.2560, 1.2195, 1.1178, 0.9846], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396,
0:         -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396, -0.2396,
0:         -0.2396], device='cuda:0')
0: [DEBUG] Epoch 16, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2396,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2396,     nan, -0.2396,     nan, -0.2396, -0.2396,     nan,     nan,
0:             nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2396, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2396,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 16, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0362, 1.0371, 1.0504, 1.0706, 1.0896, 1.0858, 1.0856, 1.0692, 1.0402, 1.0305, 1.0349, 1.0537, 1.0987, 1.1572,
0:         1.2079, 1.2590, 1.2896, 1.3038, 1.0475, 1.0344, 1.0407, 1.0486, 1.0396, 1.0358, 1.0285], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2527, 0.3073, 0.3413, 0.3594, 0.3713, 0.3962, 0.4175, 0.4468, 0.4785, 0.5028, 0.5355, 0.5633, 0.5812, 0.5721,
0:         0.5586, 0.5709, 0.6242, 0.7090, 0.2389, 0.3082, 0.3441, 0.3606, 0.3664, 0.3743, 0.3933], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5981, -0.5879, -0.5984, -0.6025, -0.6134, -0.6123, -0.6223, -0.6221, -0.6240, -0.6329, -0.6437, -0.6504,
0:         -0.6511, -0.6404, -0.6311, -0.6134, -0.5904, -0.5689, -0.6320, -0.6268, -0.6248, -0.6262, -0.6326, -0.6389,
0:         -0.6393], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0265,  0.0447,  0.1976,  0.2492,  0.1718,  0.2909,  0.2457, -0.0024, -0.0155,  0.0130, -0.1341, -0.4949,
0:         -0.6047, -0.1405,  0.1620,  0.2869,  0.5251,  0.4294,  0.1484, -0.0304, -0.0027,  0.0515,  0.0278,  0.1799,
0:          0.1213], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.0387, 0.9899, 0.9625, 0.9733, 1.0176, 1.0645, 1.0866, 1.0671, 1.0325, 1.0088, 1.0002, 0.9927, 0.9605, 0.9045,
0:         0.8480, 0.8292, 0.8721, 0.9627, 1.0637, 1.1421, 1.1969, 1.2466, 1.3095, 1.3643, 1.3754], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2260, -0.2209, -0.2239, -0.2077, -0.2025, -0.2041, -0.1971, -0.2065, -0.2030, -0.2367, -0.2296, -0.2269,
0:         -0.2127, -0.2055, -0.1964, -0.2037, -0.1998, -0.2109, -0.2477, -0.2404, -0.2222, -0.2144, -0.2008, -0.2063,
0:         -0.2079], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1530572921037674; velocity_v: 0.18588986992835999; specific_humidity: 0.1759905070066452; velocity_z: 0.5947362184524536; temperature: 0.16473625600337982; total_precip: 0.5825380682945251; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15936677157878876; velocity_v: 0.2014407515525818; specific_humidity: 0.1888810694217682; velocity_z: 0.5844910144805908; temperature: 0.15703324973583221; total_precip: 0.7303735017776489; 
0: epoch: 16 [1/5 (20%)]	Loss: 0.65646 : 0.28482 :: 0.21901 (2.47 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18114160001277924; velocity_v: 0.2146679013967514; specific_humidity: 0.17679232358932495; velocity_z: 0.5971909761428833; temperature: 0.16028785705566406; total_precip: 0.8007817268371582; 
0: epoch: 16 [2/5 (40%)]	Loss: 0.80078 : 0.31706 :: 0.22021 (15.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13718418776988983; velocity_v: 0.13170768320560455; specific_humidity: 0.17292210459709167; velocity_z: 0.48928025364875793; temperature: 0.14938019216060638; total_precip: 0.5813354253768921; 
0: epoch: 16 [3/5 (60%)]	Loss: 0.58134 : 0.24098 :: 0.21206 (15.01 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17746590077877045; velocity_v: 0.1929551213979721; specific_humidity: 0.17276223003864288; velocity_z: 0.6246570348739624; temperature: 0.14398327469825745; total_precip: 0.5618124008178711; 
0: epoch: 16 [4/5 (80%)]	Loss: 0.56181 : 0.27452 :: 0.21720 (14.75 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 16 : 0.3112679123878479
0: validation loss for velocity_u : 0.10032821446657181
0: validation loss for velocity_v : 0.13193614780902863
0: validation loss for specific_humidity : 0.14160098135471344
0: validation loss for velocity_z : 0.6834291815757751
0: validation loss for temperature : 0.1069449782371521
0: validation loss for total_precip : 0.7033681869506836
0: 17 : 20:33:23 :: batch_size = 96, lr = 1.3809311136779726e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 17, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3480, 1.4066, 1.4540, 1.4902, 1.5162, 1.5331, 1.5410, 1.5396, 1.5301, 1.5148, 1.4955, 1.4748, 1.4566, 1.4456,
0:         1.4438, 1.4510, 1.4663, 1.4904, 1.4830, 1.5230, 1.5526, 1.5722, 1.5822, 1.5841, 1.5788], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7610, 0.8129, 0.8337, 0.8305, 0.8111, 0.7814, 0.7464, 0.7113, 0.6808, 0.6570, 0.6400, 0.6297, 0.6258, 0.6275,
0:         0.6339, 0.6450, 0.6620, 0.6844, 0.8232, 0.8398, 0.8289, 0.8006, 0.7636, 0.7234, 0.6846], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6398, -0.6107, -0.5795, -0.5613, -0.5467, -0.5430, -0.5444, -0.5427, -0.5407, -0.5381, -0.5348, -0.5321,
0:         -0.5287, -0.5279, -0.5248, -0.5249, -0.5226, -0.5232, -0.5995, -0.5755, -0.5505, -0.5399, -0.5397, -0.5396,
0:         -0.5388], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.0762, -1.1681, -1.2294, -1.1966, -1.0762, -0.8726, -0.5925, -0.2838, -0.0058,  0.2262,  0.3991,  0.4735,
0:          0.4144,  0.2743,  0.1364,  0.0248, -0.0868, -0.1788, -1.3323, -1.2491, -1.1769, -1.0324, -0.7982, -0.4940,
0:         -0.1503], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-1.0622, -1.0254, -0.9797, -0.9293, -0.8779, -0.8284, -0.7822, -0.7404, -0.7030, -0.6681, -0.6340, -0.6011,
0:         -0.5709, -0.5429, -0.5162, -0.4912, -0.4683, -0.4477, -0.4274, -0.4059, -0.3823, -0.3575, -0.3318, -0.3054,
0:         -0.2786], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2408, -0.2348, -0.2252, -0.2048, -0.1748, -0.1411, -0.1027, -0.0763, -0.0487, -0.2360, -0.2240, -0.2096,
0:         -0.1820, -0.1531, -0.1207, -0.0919, -0.0703, -0.0643, -0.2348, -0.2192, -0.2024, -0.1723, -0.1435, -0.1159,
0:         -0.1015], device='cuda:0')
0: [DEBUG] Epoch 17, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2492,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2504,     nan,     nan, -0.2480,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2456,     nan,     nan,     nan,     nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2456,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2468,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2504, -0.2504,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2504,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2480,     nan, -0.2456,     nan,     nan,     nan, -0.2456,
0:             nan,     nan,     nan,     nan,     nan, -0.2408,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2288,     nan,     nan, -0.2480,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2264,     nan,     nan,     nan,     nan, -0.2408, -0.2504,     nan, -0.2504, -0.2504,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2504,     nan,     nan,     nan,     nan,     nan, -0.2432,     nan,
0:         -0.2456,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2408,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1567,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2048,     nan,     nan, -0.2408, -0.2432,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2408])
0: [DEBUG] Epoch 17, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6175, 0.6281, 0.6679, 0.7016, 0.7261, 0.7165, 0.7058, 0.6751, 0.6390, 0.6136, 0.5775, 0.5487, 0.5167, 0.5023,
0:         0.4969, 0.4981, 0.5032, 0.4803, 0.6766, 0.6670, 0.6856, 0.7083, 0.7190, 0.7106, 0.6942], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0794, 1.0640, 1.0257, 0.9810, 0.9377, 0.9216, 0.8993, 0.8812, 0.8594, 0.8301, 0.8096, 0.8073, 0.8066, 0.7959,
0:         0.7814, 0.7932, 0.8493, 0.9396, 1.0663, 1.0635, 1.0329, 0.9980, 0.9653, 0.9421, 0.9348], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5214, -0.5324, -0.5428, -0.5414, -0.5357, -0.5272, -0.5300, -0.5296, -0.5353, -0.5460, -0.5539, -0.5628,
0:         -0.5670, -0.5721, -0.5842, -0.5925, -0.6044, -0.6086, -0.5387, -0.5422, -0.5472, -0.5418, -0.5382, -0.5367,
0:         -0.5366], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3415, 0.4061, 0.4929, 0.4028, 0.2869, 0.3246, 0.3201, 0.2522, 0.2116, 0.1145, 0.0862, 0.1518, 0.1417, 0.1644,
0:         0.1943, 0.2129, 0.2913, 0.2205, 0.3606, 0.2906, 0.2557, 0.1958, 0.1611, 0.2416, 0.2832], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4252, 0.4319, 0.4368, 0.4630, 0.5269, 0.6204, 0.7125, 0.7838, 0.8207, 0.8320, 0.8241, 0.8009, 0.7559, 0.6951,
0:         0.5982, 0.4865, 0.3886, 0.3488, 0.3844, 0.4761, 0.5628, 0.5897, 0.5510, 0.4871, 0.4488], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2162, -0.2133, -0.2216, -0.2065, -0.2022, -0.2039, -0.1944, -0.2006, -0.2013, -0.2298, -0.2230, -0.2218,
0:         -0.2133, -0.2057, -0.1979, -0.2014, -0.2007, -0.2073, -0.2393, -0.2329, -0.2200, -0.2100, -0.2030, -0.2054,
0:         -0.2064], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.17404024302959442; velocity_v: 0.17470839619636536; specific_humidity: 0.20898295938968658; velocity_z: 0.6424765586853027; temperature: 0.1719529628753662; total_precip: 0.9318852424621582; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1658955216407776; velocity_v: 0.1978132724761963; specific_humidity: 0.15499624609947205; velocity_z: 0.5046271681785583; temperature: 0.13719922304153442; total_precip: 0.48079827427864075; 
0: epoch: 17 [1/5 (20%)]	Loss: 0.70634 : 0.29184 :: 0.21031 (2.67 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17833003401756287; velocity_v: 0.22916178405284882; specific_humidity: 0.17592549324035645; velocity_z: 0.7727338671684265; temperature: 0.14805680513381958; total_precip: 0.987258791923523; 
0: epoch: 17 [2/5 (40%)]	Loss: 0.98726 : 0.37662 :: 0.22016 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1778150200843811; velocity_v: 0.18286721408367157; specific_humidity: 0.16091309487819672; velocity_z: 0.6655939817428589; temperature: 0.15342120826244354; total_precip: 0.9598903059959412; 
0: epoch: 17 [3/5 (60%)]	Loss: 0.95989 : 0.34595 :: 0.21771 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18316751718521118; velocity_v: 0.19362561404705048; specific_humidity: 0.1941060572862625; velocity_z: 0.6448810696601868; temperature: 0.19575780630111694; total_precip: 0.6311660408973694; 
0: epoch: 17 [4/5 (80%)]	Loss: 0.63117 : 0.30109 :: 0.21812 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 17 : 0.28903502225875854
0: validation loss for velocity_u : 0.1084323450922966
0: validation loss for velocity_v : 0.13400043547153473
0: validation loss for specific_humidity : 0.1592615693807602
0: validation loss for velocity_z : 0.5745691657066345
0: validation loss for temperature : 0.11510636657476425
0: validation loss for total_precip : 0.6428400874137878
0: 18 : 20:37:17 :: batch_size = 96, lr = 1.3472498670029002e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 18, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.7496, -1.7368, -1.7117, -1.6832, -1.6654, -1.6573, -1.6409, -1.5990, -1.5385, -1.4833, -1.4446, -1.4194,
0:         -1.4042, -1.3924, -1.3740, -1.3428, -1.3034, -1.2601, -1.6295, -1.6265, -1.6055, -1.5738, -1.5569, -1.5601,
0:         -1.5638], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3858, -0.3794, -0.3784, -0.3847, -0.3964, -0.4083, -0.4132, -0.4007, -0.3727, -0.3432, -0.3186, -0.3014,
0:         -0.2971, -0.3053, -0.3194, -0.3307, -0.3373, -0.3385, -0.3366, -0.3291, -0.3235, -0.3211, -0.3246, -0.3356,
0:         -0.3512], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3740, -0.3833, -0.3915, -0.3944, -0.3955, -0.3704, -0.3482, -0.3217, -0.3067, -0.2948, -0.2760, -0.2544,
0:         -0.2390, -0.2170, -0.2055, -0.2132, -0.2078, -0.2183, -0.3930, -0.3892, -0.3842, -0.3842, -0.3605, -0.3355,
0:         -0.2963], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1806, -0.1296, -0.0828, -0.0079,  0.0291,  0.3061,  0.6711,  0.4962,  0.0182, -0.2295, -0.2588, -0.3121,
0:         -0.3708, -0.2349, -0.1665, -0.3143, -0.4522, -0.5663, -0.2958, -0.1437, -0.2219, -0.4685, -0.5131, -0.0350,
0:          0.4821], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.8846, 1.8912, 1.8834, 1.8669, 1.8447, 1.8171, 1.7739, 1.7169, 1.6839, 1.6907, 1.7086, 1.7184, 1.7197, 1.7130,
0:         1.6978, 1.6917, 1.7059, 1.7455, 1.8100, 1.8770, 1.9406, 1.9770, 1.9604, 1.9454, 1.9705], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417,
0:         -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417, -0.2417,
0:         -0.2417], device='cuda:0')
0: [DEBUG] Epoch 18, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan, -0.2417, -0.2417,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan, -0.2417,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,
0:         -0.2417,     nan,     nan, -0.2417, -0.2417,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2417,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2417,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan, -0.2417,     nan,     nan,     nan,
0:             nan,     nan, -0.2417, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2417,     nan])
0: [DEBUG] Epoch 18, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2008, -1.1247, -1.0550, -1.0184, -1.0249, -1.0544, -1.0626, -1.0311, -0.9864, -0.9409, -0.9372, -0.9802,
0:         -1.0309, -1.0320, -0.9720, -0.8615, -0.7560, -0.6831, -1.1493, -1.0987, -1.0413, -1.0208, -1.0654, -1.1107,
0:         -1.1412], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2444,  0.2743,  0.2620,  0.2192,  0.1680,  0.1169,  0.0763,  0.0481,  0.0267, -0.0037, -0.0272, -0.0493,
0:         -0.0719, -0.1076, -0.1372, -0.1312, -0.0758,  0.0089,  0.1781,  0.2328,  0.2320,  0.1971,  0.1419,  0.0928,
0:          0.0617], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2550, -0.2693, -0.2919, -0.3088, -0.3276, -0.3474, -0.3730, -0.3920, -0.4103, -0.4316, -0.4489, -0.4557,
0:         -0.4521, -0.4484, -0.4473, -0.4467, -0.4528, -0.4561, -0.3114, -0.3247, -0.3385, -0.3510, -0.3673, -0.3907,
0:         -0.4101], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1042,  0.0100,  0.1954,  0.0728, -0.2088, -0.1771, -0.0284,  0.0154,  0.0115, -0.0293,  0.0468,  0.0929,
0:         -0.0403, -0.0973, -0.1469, -0.1169,  0.0957,  0.0646, -0.0739, -0.0873, -0.0313, -0.1218, -0.3195, -0.3281,
0:         -0.2786], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.4635, 0.4494, 0.4596, 0.4854, 0.5340, 0.5869, 0.6388, 0.6711, 0.6789, 0.6627, 0.6283, 0.5985, 0.5882, 0.6007,
0:         0.6216, 0.6429, 0.6569, 0.6821, 0.7277, 0.7864, 0.8412, 0.8749, 0.8864, 0.8820, 0.8725], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2079, -0.2023, -0.2102, -0.1977, -0.1965, -0.1986, -0.1940, -0.2008, -0.2019, -0.2226, -0.2127, -0.2131,
0:         -0.2048, -0.1999, -0.1973, -0.2019, -0.2021, -0.2096, -0.2317, -0.2259, -0.2154, -0.2065, -0.1991, -0.2038,
0:         -0.2038], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1513136476278305; velocity_v: 0.18327690660953522; specific_humidity: 0.1364072561264038; velocity_z: 0.6542024612426758; temperature: 0.1283467710018158; total_precip: 0.5534220337867737; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14190760254859924; velocity_v: 0.1628846675157547; specific_humidity: 0.15585392713546753; velocity_z: 0.4414210021495819; temperature: 0.13022470474243164; total_precip: 0.5799844861030579; 
0: epoch: 18 [1/5 (20%)]	Loss: 0.56670 : 0.24933 :: 0.20906 (2.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14500699937343597; velocity_v: 0.16228090226650238; specific_humidity: 0.17212721705436707; velocity_z: 0.4566084146499634; temperature: 0.13477124273777008; total_precip: 0.7227315306663513; 
0: epoch: 18 [2/5 (40%)]	Loss: 0.72273 : 0.26313 :: 0.20717 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13906706869602203; velocity_v: 0.14389926195144653; specific_humidity: 0.16742873191833496; velocity_z: 0.5802553296089172; temperature: 0.15107890963554382; total_precip: 0.9529948830604553; 
0: epoch: 18 [3/5 (60%)]	Loss: 0.95299 : 0.31998 :: 0.20877 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15134598314762115; velocity_v: 0.18901631236076355; specific_humidity: 0.1747864931821823; velocity_z: 0.6182805299758911; temperature: 0.15776783227920532; total_precip: 0.9121252298355103; 
0: epoch: 18 [4/5 (80%)]	Loss: 0.91213 : 0.32934 :: 0.21357 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 18 : 0.3080815374851227
0: validation loss for velocity_u : 0.10537814348936081
0: validation loss for velocity_v : 0.10813694447278976
0: validation loss for specific_humidity : 0.14345617592334747
0: validation loss for velocity_z : 0.48490110039711
0: validation loss for temperature : 0.11710824817419052
0: validation loss for total_precip : 0.8895090222358704
0: 19 : 20:41:11 :: batch_size = 96, lr = 1.3143901141491711e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 19, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.8095, 2.6592, 2.5226, 2.4070, 2.3154, 2.2458, 2.1941, 2.1561, 2.1277, 2.1061, 2.0907, 2.0848, 2.0927, 2.1166,
0:         2.1555, 2.2062, 2.2616, 2.3122, 2.5435, 2.4097, 2.2966, 2.2121, 2.1555, 2.1213, 2.1031], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6663,  0.6067,  0.5617,  0.5191,  0.4744,  0.4271,  0.3776,  0.3261,  0.2718,  0.2149,  0.1581,  0.1057,
0:          0.0624,  0.0296,  0.0073, -0.0040, -0.0012,  0.0177,  0.5475,  0.4960,  0.4612,  0.4298,  0.3961,  0.3609,
0:          0.3257], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4889, 0.4959, 0.5400, 0.5863, 0.6328, 0.6829, 0.7005, 0.7054, 0.7009, 0.7086, 0.7178, 0.7312, 0.7622, 0.7912,
0:         0.8413, 0.9155, 0.9506, 0.9724, 0.5798, 0.5990, 0.6468, 0.7033, 0.7549, 0.8080, 0.8464], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1397, 1.5620, 1.8473, 1.9977, 2.0640, 2.0352, 1.9070, 1.7367, 1.5908, 1.4647, 1.3431, 1.2436, 1.1728, 1.0822,
0:         0.9340, 0.7770, 0.6842, 0.6554, 1.1308, 1.3697, 1.5421, 1.6615, 1.7411, 1.7456, 1.6660], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([2.4542, 2.4448, 2.4460, 2.4507, 2.4538, 2.4525, 2.4466, 2.4372, 2.4261, 2.4131, 2.3977, 2.3799, 2.3609, 2.3418,
0:         2.3250, 2.3117, 2.3017, 2.2923, 2.2804, 2.2653, 2.2480, 2.2307, 2.2146, 2.2011, 2.1904], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([1.4408, 1.4053, 1.2788, 1.1734, 1.1734, 1.2277, 1.3043, 1.3154, 1.2410, 1.3653, 1.3032, 1.1800, 0.9592, 0.9647,
0:         0.8161, 0.7428, 0.8172, 0.8294, 1.9046, 1.7892, 1.4508, 1.1889, 0.9858, 0.7217, 0.5520], device='cuda:0')
0: [DEBUG] Epoch 19, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,  0.8005,  0.9292,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          1.2455,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.6529,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.0761,     nan,     nan,     nan,     nan,     nan,     nan, -0.2203,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1959,     nan,     nan,     nan, -0.1360,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.3300,  0.2168,     nan,  0.2324,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1893,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1893,     nan,     nan,     nan,     nan, -0.0628,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0717,     nan,     nan,  0.3145,     nan,     nan,
0:             nan,  0.1547,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1836,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.5142,     nan,     nan,     nan,     nan, -0.2048,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1782,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1937,     nan,     nan, -0.1959,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1582,     nan,     nan])
0: [DEBUG] Epoch 19, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.3341, 2.2492, 2.1656, 2.1049, 2.0740, 2.0475, 2.0524, 2.0348, 2.0082, 2.0061, 2.0052, 2.0675, 2.1350, 2.2281,
0:         2.3189, 2.3879, 2.4828, 2.5700, 2.3208, 2.2218, 2.1384, 2.0975, 2.0904, 2.1122, 2.1263], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8342, 0.8099, 0.7730, 0.7394, 0.7295, 0.7457, 0.7858, 0.8408, 0.8887, 0.9208, 0.9469, 0.9698, 1.0020, 1.0286,
0:         1.0709, 1.1273, 1.2005, 1.2763, 0.8948, 0.8831, 0.8541, 0.8289, 0.8244, 0.8580, 0.9243], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6634, -0.6600, -0.6577, -0.6524, -0.6491, -0.6490, -0.6598, -0.6684, -0.6787, -0.6864, -0.6888, -0.6864,
0:         -0.6791, -0.6727, -0.6736, -0.6790, -0.6874, -0.6935, -0.6734, -0.6732, -0.6698, -0.6623, -0.6584, -0.6602,
0:         -0.6647], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3996, 0.6108, 0.8283, 0.8295, 0.8078, 0.9252, 1.0069, 1.0888, 1.2936, 1.4732, 1.5777, 1.7414, 1.8646, 1.8496,
0:         1.8235, 1.9028, 1.9782, 1.8536, 0.4127, 0.4264, 0.4621, 0.4737, 0.5678, 0.7737, 0.9317], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.8253, -1.8341, -1.7985, -1.7051, -1.5655, -1.4005, -1.2398, -1.0989, -0.9787, -0.8745, -0.7818, -0.7095,
0:         -0.6836, -0.6806, -0.7137, -0.7463, -0.7506, -0.6879, -0.5602, -0.3970, -0.2385, -0.1345, -0.0749, -0.0341,
0:          0.0166], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.5442, 0.5382, 0.5445, 0.4995, 0.4670, 0.4258, 0.4038, 0.3757, 0.3747, 0.6198, 0.6220, 0.5927, 0.5766, 0.5134,
0:         0.4931, 0.4708, 0.4500, 0.4371, 0.6336, 0.6275, 0.6436, 0.6005, 0.5782, 0.5670, 0.5310], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.16799184679985046; velocity_v: 0.20868335664272308; specific_humidity: 0.16960830986499786; velocity_z: 0.6995053887367249; temperature: 0.1882610321044922; total_precip: 1.1042399406433105; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16560229659080505; velocity_v: 0.19670362770557404; specific_humidity: 0.1656264066696167; velocity_z: 0.7002108097076416; temperature: 0.1633049100637436; total_precip: 0.9021005630493164; 
0: epoch: 19 [1/5 (20%)]	Loss: 1.00317 : 0.36465 :: 0.21791 (2.54 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15697912871837616; velocity_v: 0.1892486810684204; specific_humidity: 0.19009988009929657; velocity_z: 0.7042505145072937; temperature: 0.22584709525108337; total_precip: 1.0649783611297607; 
0: epoch: 19 [2/5 (40%)]	Loss: 1.06498 : 0.38201 :: 0.21989 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15950796008110046; velocity_v: 0.17134596407413483; specific_humidity: 0.17276915907859802; velocity_z: 0.5446444749832153; temperature: 0.15703663229942322; total_precip: 0.7184067964553833; 
0: epoch: 19 [3/5 (60%)]	Loss: 0.71841 : 0.28349 :: 0.21568 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16258089244365692; velocity_v: 0.1869339942932129; specific_humidity: 0.15698504447937012; velocity_z: 0.6946021914482117; temperature: 0.16142764687538147; total_precip: 0.6949396729469299; 
0: epoch: 19 [4/5 (80%)]	Loss: 0.69494 : 0.30581 :: 0.21506 (15.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 19 : 0.2999805212020874
0: validation loss for velocity_u : 0.08718127757310867
0: validation loss for velocity_v : 0.10779271274805069
0: validation loss for specific_humidity : 0.1302962750196457
0: validation loss for velocity_z : 0.567736029624939
0: validation loss for temperature : 0.1251121163368225
0: validation loss for total_precip : 0.7817652225494385
0: 20 : 20:45:01 :: batch_size = 96, lr = 1.2823318186821183e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 20, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1048, -1.1350, -1.2530, -1.4264, -1.5748, -1.5868, -1.4770, -1.4665, -1.5173, -1.4593, -1.3962, -1.3986,
0:         -1.4554, -1.5633, -1.6872, -1.8042, -1.8975, -1.9543, -0.9994, -1.0262, -1.1407, -1.3066, -1.4561, -1.4928,
0:         -1.3920], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8499, -0.7945, -0.7181, -0.6419, -0.6716, -0.7642, -0.7474, -0.6450, -0.5664, -0.5144, -0.4365, -0.2921,
0:         -0.1433, -0.0409,  0.0174,  0.0422,  0.0403, -0.0159, -0.7127, -0.6722, -0.6210, -0.5257, -0.5173, -0.6344,
0:         -0.6642], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([3.5636, 3.5962, 3.7127, 3.8550, 3.9352, 3.9792, 4.1277, 4.2302, 4.4553, 4.7000, 4.7424, 4.7908, 4.8112, 4.7531,
0:         4.6618, 4.6174, 4.6824, 4.8322, 3.5800, 3.6024, 3.6671, 3.7882, 3.9054, 4.0049, 4.0832], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7144, -1.0634, -2.4629, -2.8243, -2.1330, -4.5347, -6.5110, -5.1935, -4.6963, -4.1980, -3.3069, -3.0690,
0:         -2.9769, -3.1745, -3.2194, -3.9017, -4.2305, -3.6537,  0.5785,  0.6144, -0.6594, -1.2946, -0.2576, -1.7144,
0:         -4.5044], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-2.1576, -2.1604, -1.8916, -1.7492, -1.4137, -1.0901, -0.8171, -0.2108,  0.6718,  1.4423,  1.7797,  1.7611,
0:          1.5594,  1.3472,  1.2070,  1.1469,  1.2359,  1.4228,  1.5881,  1.7094,  1.8640,  2.0118,  1.8545,  1.5610,
0:          1.4542], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([1.1484, 2.1725, 3.3440, 4.5590, 5.9889, 7.3995, 6.5131, 5.2643, 5.1870, 0.9383, 1.7812, 3.0276, 4.3899, 6.1025,
0:         7.5058, 8.0879, 7.5034, 6.9575, 1.0107, 1.3392, 2.5155, 3.9479, 5.6967, 7.8102, 9.2087], device='cuda:0')
0: [DEBUG] Epoch 20, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0796, 0.2728,    nan,    nan,
0:            nan,    nan,    nan, 0.3006,    nan, 1.1556, 1.1291,    nan,    nan, 0.3887,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.2595,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.8006,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.5626,    nan,    nan,    nan,    nan, 0.6110,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.2281,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:         1.2426,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.0083,    nan,
0:            nan,    nan,    nan,    nan, 0.1243,    nan,    nan, 0.7571,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 1.8851,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.5204,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan, 1.3489,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan, 1.4866,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.6871,    nan,    nan,    nan,
0:            nan, 1.1870,    nan,    nan,    nan,    nan,    nan,    nan, 2.1242,    nan, 1.3223,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan, 1.5928,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.3271,    nan,
0:            nan,    nan,    nan,    nan,    nan])
0: [DEBUG] Epoch 20, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8864, -0.7814, -0.6774, -0.6200, -0.5988, -0.5788, -0.5177, -0.4181, -0.3223, -0.2496, -0.2450, -0.2989,
0:         -0.3815, -0.4320, -0.4441, -0.4030, -0.3528, -0.2993, -0.8454, -0.7704, -0.6952, -0.6813, -0.7085, -0.7112,
0:         -0.6638], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2456, 0.3589, 0.4452, 0.4893, 0.4923, 0.4757, 0.4363, 0.4040, 0.3812, 0.3585, 0.3562, 0.3721, 0.4003, 0.4277,
0:         0.4601, 0.5110, 0.5745, 0.6376, 0.2310, 0.3681, 0.4672, 0.5146, 0.5127, 0.4810, 0.4484], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([4.8453, 4.8459, 4.8118, 4.7771, 4.7735, 4.7483, 4.7566, 4.7101, 4.6455, 4.5423, 4.4582, 4.3622, 4.3510, 4.3830,
0:         4.4177, 4.5483, 4.6967, 4.8479, 4.7633, 4.7698, 4.7527, 4.7163, 4.7049, 4.7008, 4.6848], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2003, -0.1649,  0.9235,  1.2055,  0.6223, -0.7950, -3.9747, -7.6474, -8.3851, -5.7871, -2.4724, -0.0425,
0:          1.0130,  0.6607, -0.1756, -0.6109, -0.8119, -1.1505, -0.5481,  0.1458,  0.6239,  0.4873,  0.1382, -0.2690,
0:         -1.7144], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.2871, 1.4428, 1.6030, 1.7428, 1.8570, 1.9431, 1.9965, 2.0305, 2.0598, 2.0856, 2.0874, 2.0383, 1.9064, 1.6801,
0:         1.3658, 1.0124, 0.6813, 0.4465, 0.3401, 0.3369, 0.3817, 0.4084, 0.3959, 0.3781, 0.3889], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([2.1632, 1.9411, 1.7389, 1.5264, 1.4607, 1.4137, 1.5047, 1.5116, 1.5861, 2.2588, 2.0201, 1.7177, 1.4577, 1.3711,
0:         1.3076, 1.2928, 1.3869, 1.4156, 2.4018, 2.1098, 1.8630, 1.5412, 1.3353, 1.2494, 1.2259], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.1406363844871521; velocity_v: 0.17685654759407043; specific_humidity: 0.17276249825954437; velocity_z: 0.6311955451965332; temperature: 0.15317435562610626; total_precip: 1.0408110618591309; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14980489015579224; velocity_v: 0.17708711326122284; specific_humidity: 0.19975855946540833; velocity_z: 0.5545053482055664; temperature: 0.17457853257656097; total_precip: 0.8822365403175354; 
0: epoch: 20 [1/5 (20%)]	Loss: 0.96152 : 0.33360 :: 0.21549 (2.78 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15941394865512848; velocity_v: 0.1850852370262146; specific_humidity: 0.16164806485176086; velocity_z: 0.6565757393836975; temperature: 0.15927846729755402; total_precip: 0.7542656660079956; 
0: epoch: 20 [2/5 (40%)]	Loss: 0.75427 : 0.30754 :: 0.22212 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14974583685398102; velocity_v: 0.17974455654621124; specific_humidity: 0.1838163584470749; velocity_z: 0.5178803205490112; temperature: 0.18575336039066315; total_precip: 0.7851471304893494; 
0: epoch: 20 [3/5 (60%)]	Loss: 0.78515 : 0.29577 :: 0.21365 (15.24 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14732904732227325; velocity_v: 0.16161152720451355; specific_humidity: 0.14431673288345337; velocity_z: 0.5379440784454346; temperature: 0.15420901775360107; total_precip: 0.4951327443122864; 
0: epoch: 20 [4/5 (80%)]	Loss: 0.49513 : 0.23742 :: 0.21395 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 20 : 0.2639929950237274
0: validation loss for velocity_u : 0.11599112302064896
0: validation loss for velocity_v : 0.12210821360349655
0: validation loss for specific_humidity : 0.14104458689689636
0: validation loss for velocity_z : 0.5227059125900269
0: validation loss for temperature : 0.11288473755121231
0: validation loss for total_precip : 0.5692232847213745
0: 21 : 20:48:50 :: batch_size = 96, lr = 1.2510554328606033e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 21, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2980, -0.2831, -0.2672, -0.2536, -0.2441, -0.2372, -0.2338, -0.2347, -0.2400, -0.2509, -0.2677, -0.2908,
0:         -0.3190, -0.3485, -0.3752, -0.3997, -0.4236, -0.4466, -0.3076, -0.2895, -0.2719, -0.2563, -0.2437, -0.2342,
0:         -0.2273], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4241, -0.4272, -0.4344, -0.4467, -0.4669, -0.4970, -0.5347, -0.5756, -0.6122, -0.6386, -0.6532, -0.6540,
0:         -0.6446, -0.6296, -0.6094, -0.5881, -0.5670, -0.5455, -0.4184, -0.4225, -0.4278, -0.4368, -0.4516, -0.4745,
0:         -0.5050], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3623, -0.3359, -0.2797, -0.2154, -0.1372, -0.0652, -0.0124, -0.0006,  0.0013, -0.0392, -0.1140, -0.1786,
0:         -0.2627, -0.3385, -0.3944, -0.4359, -0.4807, -0.5038, -0.3742, -0.3544, -0.3236, -0.2638, -0.1961, -0.1184,
0:         -0.0574], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1074, 0.0705, 0.1118, 0.1378, 0.1335, 0.1552, 0.1161, 0.1161, 0.1639, 0.1748, 0.2356, 0.2639, 0.2660, 0.3030,
0:         0.2747, 0.2682, 0.2291, 0.1139, 0.1422, 0.1139, 0.1031, 0.1183, 0.1183, 0.0944, 0.0857], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.7877, 0.7742, 0.7581, 0.7444, 0.7309, 0.7156, 0.6976, 0.6737, 0.6448, 0.6108, 0.5726, 0.5385, 0.5137, 0.5062,
0:         0.5239, 0.5691, 0.6373, 0.7180, 0.8046, 0.8926, 0.9737, 1.0445, 1.0993, 1.1337, 1.1541], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2417, -0.1952, -0.1294, -0.0920, -0.1986, -0.1612, -0.2122, -0.2395, -0.2395, -0.2417, -0.2417, -0.1997,
0:         -0.1362, -0.1147, -0.1839, -0.2270, -0.1918, -0.1861, -0.2417, -0.2417, -0.1929, -0.1328, -0.1657, -0.2292,
0:         -0.1373], device='cuda:0')
0: [DEBUG] Epoch 21, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2236,     nan,     nan,     nan, -0.2406,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2213, -0.2360,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1793,     nan,     nan,     nan, -0.2406,     nan, -0.2417,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2383,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2304,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1668,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2406,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2417,     nan,     nan,     nan,     nan, -0.2417,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2100,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1373,     nan,     nan,     nan,     nan,     nan, -0.2406,
0:             nan, -0.2417,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 21, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8794, -0.8024, -0.7108, -0.6490, -0.6257, -0.6208, -0.5774, -0.4875, -0.4008, -0.3337, -0.3389, -0.4259,
0:         -0.5285, -0.5769, -0.5546, -0.4607, -0.3768, -0.3346, -0.8162, -0.7455, -0.6607, -0.6130, -0.6396, -0.6564,
0:         -0.6380], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1195, -0.0741, -0.0632, -0.0734, -0.1084, -0.1657, -0.2404, -0.3031, -0.3414, -0.3479, -0.3206, -0.2775,
0:         -0.2456, -0.2521, -0.2730, -0.2561, -0.1789, -0.0674, -0.1919, -0.1275, -0.1068, -0.1201, -0.1579, -0.2258,
0:         -0.2954], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6650, -0.6626, -0.6623, -0.6510, -0.6377, -0.6293, -0.6259, -0.6270, -0.6312, -0.6361, -0.6401, -0.6397,
0:         -0.6327, -0.6264, -0.6229, -0.6231, -0.6306, -0.6347, -0.6680, -0.6677, -0.6620, -0.6487, -0.6369, -0.6315,
0:         -0.6271], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2116, 0.2493, 0.4522, 0.3867, 0.2761, 0.4742, 0.5131, 0.4164, 0.4143, 0.3205, 0.2481, 0.3153, 0.4000, 0.3923,
0:         0.2762, 0.3529, 0.5167, 0.3904, 0.2730, 0.2275, 0.3221, 0.2529, 0.1723, 0.3705, 0.4402], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([2.1938, 2.1415, 2.1266, 2.1622, 2.2433, 2.3321, 2.3978, 2.4132, 2.4026, 2.3905, 2.3853, 2.3891, 2.3833, 2.3459,
0:         2.2563, 2.1349, 2.0240, 1.9780, 2.0155, 2.1044, 2.1717, 2.1619, 2.0794, 1.9815, 1.9305], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2057, -0.2074, -0.2188, -0.2142, -0.2137, -0.2158, -0.2108, -0.2088, -0.2071, -0.2156, -0.2157, -0.2202,
0:         -0.2161, -0.2130, -0.2099, -0.2118, -0.2102, -0.2127, -0.2241, -0.2176, -0.2199, -0.2138, -0.2128, -0.2139,
0:         -0.2122], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.16528673470020294; velocity_v: 0.1844000220298767; specific_humidity: 0.14975757896900177; velocity_z: 0.5834378004074097; temperature: 0.1471727192401886; total_precip: 0.7329609990119934; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1676684021949768; velocity_v: 0.19905470311641693; specific_humidity: 0.14940319955348969; velocity_z: 0.594774067401886; temperature: 0.14709161221981049; total_precip: 0.6335092186927795; 
0: epoch: 21 [1/5 (20%)]	Loss: 0.68324 : 0.28455 :: 0.21059 (2.51 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15110859274864197; velocity_v: 0.18188254535198212; specific_humidity: 0.17979511618614197; velocity_z: 0.5944439172744751; temperature: 0.16348685324192047; total_precip: 0.5959889888763428; 
0: epoch: 21 [2/5 (40%)]	Loss: 0.59599 : 0.27364 :: 0.21514 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15336976945400238; velocity_v: 0.15642981231212616; specific_humidity: 0.151039257645607; velocity_z: 0.46474847197532654; temperature: 0.15236029028892517; total_precip: 0.6652775406837463; 
0: epoch: 21 [3/5 (60%)]	Loss: 0.66528 : 0.25462 :: 0.21823 (15.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15196913480758667; velocity_v: 0.1911509484052658; specific_humidity: 0.1626475304365158; velocity_z: 0.5784355401992798; temperature: 0.19267496466636658; total_precip: 0.6723850965499878; 
0: epoch: 21 [4/5 (80%)]	Loss: 0.67239 : 0.28721 :: 0.21454 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 21 : 0.23979221284389496
0: validation loss for velocity_u : 0.11418350785970688
0: validation loss for velocity_v : 0.12096869945526123
0: validation loss for specific_humidity : 0.13583141565322876
0: validation loss for velocity_z : 0.44595372676849365
0: validation loss for temperature : 0.11274576932191849
0: validation loss for total_precip : 0.5090705156326294
0: 22 : 20:52:48 :: batch_size = 96, lr = 1.2205418857176618e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 22, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9079, -0.9302, -0.9677, -1.0058, -1.0279, -1.0246, -0.9779, -0.9282, -0.9085, -0.8925, -0.8655, -0.8310,
0:         -0.8157, -0.8432, -0.8765, -0.8996, -0.9295, -0.9761, -0.8888, -0.9146, -0.9562, -1.0073, -1.0382, -1.0467,
0:         -1.0037], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2411, -0.2650, -0.2810, -0.2842, -0.2680, -0.2486, -0.2219, -0.1920, -0.1841, -0.1837, -0.1925, -0.2249,
0:         -0.2665, -0.3017, -0.3303, -0.3529, -0.3645, -0.3760, -0.2514, -0.2750, -0.2953, -0.3111, -0.3137, -0.3066,
0:         -0.2753], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.8006, 1.6478, 1.5066, 1.4806, 1.5798, 1.8464, 2.1292, 2.3048, 1.9556, 1.8400, 1.8356, 1.8845, 1.9173, 1.8707,
0:         1.8652, 1.6831, 1.5362, 1.3630, 1.8905, 1.8175, 1.6768, 1.5658, 1.6313, 1.8343, 2.1454], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.7687,  0.5857,  0.9068,  0.7474,  1.1696,  0.2196, -1.3100, -0.0500,  0.5172,  0.4936,  0.7451,  0.5565,
0:          0.2634, -0.1331, -0.3026, -0.6160, -0.8462, -0.1196,  0.7833,  0.5261,  0.9259,  0.6272,  1.0629,  0.6025,
0:         -0.6889], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.3384, -0.3221, -0.3354, -0.4268, -0.5402, -0.7963, -1.0555, -0.9038, -0.6302, -0.6441, -0.7058, -0.7228,
0:         -0.6901, -0.6384, -0.6511, -0.6100, -0.4862, -0.2568, -0.0227,  0.0812,  0.1403,  0.0362, -0.0889,  0.0124,
0:          0.0815], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2072, -0.2119, -0.2212, -0.2259, -0.1850, -0.0544,  0.1018,  0.2243,  1.2493, -0.1827, -0.2107, -0.2282,
0:         -0.2282, -0.1780, -0.0159,  0.1986,  0.1590,  0.2709, -0.1920, -0.2165, -0.2387, -0.2410, -0.2282, -0.1267,
0:          0.0225], device='cuda:0')
0: [DEBUG] Epoch 22, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2212,
0:         -0.2072, -0.2270,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.0575,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2527,     nan,
0:         -0.1466,     nan,     nan,     nan,     nan,     nan, -0.2562,     nan,     nan,     nan,     nan, -0.2562,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  1.8545,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2538,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2468, -0.2398,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2352,     nan,     nan,     nan,
0:         -0.2562,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0125,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2562,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2562,     nan,     nan, -0.2562,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2562,     nan,     nan,     nan, -0.1221,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2084,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2562,     nan,     nan, -0.2562,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 22, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.2387, -1.1733, -1.1258, -1.1272, -1.1579, -1.2010, -1.2081, -1.1765, -1.1352, -1.1120, -1.1315, -1.1934,
0:         -1.2572, -1.2566, -1.1900, -1.0747, -0.9890, -0.9401, -1.1979, -1.1421, -1.1033, -1.1257, -1.2145, -1.2869,
0:         -1.3307], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1434,  0.1698,  0.1327,  0.0467, -0.0701, -0.1721, -0.2552, -0.2873, -0.2790, -0.2569, -0.2129, -0.1696,
0:         -0.1551, -0.1750, -0.1982, -0.1912, -0.1142,  0.0132,  0.0528,  0.1164,  0.1119,  0.0467, -0.0590, -0.1661,
0:         -0.2491], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.7028, 1.6767, 1.6316, 1.5989, 1.5618, 1.5446, 1.5400, 1.5494, 1.5532, 1.5475, 1.5327, 1.4864, 1.4325, 1.3760,
0:         1.3098, 1.2650, 1.2298, 1.1941, 1.6172, 1.5911, 1.5588, 1.5366, 1.5130, 1.4997, 1.5142], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 4.4073e-01,  6.3065e-01,  6.6248e-01,  3.8274e-01, -8.4613e-02, -2.2329e-01, -4.6141e-01, -8.2721e-01,
0:         -9.2555e-01, -9.5898e-01, -9.7270e-01, -8.7707e-01, -7.4105e-01, -9.0448e-01, -1.3174e+00, -1.2918e+00,
0:         -8.6901e-01, -6.3556e-01,  6.9663e-01,  8.6255e-01,  7.7796e-01,  4.4038e-01,  3.3300e-02,  9.6232e-04,
0:         -2.2876e-01], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.0280, 0.9855, 0.9658, 0.9738, 1.0117, 1.0606, 1.1000, 1.1280, 1.1587, 1.2072, 1.2771, 1.3782, 1.5006, 1.6283,
0:         1.7268, 1.7931, 1.8322, 1.8824, 1.9675, 2.0889, 2.2076, 2.2881, 2.3116, 2.2970, 2.2762], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.4933, 0.4833, 0.5057, 0.4857, 0.4647, 0.4948, 0.5563, 0.5726, 0.6463, 0.4245, 0.3945, 0.3588, 0.3560, 0.3598,
0:         0.3878, 0.4462, 0.5348, 0.6086, 0.3064, 0.2559, 0.2616, 0.2072, 0.2298, 0.3100, 0.3677], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.1789880245923996; velocity_v: 0.21010078489780426; specific_humidity: 0.16841909289360046; velocity_z: 0.6737169027328491; temperature: 0.1478857398033142; total_precip: 0.9736894369125366; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14410223066806793; velocity_v: 0.160857155919075; specific_humidity: 0.17478595674037933; velocity_z: 0.6090184450149536; temperature: 0.15787865221500397; total_precip: 1.1082756519317627; 
0: epoch: 22 [1/5 (20%)]	Loss: 1.04098 : 0.35474 :: 0.21729 (2.82 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1467328816652298; velocity_v: 0.1815820187330246; specific_humidity: 0.14689302444458008; velocity_z: 0.5273334980010986; temperature: 0.13522884249687195; total_precip: 0.6381683349609375; 
0: epoch: 22 [2/5 (40%)]	Loss: 0.63817 : 0.26049 :: 0.21430 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16502657532691956; velocity_v: 0.2106316238641739; specific_humidity: 0.15705272555351257; velocity_z: 0.636928141117096; temperature: 0.1863604187965393; total_precip: 0.6315449476242065; 
0: epoch: 22 [3/5 (60%)]	Loss: 0.63154 : 0.29362 :: 0.21060 (15.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17053614556789398; velocity_v: 0.18276028335094452; specific_humidity: 0.1543804556131363; velocity_z: 0.5101940631866455; temperature: 0.1593777984380722; total_precip: 0.6281878352165222; 
0: epoch: 22 [4/5 (80%)]	Loss: 0.62819 : 0.26480 :: 0.21562 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 22 : 0.2585412561893463
0: validation loss for velocity_u : 0.12434512376785278
0: validation loss for velocity_v : 0.13283871114253998
0: validation loss for specific_humidity : 0.13011689484119415
0: validation loss for velocity_z : 0.5659756660461426
0: validation loss for temperature : 0.10071852058172226
0: validation loss for total_precip : 0.4972529113292694
0: 23 : 20:56:38 :: batch_size = 96, lr = 1.1907725714318652e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 23, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.4314, -1.3477, -1.4159, -1.5226, -1.5272, -1.4730, -1.4169, -1.3739, -1.3452, -1.3064, -1.2725, -1.2467,
0:         -1.2346, -1.2183, -1.2005, -1.2124, -1.2316, -1.2485, -1.4757, -1.3952, -1.4620, -1.5945, -1.6364, -1.5834,
0:         -1.4847], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1630, 0.1109, 0.2101, 0.3311, 0.3859, 0.4026, 0.4090, 0.4177, 0.4299, 0.4200, 0.3784, 0.3206, 0.2834, 0.2890,
0:         0.3125, 0.3413, 0.3811, 0.4528, 0.1888, 0.0828, 0.1471, 0.3057, 0.4268, 0.4764, 0.4807], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.2989, 1.6793, 1.5751, 1.1596, 1.1139, 1.1586, 1.3514, 1.3622, 1.3474, 1.2626, 1.2354, 1.1594, 1.0338, 0.8719,
0:         0.7836, 0.7613, 0.7328, 0.6250, 1.1833, 1.5512, 1.4103, 0.8675, 0.6588, 0.7239, 1.0573], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5793,  0.4408, -0.0575, -0.1304, -0.2628, -0.3425, -0.2853, -0.5041, -0.5647, -0.6579, -0.6410, -0.5760,
0:         -0.4491, -0.1977,  0.0593,  0.1131, -0.2774, -0.4200, -1.6938, -0.5962, -0.6265,  0.0581,  0.1805, -0.4143,
0:         -0.1641], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.3342,  0.3490,  0.2515,  0.1659,  0.0681, -0.0742, -0.2694, -0.4793, -0.6258, -0.7119, -0.7550, -0.7364,
0:         -0.6810, -0.5930, -0.4948, -0.4288, -0.3946, -0.3203, -0.2197, -0.1595, -0.1832, -0.3238, -0.4641, -0.5511,
0:         -0.6165], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.7317,  0.5180,  0.9986,  0.4950,  0.2740, -0.2574, -0.2562, -0.2562, -0.2562,  6.5566,  6.0602,  0.9793,
0:          0.1327, -0.0714, -0.2211, -0.2332, -0.2525, -0.2550,  9.3307, 12.4261,  5.4998,  1.0627,  0.5953, -0.0943,
0:         -0.2332], device='cuda:0')
0: [DEBUG] Epoch 23, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  8.2933,     nan,  0.3416,     nan,     nan,     nan,     nan,     nan,     nan, 10.9285,
0:             nan,  0.9093,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  4.4358,  0.5276,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  3.3416,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  3.7836,
0:             nan,     nan,  1.8875,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1849,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,  2.4503,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  2.8827,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  5.5759,
0:             nan,  2.8006,     nan,     nan,     nan,     nan,  1.3006, -0.1366, -0.1052,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0871,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.3767,     nan,     nan,     nan,     nan, -0.0315,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  2.2353,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  2.7076,  0.6327,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 23, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.3008, -1.2281, -1.1818, -1.1792, -1.2159, -1.2517, -1.2347, -1.1616, -1.0841, -1.0289, -1.0412, -1.1168,
0:         -1.1894, -1.2001, -1.1225, -0.9856, -0.8650, -0.7959, -1.2456, -1.1770, -1.1354, -1.1634, -1.2545, -1.3205,
0:         -1.3404], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6432, 0.5896, 0.5433, 0.5224, 0.5275, 0.5518, 0.5751, 0.6010, 0.6182, 0.6291, 0.6422, 0.6564, 0.6690, 0.6591,
0:         0.6374, 0.6206, 0.6182, 0.6149, 0.6376, 0.6095, 0.5903, 0.5981, 0.6203, 0.6546, 0.6806], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2947, 1.3068, 1.3254, 1.3440, 1.3727, 1.4383, 1.5524, 1.6899, 1.8446, 2.0039, 2.1441, 2.2655, 2.3706, 2.4758,
0:         2.5609, 2.6555, 2.7324, 2.7755, 1.3421, 1.3662, 1.3802, 1.4076, 1.4354, 1.5002, 1.6034], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2959,  0.6046,  0.6629,  0.6217,  0.0592,  0.1825,  0.6675,  0.1186,  0.1044,  0.4867,  0.2934,  0.0477,
0:         -0.7304, -1.0306, -0.4965, -0.4008, -0.2535, -0.0849,  0.2737,  0.6053,  0.4148,  0.2648, -0.1372,  0.0372,
0:          0.1894], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.2201,  0.2697,  0.2834,  0.2603,  0.2253,  0.1935,  0.1471,  0.0692, -0.0413, -0.1796, -0.3227, -0.4450,
0:         -0.5375, -0.6052, -0.6549, -0.6984, -0.7264, -0.7174, -0.6480, -0.5135, -0.3267, -0.1303,  0.0324,  0.1357,
0:          0.1709], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([1.1284, 1.1144, 1.2016, 1.2320, 1.3142, 1.4372, 1.5502, 1.5871, 1.7049, 1.2907, 1.2616, 1.2112, 1.3201, 1.3572,
0:         1.4479, 1.5848, 1.6913, 1.7881, 1.5437, 1.4502, 1.4643, 1.4106, 1.4880, 1.5925, 1.7018], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.15329602360725403; velocity_v: 0.17662112414836884; specific_humidity: 0.18024371564388275; velocity_z: 0.6476279497146606; temperature: 0.16592705249786377; total_precip: 0.8495932221412659; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1784227192401886; velocity_v: 0.1739431470632553; specific_humidity: 0.18020758032798767; velocity_z: 0.6962210536003113; temperature: 0.1787337064743042; total_precip: 0.9873319268226624; 
0: epoch: 23 [1/5 (20%)]	Loss: 0.91846 : 0.34213 :: 0.22412 (2.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16078166663646698; velocity_v: 0.1873808652162552; specific_humidity: 0.15017588436603546; velocity_z: 0.5061168074607849; temperature: 0.152035191655159; total_precip: 0.393465131521225; 
0: epoch: 23 [2/5 (40%)]	Loss: 0.39347 : 0.22124 :: 0.22307 (15.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14878933131694794; velocity_v: 0.14859816431999207; specific_humidity: 0.16309243440628052; velocity_z: 0.49321407079696655; temperature: 0.17578130960464478; total_precip: 0.7442931532859802; 
0: epoch: 23 [3/5 (60%)]	Loss: 0.74429 : 0.27568 :: 0.20975 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.13898274302482605; velocity_v: 0.15837571024894714; specific_humidity: 0.168141707777977; velocity_z: 0.5902376770973206; temperature: 0.14471083879470825; total_precip: 0.9565397500991821; 
0: epoch: 23 [4/5 (80%)]	Loss: 0.95654 : 0.32346 :: 0.21397 (15.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 23 : 0.2809627652168274
0: validation loss for velocity_u : 0.10930800437927246
0: validation loss for velocity_v : 0.1290838122367859
0: validation loss for specific_humidity : 0.13587181270122528
0: validation loss for velocity_z : 0.5475022792816162
0: validation loss for temperature : 0.11344962567090988
0: validation loss for total_precip : 0.6505613327026367
0: 24 : 21:00:39 :: batch_size = 96, lr = 1.1617293379823076e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 24, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.2199, -1.2302, -1.2332, -1.2333, -1.2271, -1.2171, -1.1978, -1.1811, -1.1680, -1.1464, -1.1225, -1.0832,
0:         -1.0695, -1.0832, -1.0620, -1.0328, -1.0062, -0.9682, -1.2005, -1.2022, -1.1981, -1.1963, -1.1906, -1.1810,
0:         -1.1652], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1092, 0.1040, 0.0943, 0.0823, 0.0705, 0.0628, 0.0539, 0.0431, 0.0332, 0.0268, 0.0326, 0.0321, 0.0309, 0.0500,
0:         0.0748, 0.1028, 0.1276, 0.1131, 0.1084, 0.0984, 0.0862, 0.0767, 0.0717, 0.0700, 0.0669], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.0094, 0.8777, 0.8300, 0.8212, 0.8476, 0.8885, 0.9992, 1.0726, 1.1716, 1.1998, 1.3487, 1.3443, 1.2709, 1.3257,
0:         1.0625, 1.0188, 1.0116, 0.8379, 1.1162, 1.0691, 1.0204, 1.0196, 1.0621, 1.1667, 1.3093], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([  0.0977,   0.0798,  -0.0365,   0.2431,   0.1581,   0.6636,   0.2386,   0.6435,   0.4914,  -0.1125,   0.0217,
0:          -7.5000, -10.4613,  -3.0313,  -0.0454,  -0.9244,  -1.3091,  -0.4212,   0.3505,   0.2051,   0.1357,   0.5160,
0:           0.2901,   0.7061,   0.4444], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.5491, -0.4333, -0.3790, -0.4237, -0.5537, -0.7214, -0.8312, -0.8805, -0.8727, -0.8073, -0.7321, -0.7134,
0:         -1.0383, -1.3770, -1.2456, -1.0166, -0.9222, -0.9416, -1.0251, -1.1386, -1.1290, -0.9472, -0.7564, -0.6280,
0:         -0.5458], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1615, -0.0389,  0.0825,  0.1678,  0.1071, -0.0296, -0.0716, -0.0074,  0.0592, -0.1639,  0.0288,  0.1374,
0:          0.0709,  0.0078, -0.1020, -0.0704,  0.0569,  0.0113, -0.0739,  0.1549,  0.2285,  0.1036, -0.0307, -0.0821,
0:          0.0078], device='cuda:0')
0: [DEBUG] Epoch 24, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,  0.2274,     nan, -0.2363,     nan,     nan,     nan, -0.2409,  0.5707,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  2.4998,     nan,     nan,     nan,     nan, -0.2164,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2374,     nan,     nan, -0.2409,     nan,     nan, -0.2409,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1919,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.3208,     nan,     nan,     nan,     nan,     nan,     nan, -0.2327,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2409,     nan,     nan, -0.2386,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2409,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2339,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.6372])
0: [DEBUG] Epoch 24, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.0816, -0.9926, -0.9177, -0.8883, -0.9022, -0.9494, -0.9727, -0.9622, -0.9424, -0.9237, -0.9483, -1.0256,
0:         -1.1125, -1.1553, -1.1265, -1.0230, -0.9068, -0.7916, -1.0197, -0.9250, -0.8456, -0.8251, -0.8893, -0.9571,
0:         -1.0141], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3909, 0.4448, 0.4487, 0.4281, 0.4016, 0.3912, 0.3874, 0.3838, 0.3656, 0.3304, 0.2944, 0.2741, 0.2660, 0.2544,
0:         0.2480, 0.2717, 0.3294, 0.4053, 0.3677, 0.4494, 0.4728, 0.4522, 0.4257, 0.4103, 0.4087], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.3154, 2.2311, 2.1225, 2.0192, 1.9139, 1.8149, 1.7295, 1.6375, 1.5509, 1.4640, 1.3974, 1.3438, 1.3127, 1.3041,
0:         1.2836, 1.2721, 1.2517, 1.2376, 2.3456, 2.2377, 2.1176, 1.9903, 1.8672, 1.7672, 1.6937], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2106, -0.4165, -0.6257, -0.9926, -1.0698, -0.9103, -0.8733, -0.5331, -0.1614, -0.0081,  0.1180,  0.1371,
0:          0.2245,  0.3352,  0.2804,  0.4075,  0.5611,  0.3150, -0.2068, -0.3194, -0.4095, -0.7448, -0.7658, -0.6959,
0:         -0.8061], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1791, -0.1885, -0.1381, -0.0453,  0.0676,  0.1750,  0.2739,  0.3636,  0.4337,  0.4658,  0.4488,  0.3894,
0:          0.3137,  0.2547,  0.2128,  0.1811,  0.1403,  0.1047,  0.0860,  0.0753,  0.0639,  0.0169, -0.0627, -0.1469,
0:         -0.1979], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([1.8546, 1.9068, 2.0177, 2.0756, 2.1693, 2.2496, 2.3767, 2.4004, 2.4319, 1.8102, 1.8234, 1.7893, 1.8677, 1.9434,
0:         2.0540, 2.0836, 2.1549, 2.1717, 1.7429, 1.6752, 1.6859, 1.6415, 1.6415, 1.7355, 1.7350], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.15204960107803345; velocity_v: 0.18737322092056274; specific_humidity: 0.16054299473762512; velocity_z: 0.6515065431594849; temperature: 0.16603423655033112; total_precip: 0.9636842012405396; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1402369886636734; velocity_v: 0.15127654373645782; specific_humidity: 0.17372450232505798; velocity_z: 0.5770679712295532; temperature: 0.16144372522830963; total_precip: 0.8109717965126038; 
0: epoch: 24 [1/5 (20%)]	Loss: 0.88733 : 0.32051 :: 0.21265 (2.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14800356328487396; velocity_v: 0.17651230096817017; specific_humidity: 0.14374162256717682; velocity_z: 0.6825467944145203; temperature: 0.14410747587680817; total_precip: 0.4658198654651642; 
0: epoch: 24 [2/5 (40%)]	Loss: 0.46582 : 0.25708 :: 0.21284 (15.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1494031846523285; velocity_v: 0.16895273327827454; specific_humidity: 0.1316714584827423; velocity_z: 0.4491947591304779; temperature: 0.16288819909095764; total_precip: 0.5036064982414246; 
0: epoch: 24 [3/5 (60%)]	Loss: 0.50361 : 0.22552 :: 0.21817 (15.21 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1576797068119049; velocity_v: 0.18917131423950195; specific_humidity: 0.15958760678768158; velocity_z: 0.6051677465438843; temperature: 0.1671474725008011; total_precip: 0.6443012952804565; 
0: epoch: 24 [4/5 (80%)]	Loss: 0.64430 : 0.28320 :: 0.21588 (15.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 24 : 0.2618445158004761
0: validation loss for velocity_u : 0.09184926748275757
0: validation loss for velocity_v : 0.10677774995565414
0: validation loss for specific_humidity : 0.13862180709838867
0: validation loss for velocity_z : 0.503931999206543
0: validation loss for temperature : 0.10792744904756546
0: validation loss for total_precip : 0.6219586133956909
0: 25 : 21:04:42 :: batch_size = 96, lr = 1.1333944760803001e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 25, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8955, -0.8699, -0.8428, -0.8142, -0.7845, -0.7540, -0.7231, -0.6916, -0.6601, -0.6285, -0.5974, -0.5670,
0:         -0.5378, -0.5096, -0.4824, -0.4559, -0.4300, -0.4046, -0.7920, -0.7742, -0.7547, -0.7336, -0.7110, -0.6870,
0:         -0.6621], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1876, -1.1865, -1.1845, -1.1824, -1.1798, -1.1777, -1.1767, -1.1773, -1.1796, -1.1833, -1.1876, -1.1917,
0:         -1.1944, -1.1950, -1.1937, -1.1904, -1.1855, -1.1791, -1.2026, -1.2044, -1.2040, -1.2011, -1.1958, -1.1892,
0:         -1.1822], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5453, -0.5454, -0.5466, -0.5479, -0.5485, -0.5490, -0.5485, -0.5472, -0.5468, -0.5465, -0.5462, -0.5457,
0:         -0.5450, -0.5449, -0.5448, -0.5447, -0.5450, -0.5454, -0.5519, -0.5537, -0.5553, -0.5565, -0.5570, -0.5582,
0:         -0.5595], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1337, -0.2269, -0.3235, -0.4234, -0.5267, -0.6312, -0.7322, -0.8221, -0.8973, -0.9557, -0.9995, -1.0354,
0:         -1.0646, -1.0927, -1.1163, -1.1331, -1.1432, -1.1455, -0.0854, -0.1236, -0.1764, -0.2471, -0.3291, -0.4156,
0:         -0.4975], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.3118, 1.3073, 1.3050, 1.3049, 1.3064, 1.3092, 1.3133, 1.3186, 1.3243, 1.3296, 1.3339, 1.3361, 1.3360, 1.3333,
0:         1.3281, 1.3211, 1.3125, 1.3032, 1.2934, 1.2835, 1.2737, 1.2640, 1.2544, 1.2445, 1.2346], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([ 0.1156,  0.1168,  0.1143,  0.1119,  0.1131,  0.1143,  0.1181,  0.1181,  0.1193,  0.0521,  0.0496,  0.0458,
0:          0.0433,  0.0446,  0.0433,  0.0421,  0.0408,  0.0396,  0.0010, -0.0015, -0.0052, -0.0090, -0.0115, -0.0140,
0:         -0.0165], device='cuda:0')
0: [DEBUG] Epoch 25, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan, -0.2532,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2482, -0.2482,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2457,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2432,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2432,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2482,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2532,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2569,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2519,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2494,     nan,     nan,     nan, -0.2482, -0.2482,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2556,     nan,     nan,     nan,     nan, -0.2519,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2556,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2581,     nan,     nan,     nan,
0:             nan, -0.2569,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2569,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 25, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1732, -1.0987, -1.0502, -1.0497, -1.0905, -1.1260, -1.0995, -1.0013, -0.8996, -0.8154, -0.8179, -0.8994,
0:         -1.0131, -1.0791, -1.0688, -0.9893, -0.8933, -0.8244, -1.1387, -1.0628, -1.0072, -1.0159, -1.0945, -1.1418,
0:         -1.1361], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0730,  0.1329,  0.1390,  0.1036,  0.0390, -0.0162, -0.0772, -0.1103, -0.1270, -0.1421, -0.1467, -0.1485,
0:         -0.1646, -0.2083, -0.2545, -0.2761, -0.2646, -0.2323, -0.0428,  0.0442,  0.0716,  0.0457, -0.0181, -0.1003,
0:         -0.1644], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7179, -0.7105, -0.7015, -0.6916, -0.6813, -0.6750, -0.6774, -0.6800, -0.6841, -0.6904, -0.6920, -0.6924,
0:         -0.6858, -0.6790, -0.6839, -0.6857, -0.7003, -0.7039, -0.7338, -0.7165, -0.6988, -0.6832, -0.6747, -0.6769,
0:         -0.6809], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.4139,  0.4189,  0.4763,  0.3432,  0.1530,  0.1832,  0.1754,  0.0248, -0.0237, -0.0576, -0.0870,  0.0526,
0:          0.1883,  0.2086,  0.1592,  0.1707,  0.2863,  0.1886,  0.3985,  0.2486,  0.2354,  0.1431,  0.0518,  0.1222,
0:          0.0935], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.3994, -1.4450, -1.4853, -1.5220, -1.5430, -1.5519, -1.5499, -1.5441, -1.5357, -1.5287, -1.5257, -1.5220,
0:         -1.5179, -1.5071, -1.4964, -1.4895, -1.4913, -1.4981, -1.5102, -1.5238, -1.5393, -1.5562, -1.5742, -1.5906,
0:         -1.6066], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2233, -0.2235, -0.2341, -0.2359, -0.2361, -0.2357, -0.2349, -0.2327, -0.2335, -0.2285, -0.2328, -0.2327,
0:         -0.2319, -0.2335, -0.2316, -0.2363, -0.2325, -0.2412, -0.2315, -0.2275, -0.2328, -0.2284, -0.2303, -0.2349,
0:         -0.2336], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.16253530979156494; velocity_v: 0.17663389444351196; specific_humidity: 0.1508617103099823; velocity_z: 0.45133176445961; temperature: 0.1316278576850891; total_precip: 0.42153918743133545; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1481093466281891; velocity_v: 0.1840476095676422; specific_humidity: 0.13585621118545532; velocity_z: 0.6001891493797302; temperature: 0.15022896230220795; total_precip: 0.6071580052375793; 
0: epoch: 25 [1/5 (20%)]	Loss: 0.51435 : 0.24036 :: 0.21481 (2.48 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.18135026097297668; velocity_v: 0.20181888341903687; specific_humidity: 0.1410461664199829; velocity_z: 0.6044474840164185; temperature: 0.17275205254554749; total_precip: 0.5846543312072754; 
0: epoch: 25 [2/5 (40%)]	Loss: 0.58465 : 0.27736 :: 0.21364 (15.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15423159301280975; velocity_v: 0.18908752501010895; specific_humidity: 0.14274287223815918; velocity_z: 0.6113737225532532; temperature: 0.19607524573802948; total_precip: 0.5252314805984497; 
0: epoch: 25 [3/5 (60%)]	Loss: 0.52523 : 0.26561 :: 0.21590 (15.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16587433218955994; velocity_v: 0.17718453705310822; specific_humidity: 0.15301461517810822; velocity_z: 0.5841535329818726; temperature: 0.16754426062107086; total_precip: 0.5174846053123474; 
0: epoch: 25 [4/5 (80%)]	Loss: 0.51748 : 0.25714 :: 0.21736 (15.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 25 : 0.29326289892196655
0: validation loss for velocity_u : 0.09885109215974808
0: validation loss for velocity_v : 0.12058151513338089
0: validation loss for specific_humidity : 0.13284049928188324
0: validation loss for velocity_z : 0.5401177406311035
0: validation loss for temperature : 0.1216282919049263
0: validation loss for total_precip : 0.7455585598945618
0: 26 : 21:08:37 :: batch_size = 96, lr = 1.1057507083710246e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 26, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0542, 0.0716, 0.0871, 0.1026, 0.1183, 0.1309, 0.1403, 0.1441, 0.1456, 0.1459, 0.1470, 0.1467, 0.1438, 0.1384,
0:         0.1287, 0.1187, 0.1089, 0.0997, 0.0490, 0.0639, 0.0760, 0.0877, 0.0998, 0.1126, 0.1241], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2971, 0.2901, 0.2839, 0.2815, 0.2823, 0.2861, 0.2887, 0.2873, 0.2819, 0.2725, 0.2625, 0.2495, 0.2335, 0.2131,
0:         0.1883, 0.1595, 0.1279, 0.0938, 0.3229, 0.3171, 0.3129, 0.3115, 0.3113, 0.3125, 0.3133], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.0321, 0.0287, 0.0274, 0.0270, 0.0275, 0.0309, 0.0380, 0.0521, 0.0723, 0.1039, 0.1356, 0.1549, 0.1618, 0.1597,
0:         0.1540, 0.1433, 0.1325, 0.1079, 0.0529, 0.0507, 0.0492, 0.0485, 0.0492, 0.0575, 0.0645], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0701, -0.0458, -0.0613, -0.0590, -0.0170,  0.0184,  0.0338, -0.0104, -0.0347, -0.0989, -0.1497, -0.1807,
0:         -0.2117, -0.2316, -0.2692, -0.2736, -0.2515, -0.2470, -0.1497, -0.1431, -0.0966, -0.0922, -0.0590,  0.0029,
0:          0.0560], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.9814, 1.0052, 1.0312, 1.0502, 1.0687, 1.0907, 1.1193, 1.1498, 1.1737, 1.1948, 1.2169, 1.2450, 1.2805, 1.3174,
0:         1.3413, 1.3542, 1.3649, 1.3822, 1.4125, 1.4633, 1.5354, 1.6064, 1.6767, 1.7345, 1.7785], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1660, -0.1972, -0.1972, -0.2084, -0.2128, -0.2151, -0.2151, -0.2173, -0.2106, -0.1593, -0.1950, -0.1928,
0:         -0.2039, -0.2106, -0.1994, -0.1994, -0.1838, -0.1705, -0.1771, -0.1905, -0.1950, -0.2039, -0.2017, -0.1883,
0:         -0.1749], device='cuda:0')
0: [DEBUG] Epoch 26, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1705,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1571,
0:             nan,     nan,     nan,     nan,     nan, -0.2195,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2039,     nan,     nan, -0.1928,     nan, -0.2262,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2039,     nan,     nan,     nan,     nan,
0:         -0.1950,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1928,     nan,
0:             nan,     nan, -0.2151,     nan,     nan,     nan,     nan, -0.1905,     nan,     nan,     nan,     nan,
0:         -0.1816,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2284,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2284,     nan,     nan,     nan,     nan,     nan,     nan, -0.1972,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1794,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2084,     nan, -0.2006,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2061,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2151,     nan,     nan,     nan,     nan,     nan, -0.1838,
0:         -0.2195,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2240,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1816,     nan,     nan, -0.2139,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 26, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.0230, 0.0542, 0.0765, 0.0930, 0.0877, 0.0865, 0.1289, 0.1980, 0.2533, 0.2964, 0.2839, 0.2136, 0.1407, 0.1058,
0:         0.1276, 0.2167, 0.3051, 0.3795, 0.1221, 0.1443, 0.1593, 0.1442, 0.0982, 0.0821, 0.1035], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8608, 0.8424, 0.8275, 0.8321, 0.8512, 0.8798, 0.8938, 0.9020, 0.9006, 0.8842, 0.8636, 0.8412, 0.8109, 0.7619,
0:         0.7136, 0.6976, 0.7278, 0.7943, 0.8558, 0.8505, 0.8378, 0.8554, 0.8797, 0.9080, 0.9359], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.8747, 0.9588, 1.0359, 1.1211, 1.1830, 1.2440, 1.3213, 1.3844, 1.4605, 1.5065, 1.5492, 1.5542, 1.5594, 1.5666,
0:         1.5601, 1.5950, 1.6683, 1.7504, 0.8935, 0.9851, 1.0877, 1.1810, 1.2793, 1.3685, 1.4736], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0274,  0.0777,  0.2215,  0.0529, -0.1223, -0.0245, -0.0750, -0.1919, -0.1033, -0.0921, -0.1556, -0.0582,
0:          0.0328,  0.0203, -0.0553,  0.0523,  0.2542,  0.1479,  0.0154, -0.1176, -0.1137, -0.2353, -0.3005, -0.1975,
0:         -0.2032], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 3.1791e-01,  2.5094e-01,  1.8885e-01,  1.2548e-01,  6.4630e-02,  1.9154e-04, -7.0481e-02, -1.4041e-01,
0:         -1.9657e-01, -2.3310e-01, -2.5683e-01, -2.6544e-01, -2.6471e-01, -2.5569e-01, -2.5019e-01, -2.6053e-01,
0:         -2.9660e-01, -3.5087e-01, -4.1136e-01, -4.5876e-01, -4.8231e-01, -4.8258e-01, -4.6784e-01, -4.4602e-01,
0:         -4.2863e-01], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2296, -0.2269, -0.2346, -0.2334, -0.2282, -0.2272, -0.2184, -0.2081, -0.2108, -0.2322, -0.2367, -0.2326,
0:         -0.2295, -0.2249, -0.2183, -0.2161, -0.2151, -0.2191, -0.2337, -0.2266, -0.2326, -0.2216, -0.2200, -0.2253,
0:         -0.2170], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15926863253116608; velocity_v: 0.19666516780853271; specific_humidity: 0.15712369978427887; velocity_z: 0.7354015707969666; temperature: 0.16537582874298096; total_precip: 0.9669118523597717; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16842219233512878; velocity_v: 0.18298198282718658; specific_humidity: 0.16210000216960907; velocity_z: 0.6459283232688904; temperature: 0.15132348239421844; total_precip: 0.8058937191963196; 
0: epoch: 26 [1/5 (20%)]	Loss: 0.88640 : 0.33706 :: 0.22091 (2.79 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1672370731830597; velocity_v: 0.19250907003879547; specific_humidity: 0.14138393104076385; velocity_z: 0.5301721096038818; temperature: 0.13655433058738708; total_precip: 0.569886326789856; 
0: epoch: 26 [2/5 (40%)]	Loss: 0.56989 : 0.25343 :: 0.21501 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14084459841251373; velocity_v: 0.1510063260793686; specific_humidity: 0.1514361947774887; velocity_z: 0.5447158813476562; temperature: 0.1270957589149475; total_precip: 0.6071512699127197; 
0: epoch: 26 [3/5 (60%)]	Loss: 0.60715 : 0.25117 :: 0.21354 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14845752716064453; velocity_v: 0.1766303926706314; specific_humidity: 0.1524210274219513; velocity_z: 0.6180838346481323; temperature: 0.15530399978160858; total_precip: 0.6142209768295288; 
0: epoch: 26 [4/5 (80%)]	Loss: 0.61422 : 0.27392 :: 0.21260 (15.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 26 : 0.2839306890964508
0: validation loss for velocity_u : 0.1157277449965477
0: validation loss for velocity_v : 0.13073380291461945
0: validation loss for specific_humidity : 0.12145057320594788
0: validation loss for velocity_z : 0.5219035744667053
0: validation loss for temperature : 0.12435963749885559
0: validation loss for total_precip : 0.6894087791442871
0: 27 : 21:12:38 :: batch_size = 96, lr = 1.0787811788985607e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 27, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1981, 1.1866, 1.1746, 1.1627, 1.1505, 1.1380, 1.1253, 1.1122, 1.0987, 1.0851, 1.0712, 1.0569, 1.0427, 1.0282,
0:         1.0136, 0.9990, 0.9844, 0.9698, 1.2133, 1.2018, 1.1904, 1.1785, 1.1664, 1.1541, 1.1414], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8304, -0.8352, -0.8388, -0.8413, -0.8427, -0.8434, -0.8431, -0.8421, -0.8402, -0.8377, -0.8346, -0.8306,
0:         -0.8258, -0.8204, -0.8141, -0.8071, -0.7995, -0.7912, -0.8855, -0.8943, -0.9013, -0.9072, -0.9116, -0.9149,
0:         -0.9170], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6709, -0.6698, -0.6687, -0.6671, -0.6654, -0.6638, -0.6621, -0.6602, -0.6584, -0.6566, -0.6547, -0.6528,
0:         -0.6508, -0.6490, -0.6472, -0.6454, -0.6436, -0.6420, -0.6771, -0.6765, -0.6759, -0.6755, -0.6741, -0.6725,
0:         -0.6712], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0581, -0.0649, -0.0739, -0.0840, -0.0941, -0.1043, -0.1122, -0.1178, -0.1212, -0.1234, -0.1245, -0.1268,
0:         -0.1302, -0.1335, -0.1369, -0.1392, -0.1414, -0.1414, -0.0289, -0.0300, -0.0345, -0.0424, -0.0502, -0.0581,
0:         -0.0660], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.0402, 1.0328, 1.0253, 1.0171, 1.0085, 0.9997, 0.9906, 0.9807, 0.9710, 0.9610, 0.9508, 0.9406, 0.9307, 0.9212,
0:         0.9119, 0.9031, 0.8949, 0.8873, 0.8805, 0.8743, 0.8688, 0.8641, 0.8601, 0.8561, 0.8524], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2245, -0.2257, -0.2281, -0.2281, -0.2269, -0.2269, -0.2257, -0.2281, -0.2269, -0.2293, -0.2305, -0.2317,
0:         -0.2329, -0.2329, -0.2329, -0.2329, -0.2329, -0.2341, -0.2317, -0.2341, -0.2353, -0.2365, -0.2377, -0.2377,
0:         -0.2377], device='cuda:0')
0: [DEBUG] Epoch 27, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan, -0.2401,     nan,     nan,     nan,     nan, -0.2412,     nan, -0.2401,
0:         -0.2401,     nan,     nan,     nan,     nan,     nan, -0.2412,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2412,     nan,
0:             nan,     nan, -0.2424,     nan,     nan,     nan,     nan, -0.2353,     nan, -0.2365,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2090,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2389,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2365,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2317,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1839, -0.1767,     nan,
0:             nan,     nan, -0.1528,     nan,     nan,     nan,     nan,     nan,     nan, -0.2389,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2412,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2389,     nan, -0.2412,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2412,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2102,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 27, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.0721, 2.0493, 2.0546, 2.0890, 2.1332, 2.1291, 2.0961, 2.0073, 1.8626, 1.7427, 1.6353, 1.5812, 1.5572, 1.5673,
0:         1.5778, 1.5811, 1.5697, 1.5491, 2.1822, 2.1283, 2.1089, 2.1192, 2.1294, 2.1300, 2.1002], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8136, -0.7428, -0.7447, -0.7827, -0.8457, -0.8886, -0.9514, -0.9977, -1.0475, -1.0790, -1.0656, -1.0338,
0:         -1.0002, -1.0259, -1.0706, -1.1004, -1.0578, -0.9551, -0.9061, -0.7936, -0.7691, -0.8225, -0.9043, -0.9887,
0:         -1.0443], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6434, -0.6416, -0.6441, -0.6444, -0.6416, -0.6393, -0.6461, -0.6483, -0.6569, -0.6650, -0.6733, -0.6763,
0:         -0.6746, -0.6741, -0.6796, -0.6867, -0.7042, -0.7091, -0.6655, -0.6583, -0.6555, -0.6503, -0.6499, -0.6501,
0:         -0.6515], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3016, -0.2621, -0.1270, -0.1943, -0.3206, -0.2911, -0.2877, -0.3212, -0.3369, -0.4047, -0.3954, -0.2771,
0:         -0.2147, -0.2331, -0.3717, -0.3886, -0.1756, -0.2050, -0.2992, -0.3840, -0.3669, -0.4480, -0.4550, -0.3343,
0:         -0.2869], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.5099, 0.5100, 0.5045, 0.5127, 0.5574, 0.6213, 0.6753, 0.7052, 0.7129, 0.7179, 0.7324, 0.7505, 0.7483, 0.7309,
0:         0.6602, 0.5635, 0.4505, 0.3831, 0.3908, 0.4726, 0.5781, 0.6353, 0.6223, 0.5606, 0.4940], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1855, -0.1858, -0.2029, -0.2048, -0.2016, -0.2026, -0.1991, -0.1877, -0.1884, -0.1911, -0.1947, -0.1973,
0:         -0.2002, -0.2013, -0.1998, -0.2011, -0.1888, -0.1952, -0.1960, -0.1927, -0.1985, -0.1957, -0.1963, -0.2055,
0:         -0.1935], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.14768186211585999; velocity_v: 0.20059576630592346; specific_humidity: 0.14888128638267517; velocity_z: 0.7535361051559448; temperature: 0.15826676785945892; total_precip: 1.044684886932373; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17311245203018188; velocity_v: 0.20547199249267578; specific_humidity: 0.15142454206943512; velocity_z: 0.6260243654251099; temperature: 0.1412835717201233; total_precip: 0.6005803346633911; 
0: epoch: 27 [1/5 (20%)]	Loss: 0.82263 : 0.32578 :: 0.22240 (2.87 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15803280472755432; velocity_v: 0.17829474806785583; specific_humidity: 0.14092126488685608; velocity_z: 0.5116551518440247; temperature: 0.14311057329177856; total_precip: 0.521112859249115; 
0: epoch: 27 [2/5 (40%)]	Loss: 0.52111 : 0.23938 :: 0.21026 (15.07 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1646420806646347; velocity_v: 0.18472760915756226; specific_humidity: 0.1575223207473755; velocity_z: 0.6004064083099365; temperature: 0.16331273317337036; total_precip: 0.7270482182502747; 
0: epoch: 27 [3/5 (60%)]	Loss: 0.72705 : 0.29605 :: 0.21586 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16322952508926392; velocity_v: 0.1625279039144516; specific_humidity: 0.16026030480861664; velocity_z: 0.5508760213851929; temperature: 0.16507290303707123; total_precip: 0.778406023979187; 
0: epoch: 27 [4/5 (80%)]	Loss: 0.77841 : 0.29212 :: 0.22153 (15.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 27 : 0.2945980429649353
0: validation loss for velocity_u : 0.09293387085199356
0: validation loss for velocity_v : 0.09944825619459152
0: validation loss for specific_humidity : 0.15847428143024445
0: validation loss for velocity_z : 0.5057581067085266
0: validation loss for temperature : 0.11273825913667679
0: validation loss for total_precip : 0.7982351779937744
0: 28 : 21:17:06 :: batch_size = 96, lr = 1.0524694428278642e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 28, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7631, -0.7571, -0.7509, -0.7447, -0.7385, -0.7323, -0.7259, -0.7197, -0.7132, -0.7068, -0.7003, -0.6936,
0:         -0.6869, -0.6802, -0.6733, -0.6663, -0.6593, -0.6523, -0.7447, -0.7373, -0.7301, -0.7228, -0.7153, -0.7078,
0:         -0.7003], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.4368, 1.4289, 1.4215, 1.4145, 1.4082, 1.4023, 1.3969, 1.3918, 1.3875, 1.3833, 1.3798, 1.3766, 1.3737, 1.3713,
0:         1.3694, 1.3676, 1.3663, 1.3652, 1.5000, 1.4926, 1.4859, 1.4793, 1.4734, 1.4678, 1.4627], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4428, -0.4415, -0.4402, -0.4389, -0.4376, -0.4363, -0.4349, -0.4337, -0.4324, -0.4311, -0.4298, -0.4287,
0:         -0.4277, -0.4266, -0.4256, -0.4246, -0.4235, -0.4225, -0.4302, -0.4288, -0.4274, -0.4259, -0.4245, -0.4232,
0:         -0.4219], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5286, 0.5240, 0.5217, 0.5183, 0.5160, 0.5125, 0.5102, 0.5079, 0.5067, 0.5044, 0.5021, 0.4998, 0.4987, 0.4975,
0:         0.4964, 0.4952, 0.4952, 0.4964, 0.5286, 0.5286, 0.5286, 0.5286, 0.5298, 0.5298, 0.5309], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.8328, 1.8355, 1.8380, 1.8408, 1.8433, 1.8460, 1.8490, 1.8519, 1.8549, 1.8582, 1.8612, 1.8645, 1.8679, 1.8711,
0:         1.8746, 1.8781, 1.8816, 1.8850, 1.8885, 1.8923, 1.8957, 1.8994, 1.9030, 1.9065, 1.9104], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2554, -0.2554, -0.2554, -0.2554, -0.2554, -0.2554, -0.2554, -0.2554, -0.2554, -0.2554, -0.2542, -0.2542,
0:         -0.2542, -0.2542, -0.2542, -0.2542, -0.2542, -0.2542, -0.2542, -0.2542, -0.2542, -0.2542, -0.2542, -0.2542,
0:         -0.2542], device='cuda:0')
0: [DEBUG] Epoch 28, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2351,
0:             nan,     nan,     nan,     nan, -0.2375,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2399,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2399, -0.2423,     nan,     nan,     nan,     nan,     nan,     nan, -0.2399,     nan,
0:         -0.2423,     nan,     nan,     nan,     nan, -0.2471,     nan,     nan,     nan, -0.2447,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2518,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2566, -0.2566,     nan, -0.2566,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2399,     nan,     nan,     nan,     nan,     nan,     nan, -0.2423,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2471,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2208,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2304,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2447,     nan,     nan,     nan,     nan,     nan,     nan, -0.2542,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2566])
0: [DEBUG] Epoch 28, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9948, -0.9675, -0.9686, -1.0053, -1.0520, -1.0956, -1.0706, -0.9862, -0.8821, -0.7889, -0.7665, -0.8141,
0:         -0.9042, -0.9554, -0.9483, -0.8673, -0.7649, -0.6737, -0.9359, -0.9200, -0.9275, -0.9841, -1.0901, -1.1500,
0:         -1.1592], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1405,  0.1599,  0.1485,  0.1169,  0.0770,  0.0544,  0.0293,  0.0113,  0.0030, -0.0030,  0.0036,  0.0231,
0:          0.0408,  0.0332,  0.0264,  0.0333,  0.0811,  0.1459,  0.1488,  0.1917,  0.2033,  0.1853,  0.1668,  0.1548,
0:          0.1479], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4969, -0.4765, -0.4585, -0.4379, -0.4229, -0.4155, -0.4230, -0.4311, -0.4417, -0.4517, -0.4574, -0.4600,
0:         -0.4578, -0.4536, -0.4583, -0.4591, -0.4685, -0.4670, -0.5152, -0.4806, -0.4495, -0.4211, -0.4076, -0.4077,
0:         -0.4160], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2414, -0.2281, -0.0939, -0.2344, -0.3579, -0.2910, -0.2998, -0.2511, -0.1729, -0.2496, -0.2869, -0.2183,
0:         -0.1848, -0.2316, -0.3273, -0.2921, -0.1429, -0.1891, -0.2965, -0.4081, -0.3323, -0.4453, -0.4919, -0.3689,
0:         -0.3682], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([1.8929, 1.8250, 1.7695, 1.7464, 1.7670, 1.8104, 1.8629, 1.8922, 1.8990, 1.8857, 1.8571, 1.8238, 1.7829, 1.7408,
0:         1.6797, 1.6135, 1.5553, 1.5387, 1.5794, 1.6519, 1.7179, 1.7244, 1.6744, 1.5936, 1.5204], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2368, -0.2345, -0.2450, -0.2419, -0.2408, -0.2421, -0.2460, -0.2460, -0.2552, -0.2430, -0.2391, -0.2357,
0:         -0.2342, -0.2338, -0.2369, -0.2436, -0.2476, -0.2587, -0.2393, -0.2304, -0.2353, -0.2281, -0.2259, -0.2373,
0:         -0.2349], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.14926080405712128; velocity_v: 0.17717115581035614; specific_humidity: 0.1667928248643875; velocity_z: 0.7329904437065125; temperature: 0.17058652639389038; total_precip: 1.0238946676254272; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16875752806663513; velocity_v: 0.2039732187986374; specific_humidity: 0.15137210488319397; velocity_z: 0.6195672750473022; temperature: 0.1406976580619812; total_precip: 0.8239198327064514; 
0: epoch: 28 [1/5 (20%)]	Loss: 0.92391 : 0.33964 :: 0.21901 (2.75 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1731073558330536; velocity_v: 0.19079042971134186; specific_humidity: 0.14399535953998566; velocity_z: 0.665555477142334; temperature: 0.18260817229747772; total_precip: 0.653039813041687; 
0: epoch: 28 [2/5 (40%)]	Loss: 0.65304 : 0.29734 :: 0.21660 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14828723669052124; velocity_v: 0.15818949043750763; specific_humidity: 0.1698114275932312; velocity_z: 0.4992273449897766; temperature: 0.15266071259975433; total_precip: 0.6547042727470398; 
0: epoch: 28 [3/5 (60%)]	Loss: 0.65470 : 0.26073 :: 0.21330 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14922799170017242; velocity_v: 0.17150598764419556; specific_humidity: 0.14779919385910034; velocity_z: 0.5378758311271667; temperature: 0.15643322467803955; total_precip: 0.5809934735298157; 
0: epoch: 28 [4/5 (80%)]	Loss: 0.58099 : 0.25400 :: 0.21729 (15.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 28 : 0.3459209203720093
0: validation loss for velocity_u : 0.11247541755437851
0: validation loss for velocity_v : 0.11489716917276382
0: validation loss for specific_humidity : 0.13882724940776825
0: validation loss for velocity_z : 0.55683434009552
0: validation loss for temperature : 0.11998176574707031
0: validation loss for total_precip : 1.0325099229812622
0: 29 : 21:21:02 :: batch_size = 96, lr = 1.0267994564174285e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 29, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7458, -0.7686, -0.7960, -0.8257, -0.8585, -0.8968, -0.9379, -0.9753, -1.0042, -1.0284, -1.0514, -1.0756,
0:         -1.1007, -1.1246, -1.1470, -1.1665, -1.1849, -1.2038, -0.7694, -0.7836, -0.7995, -0.8158, -0.8370, -0.8664,
0:         -0.9010], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2777, -0.3564, -0.4318, -0.5025, -0.5610, -0.6001, -0.6235, -0.6311, -0.6215, -0.5975, -0.5573, -0.5068,
0:         -0.4526, -0.4029, -0.3631, -0.3285, -0.2965, -0.2671, -0.3303, -0.4086, -0.4840, -0.5514, -0.6034, -0.6366,
0:         -0.6555], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4254, -0.3514, -0.3053, -0.2899, -0.2881, -0.2873, -0.2928, -0.3063, -0.3241, -0.3301, -0.3297, -0.3249,
0:         -0.3212, -0.3230, -0.3276, -0.3341, -0.3435, -0.3517, -0.4255, -0.3513, -0.2939, -0.2752, -0.2768, -0.2805,
0:         -0.2860], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.0242,  0.1437,  0.1637,  0.0353, -0.1042, -0.1441, -0.0931,  0.0065,  0.1216,  0.2434,  0.3275,  0.3496,
0:          0.3452,  0.3098,  0.2544,  0.2013,  0.1482,  0.1127,  0.0707,  0.1282,  0.1282, -0.0024, -0.1463, -0.1529,
0:         -0.0555], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.5553, -0.6996, -0.7406, -0.7210, -0.6671, -0.6062, -0.5433, -0.4589, -0.3558, -0.2530, -0.1670, -0.0949,
0:         -0.0179,  0.0678,  0.1593,  0.2453,  0.3190,  0.3827,  0.4378,  0.4912,  0.5444,  0.6016,  0.6640,  0.7230,
0:          0.7766], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.0992, -0.0582, -0.1356, -0.2313, -0.2381, -0.2381, -0.2381, -0.2381, -0.2358, -0.1265, -0.0719, -0.1333,
0:         -0.2244, -0.2335, -0.2381, -0.2381, -0.2381, -0.2381, -0.1812, -0.1333, -0.2130, -0.2267, -0.2335, -0.2381,
0:         -0.2381], device='cuda:0')
0: [DEBUG] Epoch 29, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([ 0.2549,     nan,     nan,     nan,     nan,  0.6477,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0021,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1800,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2233,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2014,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1880,     nan,     nan,     nan,     nan,     nan,  0.0056,     nan,
0:             nan,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1015,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.5293,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1333,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2290,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2313, -0.2358,
0:             nan,     nan,     nan,     nan, -0.0559,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2039, -0.1994, -0.1925,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 29, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.3808, -1.2918, -1.2484, -1.2547, -1.3129, -1.3638, -1.3315, -1.2187, -1.0854, -0.9905, -0.9931, -1.0966,
0:         -1.2317, -1.2982, -1.2592, -1.1183, -0.9776, -0.8571, -1.2669, -1.1827, -1.1335, -1.1667, -1.2767, -1.3485,
0:         -1.3566], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0495,  0.0538,  0.0059, -0.0642, -0.1425, -0.2040, -0.2555, -0.2876, -0.3014, -0.3210, -0.3229, -0.3234,
0:         -0.3315, -0.3734, -0.4363, -0.4736, -0.4776, -0.4553, -0.0061,  0.0187, -0.0196, -0.0983, -0.1838, -0.2652,
0:         -0.3110], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6731, -0.6745, -0.6830, -0.6809, -0.6737, -0.6681, -0.6699, -0.6762, -0.6822, -0.6913, -0.6914, -0.6837,
0:         -0.6602, -0.6381, -0.6223, -0.6107, -0.6098, -0.6099, -0.6585, -0.6574, -0.6594, -0.6567, -0.6533, -0.6519,
0:         -0.6509], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2872,  0.2339,  0.3773,  0.3097,  0.1333,  0.2438,  0.1884,  0.0746,  0.2431,  0.2356,  0.1074,  0.1455,
0:          0.2145,  0.2338,  0.1034,  0.1234,  0.3181,  0.2092,  0.2778,  0.0772,  0.0747,  0.0306, -0.0190,  0.1284,
0:          0.1310], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.8815, 0.9148, 0.9622, 1.0217, 1.1034, 1.1823, 1.2377, 1.2506, 1.2366, 1.2165, 1.2031, 1.2018, 1.1882, 1.1403,
0:         1.0490, 0.9356, 0.8392, 0.8009, 0.8279, 0.8913, 0.9414, 0.9423, 0.8995, 0.8486, 0.8206], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0322, -0.0267, -0.0508, -0.0666, -0.0719, -0.0843, -0.0991, -0.0945, -0.1090, -0.0426, -0.0513, -0.0655,
0:         -0.0780, -0.0818, -0.1058, -0.1096, -0.1149, -0.1287, -0.0740, -0.0791, -0.0902, -0.0985, -0.1080, -0.1218,
0:         -0.1248], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1754993200302124; velocity_v: 0.1784345656633377; specific_humidity: 0.1530030518770218; velocity_z: 0.6589925289154053; temperature: 0.16089271008968353; total_precip: 0.5878894329071045; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16097551584243774; velocity_v: 0.19165897369384766; specific_humidity: 0.157179057598114; velocity_z: 0.5998321771621704; temperature: 0.17875966429710388; total_precip: 0.6842183470726013; 
0: epoch: 29 [1/5 (20%)]	Loss: 0.63605 : 0.28642 :: 0.21468 (2.41 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15500949323177338; velocity_v: 0.1849772036075592; specific_humidity: 0.1566312313079834; velocity_z: 0.5862959623336792; temperature: 0.15471531450748444; total_precip: 0.591849148273468; 
0: epoch: 29 [2/5 (40%)]	Loss: 0.59185 : 0.26775 :: 0.21687 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17124341428279877; velocity_v: 0.19205668568611145; specific_humidity: 0.157974973320961; velocity_z: 0.6401065587997437; temperature: 0.1584230363368988; total_precip: 0.7047061920166016; 
0: epoch: 29 [3/5 (60%)]	Loss: 0.70471 : 0.29983 :: 0.22028 (15.36 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16816142201423645; velocity_v: 0.1854839026927948; specific_humidity: 0.1457068771123886; velocity_z: 0.6451315879821777; temperature: 0.15219442546367645; total_precip: 0.5915390253067017; 
0: epoch: 29 [4/5 (80%)]	Loss: 0.59154 : 0.27706 :: 0.22158 (15.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 29 : 0.2899017333984375
0: validation loss for velocity_u : 0.10327881574630737
0: validation loss for velocity_v : 0.11551952362060547
0: validation loss for specific_humidity : 0.12813688814640045
0: validation loss for velocity_z : 0.5341242551803589
0: validation loss for temperature : 0.10737094283103943
0: validation loss for total_precip : 0.7509796023368835
0: 30 : 21:25:25 :: batch_size = 96, lr = 1.0017555672365157e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 30, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2264, 0.2048, 0.1873, 0.1742, 0.1644, 0.1555, 0.1467, 0.1391, 0.1340, 0.1329, 0.1350, 0.1383, 0.1420, 0.1444,
0:         0.1452, 0.1434, 0.1378, 0.1322, 0.2600, 0.2408, 0.2227, 0.2070, 0.1945, 0.1828, 0.1713], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1288,  0.0721,  0.0237, -0.0148, -0.0434, -0.0677, -0.0858, -0.0929, -0.0869, -0.0648, -0.0287,  0.0158,
0:          0.0632,  0.1074,  0.1463,  0.1793,  0.2092,  0.2392,  0.2286,  0.1770,  0.1284,  0.0852,  0.0495,  0.0177,
0:         -0.0098], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3411, -0.3391, -0.3332, -0.3297, -0.3300, -0.3320, -0.3374, -0.3422, -0.3438, -0.3406, -0.3384, -0.3353,
0:         -0.3334, -0.3342, -0.3372, -0.3433, -0.3518, -0.3581, -0.2923, -0.2959, -0.2951, -0.2903, -0.2847, -0.2821,
0:         -0.2837], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6461,  0.6483,  0.6243,  0.5110,  0.3147,  0.0553, -0.1452, -0.2324, -0.2434, -0.1409, -0.0253,  0.1098,
0:          0.2493,  0.3104,  0.3911,  0.4020,  0.3344,  0.2951,  0.8314,  0.8423,  0.9164,  0.9034,  0.8227,  0.6418,
0:          0.3932], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2204, -0.2995, -0.3696, -0.4149, -0.4176, -0.3860, -0.3308, -0.2648, -0.2007, -0.1448, -0.1021, -0.0700,
0:         -0.0492, -0.0324, -0.0155, -0.0039,  0.0043,  0.0059,  0.0077,  0.0187,  0.0407,  0.0769,  0.1286,  0.1875,
0:          0.2422], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2449, -0.2449, -0.2449, -0.2449, -0.2449, -0.2449, -0.2449, -0.2425, -0.2425, -0.2449, -0.2449, -0.2449,
0:         -0.2449, -0.2449, -0.2449, -0.2425, -0.2378, -0.2331, -0.2449, -0.2449, -0.2449, -0.2425, -0.2449, -0.2425,
0:         -0.2331], device='cuda:0')
0: [DEBUG] Epoch 30, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan, -0.2449,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan, -0.1535, -0.1383,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1722,     nan,     nan,     nan,
0:             nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1898,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2355,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2449,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2039,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2449,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:          0.0596,     nan,     nan])
0: [DEBUG] Epoch 30, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2747, 0.2910, 0.3337, 0.3738, 0.3943, 0.3777, 0.3565, 0.3192, 0.2736, 0.2454, 0.2153, 0.1758, 0.1441, 0.1234,
0:         0.1169, 0.1325, 0.1598, 0.1844, 0.3868, 0.3663, 0.3806, 0.3987, 0.3923, 0.3766, 0.3522], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.5136, 0.5152, 0.4891, 0.4487, 0.4033, 0.3804, 0.3661, 0.3711, 0.3815, 0.3889, 0.3997, 0.4115, 0.4191, 0.4091,
0:         0.3974, 0.4137, 0.4671, 0.5477, 0.4936, 0.5064, 0.4769, 0.4254, 0.3694, 0.3245, 0.3037], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3780, -0.3646, -0.3514, -0.3164, -0.2757, -0.2231, -0.1729, -0.1236, -0.0790, -0.0423, -0.0116,  0.0135,
0:          0.0323,  0.0464,  0.0288,  0.0041, -0.0496, -0.0972, -0.3862, -0.3767, -0.3597, -0.3379, -0.3067, -0.2694,
0:         -0.2189], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1082, -0.0511,  0.0844,  0.0231, -0.1946, -0.2079, -0.2563, -0.2989, -0.1475, -0.1586, -0.1460,  0.0567,
0:          0.0886,  0.0073, -0.1370, -0.1952, -0.0509, -0.0789, -0.0796, -0.1263, -0.1547, -0.2286, -0.3282, -0.3084,
0:         -0.3814], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.7259, 0.6971, 0.6728, 0.6602, 0.6609, 0.6672, 0.6646, 0.6541, 0.6373, 0.6227, 0.6149, 0.6082, 0.5913, 0.5611,
0:         0.5114, 0.4533, 0.4007, 0.3766, 0.3844, 0.4119, 0.4309, 0.4132, 0.3599, 0.2904, 0.2277], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2231, -0.2251, -0.2357, -0.2365, -0.2290, -0.2346, -0.2319, -0.2232, -0.2274, -0.2368, -0.2366, -0.2336,
0:         -0.2329, -0.2294, -0.2290, -0.2280, -0.2315, -0.2373, -0.2321, -0.2273, -0.2340, -0.2252, -0.2258, -0.2321,
0:         -0.2223], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.16073253750801086; velocity_v: 0.16142462193965912; specific_humidity: 0.1586214154958725; velocity_z: 0.5237717032432556; temperature: 0.16662919521331787; total_precip: 0.5179716944694519; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14009711146354675; velocity_v: 0.17248816788196564; specific_humidity: 0.1607138216495514; velocity_z: 0.6427299976348877; temperature: 0.15972977876663208; total_precip: 0.6473661661148071; 
0: epoch: 30 [1/5 (20%)]	Loss: 0.58267 : 0.26389 :: 0.21154 (2.64 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15873576700687408; velocity_v: 0.18668833374977112; specific_humidity: 0.16698938608169556; velocity_z: 0.5679910182952881; temperature: 0.1600581258535385; total_precip: 0.6205282211303711; 
0: epoch: 30 [2/5 (40%)]	Loss: 0.62053 : 0.27180 :: 0.21884 (15.11 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1655607968568802; velocity_v: 0.1565607339143753; specific_humidity: 0.16032342612743378; velocity_z: 0.518893301486969; temperature: 0.1598132848739624; total_precip: 0.5495678186416626; 
0: epoch: 30 [3/5 (60%)]	Loss: 0.54957 : 0.24738 :: 0.22462 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16659845411777496; velocity_v: 0.1859990954399109; specific_humidity: 0.13628531992435455; velocity_z: 0.6041876673698425; temperature: 0.18327663838863373; total_precip: 0.6398276090621948; 
0: epoch: 30 [4/5 (80%)]	Loss: 0.63983 : 0.28189 :: 0.21856 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 30 : 0.3354231119155884
0: validation loss for velocity_u : 0.1260172575712204
0: validation loss for velocity_v : 0.1423848420381546
0: validation loss for specific_humidity : 0.1376757174730301
0: validation loss for velocity_z : 0.6940740942955017
0: validation loss for temperature : 0.12305045127868652
0: validation loss for total_precip : 0.7893362641334534
0: 31 : 21:29:19 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 31, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9415, -0.9486, -0.9560, -0.9636, -0.9708, -0.9755, -0.9752, -0.9693, -0.9587, -0.9440, -0.9257, -0.9039,
0:         -0.8790, -0.8510, -0.8215, -0.7930, -0.7664, -0.7413, -1.0183, -1.0256, -1.0317, -1.0362, -1.0389, -1.0381,
0:         -1.0320], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.5998, -1.6774, -1.7555, -1.8351, -1.9135, -1.9866, -2.0511, -2.1043, -2.1461, -2.1773, -2.1980, -2.2052,
0:         -2.1955, -2.1698, -2.1321, -2.0877, -2.0392, -1.9876, -1.5691, -1.6472, -1.7252, -1.8017, -1.8749, -1.9416,
0:         -2.0003], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4378, -0.4278, -0.4165, -0.4152, -0.4185, -0.4376, -0.4592, -0.4894, -0.5204, -0.5534, -0.5799, -0.6036,
0:         -0.6160, -0.6250, -0.6264, -0.6273, -0.6235, -0.6131, -0.4312, -0.4244, -0.4184, -0.4188, -0.4261, -0.4437,
0:         -0.4673], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2970, 0.2991, 0.3056, 0.3400, 0.3787, 0.3852, 0.3744, 0.3594, 0.3249, 0.2668, 0.2281, 0.2302, 0.2281, 0.2023,
0:         0.1850, 0.1915, 0.1721, 0.1097, 0.2044, 0.2130, 0.2152, 0.2388, 0.2840, 0.3206, 0.3292], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.0075, 0.9971, 0.9873, 0.9802, 0.9769, 0.9760, 0.9751, 0.9726, 0.9672, 0.9578, 0.9439, 0.9251, 0.9001, 0.8675,
0:         0.8274, 0.7825, 0.7361, 0.6899, 0.6445, 0.6018, 0.5610, 0.5230, 0.4869, 0.4519, 0.4184], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2483, -0.2483, -0.2483, -0.2460, -0.2460, -0.2460, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483,
0:         -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483, -0.2483,
0:         -0.2483], device='cuda:0')
0: [DEBUG] Epoch 31, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2366,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448, -0.2401,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2413,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2331,
0:             nan,     nan,     nan, -0.2483, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2424,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2436,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2436, -0.2460,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2424, -0.2436, -0.2460,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2460, -0.2471,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2424,
0:             nan,     nan,     nan,     nan,     nan, -0.2483,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2460,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2424,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2471,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 31, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7443, -0.6771, -0.6096, -0.5983, -0.6507, -0.7177, -0.7375, -0.6903, -0.6359, -0.5866, -0.6078, -0.6982,
0:         -0.8126, -0.8867, -0.8943, -0.8389, -0.7869, -0.7646, -0.6581, -0.6049, -0.5522, -0.5668, -0.6523, -0.7349,
0:         -0.7707], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.8544, -0.8399, -0.9274, -1.0501, -1.1725, -1.2529, -1.3315, -1.3707, -1.3859, -1.3656, -1.3012, -1.2205,
0:         -1.1686, -1.1915, -1.2638, -1.3373, -1.3530, -1.3138, -0.9587, -0.9103, -0.9760, -1.0972, -1.2170, -1.3202,
0:         -1.3804], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2419, -0.2692, -0.3224, -0.3813, -0.4464, -0.5079, -0.5703, -0.6166, -0.6548, -0.6800, -0.6994, -0.7119,
0:         -0.7154, -0.7190, -0.7304, -0.7371, -0.7548, -0.7661, -0.2428, -0.2701, -0.3207, -0.3781, -0.4527, -0.5217,
0:         -0.5839], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6610, 0.7080, 0.8420, 0.7292, 0.5737, 0.7661, 0.8409, 0.7126, 0.7754, 0.7871, 0.6949, 0.8131, 0.9336, 0.8544,
0:         0.7448, 0.8259, 0.9829, 0.9230, 0.6746, 0.6106, 0.6527, 0.5889, 0.4950, 0.6410, 0.7379], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.6287,  0.5609,  0.5073,  0.4707,  0.4470,  0.4195,  0.3782,  0.3141,  0.2392,  0.1612,  0.0844,  0.0126,
0:         -0.0586, -0.1306, -0.2117, -0.2966, -0.3716, -0.4239, -0.4476, -0.4545, -0.4658, -0.4939, -0.5389, -0.5907,
0:         -0.6374], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2425, -0.2398, -0.2489, -0.2417, -0.2339, -0.2412, -0.2436, -0.2357, -0.2419, -0.2479, -0.2428, -0.2363,
0:         -0.2325, -0.2293, -0.2300, -0.2318, -0.2354, -0.2443, -0.2381, -0.2314, -0.2308, -0.2189, -0.2165, -0.2248,
0:         -0.2182], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.17753833532333374; velocity_v: 0.2323586344718933; specific_humidity: 0.13431988656520844; velocity_z: 0.638545036315918; temperature: 0.15264645218849182; total_precip: 0.7409856915473938; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1814378947019577; velocity_v: 0.17945121228694916; specific_humidity: 0.14681196212768555; velocity_z: 0.5797542929649353; temperature: 0.1978691667318344; total_precip: 0.7274097204208374; 
0: epoch: 31 [1/5 (20%)]	Loss: 0.73420 : 0.30369 :: 0.22037 (2.73 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15455134212970734; velocity_v: 0.16141679883003235; specific_humidity: 0.14016535878181458; velocity_z: 0.4634013772010803; temperature: 0.13305723667144775; total_precip: 0.3885258436203003; 
0: epoch: 31 [2/5 (40%)]	Loss: 0.38853 : 0.20458 :: 0.21431 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1557200700044632; velocity_v: 0.16802650690078735; specific_humidity: 0.14565534889698029; velocity_z: 0.5633376240730286; temperature: 0.1497148871421814; total_precip: 0.7308292984962463; 
0: epoch: 31 [3/5 (60%)]	Loss: 0.73083 : 0.28239 :: 0.21596 (15.34 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14469781517982483; velocity_v: 0.1639973372220993; specific_humidity: 0.14042437076568604; velocity_z: 0.49745503067970276; temperature: 0.1472626030445099; total_precip: 0.5470766425132751; 
0: epoch: 31 [4/5 (80%)]	Loss: 0.54708 : 0.23758 :: 0.21496 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 31 : 0.26247939467430115
0: validation loss for velocity_u : 0.1001833900809288
0: validation loss for velocity_v : 0.13012397289276123
0: validation loss for specific_humidity : 0.10480359941720963
0: validation loss for velocity_z : 0.5890409350395203
0: validation loss for temperature : 0.09150636941194534
0: validation loss for total_precip : 0.5592183470726013
0: 32 : 21:33:13 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 32, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-1.1537, -1.1346, -1.1153, -1.0968, -1.0802, -1.0653, -1.0520, -1.0429, -1.0414, -1.0463, -1.0545, -1.0664,
0:         -1.0834, -1.1039, -1.1229, -1.1387, -1.1513, -1.1605, -1.2405, -1.2138, -1.1868, -1.1608, -1.1383, -1.1197,
0:         -1.1045], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1531, -0.1775, -0.1963, -0.2104, -0.2231, -0.2379, -0.2562, -0.2803, -0.3133, -0.3567, -0.4092, -0.4683,
0:         -0.5337, -0.6045, -0.6782, -0.7533, -0.8285, -0.9018, -0.1138, -0.1386, -0.1572, -0.1703, -0.1812, -0.1943,
0:         -0.2125], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2620, -0.2744, -0.2866, -0.2924, -0.2971, -0.2991, -0.3011, -0.3003, -0.3059, -0.3092, -0.3233, -0.3410,
0:         -0.3519, -0.3622, -0.3700, -0.3696, -0.3636, -0.3533, -0.2662, -0.2782, -0.2836, -0.2888, -0.2909, -0.2965,
0:         -0.3000], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0563, -0.0308, -0.0010,  0.0310,  0.0501,  0.0523,  0.0650,  0.1119,  0.1608,  0.1864,  0.2332,  0.3376,
0:          0.4525,  0.5121,  0.5505,  0.6314,  0.7187,  0.7357, -0.0457, -0.0585, -0.0797, -0.0691, -0.0436, -0.0542,
0:         -0.0840], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([2.2119, 2.2035, 2.1949, 2.1861, 2.1756, 2.1624, 2.1464, 2.1293, 2.1115, 2.0922, 2.0715, 2.0513, 2.0304, 2.0074,
0:         1.9813, 1.9522, 1.9193, 1.8810, 1.8381, 1.7932, 1.7466, 1.6974, 1.6454, 1.5917, 1.5353], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390,
0:         -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390, -0.2390,
0:         -0.2390], device='cuda:0')
0: [DEBUG] Epoch 32, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489, -0.2489,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2489,     nan, -0.2489,     nan,     nan,     nan,     nan,     nan, -0.2489,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2489,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2489,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2489,     nan,     nan,     nan, -0.2489,     nan, -0.2489,     nan,
0:             nan,     nan,     nan,     nan, -0.2489,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2489,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489,     nan,     nan,
0:             nan,     nan, -0.2489,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489, -0.2489,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2489, -0.2489,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2489, -0.2489,     nan,     nan,
0:             nan,     nan, -0.2489])
0: [DEBUG] Epoch 32, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.9663, -0.8617, -0.7733, -0.7451, -0.7733, -0.8183, -0.8104, -0.7472, -0.6736, -0.6082, -0.6128, -0.7006,
0:         -0.8088, -0.8797, -0.8934, -0.8188, -0.7307, -0.6520, -0.8915, -0.7762, -0.6787, -0.6658, -0.7388, -0.8065,
0:         -0.8309], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5958, -0.5680, -0.5992, -0.6694, -0.7720, -0.8663, -0.9558, -1.0069, -1.0287, -1.0371, -1.0282, -1.0115,
0:         -1.0119, -1.0499, -1.1026, -1.1225, -1.1049, -1.0441, -0.6525, -0.5922, -0.6016, -0.6747, -0.7876, -0.9168,
0:         -1.0119], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5104, -0.5359, -0.5667, -0.5882, -0.6067, -0.6239, -0.6410, -0.6579, -0.6696, -0.6728, -0.6682, -0.6523,
0:         -0.6269, -0.6022, -0.5891, -0.5783, -0.5772, -0.5766, -0.5040, -0.5277, -0.5544, -0.5711, -0.5938, -0.6158,
0:         -0.6359], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2659, 0.2864, 0.4096, 0.4670, 0.4338, 0.4896, 0.5605, 0.6815, 0.8706, 1.0168, 1.1878, 1.3943, 1.4696, 1.3464,
0:         1.1424, 1.0889, 1.0955, 0.8904, 0.3323, 0.1925, 0.1972, 0.2481, 0.2858, 0.4049, 0.4807], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.0368,  0.0089, -0.0167, -0.0427, -0.0560, -0.0652, -0.0727, -0.0891, -0.1151, -0.1460, -0.1804, -0.2160,
0:         -0.2565, -0.2955, -0.3384, -0.3734, -0.4021, -0.4166, -0.4228, -0.4292, -0.4396, -0.4559, -0.4719, -0.4883,
0:         -0.5099], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2385, -0.2453, -0.2604, -0.2565, -0.2502, -0.2555, -0.2584, -0.2525, -0.2515, -0.2521, -0.2505, -0.2520,
0:         -0.2475, -0.2482, -0.2508, -0.2503, -0.2488, -0.2584, -0.2448, -0.2428, -0.2496, -0.2410, -0.2400, -0.2453,
0:         -0.2450], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15359234809875488; velocity_v: 0.1758231371641159; specific_humidity: 0.1490498185157776; velocity_z: 0.5838611721992493; temperature: 0.1984393447637558; total_precip: 0.6525108218193054; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17308703064918518; velocity_v: 0.2177760899066925; specific_humidity: 0.1430584043264389; velocity_z: 0.6427152156829834; temperature: 0.15440861880779266; total_precip: 0.5824062824249268; 
0: epoch: 32 [1/5 (20%)]	Loss: 0.61746 : 0.28126 :: 0.21666 (2.43 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15637511014938354; velocity_v: 0.1969469040632248; specific_humidity: 0.14283917844295502; velocity_z: 0.49814558029174805; temperature: 0.16040003299713135; total_precip: 0.5398101210594177; 
0: epoch: 32 [2/5 (40%)]	Loss: 0.53981 : 0.24585 :: 0.21956 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.162338525056839; velocity_v: 0.1826302409172058; specific_humidity: 0.14767393469810486; velocity_z: 0.5642676949501038; temperature: 0.14297941327095032; total_precip: 0.9822309613227844; 
0: epoch: 32 [3/5 (60%)]	Loss: 0.98223 : 0.32712 :: 0.21470 (15.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16828499734401703; velocity_v: 0.18702997267246246; specific_humidity: 0.14433322846889496; velocity_z: 0.5131749510765076; temperature: 0.16314511001110077; total_precip: 0.3749348819255829; 
0: epoch: 32 [4/5 (80%)]	Loss: 0.37493 : 0.22099 :: 0.21688 (15.35 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 32 : 0.2782658338546753
0: validation loss for velocity_u : 0.10818702727556229
0: validation loss for velocity_v : 0.1364891529083252
0: validation loss for specific_humidity : 0.12469280511140823
0: validation loss for velocity_z : 0.5202599167823792
0: validation loss for temperature : 0.11224565654993057
0: validation loss for total_precip : 0.6677204966545105
0: 33 : 21:37:10 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 33, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4524, -0.4533, -0.4544, -0.4556, -0.4570, -0.4587, -0.4607, -0.4632, -0.4661, -0.4695, -0.4735, -0.4778,
0:         -0.4826, -0.4880, -0.4937, -0.5000, -0.5068, -0.5139, -0.4605, -0.4615, -0.4624, -0.4635, -0.4645, -0.4659,
0:         -0.4678], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7119, -0.7138, -0.7159, -0.7179, -0.7204, -0.7231, -0.7257, -0.7288, -0.7321, -0.7354, -0.7389, -0.7424,
0:         -0.7459, -0.7490, -0.7519, -0.7546, -0.7564, -0.7579, -0.7210, -0.7235, -0.7259, -0.7286, -0.7317, -0.7350,
0:         -0.7387], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6607, -0.6616, -0.6627, -0.6637, -0.6647, -0.6658, -0.6668, -0.6675, -0.6682, -0.6689, -0.6697, -0.6704,
0:         -0.6711, -0.6715, -0.6721, -0.6725, -0.6729, -0.6734, -0.6576, -0.6588, -0.6600, -0.6613, -0.6625, -0.6639,
0:         -0.6652], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0597, -0.0740, -0.0882, -0.1024, -0.1145, -0.1243, -0.1320, -0.1385, -0.1429, -0.1484, -0.1528, -0.1572,
0:         -0.1626, -0.1681, -0.1736, -0.1780, -0.1790, -0.1790, -0.0433, -0.0565, -0.0707, -0.0827, -0.0937, -0.1035,
0:         -0.1123], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.6612, -0.6525, -0.6438, -0.6359, -0.6284, -0.6210, -0.6146, -0.6085, -0.6027, -0.5974, -0.5926, -0.5882,
0:         -0.5843, -0.5806, -0.5773, -0.5743, -0.5717, -0.5695, -0.5669, -0.5649, -0.5623, -0.5599, -0.5574, -0.5546,
0:         -0.5516], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472,
0:         -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472,
0:         -0.2472], device='cuda:0')
0: [DEBUG] Epoch 33, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.1474,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1990,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2260,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2331,     nan,     nan,     nan,     nan,     nan, -0.2389,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2425,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1356,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1744,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1932,     nan,     nan,     nan, -0.2096,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2354, -0.2354,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1462,
0:             nan,     nan,     nan,     nan, -0.1438,     nan, -0.1462,     nan, -0.1485,     nan,     nan,     nan,
0:             nan, -0.1626,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1979,     nan,     nan,     nan,     nan,
0:         -0.1990,     nan,     nan,     nan, -0.2143,     nan, -0.2143,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2389,     nan,     nan,     nan, -0.2389,     nan,     nan,     nan,     nan,
0:             nan, -0.2401,     nan])
0: [DEBUG] Epoch 33, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2323, -0.1621, -0.1114, -0.0868, -0.1065, -0.1423, -0.1431, -0.1119, -0.0741, -0.0376, -0.0502, -0.1111,
0:         -0.1735, -0.2001, -0.1625, -0.0746,  0.0059,  0.0521, -0.2391, -0.1869, -0.1392, -0.1283, -0.1840, -0.2272,
0:         -0.2526], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4415, -0.4306, -0.4776, -0.5563, -0.6526, -0.7409, -0.8283, -0.8723, -0.8889, -0.8888, -0.8518, -0.8068,
0:         -0.7856, -0.8115, -0.8608, -0.8843, -0.8313, -0.7218, -0.5859, -0.5229, -0.5450, -0.6136, -0.7142, -0.8118,
0:         -0.8764], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7593, -0.7461, -0.7279, -0.7074, -0.6842, -0.6690, -0.6673, -0.6644, -0.6701, -0.6818, -0.6943, -0.6978,
0:         -0.6918, -0.6837, -0.6844, -0.6856, -0.7011, -0.7079, -0.7502, -0.7277, -0.7066, -0.6851, -0.6699, -0.6625,
0:         -0.6589], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2257,  0.2112,  0.2854,  0.1662,  0.0157,  0.0644,  0.0619,  0.0118,  0.0501, -0.0019, -0.0735,  0.0202,
0:          0.1084,  0.0304, -0.1521, -0.0976,  0.1249,  0.0298,  0.1360,  0.0366,  0.0748, -0.0286, -0.1176, -0.0317,
0:          0.0116], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.8831, -0.9375, -0.9464, -0.9374, -0.9200, -0.9199, -0.9273, -0.9306, -0.9295, -0.9376, -0.9668, -1.0092,
0:         -1.0288, -0.9898, -0.8908, -0.7742, -0.7017, -0.7135, -0.7982, -0.9145, -0.9975, -1.0130, -0.9598, -0.8856,
0:         -0.8268], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2388, -0.2463, -0.2568, -0.2514, -0.2416, -0.2456, -0.2508, -0.2463, -0.2444, -0.2501, -0.2459, -0.2448,
0:         -0.2400, -0.2390, -0.2368, -0.2400, -0.2447, -0.2538, -0.2391, -0.2336, -0.2353, -0.2282, -0.2263, -0.2342,
0:         -0.2322], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1493351012468338; velocity_v: 0.1671173870563507; specific_humidity: 0.1305517852306366; velocity_z: 0.5628930330276489; temperature: 0.1308733969926834; total_precip: 0.6118565201759338; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15883322060108185; velocity_v: 0.1955225169658661; specific_humidity: 0.1256476491689682; velocity_z: 0.6881454586982727; temperature: 0.17702603340148926; total_precip: 0.4955521821975708; 
0: epoch: 33 [1/5 (20%)]	Loss: 0.55370 : 0.26291 :: 0.21579 (2.61 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15733174979686737; velocity_v: 0.16541700065135956; specific_humidity: 0.1696470081806183; velocity_z: 0.5821799039840698; temperature: 0.19517648220062256; total_precip: 0.6338002681732178; 
0: epoch: 33 [2/5 (40%)]	Loss: 0.63380 : 0.27856 :: 0.21965 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15844117105007172; velocity_v: 0.19875019788742065; specific_humidity: 0.1386307030916214; velocity_z: 0.6019007563591003; temperature: 0.15608499944210052; total_precip: 0.7098426222801208; 
0: epoch: 33 [3/5 (60%)]	Loss: 0.70984 : 0.28987 :: 0.22108 (15.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1732676923274994; velocity_v: 0.18851767480373383; specific_humidity: 0.15007348358631134; velocity_z: 0.517691969871521; temperature: 0.14737266302108765; total_precip: 0.3803116977214813; 
0: epoch: 33 [4/5 (80%)]	Loss: 0.38031 : 0.22308 :: 0.21743 (15.20 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 33 : 0.2547791600227356
0: validation loss for velocity_u : 0.11236408352851868
0: validation loss for velocity_v : 0.10682635754346848
0: validation loss for specific_humidity : 0.12402715533971786
0: validation loss for velocity_z : 0.4528057277202606
0: validation loss for temperature : 0.11933096498250961
0: validation loss for total_precip : 0.6133209466934204
0: 34 : 21:41:03 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 34, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0357, -0.0204,  0.0124,  0.0206,  0.0166, -0.0242, -0.0988, -0.1426, -0.1143, -0.0599, -0.0277, -0.0159,
0:         -0.0247, -0.0435, -0.0671, -0.0947, -0.0970, -0.0689, -0.2064, -0.1620, -0.1165, -0.0885, -0.0885, -0.1321,
0:         -0.1953], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4336, 0.3718, 0.3057, 0.2627, 0.2420, 0.2370, 0.2625, 0.3000, 0.3274, 0.3501, 0.3626, 0.3574, 0.3246, 0.2560,
0:         0.1707, 0.0832, 0.0334, 0.0768, 0.5990, 0.4922, 0.4169, 0.4062, 0.4375, 0.4853, 0.5216], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.3711, 2.3291, 2.2946, 2.2561, 2.2417, 2.1897, 2.3621, 2.4302, 2.5654, 2.5155, 2.4429, 2.3388, 2.1902, 2.0472,
0:         1.9553, 1.8497, 1.8469, 2.0397, 2.4553, 2.3939, 2.3581, 2.4088, 2.4938, 2.5684, 2.6277], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6161,  1.7758,  1.5214,  0.8253,  0.7828,  0.9262,  0.8298,  0.3781,  0.0419, -0.0411, -0.1801, -0.3785,
0:         -0.2619, -0.1857, -0.2249, -0.0893, -0.0669,  0.1304,  0.0037,  0.9408,  0.9083,  0.5014,  0.0441,  0.2324,
0:          0.3826], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.3324, -0.1583,  0.0331,  0.1462,  0.1473,  0.0837,  0.0659,  0.0946,  0.1952,  0.3077,  0.4103,  0.5410,
0:          0.6237,  0.6780,  0.7067,  0.6870,  0.6783,  0.6568,  0.6394,  0.6307,  0.6320,  0.7256,  0.8365,  0.8744,
0:          0.8287], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2563, -0.2576, -0.2576, -0.2588, -0.2588, -0.2588, -0.1783, -0.1251, -0.0371, -0.2502, -0.2539, -0.2539,
0:         -0.2539, -0.2588, -0.2588, -0.2477, -0.1808, -0.0632, -0.2328, -0.2365, -0.2093, -0.1697, -0.2514, -0.2452,
0:         -0.2118], device='cuda:0')
0: [DEBUG] Epoch 34, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([7.0515,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.9313,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 4.7072,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan, 2.6589,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 6.6577,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.0595,    nan,
0:            nan,    nan,    nan, 1.6037,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:         0.8780,    nan,    nan,    nan,    nan,    nan, 7.0490,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 1.5468,    nan,    nan,    nan,
0:            nan,    nan, 1.2099,    nan,    nan,    nan,    nan, 2.7480,    nan,    nan, 0.9833,    nan,    nan,    nan,
0:         1.1171,    nan,    nan,    nan,    nan, 0.0929,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:         2.9400,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 2.2093, 1.8229, 1.2619,    nan,    nan,    nan,
0:            nan, 5.2063,    nan,    nan, 1.1356,    nan,    nan,    nan,    nan, 8.5636,    nan, 1.9567,    nan,    nan,
0:            nan,    nan, 2.0731,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 3.7586,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan, 2.2341,    nan, 2.9561,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:         0.4347,    nan, 1.4638,    nan,    nan])
0: [DEBUG] Epoch 34, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0147,  0.0353,  0.0787,  0.1123,  0.1243,  0.1385,  0.1840,  0.2606,  0.3240,  0.3719,  0.3610,  0.2937,
0:          0.2219,  0.1866,  0.2038,  0.2741,  0.3444,  0.4018, -0.0161,  0.0447,  0.0945,  0.1088,  0.0866,  0.0920,
0:          0.1293], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6271, 0.5855, 0.5313, 0.4857, 0.4482, 0.4406, 0.4414, 0.4534, 0.4576, 0.4449, 0.4272, 0.4098, 0.3945, 0.3706,
0:         0.3494, 0.3631, 0.4046, 0.4634, 0.5500, 0.5297, 0.4956, 0.4581, 0.4399, 0.4456, 0.4763], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.3963, 2.4163, 2.3844, 2.3104, 2.2050, 2.0817, 1.9741, 1.8848, 1.8109, 1.7439, 1.6887, 1.6050, 1.5350, 1.4743,
0:         1.4078, 1.3847, 1.4180, 1.4971, 2.4014, 2.4178, 2.4041, 2.3458, 2.2549, 2.1574, 2.0967], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.9364, -1.4937, -1.6121, -2.5292, -3.5930, -3.8240, -3.1857, -2.2499, -1.5093, -1.3813, -1.4211, -1.0894,
0:         -0.8435, -0.7501, -0.4210,  0.0567,  0.3463,  0.3390, -1.7955, -1.6301, -1.9975, -2.9097, -3.8240, -3.9081,
0:         -3.0601], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.0401, 0.0608, 0.1164, 0.1852, 0.2545, 0.3155, 0.3689, 0.4159, 0.4754, 0.5328, 0.5705, 0.5904, 0.6031, 0.6095,
0:         0.6108, 0.6025, 0.5912, 0.5980, 0.6430, 0.7166, 0.7943, 0.8334, 0.8177, 0.7682, 0.7159], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([5.8112, 6.7103, 7.4818, 7.8745, 7.8283, 7.3708, 6.6431, 5.7051, 4.7775, 5.7414, 6.5209, 7.2357, 7.7158, 7.6362,
0:         7.1972, 6.3402, 5.3717, 4.4768, 5.0514, 5.7495, 6.3700, 6.6304, 6.6520, 6.2560, 5.4262], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.16421079635620117; velocity_v: 0.19207535684108734; specific_humidity: 0.14887771010398865; velocity_z: 0.5900079011917114; temperature: 0.17515940964221954; total_precip: 0.6427100896835327; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17685043811798096; velocity_v: 0.21872767806053162; specific_humidity: 0.1406724452972412; velocity_z: 0.7786670327186584; temperature: 0.19643907248973846; total_precip: 0.7361996173858643; 
0: epoch: 34 [1/5 (20%)]	Loss: 0.68945 : 0.30852 :: 0.22284 (2.45 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1874261200428009; velocity_v: 0.20460999011993408; specific_humidity: 0.12228837609291077; velocity_z: 0.5240152478218079; temperature: 0.1486188769340515; total_precip: 0.6285861134529114; 
0: epoch: 34 [2/5 (40%)]	Loss: 0.62859 : 0.26686 :: 0.22161 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1515626609325409; velocity_v: 0.19280961155891418; specific_humidity: 0.1553928256034851; velocity_z: 0.6263632774353027; temperature: 0.16389931738376617; total_precip: 0.7715278267860413; 
0: epoch: 34 [3/5 (60%)]	Loss: 0.77153 : 0.30559 :: 0.21285 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14760856330394745; velocity_v: 0.15553690493106842; specific_humidity: 0.1466589868068695; velocity_z: 0.47312843799591064; temperature: 0.1466652750968933; total_precip: 0.6380109786987305; 
0: epoch: 34 [4/5 (80%)]	Loss: 0.63801 : 0.24868 :: 0.21588 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 34 : 0.28059253096580505
0: validation loss for velocity_u : 0.10224372148513794
0: validation loss for velocity_v : 0.12135710567235947
0: validation loss for specific_humidity : 0.12693935632705688
0: validation loss for velocity_z : 0.49925336241722107
0: validation loss for temperature : 0.11647996306419373
0: validation loss for total_precip : 0.7172817587852478
0: 35 : 21:45:00 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 35, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.5078, 0.5149, 0.5197, 0.5233, 0.5257, 0.5278, 0.5297, 0.5317, 0.5343, 0.5376, 0.5420, 0.5472, 0.5526, 0.5585,
0:         0.5649, 0.5717, 0.5788, 0.5861, 0.5452, 0.5459, 0.5454, 0.5444, 0.5433, 0.5428, 0.5428], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.7075, 1.7566, 1.8037, 1.8480, 1.8893, 1.9284, 1.9650, 1.9991, 2.0302, 2.0578, 2.0820, 2.1029, 2.1209, 2.1353,
0:         2.1468, 2.1558, 2.1637, 2.1710, 1.7548, 1.8007, 1.8448, 1.8873, 1.9278, 1.9660, 2.0022], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4919, -0.4807, -0.4700, -0.4615, -0.4530, -0.4482, -0.4434, -0.4405, -0.4381, -0.4365, -0.4359, -0.4354,
0:         -0.4353, -0.4352, -0.4351, -0.4349, -0.4358, -0.4371, -0.4822, -0.4718, -0.4617, -0.4561, -0.4506, -0.4480,
0:         -0.4456], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3911, 0.4043, 0.4043, 0.3999, 0.4021, 0.4043, 0.4065, 0.4021, 0.4021, 0.4109, 0.4285, 0.4439, 0.4483, 0.4461,
0:         0.4417, 0.4329, 0.4197, 0.3955, 0.3624, 0.3735, 0.3757, 0.3801, 0.3911, 0.4043, 0.4065], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([ 0.5765,  0.5592,  0.5416,  0.5242,  0.5054,  0.4858,  0.4650,  0.4434,  0.4208,  0.3966,  0.3711,  0.3438,
0:          0.3151,  0.2845,  0.2514,  0.2162,  0.1789,  0.1401,  0.0997,  0.0580,  0.0158, -0.0260, -0.0665, -0.1061,
0:         -0.1444], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2025, -0.2093, -0.2161, -0.2184, -0.2218, -0.2230, -0.2253, -0.2253, -0.2264, -0.1865, -0.1991, -0.2082,
0:         -0.2139, -0.2184, -0.2196, -0.2196, -0.2207, -0.2207, -0.1376, -0.1774, -0.2082, -0.2105, -0.2150, -0.2150,
0:         -0.2161], device='cuda:0')
0: [DEBUG] Epoch 35, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([0.3226,    nan,    nan,    nan,    nan,    nan,    nan, 0.1267,    nan,    nan,    nan,    nan,    nan,    nan,
0:         0.2007,    nan,    nan, 0.0788,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.4672,
0:            nan,    nan,    nan,    nan, 0.3157, 0.2952,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.2998,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan, 0.3043,    nan, 0.2633,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:         0.2463,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.2007,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan, 0.2554, 0.2075,    nan,    nan,    nan, 0.5652,    nan,    nan,
0:            nan,    nan,    nan,    nan, 0.2064, 0.6324,    nan,    nan, 0.4957,    nan, 0.3727,    nan,    nan, 0.2542,
0:            nan,    nan,    nan,    nan,    nan,    nan, 0.3112,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan, 0.0264,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.4741,    nan,
0:         0.5834,    nan,    nan, 0.2144,    nan, 0.1620,    nan,    nan, 0.5253,    nan,    nan,    nan,    nan,    nan,
0:         0.2725,    nan,    nan,    nan,    nan,    nan,    nan, 0.4786,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,
0:            nan,    nan,    nan,    nan,    nan])
0: [DEBUG] Epoch 35, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.7282, 0.7521, 0.7934, 0.8179, 0.8290, 0.8094, 0.7906, 0.7609, 0.7191, 0.6772, 0.6123, 0.5312, 0.4563, 0.4041,
0:         0.3847, 0.4025, 0.4374, 0.4633, 0.7820, 0.7768, 0.7920, 0.7961, 0.7744, 0.7476, 0.7159], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0234,  0.0580,  0.0835,  0.0448, -0.0175, -0.0639, -0.0924, -0.0892, -0.0619, -0.0485, -0.0319, -0.0193,
0:         -0.0155, -0.0328, -0.0496, -0.0558, -0.0394, -0.0018, -0.1362, -0.0302,  0.0278,  0.0183, -0.0202, -0.0585,
0:         -0.0641], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4009, -0.3887, -0.3731, -0.3527, -0.3324, -0.3136, -0.3124, -0.3177, -0.3333, -0.3588, -0.3858, -0.4163,
0:         -0.4412, -0.4630, -0.4883, -0.4988, -0.5150, -0.5119, -0.4257, -0.4036, -0.3842, -0.3605, -0.3430, -0.3319,
0:         -0.3301], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1167,  0.0419,  0.2841,  0.1575,  0.0120,  0.1845,  0.2162,  0.1424,  0.1422,  0.1028,  0.1069,  0.1999,
0:          0.2580,  0.1457, -0.0640, -0.0762,  0.0236, -0.0898,  0.0165, -0.0633,  0.0572,  0.0485,  0.0371,  0.2147,
0:          0.2328], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.5151, 0.4742, 0.4586, 0.4687, 0.4997, 0.5285, 0.5425, 0.5332, 0.5158, 0.5018, 0.4972, 0.5063, 0.5217, 0.5252,
0:         0.5073, 0.4662, 0.4252, 0.4172, 0.4597, 0.5287, 0.5817, 0.5864, 0.5474, 0.4982, 0.4666], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([0.2706, 0.2723, 0.2679, 0.2648, 0.2509, 0.2388, 0.2533, 0.2257, 0.2361, 0.3202, 0.3235, 0.3106, 0.3154, 0.2974,
0:         0.2807, 0.2857, 0.2904, 0.2682, 0.3371, 0.3411, 0.3784, 0.3463, 0.3616, 0.3565, 0.3329], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0: velocity_u: 0.17102885246276855; velocity_v: 0.1707693636417389; specific_humidity: 0.1516536921262741; velocity_z: 0.5217192769050598; temperature: 0.14837858080863953; total_precip: 0.5668783783912659; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1473851352930069; velocity_v: 0.18315647542476654; specific_humidity: 0.17392511665821075; velocity_z: 0.6604428291320801; temperature: 0.19060149788856506; total_precip: 0.8503782153129578; 
0: epoch: 35 [1/5 (20%)]	Loss: 0.70863 : 0.29049 :: 0.21988 (2.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1852569282054901; velocity_v: 0.1939583271741867; specific_humidity: 0.12755444645881653; velocity_z: 0.5908926129341125; temperature: 0.15898294746875763; total_precip: 0.5162903070449829; 
0: epoch: 35 [2/5 (40%)]	Loss: 0.51629 : 0.25815 :: 0.22238 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14632423222064972; velocity_v: 0.16753850877285004; specific_humidity: 0.14584578573703766; velocity_z: 0.5114288330078125; temperature: 0.14543360471725464; total_precip: 0.5650448203086853; 
0: epoch: 35 [3/5 (60%)]	Loss: 0.56504 : 0.24441 :: 0.21561 (15.25 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16773554682731628; velocity_v: 0.19466516375541687; specific_humidity: 0.15697644650936127; velocity_z: 0.5620232224464417; temperature: 0.1973997801542282; total_precip: 0.6372899413108826; 
0: epoch: 35 [4/5 (80%)]	Loss: 0.63729 : 0.28099 :: 0.22543 (15.08 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 35 : 0.25231003761291504
0: validation loss for velocity_u : 0.10258591920137405
0: validation loss for velocity_v : 0.10386810451745987
0: validation loss for specific_humidity : 0.1196870505809784
0: validation loss for velocity_z : 0.49522438645362854
0: validation loss for temperature : 0.10978101938962936
0: validation loss for total_precip : 0.5827136635780334
0: 36 : 21:48:51 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 36, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.1603, 1.1846, 1.1976, 1.1974, 1.1853, 1.1626, 1.1345, 1.1110, 1.0977, 1.0922, 1.0927, 1.0982, 1.1047, 1.1100,
0:         1.1144, 1.1166, 1.1133, 1.1029, 1.1912, 1.2094, 1.2148, 1.2069, 1.1872, 1.1585, 1.1277], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.3656, 2.4948, 2.6204, 2.7458, 2.8769, 3.0180, 3.1665, 3.3117, 3.4380, 3.5323, 3.5905, 3.6161, 3.6125, 3.5755,
0:         3.4915, 3.3473, 3.1366, 2.8653, 2.4197, 2.5436, 2.6663, 2.7917, 2.9254, 3.0707, 3.2229], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4572, -0.4692, -0.4796, -0.4864, -0.4869, -0.4883, -0.4941, -0.5243, -0.5632, -0.6208, -0.6844, -0.7409,
0:         -0.7879, -0.8082, -0.8216, -0.8235, -0.8209, -0.8167, -0.4481, -0.4590, -0.4670, -0.4731, -0.4741, -0.4831,
0:         -0.4906], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6938, 0.6549, 0.5306, 0.3760, 0.2949, 0.3371, 0.5122, 0.7695, 0.9911, 1.1402, 1.2635, 1.3651, 1.4299, 1.4710,
0:         1.4472, 1.2970, 1.0451, 0.7338, 0.5576, 0.5587, 0.4917, 0.4009, 0.3522, 0.3706, 0.4938], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2319, -0.2489, -0.2684, -0.2894, -0.3085, -0.3198, -0.3178, -0.3037, -0.2852, -0.2712, -0.2690, -0.2863,
0:         -0.3318, -0.4127, -0.5332, -0.6949, -0.8942, -1.1211, -1.3603, -1.5914, -1.7937, -1.9558, -2.0784, -2.1689,
0:         -2.2348], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1594, -0.1346, -0.1391, -0.1695, -0.1583, -0.1673, -0.1797, -0.1009,  0.0195, -0.1875, -0.1504, -0.1560,
0:         -0.1965, -0.1695, -0.1324, -0.1493, -0.0593, -0.0683, -0.1628, -0.1819, -0.1898, -0.1842, -0.1808, -0.1391,
0:         -0.0705], device='cuda:0')
0: [DEBUG] Epoch 36, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2438,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2460,
0:             nan, -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2460,
0:         -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2033, -0.1853,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2055,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2460,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2460,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2438,     nan, -0.2438,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2438,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2449,     nan,     nan,     nan,     nan, -0.2460,     nan, -0.2460, -0.2460, -0.2460,
0:         -0.2460, -0.2460,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2460,     nan,     nan,     nan,     nan,     nan,     nan, -0.2460,     nan, -0.2460,     nan,     nan,
0:         -0.2460,     nan,     nan])
0: [DEBUG] Epoch 36, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4242, 1.3504, 1.2999, 1.2755, 1.2789, 1.2633, 1.2645, 1.2266, 1.1759, 1.1379, 1.0879, 1.0707, 1.0582, 1.0651,
0:         1.0669, 1.0666, 1.0802, 1.0850, 1.4307, 1.3303, 1.2742, 1.2548, 1.2512, 1.2586, 1.2599], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0815,  0.1160,  0.1319,  0.1232,  0.1086,  0.0960,  0.0805,  0.0710,  0.0640,  0.0625,  0.0688,  0.0777,
0:          0.0872,  0.0710,  0.0561,  0.0538,  0.0944,  0.1666, -0.0212,  0.0366,  0.0714,  0.0781,  0.0749,  0.0719,
0:          0.0680], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5651, -0.5581, -0.5438, -0.5203, -0.4994, -0.4883, -0.4889, -0.4970, -0.5140, -0.5285, -0.5373, -0.5465,
0:         -0.5418, -0.5357, -0.5354, -0.5304, -0.5339, -0.5298, -0.6040, -0.5933, -0.5783, -0.5519, -0.5325, -0.5195,
0:         -0.5179], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0148,  0.0667,  0.2273,  0.1907,  0.0451,  0.0922,  0.1354,  0.1301,  0.1806,  0.1781,  0.2123,  0.3100,
0:          0.3591,  0.3817,  0.3388,  0.3688,  0.5266,  0.5003,  0.0282, -0.0296,  0.0047, -0.0278, -0.0427,  0.0704,
0:          0.1040], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.7332, 0.6821, 0.6437, 0.6271, 0.6361, 0.6517, 0.6577, 0.6400, 0.6104, 0.5774, 0.5433, 0.5120, 0.4798, 0.4381,
0:         0.3833, 0.3187, 0.2626, 0.2411, 0.2591, 0.2993, 0.3269, 0.3203, 0.2873, 0.2573, 0.2452], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2215, -0.2293, -0.2401, -0.2366, -0.2292, -0.2338, -0.2385, -0.2302, -0.2302, -0.2317, -0.2273, -0.2261,
0:         -0.2257, -0.2301, -0.2287, -0.2289, -0.2316, -0.2353, -0.2182, -0.2142, -0.2174, -0.2137, -0.2142, -0.2254,
0:         -0.2230], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.14199517667293549; velocity_v: 0.15804634988307953; specific_humidity: 0.17320851981639862; velocity_z: 0.5425494909286499; temperature: 0.14201802015304565; total_precip: 0.6870037913322449; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15652450919151306; velocity_v: 0.17946916818618774; specific_humidity: 0.12836475670337677; velocity_z: 0.5593717694282532; temperature: 0.12600630521774292; total_precip: 0.5108966827392578; 
0: epoch: 36 [1/5 (20%)]	Loss: 0.59895 : 0.25562 :: 0.21559 (2.21 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16332457959651947; velocity_v: 0.15956322848796844; specific_humidity: 0.13779722154140472; velocity_z: 0.4663006663322449; temperature: 0.15120086073875427; total_precip: 0.6418069005012512; 
0: epoch: 36 [2/5 (40%)]	Loss: 0.64181 : 0.25049 :: 0.21701 (15.02 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14846564829349518; velocity_v: 0.17272667586803436; specific_humidity: 0.14616242051124573; velocity_z: 0.5828700661659241; temperature: 0.14154918491840363; total_precip: 0.7043318748474121; 
0: epoch: 36 [3/5 (60%)]	Loss: 0.70433 : 0.27931 :: 0.21973 (14.78 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16156397759914398; velocity_v: 0.17299897968769073; specific_humidity: 0.13274715840816498; velocity_z: 0.47099870443344116; temperature: 0.14050999283790588; total_precip: 0.5240492820739746; 
0: epoch: 36 [4/5 (80%)]	Loss: 0.52405 : 0.23130 :: 0.21590 (15.09 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 36 : 0.2566194236278534
0: validation loss for velocity_u : 0.11315557360649109
0: validation loss for velocity_v : 0.11642611026763916
0: validation loss for specific_humidity : 0.11709357053041458
0: validation loss for velocity_z : 0.5082821846008301
0: validation loss for temperature : 0.11442146450281143
0: validation loss for total_precip : 0.5703379511833191
0: 37 : 21:52:51 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 37, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2298, 0.2171, 0.2042, 0.1885, 0.1704, 0.1497, 0.1273, 0.1070, 0.0894, 0.0772, 0.0708, 0.0705, 0.0772, 0.0900,
0:         0.1087, 0.1321, 0.1593, 0.1906, 0.1839, 0.1685, 0.1536, 0.1378, 0.1213, 0.1046, 0.0894], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1058, 0.1269, 0.1472, 0.1633, 0.1771, 0.1864, 0.1938, 0.1996, 0.2036, 0.2099, 0.2155, 0.2223, 0.2286, 0.2334,
0:         0.2370, 0.2394, 0.2412, 0.2400, 0.0861, 0.1060, 0.1221, 0.1343, 0.1446, 0.1526, 0.1605], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.4511, 0.4546, 0.4759, 0.4994, 0.5261, 0.5607, 0.5726, 0.5867, 0.5742, 0.5587, 0.5335, 0.5035, 0.4692, 0.4367,
0:         0.4120, 0.3964, 0.3873, 0.3795, 0.5298, 0.5411, 0.5704, 0.5926, 0.6113, 0.6153, 0.6255], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.8152,  0.7464,  0.6580,  0.6099,  0.5892,  0.5509,  0.5258,  0.4581,  0.3882,  0.3380,  0.2364,  0.1829,
0:          0.0911,  0.0354, -0.0028, -0.0356, -0.0421, -0.1011,  0.6896,  0.6023,  0.5575,  0.5477,  0.5564,  0.5673,
0:          0.5487], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.5405, -0.5085, -0.4807, -0.4537, -0.4263, -0.3918, -0.3515, -0.3017, -0.2513, -0.2017, -0.1565, -0.1221,
0:         -0.0906, -0.0696, -0.0560, -0.0501, -0.0496, -0.0504, -0.0530, -0.0521, -0.0526, -0.0476, -0.0388, -0.0283,
0:         -0.0149], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2167, -0.2363, -0.2363, -0.2375, -0.2375, -0.2375, -0.2375, -0.2375, -0.2375, -0.2213, -0.2259, -0.2317,
0:         -0.2375, -0.2375, -0.2375, -0.2375, -0.2375, -0.2375, -0.0943, -0.2225, -0.2306, -0.2375, -0.2375, -0.2375,
0:         -0.2375], device='cuda:0')
0: [DEBUG] Epoch 37, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1060,     nan,     nan, -0.2222,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2278,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2142,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2370,     nan,     nan,     nan, -0.1151,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2199,     nan,     nan, -0.1948,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2165,     nan,     nan,     nan,     nan,     nan, -0.2381,
0:             nan,     nan,     nan,     nan,     nan, -0.2370,     nan,     nan,     nan, -0.2381,     nan,     nan,
0:         -0.2130,     nan,     nan, -0.2165,     nan,     nan,     nan,     nan,     nan, -0.2153,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2267, -0.2256,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0935,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2165, -0.1527, -0.1163,     nan,     nan,     nan,     nan, -0.2381,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2005,     nan, -0.2210,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 37, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3504, -0.3364, -0.3473, -0.3653, -0.3949, -0.4142, -0.3744, -0.2914, -0.2257, -0.1895, -0.2336, -0.3495,
0:         -0.4723, -0.5278, -0.4977, -0.3832, -0.2817, -0.2198, -0.3309, -0.3197, -0.3237, -0.3513, -0.4158, -0.4333,
0:         -0.4039], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2622, 0.2943, 0.2929, 0.2808, 0.2590, 0.2406, 0.2191, 0.2052, 0.1884, 0.1707, 0.1522, 0.1419, 0.1369, 0.1179,
0:         0.1035, 0.1246, 0.1767, 0.2484, 0.2147, 0.2659, 0.2805, 0.2731, 0.2561, 0.2375, 0.2238], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4760, -0.4749, -0.4804, -0.4828, -0.4865, -0.4942, -0.5041, -0.5158, -0.5207, -0.5243, -0.5165, -0.5071,
0:         -0.4876, -0.4694, -0.4681, -0.4677, -0.4815, -0.4912, -0.4739, -0.4713, -0.4724, -0.4721, -0.4770, -0.4883,
0:         -0.4909], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1969, 0.2008, 0.3570, 0.3180, 0.1631, 0.2266, 0.1529, 0.0354, 0.1345, 0.0894, 0.0970, 0.2707, 0.2431, 0.2803,
0:         0.2981, 0.3164, 0.5499, 0.4775, 0.1812, 0.0606, 0.1015, 0.0821, 0.0197, 0.1284, 0.1368], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.3078, -0.3011, -0.3014, -0.3226, -0.3399, -0.3471, -0.3385, -0.3201, -0.3023, -0.2907, -0.2928, -0.3077,
0:         -0.3325, -0.3557, -0.3697, -0.3645, -0.3474, -0.3136, -0.2718, -0.2278, -0.1766, -0.1259, -0.0698, -0.0210,
0:          0.0050], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1935, -0.2024, -0.2114, -0.2132, -0.2122, -0.2101, -0.2170, -0.2192, -0.2228, -0.2040, -0.2044, -0.2085,
0:         -0.2070, -0.2110, -0.2154, -0.2153, -0.2204, -0.2302, -0.2075, -0.2015, -0.2067, -0.2037, -0.2064, -0.2145,
0:         -0.2163], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1464272439479828; velocity_v: 0.18753281235694885; specific_humidity: 0.1356162279844284; velocity_z: 0.5890920758247375; temperature: 0.16608351469039917; total_precip: 0.5828878879547119; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16501156985759735; velocity_v: 0.1650812327861786; specific_humidity: 0.13601164519786835; velocity_z: 0.6851288080215454; temperature: 0.1553678661584854; total_precip: 0.5062164664268494; 
0: epoch: 37 [1/5 (20%)]	Loss: 0.54455 : 0.26403 :: 0.22340 (2.39 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14897660911083221; velocity_v: 0.17219571769237518; specific_humidity: 0.14659781754016876; velocity_z: 0.5452707409858704; temperature: 0.19188569486141205; total_precip: 0.5494868159294128; 
0: epoch: 37 [2/5 (40%)]	Loss: 0.54949 : 0.25526 :: 0.21788 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15037497878074646; velocity_v: 0.15970450639724731; specific_humidity: 0.1533835232257843; velocity_z: 0.5954874157905579; temperature: 0.16682489216327667; total_precip: 0.8534795641899109; 
0: epoch: 37 [3/5 (60%)]	Loss: 0.85348 : 0.30894 :: 0.22132 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16598916053771973; velocity_v: 0.18945324420928955; specific_humidity: 0.1296520233154297; velocity_z: 0.5149003267288208; temperature: 0.15259931981563568; total_precip: 0.5618530511856079; 
0: epoch: 37 [4/5 (80%)]	Loss: 0.56185 : 0.24931 :: 0.22087 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 37 : 0.24482271075248718
0: validation loss for velocity_u : 0.1031390130519867
0: validation loss for velocity_v : 0.11803188920021057
0: validation loss for specific_humidity : 0.11809196323156357
0: validation loss for velocity_z : 0.5346400141716003
0: validation loss for temperature : 0.1129278838634491
0: validation loss for total_precip : 0.48210546374320984
0: 38 : 21:56:47 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 38, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6957, 0.7114, 0.7528, 0.8015, 0.8543, 0.9010, 0.9251, 0.9372, 0.9852, 1.0763, 1.1142, 1.0359, 0.9190, 0.8460,
0:         0.8157, 0.8074, 0.7936, 0.7482, 0.7176, 0.7135, 0.7330, 0.7724, 0.8226, 0.8558, 0.8695], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3253, 1.3098, 1.2953, 1.2770, 1.2418, 1.1997, 1.1666, 1.1419, 1.1123, 1.0814, 1.0690, 1.0721, 1.0785, 1.0810,
0:         1.0694, 1.0516, 1.0383, 1.0087, 1.3183, 1.3058, 1.2866, 1.2563, 1.2184, 1.1873, 1.1583], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4514, -0.5405, -0.5792, -0.5842, -0.5927, -0.6048, -0.6197, -0.6362, -0.6500, -0.6591, -0.6602, -0.6576,
0:         -0.6497, -0.6273, -0.6068, -0.5991, -0.6078, -0.5966, -0.3821, -0.4836, -0.5519, -0.5727, -0.5830, -0.5973,
0:         -0.6131], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 3.1498e-01,  2.8144e-01,  3.7981e-01,  9.2531e-01,  1.2517e+00,  7.9788e-01, -2.4708e-04, -5.4798e-01,
0:         -2.3722e-01,  1.8039e+00,  4.2251e+00,  3.4985e+00,  5.1173e-02, -1.6345e+00, -5.8822e-01,  4.3347e-01,
0:          4.6700e-01,  2.5238e-01,  1.4283e-01,  1.5401e-01,  2.0767e-01,  5.9667e-01,  9.8343e-01,  7.8893e-01,
0:          1.7860e-01], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([1.0941, 1.0853, 1.0867, 1.1161, 1.1710, 1.2214, 1.2451, 1.2379, 1.2363, 1.3290, 1.5423, 1.7386, 1.7555, 1.6204,
0:         1.5037, 1.4992, 1.5447, 1.5431, 1.4847, 1.4125, 1.3466, 1.2834, 1.2279, 1.1989, 1.2097], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513,
0:         -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513, -0.2513,
0:         -0.2513], device='cuda:0')
0: [DEBUG] Epoch 38, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1480,     nan,     nan,     nan,     nan,
0:             nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2513,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,
0:             nan,     nan,     nan,     nan, -0.2278,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2384,
0:         -0.2513,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,
0:             nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,
0:             nan,     nan,     nan,  1.3339,     nan, -0.1632,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2513,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2513,     nan,     nan, -0.2490,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2513,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,  0.0446,     nan,     nan,     nan,  0.5073,  0.8408,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 38, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.0641, 2.0434, 2.0482, 2.0721, 2.1070, 2.1154, 2.1349, 2.1241, 2.1043, 2.0855, 2.0508, 2.0385, 2.0242, 2.0191,
0:         2.0244, 2.0154, 2.0051, 1.9778, 2.1360, 2.0797, 2.0533, 2.0650, 2.0926, 2.1241, 2.1569], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7104, -0.6787, -0.6923, -0.7466, -0.8226, -0.8870, -0.9567, -0.9914, -1.0001, -0.9966, -0.9587, -0.9010,
0:         -0.8435, -0.8032, -0.7584, -0.6987, -0.6078, -0.4832, -0.7927, -0.7275, -0.7153, -0.7495, -0.8197, -0.9049,
0:         -0.9717], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0203,  0.0309,  0.0527,  0.0706,  0.0893,  0.0985,  0.1002,  0.1079,  0.1186,  0.1481,  0.1889,  0.2354,
0:          0.2901,  0.3443,  0.3888,  0.4428,  0.4889,  0.5429, -0.0203, -0.0114,  0.0056,  0.0334,  0.0432,  0.0511,
0:          0.0589], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1747,  0.4235,  0.7227,  0.8282,  0.2489, -0.7019, -1.1805, -1.1625, -1.0396, -1.0135, -1.1445, -1.3919,
0:         -1.6838, -1.8146, -1.7049, -1.4520, -1.4157, -1.6321,  0.0889,  0.1792,  0.3448,  0.3710, -0.0201, -0.4927,
0:         -0.6313], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.3489, 0.3599, 0.3799, 0.4197, 0.4816, 0.5489, 0.6140, 0.6658, 0.7048, 0.7375, 0.7692, 0.8022, 0.8300, 0.8511,
0:         0.8485, 0.8271, 0.7979, 0.7930, 0.8259, 0.8948, 0.9692, 1.0212, 1.0443, 1.0454, 1.0482], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1300, -0.1297, -0.1342, -0.1286, -0.1308, -0.1317, -0.1395, -0.1584, -0.1618, -0.1343, -0.1339, -0.1486,
0:         -0.1347, -0.1389, -0.1510, -0.1502, -0.1577, -0.1517, -0.1384, -0.1465, -0.1445, -0.1443, -0.1447, -0.1500,
0:         -0.1453], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15436741709709167; velocity_v: 0.16695654392242432; specific_humidity: 0.14456728100776672; velocity_z: 0.6104292869567871; temperature: 0.15769141912460327; total_precip: 0.8764423727989197; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17366448044776917; velocity_v: 0.19370940327644348; specific_humidity: 0.13131822645664215; velocity_z: 0.6683701872825623; temperature: 0.15612006187438965; total_precip: 0.6061064600944519; 
0: epoch: 38 [1/5 (20%)]	Loss: 0.74127 : 0.29941 :: 0.22811 (2.47 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17386987805366516; velocity_v: 0.18716906011104584; specific_humidity: 0.14421319961547852; velocity_z: 0.7207180261611938; temperature: 0.1615707129240036; total_precip: 0.9034412503242493; 
0: epoch: 38 [2/5 (40%)]	Loss: 0.90344 : 0.34483 :: 0.22270 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15181055665016174; velocity_v: 0.1661434918642044; specific_humidity: 0.14760810136795044; velocity_z: 0.6638933420181274; temperature: 0.1798950731754303; total_precip: 1.1267437934875488; 
0: epoch: 38 [3/5 (60%)]	Loss: 1.12674 : 0.36879 :: 0.21701 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16878527402877808; velocity_v: 0.19206836819648743; specific_humidity: 0.15045300126075745; velocity_z: 0.6424109935760498; temperature: 0.21666887402534485; total_precip: 0.8720414042472839; 
0: epoch: 38 [4/5 (80%)]	Loss: 0.87204 : 0.33508 :: 0.22003 (15.10 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 38 : 0.25679051876068115
0: validation loss for velocity_u : 0.10413642972707748
0: validation loss for velocity_v : 0.11437755823135376
0: validation loss for specific_humidity : 0.1157483160495758
0: validation loss for velocity_z : 0.5126197338104248
0: validation loss for temperature : 0.10789841413497925
0: validation loss for total_precip : 0.5859627723693848
0: 39 : 22:00:41 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 39, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2368, 0.2375, 0.2362, 0.2326, 0.2272, 0.2207, 0.2139, 0.2076, 0.2030, 0.2007, 0.2013, 0.2051, 0.2124, 0.2238,
0:         0.2399, 0.2608, 0.2859, 0.3140, 0.2019, 0.2051, 0.2061, 0.2051, 0.2024, 0.1985, 0.1944], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.5597, 2.4888, 2.4171, 2.3460, 2.2771, 2.2120, 2.1523, 2.0993, 2.0554, 2.0214, 1.9980, 1.9847, 1.9803, 1.9826,
0:         1.9907, 2.0037, 2.0206, 2.0402, 2.5784, 2.5027, 2.4263, 2.3516, 2.2803, 2.2146, 2.1561], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.7547, -0.7478, -0.7394, -0.7307, -0.7217, -0.7119, -0.7030, -0.6949, -0.6871, -0.6825, -0.6783, -0.6760,
0:         -0.6754, -0.6753, -0.6773, -0.6793, -0.6853, -0.6918, -0.7513, -0.7439, -0.7366, -0.7281, -0.7197, -0.7119,
0:         -0.7038], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.3220, 0.3186, 0.3254, 0.3445, 0.3613, 0.3647, 0.3568, 0.3478, 0.3467, 0.3546, 0.3591, 0.3557, 0.3467, 0.3355,
0:         0.3220, 0.3074, 0.2906, 0.2748, 0.4051, 0.3894, 0.3793, 0.3804, 0.3894, 0.3928, 0.3838], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.7451, -0.8141, -0.8804, -0.9433, -1.0018, -1.0553, -1.1036, -1.1464, -1.1831, -1.2136, -1.2379, -1.2562,
0:         -1.2690, -1.2768, -1.2801, -1.2803, -1.2780, -1.2754, -1.2730, -1.2729, -1.2758, -1.2829, -1.2963, -1.3164,
0:         -1.3447], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2550, -0.2550, -0.2538, -0.2538, -0.2527, -0.2515, -0.2468, -0.2433, -0.2398, -0.2550, -0.2538, -0.2538,
0:         -0.2527, -0.2515, -0.2480, -0.2398, -0.2328, -0.2317, -0.2550, -0.2538, -0.2538, -0.2515, -0.2492, -0.2445,
0:         -0.2387], device='cuda:0')
0: [DEBUG] Epoch 39, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2224,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2247,     nan,     nan,     nan, -0.2189,     nan,     nan,     nan,
0:             nan,     nan, -0.2282, -0.2270,     nan,     nan, -0.2282,     nan,     nan,     nan, -0.2235,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2165,     nan,     nan,     nan,     nan,     nan,
0:         -0.2247,     nan,     nan,     nan,     nan, -0.2107,     nan,     nan,     nan, -0.2107,     nan,     nan,
0:             nan,     nan, -0.1979, -0.1979,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1745,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2200,     nan,
0:         -0.2247,     nan,     nan,     nan, -0.2165, -0.2189,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2235,     nan,     nan,
0:             nan, -0.2259,     nan,     nan,     nan,     nan,     nan, -0.2212,     nan, -0.2200,     nan,     nan,
0:             nan,     nan,     nan, -0.2177,     nan,     nan,     nan, -0.2177,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2037,     nan,
0:             nan,     nan, -0.1990, -0.2002,     nan,     nan, -0.2060,     nan,     nan, -0.1920,     nan,     nan,
0:             nan,     nan,     nan, -0.2165, -0.2130, -0.2107,     nan,     nan,     nan,     nan, -0.2165,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2177,     nan,     nan,
0:         -0.2235,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2235,     nan,
0:             nan,     nan,     nan,     nan, -0.2247,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2177,     nan,     nan, -0.2130,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2142,     nan,     nan,     nan,     nan,     nan,     nan, -0.2119,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 39, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9480, 0.9439, 0.9539, 0.9811, 1.0135, 1.0305, 1.0479, 1.0350, 1.0031, 0.9794, 0.9623, 0.9685, 1.0046, 1.0619,
0:         1.1353, 1.2065, 1.2736, 1.3227, 0.9959, 0.9696, 0.9700, 0.9801, 0.9913, 1.0029, 1.0080], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.1028, 1.1252, 1.1182, 1.0852, 1.0251, 0.9771, 0.9340, 0.9034, 0.8710, 0.8293, 0.7752, 0.7186, 0.6626, 0.5996,
0:         0.5444, 0.5027, 0.4876, 0.4958, 1.1503, 1.1847, 1.1852, 1.1465, 1.0907, 1.0509, 1.0265], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7968, -0.7969, -0.7912, -0.7804, -0.7655, -0.7520, -0.7491, -0.7448, -0.7474, -0.7487, -0.7501, -0.7507,
0:         -0.7445, -0.7390, -0.7426, -0.7444, -0.7522, -0.7502, -0.7928, -0.7854, -0.7729, -0.7564, -0.7444, -0.7414,
0:         -0.7387], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0098,  0.0266,  0.1805,  0.0484, -0.1539, -0.1233, -0.1891, -0.3031, -0.3213, -0.4424, -0.4631, -0.2363,
0:         -0.1118, -0.2023, -0.3289, -0.2590, -0.0646, -0.1067,  0.1470,  0.0515,  0.0843, -0.0461, -0.1725, -0.1167,
0:         -0.1448], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0885, -0.1135, -0.1219, -0.1262, -0.1222, -0.1234, -0.1217, -0.1164, -0.1084, -0.1016, -0.1042, -0.1107,
0:         -0.1051, -0.0764, -0.0231,  0.0364,  0.0785,  0.0914,  0.0818,  0.0647,  0.0548,  0.0638,  0.0897,  0.1209,
0:          0.1499], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2208, -0.2234, -0.2234, -0.2223, -0.2218, -0.2171, -0.2306, -0.2302, -0.2415, -0.2226, -0.2151, -0.2149,
0:         -0.2083, -0.2112, -0.2215, -0.2233, -0.2244, -0.2332, -0.2168, -0.2083, -0.2083, -0.2033, -0.1974, -0.2074,
0:         -0.2141], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15968157351016998; velocity_v: 0.18884645402431488; specific_humidity: 0.13680601119995117; velocity_z: 0.6608451008796692; temperature: 0.15620848536491394; total_precip: 0.7989822626113892; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1532410830259323; velocity_v: 0.17595265805721283; specific_humidity: 0.14165502786636353; velocity_z: 0.6046685576438904; temperature: 0.14149251580238342; total_precip: 0.7003018856048584; 
0: epoch: 39 [1/5 (20%)]	Loss: 0.74964 : 0.29806 :: 0.21804 (2.38 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1549527496099472; velocity_v: 0.1881040781736374; specific_humidity: 0.14282913506031036; velocity_z: 0.7067379951477051; temperature: 0.14915236830711365; total_precip: 0.9195238351821899; 
0: epoch: 39 [2/5 (40%)]	Loss: 0.91952 : 0.33939 :: 0.22345 (14.73 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14769087731838226; velocity_v: 0.1766548901796341; specific_humidity: 0.1325734704732895; velocity_z: 0.6765442490577698; temperature: 0.14246425032615662; total_precip: 0.41538527607917786; 
0: epoch: 39 [3/5 (60%)]	Loss: 0.41539 : 0.24526 :: 0.21752 (15.30 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1520988494157791; velocity_v: 0.18136155605316162; specific_humidity: 0.1525658667087555; velocity_z: 0.6145387291908264; temperature: 0.15018992125988007; total_precip: 0.7376806139945984; 
0: epoch: 39 [4/5 (80%)]	Loss: 0.73768 : 0.29409 :: 0.21971 (14.60 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 39 : 0.3274097740650177
0: validation loss for velocity_u : 0.12516628205776215
0: validation loss for velocity_v : 0.11653365939855576
0: validation loss for specific_humidity : 0.12306582927703857
0: validation loss for velocity_z : 0.6017559170722961
0: validation loss for temperature : 0.1190328523516655
0: validation loss for total_precip : 0.8789039254188538
0: 40 : 22:04:39 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 40, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3185, 1.2877, 1.2659, 1.2463, 1.2219, 1.1883, 1.1446, 1.0950, 1.0464, 1.0049, 0.9747, 0.9564, 0.9480, 0.9457,
0:         0.9469, 0.9501, 0.9551, 0.9598, 1.3295, 1.2939, 1.2660, 1.2383, 1.2042, 1.1605, 1.1093], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.3463, 1.3181, 1.2772, 1.2227, 1.1571, 1.0870, 1.0221, 0.9719, 0.9443, 0.9464, 0.9846, 1.0634, 1.1812, 1.3304,
0:         1.5003, 1.6799, 1.8596, 2.0317, 1.2554, 1.2172, 1.1631, 1.0963, 1.0233, 0.9538, 0.8983], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0636, -0.0864, -0.1091, -0.1342, -0.1595, -0.1910, -0.2224, -0.2802, -0.3382, -0.4212, -0.5044, -0.5750,
0:         -0.6455, -0.6841, -0.7228, -0.7342, -0.7455, -0.7466, -0.0885, -0.1108, -0.1333, -0.1598, -0.1861, -0.2210,
0:         -0.2571], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.1461,  0.2404,  0.2254,  0.1182, -0.0233, -0.1347, -0.1819, -0.1669, -0.0919,  0.0368,  0.1975,  0.3840,
0:          0.6005,  0.8513,  1.0892,  1.2457,  1.2992,  1.2842,  0.1332,  0.1675,  0.0946, -0.0383, -0.1604, -0.2226,
0:         -0.2162], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.9603, 0.9058, 0.8438, 0.7701, 0.6842, 0.5897, 0.4937, 0.4057, 0.3337, 0.2824, 0.2524, 0.2414, 0.2461, 0.2617,
0:         0.2841, 0.3102, 0.3365, 0.3607, 0.3812, 0.3985, 0.4140, 0.4285, 0.4426, 0.4554, 0.4661], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2345, -0.2392, -0.2415, -0.2415, -0.2438, -0.2461, -0.2461, -0.2461, -0.2438, -0.2369, -0.2392, -0.2438,
0:         -0.2299, -0.2183, -0.2276, -0.2369, -0.1952, -0.1651, -0.2392, -0.2415, -0.2461, -0.2392, -0.2369, -0.1906,
0:         -0.1697], device='cuda:0')
0: [DEBUG] Epoch 40, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.2341,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2322,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0343,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0031,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.1300,  0.0316, -0.0297,     nan, -0.1443,     nan,
0:             nan,     nan,     nan, -0.0783,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0930,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.3545,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,  0.1636,  0.0837,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1813,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1570,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 40, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([2.8880, 2.7975, 2.7045, 2.6425, 2.6205, 2.5905, 2.5743, 2.5076, 2.4244, 2.3478, 2.2866, 2.2959, 2.3192, 2.3735,
0:         2.4014, 2.4001, 2.4184, 2.4302, 2.9046, 2.7940, 2.6992, 2.6490, 2.6326, 2.6435, 2.6384], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.2511, 1.2500, 1.2310, 1.2128, 1.2158, 1.2658, 1.3379, 1.4289, 1.5131, 1.5807, 1.6376, 1.6791, 1.7144, 1.7261,
0:         1.7339, 1.7532, 1.7940, 1.8498, 1.3151, 1.3240, 1.3134, 1.3067, 1.3268, 1.3929, 1.4925], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.3368,  0.3219,  0.3048,  0.2864,  0.2752,  0.2589,  0.2356,  0.2115,  0.1745,  0.1341,  0.0915,  0.0422,
0:         -0.0035, -0.0460, -0.0891, -0.1109, -0.1295, -0.1177,  0.2744,  0.2696,  0.2518,  0.2420,  0.2218,  0.1951,
0:          0.1719], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.1053,  0.1000,  0.2804,  0.1937,  0.0742,  0.0975,  0.0312, -0.0402,  0.0197,  0.0583,  0.0536,  0.1374,
0:          0.2665,  0.2680,  0.2031,  0.1742,  0.1627,  0.0881, -0.0121, -0.0188,  0.0273, -0.0131,  0.0074,  0.0989,
0:          0.0442], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([2.4056, 2.3317, 2.2797, 2.2958, 2.3712, 2.4656, 2.5345, 2.5504, 2.5300, 2.5086, 2.5012, 2.5181, 2.5315, 2.5130,
0:         2.4232, 2.2769, 2.1237, 2.0403, 2.0582, 2.1435, 2.2069, 2.1888, 2.0951, 1.9863, 1.9318], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0994, -0.1007, -0.1071, -0.1145, -0.1136, -0.1163, -0.1201, -0.1179, -0.1247, -0.1134, -0.1070, -0.1131,
0:         -0.1142, -0.1215, -0.1244, -0.1262, -0.1208, -0.1258, -0.1309, -0.1221, -0.1237, -0.1222, -0.1205, -0.1256,
0:         -0.1269], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.18059904873371124; velocity_v: 0.20222781598567963; specific_humidity: 0.14498774707317352; velocity_z: 0.6473314762115479; temperature: 0.1480395495891571; total_precip: 0.7531219720840454; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14319586753845215; velocity_v: 0.17621950805187225; specific_humidity: 0.1495710015296936; velocity_z: 0.587775707244873; temperature: 0.1573750376701355; total_precip: 0.7570502758026123; 
0: epoch: 40 [1/5 (20%)]	Loss: 0.75509 : 0.30013 :: 0.21669 (2.62 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1730000525712967; velocity_v: 0.2113180309534073; specific_humidity: 0.15214316546916962; velocity_z: 0.6963625550270081; temperature: 0.17332376539707184; total_precip: 0.6980008482933044; 
0: epoch: 40 [2/5 (40%)]	Loss: 0.69800 : 0.31254 :: 0.22200 (15.26 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15133626759052277; velocity_v: 0.1897902488708496; specific_humidity: 0.12590090930461884; velocity_z: 0.579096794128418; temperature: 0.14357241988182068; total_precip: 0.609331488609314; 
0: epoch: 40 [3/5 (60%)]	Loss: 0.60933 : 0.26439 :: 0.21156 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16537508368492126; velocity_v: 0.19710837304592133; specific_humidity: 0.12089125066995621; velocity_z: 0.6431356072425842; temperature: 0.13339395821094513; total_precip: 0.7571752667427063; 
0: epoch: 40 [4/5 (80%)]	Loss: 0.75718 : 0.30015 :: 0.21616 (15.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 40 : 0.28491464257240295
0: validation loss for velocity_u : 0.10425441712141037
0: validation loss for velocity_v : 0.12512190639972687
0: validation loss for specific_humidity : 0.11722106486558914
0: validation loss for velocity_z : 0.5617898106575012
0: validation loss for temperature : 0.10902876406908035
0: validation loss for total_precip : 0.6920716166496277
0: 41 : 22:08:37 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 41, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0208, -0.0237, -0.0263, -0.0255, -0.0199, -0.0097,  0.0041,  0.0204,  0.0375,  0.0521,  0.0612,  0.0644,
0:          0.0632,  0.0585,  0.0513,  0.0427,  0.0350,  0.0304, -0.0401, -0.0494, -0.0611, -0.0704, -0.0730, -0.0677,
0:         -0.0560], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.6326,  0.5474,  0.4619,  0.3779,  0.2947,  0.2119,  0.1308,  0.0531, -0.0222, -0.0982, -0.1777, -0.2604,
0:         -0.3449, -0.4295, -0.5119, -0.5881, -0.6538, -0.7083,  0.6462,  0.5599,  0.4717,  0.3852,  0.3018,  0.2205,
0:          0.1404], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5880, -0.5832, -0.5778, -0.5777, -0.5804, -0.5839, -0.5860, -0.5861, -0.5831, -0.5776, -0.5741, -0.5690,
0:         -0.5592, -0.5546, -0.5522, -0.5500, -0.5444, -0.5371, -0.5800, -0.5755, -0.5725, -0.5770, -0.5807, -0.5849,
0:         -0.5885], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.6558, -0.6383, -0.6383, -0.6492, -0.6830, -0.7592, -0.8529, -0.8975, -0.8648, -0.7908, -0.7342, -0.7037,
0:         -0.6710, -0.6296, -0.6057, -0.6220, -0.6721, -0.7363, -0.6111, -0.5665, -0.5545, -0.5708, -0.6057, -0.6667,
0:         -0.7440], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0478, -0.0439, -0.0374, -0.0312, -0.0298, -0.0352, -0.0470, -0.0614, -0.0743, -0.0838, -0.0896, -0.0932,
0:         -0.0964, -0.1004, -0.1059, -0.1128, -0.1210, -0.1292, -0.1373, -0.1463, -0.1568, -0.1679, -0.1774, -0.1842,
0:         -0.1883], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2372, -0.2360, -0.2349, -0.2326, -0.2360, -0.2360, -0.2360, -0.2383, -0.2372, -0.2280, -0.2269, -0.2315,
0:         -0.2360, -0.2418, -0.2418, -0.2418, -0.2418, -0.2395, -0.2166, -0.2235, -0.2349, -0.2372, -0.2395, -0.2395,
0:         -0.2395], device='cuda:0')
0: [DEBUG] Epoch 41, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1343,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2349, -0.2349,
0:             nan,     nan,     nan, -0.2360,     nan,     nan,     nan, -0.2109,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2120,     nan,     nan,     nan,     nan, -0.2326,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2143,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2200,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2372,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2349, -0.2326,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2166,     nan, -0.2177,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2132,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1926,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2372,     nan,     nan,     nan, -0.1880,     nan,     nan, -0.2418,
0:             nan,     nan, -0.2349,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2063,     nan,     nan,     nan,     nan, -0.2189,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2246,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 41, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3793, -0.3161, -0.2454, -0.1982, -0.1772, -0.1681, -0.1370, -0.0797, -0.0312,  0.0167,  0.0182, -0.0238,
0:         -0.0748, -0.0984, -0.0900, -0.0434,  0.0009,  0.0448, -0.3218, -0.2781, -0.2185, -0.1980, -0.2214, -0.2386,
0:         -0.2267], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2243,  0.2223,  0.1816,  0.1332,  0.0725,  0.0196, -0.0374, -0.0773, -0.1133, -0.1503, -0.1771, -0.2073,
0:         -0.2447, -0.3095, -0.3621, -0.3615, -0.2842, -0.1532,  0.2153,  0.2457,  0.2248,  0.1752,  0.1130,  0.0358,
0:         -0.0246], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5600, -0.5558, -0.5532, -0.5518, -0.5505, -0.5528, -0.5639, -0.5759, -0.5885, -0.6029, -0.6180, -0.6324,
0:         -0.6365, -0.6394, -0.6474, -0.6482, -0.6584, -0.6596, -0.5743, -0.5640, -0.5571, -0.5505, -0.5548, -0.5653,
0:         -0.5762], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.1898, 0.2654, 0.3800, 0.3320, 0.2530, 0.2917, 0.2883, 0.2650, 0.2678, 0.2816, 0.3358, 0.4225, 0.5043, 0.4715,
0:         0.3700, 0.4443, 0.5520, 0.4353, 0.1580, 0.0646, 0.1091, 0.1221, 0.1397, 0.2093, 0.2119], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.2866, 0.2692, 0.2687, 0.2819, 0.3107, 0.3412, 0.3710, 0.3899, 0.3994, 0.4059, 0.4128, 0.4318, 0.4665, 0.5129,
0:         0.5577, 0.5927, 0.6120, 0.6266, 0.6433, 0.6647, 0.6827, 0.6962, 0.7060, 0.7120, 0.7127], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2562, -0.2646, -0.2677, -0.2652, -0.2577, -0.2511, -0.2592, -0.2516, -0.2583, -0.2664, -0.2589, -0.2610,
0:         -0.2499, -0.2517, -0.2495, -0.2526, -0.2524, -0.2604, -0.2628, -0.2516, -0.2527, -0.2401, -0.2327, -0.2422,
0:         -0.2427], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15434180200099945; velocity_v: 0.19093462824821472; specific_humidity: 0.14771203696727753; velocity_z: 0.662050187587738; temperature: 0.15338824689388275; total_precip: 0.5912166237831116; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16644561290740967; velocity_v: 0.20213639736175537; specific_humidity: 0.15456359088420868; velocity_z: 0.6380408406257629; temperature: 0.16017399728298187; total_precip: 0.9225592613220215; 
0: epoch: 41 [1/5 (20%)]	Loss: 0.75689 : 0.30776 :: 0.22449 (2.21 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16873633861541748; velocity_v: 0.20757922530174255; specific_humidity: 0.14140678942203522; velocity_z: 0.6952979564666748; temperature: 0.1595691591501236; total_precip: 0.4368172883987427; 
0: epoch: 41 [2/5 (40%)]	Loss: 0.43682 : 0.26390 :: 0.22028 (15.13 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16817016899585724; velocity_v: 0.21448026597499847; specific_humidity: 0.1309659481048584; velocity_z: 0.6935587525367737; temperature: 0.16421711444854736; total_precip: 0.7209024429321289; 
0: epoch: 41 [3/5 (60%)]	Loss: 0.72090 : 0.31203 :: 0.21654 (15.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14941756427288055; velocity_v: 0.1637076437473297; specific_humidity: 0.14523470401763916; velocity_z: 0.5814803242683411; temperature: 0.15989848971366882; total_precip: 0.6473824977874756; 
0: epoch: 41 [4/5 (80%)]	Loss: 0.64738 : 0.27074 :: 0.22045 (15.29 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 41 : 0.2965693175792694
0: validation loss for velocity_u : 0.1143382117152214
0: validation loss for velocity_v : 0.1309177279472351
0: validation loss for specific_humidity : 0.12004805356264114
0: validation loss for velocity_z : 0.6765084862709045
0: validation loss for temperature : 0.10709542781114578
0: validation loss for total_precip : 0.6305081248283386
0: 42 : 22:12:41 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 42, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([2.5850, 2.5878, 2.5904, 2.5916, 2.5904, 2.5867, 2.5795, 2.5685, 2.5546, 2.5393, 2.5239, 2.5085, 2.4941, 2.4812,
0:         2.4704, 2.4627, 2.4576, 2.4543, 2.6183, 2.6149, 2.6086, 2.5996, 2.5878, 2.5749, 2.5615], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1015, -0.1002, -0.0980, -0.0969, -0.0976, -0.1006, -0.1077, -0.1196, -0.1347, -0.1515, -0.1692, -0.1852,
0:         -0.1972, -0.2052, -0.2091, -0.2087, -0.2052, -0.2020, -0.1248, -0.1127, -0.1015, -0.0928, -0.0872, -0.0861,
0:         -0.0907], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1139, -0.1119, -0.1123, -0.1130, -0.1130, -0.1132, -0.1137, -0.1141, -0.1145, -0.1130, -0.1072, -0.1072,
0:         -0.1056, -0.1119, -0.1179, -0.1222, -0.1253, -0.1278, -0.1363, -0.1341, -0.1323, -0.1315, -0.1308, -0.1308,
0:         -0.1310], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4827, -0.5205, -0.5650, -0.5761, -0.5761, -0.6028, -0.6473, -0.6896, -0.7519, -0.8431, -0.9299, -0.9989,
0:         -1.0812, -1.1680, -1.2369, -1.2948, -1.3393, -1.3437, -0.8832, -0.9121, -0.9299, -0.9076, -0.8743, -0.8654,
0:         -0.8787], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.0863, -0.0696, -0.0549, -0.0444, -0.0364, -0.0300, -0.0257, -0.0235, -0.0228, -0.0232, -0.0241, -0.0227,
0:         -0.0158, -0.0041,  0.0084,  0.0183,  0.0258,  0.0319,  0.0359,  0.0378,  0.0371,  0.0340,  0.0278,  0.0190,
0:          0.0076], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([0.1128, 0.0326, 0.0303, 0.0586, 0.0645, 0.1104, 0.1611, 0.2213, 0.2790, 0.5573, 0.4795, 0.4146, 0.3604, 0.3368,
0:         0.3781, 0.4382, 0.5455, 0.5985, 0.9947, 0.9345, 0.8768, 0.8001, 0.7153, 0.7601, 0.8273], device='cuda:0')
0: [DEBUG] Epoch 42, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0794,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,  0.0197,     nan,  0.0161,     nan,     nan,     nan,
0:          0.0680,     nan,     nan,     nan,     nan,     nan, -0.0310,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0546,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0892,     nan,
0:         -0.1006,     nan, -0.1171,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,  0.2767,     nan,     nan,     nan,     nan,     nan, -0.0169,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0204,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1454,     nan,     nan,     nan,     nan,     nan,     nan,  0.0480,     nan, -0.1371, -0.1289,
0:             nan,     nan,     nan,     nan,     nan,  0.2543,     nan,  0.0421,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,  0.0562,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.0145,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1560, -0.1171,
0:             nan,     nan,     nan,     nan, -0.0605,     nan,     nan,     nan, -0.1218,     nan, -0.0688,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 42, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([3.3836, 3.4237, 3.4639, 3.4732, 3.4768, 3.4438, 3.3810, 3.2710, 3.1413, 3.0239, 2.9123, 2.8731, 2.8580, 2.8710,
0:         2.8847, 2.8694, 2.8508, 2.8119, 3.4544, 3.4650, 3.4851, 3.4993, 3.5054, 3.4899, 3.4362], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4036, -0.3753, -0.3648, -0.3674, -0.3715, -0.3780, -0.3877, -0.3800, -0.3703, -0.3510, -0.3159, -0.2735,
0:         -0.2528, -0.2606, -0.2789, -0.2788, -0.2290, -0.1554, -0.5000, -0.4277, -0.3947, -0.3933, -0.4078, -0.4328,
0:         -0.4601], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.7064, -0.7102, -0.7063, -0.6993, -0.6813, -0.6676, -0.6630, -0.6575, -0.6625, -0.6702, -0.6777, -0.6898,
0:         -0.6948, -0.6960, -0.7015, -0.7014, -0.7034, -0.6942, -0.6955, -0.6952, -0.6950, -0.6822, -0.6717, -0.6649,
0:         -0.6592], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3314, -0.3535, -0.2578, -0.3375, -0.4819, -0.4294, -0.3626, -0.3368, -0.2137, -0.1151, -0.0672,  0.0980,
0:          0.2468,  0.2166,  0.1677,  0.1791,  0.1715,  0.1457, -0.1932, -0.4121, -0.5244, -0.5925, -0.6361, -0.5144,
0:         -0.4022], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.6374, -0.6441, -0.6378, -0.6319, -0.6095, -0.5832, -0.5500, -0.5210, -0.5104, -0.5229, -0.5522, -0.5830,
0:         -0.5887, -0.5556, -0.4968, -0.4395, -0.4160, -0.4339, -0.4807, -0.5339, -0.5705, -0.5730, -0.5488, -0.5272,
0:         -0.5230], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0573, -0.0613, -0.0701, -0.0925, -0.0926, -0.0907, -0.0878, -0.0672, -0.0621, -0.0556, -0.0587, -0.0715,
0:         -0.0789, -0.0981, -0.0977, -0.0945, -0.0853, -0.0674, -0.0652, -0.0763, -0.0787, -0.0877, -0.1054, -0.1069,
0:         -0.0999], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.14768226444721222; velocity_v: 0.1562473177909851; specific_humidity: 0.1388028860092163; velocity_z: 0.4745936989784241; temperature: 0.13036465644836426; total_precip: 0.5430504679679871; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15139137208461761; velocity_v: 0.15886235237121582; specific_humidity: 0.1517217606306076; velocity_z: 0.5569647550582886; temperature: 0.18312111496925354; total_precip: 0.49989643692970276; 
0: epoch: 42 [1/5 (20%)]	Loss: 0.52147 : 0.23804 :: 0.21414 (2.53 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16251705586910248; velocity_v: 0.1896149069070816; specific_humidity: 0.14216530323028564; velocity_z: 0.5475976467132568; temperature: 0.14925304055213928; total_precip: 0.5284204483032227; 
0: epoch: 42 [2/5 (40%)]	Loss: 0.52842 : 0.24852 :: 0.22494 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16971805691719055; velocity_v: 0.17066049575805664; specific_humidity: 0.1463286131620407; velocity_z: 0.541217565536499; temperature: 0.1472078412771225; total_precip: 0.6871970295906067; 
0: epoch: 42 [3/5 (60%)]	Loss: 0.68720 : 0.27308 :: 0.22297 (15.28 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1285552829504013; velocity_v: 0.14706790447235107; specific_humidity: 0.14844954013824463; velocity_z: 0.5597022771835327; temperature: 0.16100959479808807; total_precip: 0.5480018258094788; 
0: epoch: 42 [4/5 (80%)]	Loss: 0.54800 : 0.24524 :: 0.21698 (15.23 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 42 : 0.2656537592411041
0: validation loss for velocity_u : 0.10283946245908737
0: validation loss for velocity_v : 0.12660810351371765
0: validation loss for specific_humidity : 0.13050906360149384
0: validation loss for velocity_z : 0.5125043988227844
0: validation loss for temperature : 0.13310347497463226
0: validation loss for total_precip : 0.5883581638336182
0: 43 : 22:16:37 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 43, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([ 0.2444,  0.2201,  0.1964,  0.1735,  0.1516,  0.1309,  0.1114,  0.0930,  0.0760,  0.0598,  0.0450,  0.0310,
0:          0.0181,  0.0059, -0.0055, -0.0163, -0.0266, -0.0366,  0.2146,  0.1882,  0.1626,  0.1376,  0.1140,  0.0914,
0:          0.0702], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-2.2077, -2.2410, -2.2784, -2.3197, -2.3641, -2.4112, -2.4602, -2.5106, -2.5616, -2.6126, -2.6632, -2.7128,
0:         -2.7614, -2.8083, -2.8533, -2.8965, -2.9372, -2.9752, -2.2519, -2.2846, -2.3211, -2.3610, -2.4042, -2.4494,
0:         -2.4965], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3310, -0.3348, -0.3379, -0.3411, -0.3443, -0.3468, -0.3487, -0.3513, -0.3538, -0.3564, -0.3587, -0.3608,
0:         -0.3621, -0.3634, -0.3646, -0.3661, -0.3674, -0.3690, -0.3282, -0.3321, -0.3353, -0.3387, -0.3430, -0.3473,
0:         -0.3514], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.1934, 0.2730, 0.3494, 0.4201, 0.4852, 0.5424, 0.5918, 0.6344, 0.6703, 0.7006, 0.7253, 0.7466, 0.7624, 0.7747,
0:         0.7848, 0.7927, 0.7983, 0.8039, 0.3740, 0.4492, 0.5177, 0.5783, 0.6311, 0.6793, 0.7242], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.6322, 0.6245, 0.6186, 0.6146, 0.6125, 0.6122, 0.6134, 0.6158, 0.6194, 0.6238, 0.6284, 0.6334, 0.6379, 0.6421,
0:         0.6455, 0.6481, 0.6494, 0.6498, 0.6485, 0.6460, 0.6423, 0.6373, 0.6310, 0.6235, 0.6150], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2260, -0.2248, -0.2236, -0.2236, -0.2224, -0.2211, -0.2187, -0.2163, -0.2139, -0.2151, -0.2139, -0.2127,
0:         -0.2127, -0.2139, -0.2151, -0.2151, -0.2115, -0.2067, -0.2127, -0.2115, -0.2103, -0.2103, -0.2115, -0.2115,
0:         -0.2042], device='cuda:0')
0: [DEBUG] Epoch 43, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2091,     nan,     nan, -0.1608,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1704,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2042, -0.2030,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2260,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.1958,     nan,     nan,     nan, -0.1885,     nan,     nan,     nan, -0.1583, -0.1692,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.1112,     nan,     nan,     nan, -0.1559,     nan,
0:             nan,     nan, -0.2067,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1680,     nan,
0:         -0.1535,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1825,     nan,     nan,     nan, -0.1994,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2308,     nan,     nan,     nan,
0:         -0.2030,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1849,     nan,
0:         -0.2042,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1765,     nan, -0.1982,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1740,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1608, -0.1668,     nan,     nan,     nan, -0.1402,     nan, -0.1366,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1801,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1922,     nan,     nan,     nan,     nan,     nan, -0.0955,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 43, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3474, 0.3880, 0.4344, 0.4607, 0.4768, 0.4734, 0.4877, 0.4973, 0.4986, 0.4971, 0.4774, 0.4413, 0.4147, 0.4061,
0:         0.4148, 0.4440, 0.4620, 0.4781, 0.4092, 0.4252, 0.4531, 0.4596, 0.4365, 0.4274, 0.4336], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3954, -0.3528, -0.3822, -0.4569, -0.5606, -0.6664, -0.8030, -0.9066, -0.9903, -1.0263, -1.0213, -1.0131,
0:         -1.0406, -1.1289, -1.2478, -1.3348, -1.3103, -1.2040, -0.4312, -0.3667, -0.3758, -0.4328, -0.5206, -0.6307,
0:         -0.7513], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5162, -0.5102, -0.5006, -0.4905, -0.4776, -0.4706, -0.4740, -0.4791, -0.4906, -0.5064, -0.5220, -0.5335,
0:         -0.5374, -0.5385, -0.5462, -0.5492, -0.5631, -0.5699, -0.5339, -0.5249, -0.5176, -0.5031, -0.4949, -0.4922,
0:         -0.4931], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6982, -0.6103, -0.3775, -0.4204, -0.5981, -0.6039, -0.5984, -0.6521, -0.5293, -0.3681, -0.3627, -0.2958,
0:         -0.1323, -0.0984, -0.1283, -0.0679,  0.0343,  0.1214, -0.8828, -0.9297, -0.8145, -0.7771, -0.8401, -0.8021,
0:         -0.7178], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.0933, 0.0398, 0.0131, 0.0157, 0.0496, 0.0844, 0.1169, 0.1320, 0.1382, 0.1409, 0.1391, 0.1329, 0.1170, 0.0943,
0:         0.0591, 0.0260, 0.0051, 0.0158, 0.0616, 0.1216, 0.1671, 0.1779, 0.1649, 0.1462, 0.1401], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2243, -0.2324, -0.2329, -0.2344, -0.2294, -0.2235, -0.2311, -0.2265, -0.2294, -0.2397, -0.2364, -0.2338,
0:         -0.2312, -0.2336, -0.2315, -0.2331, -0.2345, -0.2411, -0.2410, -0.2342, -0.2339, -0.2243, -0.2171, -0.2287,
0:         -0.2273], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1544899344444275; velocity_v: 0.1621846854686737; specific_humidity: 0.1353132128715515; velocity_z: 0.5119167566299438; temperature: 0.15405842661857605; total_precip: 0.7123777866363525; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16847236454486847; velocity_v: 0.19764931499958038; specific_humidity: 0.1641731560230255; velocity_z: 0.7046299576759338; temperature: 0.2324652373790741; total_precip: 0.8036902546882629; 
0: epoch: 43 [1/5 (20%)]	Loss: 0.75803 : 0.30419 :: 0.22053 (2.61 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14776138961315155; velocity_v: 0.1772453486919403; specific_humidity: 0.1509411782026291; velocity_z: 0.5299551486968994; temperature: 0.16800887882709503; total_precip: 0.750649631023407; 
0: epoch: 43 [2/5 (40%)]	Loss: 0.75065 : 0.28324 :: 0.22200 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14803887903690338; velocity_v: 0.17297889292240143; specific_humidity: 0.16056065261363983; velocity_z: 0.5279781222343445; temperature: 0.22312086820602417; total_precip: 0.631598949432373; 
0: epoch: 43 [3/5 (60%)]	Loss: 0.63160 : 0.27091 :: 0.21859 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14738233387470245; velocity_v: 0.18677374720573425; specific_humidity: 0.12523093819618225; velocity_z: 0.6169341802597046; temperature: 0.15153779089450836; total_precip: 0.6315850615501404; 
0: epoch: 43 [4/5 (80%)]	Loss: 0.63159 : 0.27283 :: 0.21842 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 43 : 0.2507116198539734
0: validation loss for velocity_u : 0.10313116759061813
0: validation loss for velocity_v : 0.11858775466680527
0: validation loss for specific_humidity : 0.11235323548316956
0: validation loss for velocity_z : 0.4584156274795532
0: validation loss for temperature : 0.09848877042531967
0: validation loss for total_precip : 0.6132932305335999
0: 44 : 22:20:33 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 44, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.7102, 1.7227, 1.7387, 1.7567, 1.7747, 1.7910, 1.8038, 1.8127, 1.8165, 1.8136, 1.8030, 1.7853, 1.7629, 1.7368,
0:         1.7086, 1.6800, 1.6525, 1.6272, 1.7599, 1.7589, 1.7638, 1.7739, 1.7888, 1.8073, 1.8280], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.9125, -0.9636, -1.0161, -1.0652, -1.1074, -1.1418, -1.1693, -1.1923, -1.2109, -1.2254, -1.2370, -1.2465,
0:         -1.2521, -1.2519, -1.2450, -1.2320, -1.2132, -1.1894, -0.8794, -0.9104, -0.9522, -1.0004, -1.0505, -1.0977,
0:         -1.1387], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.6105, 0.6481, 0.7025, 0.7277, 0.7255, 0.7302, 0.7235, 0.6986, 0.6768, 0.6516, 0.6109, 0.5725, 0.5367, 0.4990,
0:         0.4673, 0.4329, 0.4056, 0.3844, 0.5423, 0.5802, 0.6343, 0.6727, 0.7092, 0.7176, 0.7145], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0612, -0.0747, -0.0860, -0.1210, -0.1808, -0.2463, -0.3388, -0.4753, -0.5927, -0.6254, -0.5961, -0.5498,
0:         -0.4787, -0.3805, -0.2846, -0.1977, -0.0950,  0.0065, -0.0860, -0.0984, -0.1097, -0.1233, -0.1334, -0.1154,
0:         -0.0950], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([0.1124, 0.0743, 0.0451, 0.0263, 0.0167, 0.0160, 0.0203, 0.0268, 0.0351, 0.0445, 0.0544, 0.0656, 0.0801, 0.0963,
0:         0.1131, 0.1299, 0.1460, 0.1617, 0.1777, 0.1955, 0.2145, 0.2360, 0.2612, 0.2896, 0.3188], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2404, -0.2404, -0.2404, -0.2404, -0.2404, -0.2404, -0.2404, -0.2404, -0.2404, -0.2404, -0.2275, -0.2275,
0:         -0.2181, -0.2369, -0.2404, -0.2404, -0.2404, -0.2404, -0.2404, -0.2251, -0.2110, -0.1711, -0.1899, -0.2145,
0:         -0.2404], device='cuda:0')
0: [DEBUG] Epoch 44, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0055,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0125,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1699,     nan,     nan,     nan, -0.1676,     nan,     nan, -0.2404,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1441,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.1229,     nan, -0.0618, -0.0642,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1652,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0325,     nan,     nan,     nan,     nan,
0:         -0.2169,     nan,     nan, -0.1652,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1523, -0.1370,     nan,     nan,     nan,     nan,     nan,     nan, -0.2404,     nan,     nan,     nan,
0:         -0.1452,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.1030, -0.1335,     nan, -0.0818,     nan,     nan,     nan, -0.1723,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1570,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1323,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.0290,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0748,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0431,     nan,     nan])
0: [DEBUG] Epoch 44, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.4881, 1.4805, 1.4998, 1.5089, 1.5077, 1.4743, 1.4358, 1.3639, 1.2818, 1.2231, 1.1688, 1.1619, 1.1620, 1.1776,
0:         1.1886, 1.1870, 1.1975, 1.1878, 1.5014, 1.4696, 1.4722, 1.4827, 1.4848, 1.4644, 1.4330], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-1.1731, -1.2278, -1.3430, -1.4670, -1.5720, -1.6609, -1.7861, -1.9009, -1.9976, -2.0425, -2.0096, -1.9343,
0:         -1.8795, -1.9066, -2.0096, -2.1138, -2.1544, -2.1108, -1.3665, -1.3706, -1.4570, -1.5593, -1.6593, -1.7632,
0:         -1.8739], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.6341, -0.6015, -0.5367, -0.4462, -0.3374, -0.2246, -0.1141, -0.0061,  0.0897,  0.1688,  0.2303,  0.2756,
0:          0.2982,  0.3054,  0.2963,  0.2807,  0.2613,  0.2519, -0.6751, -0.6554, -0.6121, -0.5351, -0.4457, -0.3420,
0:         -0.2267], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.3766, -0.2640, -0.0576, -0.1149, -0.3460, -0.4566, -0.5800, -0.7599, -0.8239, -0.8562, -0.8860, -0.7573,
0:         -0.5548, -0.5328, -0.5933, -0.5770, -0.5852, -0.6000, -0.4218, -0.4725, -0.4031, -0.2889, -0.2339, -0.2026,
0:         -0.2581], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.5988, -1.5560, -1.5104, -1.4791, -1.4466, -1.4181, -1.3835, -1.3486, -1.3197, -1.2990, -1.2868, -1.2764,
0:         -1.2640, -1.2274, -1.1797, -1.1193, -1.0656, -1.0189, -0.9820, -0.9452, -0.9001, -0.8335, -0.7429, -0.6441,
0:         -0.5449], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.1564, -0.1621, -0.1690, -0.1707, -0.1669, -0.1591, -0.1623, -0.1425, -0.1361, -0.1754, -0.1760, -0.1752,
0:         -0.1741, -0.1795, -0.1725, -0.1631, -0.1576, -0.1555, -0.1926, -0.1876, -0.1928, -0.1848, -0.1787, -0.1860,
0:         -0.1736], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15863512456417084; velocity_v: 0.20494741201400757; specific_humidity: 0.11570671200752258; velocity_z: 0.5869582295417786; temperature: 0.13807804882526398; total_precip: 0.6447534561157227; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17870190739631653; velocity_v: 0.2490406036376953; specific_humidity: 0.13909615576267242; velocity_z: 0.6356843709945679; temperature: 0.16671216487884521; total_precip: 0.6306346654891968; 
0: epoch: 44 [1/5 (20%)]	Loss: 0.63769 : 0.28325 :: 0.21950 (2.68 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17806857824325562; velocity_v: 0.24399062991142273; specific_humidity: 0.16434414684772491; velocity_z: 0.7401763796806335; temperature: 0.2006508708000183; total_precip: 1.0735474824905396; 
0: epoch: 44 [2/5 (40%)]	Loss: 1.07355 : 0.39384 :: 0.22525 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1852453500032425; velocity_v: 0.20091673731803894; specific_humidity: 0.15211495757102966; velocity_z: 0.5774213671684265; temperature: 0.20075170695781708; total_precip: 0.5098766088485718; 
0: epoch: 44 [3/5 (60%)]	Loss: 0.50988 : 0.26568 :: 0.22189 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14669710397720337; velocity_v: 0.15708166360855103; specific_humidity: 0.15456517040729523; velocity_z: 0.6132239103317261; temperature: 0.1606871783733368; total_precip: 0.6980330348014832; 
0: epoch: 44 [4/5 (80%)]	Loss: 0.69803 : 0.28409 :: 0.21848 (15.15 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 44 : 0.28889474272727966
0: validation loss for velocity_u : 0.12188839912414551
0: validation loss for velocity_v : 0.14358381927013397
0: validation loss for specific_humidity : 0.10898494720458984
0: validation loss for velocity_z : 0.6248586773872375
0: validation loss for temperature : 0.10860509425401688
0: validation loss for total_precip : 0.6254472732543945
0: 45 : 22:24:36 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 45, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.9480, 0.9641, 0.9793, 0.9955, 1.0169, 1.0463, 1.0826, 1.1225, 1.1604, 1.1944, 1.2262, 1.2577, 1.2890, 1.3200,
0:         1.3524, 1.3865, 1.4208, 1.4541, 0.9178, 0.9398, 0.9587, 0.9745, 0.9937, 1.0193, 1.0498], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8852, -1.2024, -1.4509, -1.6095, -1.6889, -1.7186, -1.7250, -1.7270, -1.7449, -1.7982, -1.8922, -2.0139,
0:         -2.1421, -2.2576, -2.3489, -2.4127, -2.4526, -2.4742, -0.9593, -1.3087, -1.5775, -1.7345, -1.7885, -1.7769,
0:         -1.7385], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.8116, -0.7915, -0.7500, -0.7006, -0.6508, -0.6090, -0.5797, -0.5532, -0.5338, -0.5144, -0.4927, -0.4733,
0:         -0.4533, -0.4325, -0.4129, -0.3928, -0.3740, -0.3567, -0.8105, -0.7916, -0.7504, -0.6986, -0.6509, -0.6094,
0:         -0.5843], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5575, -0.7418, -0.9115, -0.9540, -0.9004, -0.8970, -1.0433, -1.3292, -1.6854, -2.0650, -2.4380, -2.7752,
0:         -3.0734, -3.3604, -3.6474, -3.8953, -4.0750, -4.2102, -0.6927, -0.8222, -0.9551, -1.0388, -1.0612, -1.0612,
0:         -1.0924], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.4808, -0.5739, -0.6307, -0.6393, -0.6010, -0.5215, -0.4143, -0.2989, -0.1908, -0.0925,  0.0012,  0.0924,
0:          0.1801,  0.2627,  0.3395,  0.4110,  0.4779,  0.5400,  0.5983,  0.6555,  0.7133,  0.7676,  0.8122,  0.8472,
0:          0.8780], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.1974, -0.1985, -0.1443, -0.0424,  0.1148,  0.2831,  0.4558,  0.4491,  0.4347, -0.2074, -0.2196, -0.1686,
0:         -0.1365, -0.0181,  0.1458,  0.3584,  0.4967,  0.5266, -0.2096, -0.2218, -0.2162, -0.1886, -0.1033,  0.0594,
0:          0.2676], device='cuda:0')
0: [DEBUG] Epoch 45, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.0203,     nan,     nan,     nan,     nan,     nan, -0.2173,     nan,     nan,     nan,     nan, -0.2041,
0:             nan,     nan,     nan,     nan,     nan,     nan, -0.2085,     nan,     nan,     nan,     nan,     nan,
0:         -0.1708,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2196,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2107,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1952,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.1244,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2041,     nan,     nan,     nan, -0.0911,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1919,     nan,     nan,     nan,
0:         -0.1576,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.0956,     nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0484,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0756,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1985,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.0734,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.1354,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 45, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.9400, 0.9003, 0.8795, 0.8615, 0.8587, 0.8461, 0.8378, 0.8069, 0.7550, 0.7182, 0.6722, 0.6642, 0.6585, 0.6703,
0:         0.6789, 0.6814, 0.7071, 0.7225, 0.9699, 0.9046, 0.8618, 0.8353, 0.8322, 0.8352, 0.8394], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4984, -0.4330, -0.4021, -0.4044, -0.4233, -0.4424, -0.4739, -0.4890, -0.4950, -0.4988, -0.4890, -0.4742,
0:         -0.4735, -0.4914, -0.5043, -0.4874, -0.4209, -0.3191, -0.5892, -0.4929, -0.4323, -0.4161, -0.4315, -0.4685,
0:         -0.5067], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5827, -0.5848, -0.5841, -0.5815, -0.5744, -0.5708, -0.5765, -0.5831, -0.5952, -0.6060, -0.6174, -0.6286,
0:         -0.6369, -0.6479, -0.6695, -0.6867, -0.7131, -0.7249, -0.5916, -0.5868, -0.5840, -0.5712, -0.5630, -0.5607,
0:         -0.5618], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1219,  0.1815,  0.2709,  0.2205,  0.0548,  0.0214, -0.0342, -0.1435, -0.0122,  0.0952,  0.1187,  0.2985,
0:          0.4230,  0.4051,  0.3061,  0.2849,  0.4302,  0.4087,  0.0171, -0.0633, -0.1030, -0.0766, -0.0644, -0.0215,
0:         -0.0441], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([0.6373, 0.5710, 0.5385, 0.5528, 0.6124, 0.6879, 0.7548, 0.7891, 0.7936, 0.7785, 0.7533, 0.7298, 0.7040, 0.6685,
0:         0.6116, 0.5374, 0.4644, 0.4313, 0.4575, 0.5265, 0.5970, 0.6279, 0.6157, 0.5840, 0.5623], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2267, -0.2321, -0.2361, -0.2345, -0.2290, -0.2227, -0.2266, -0.2190, -0.2141, -0.2268, -0.2296, -0.2207,
0:         -0.2231, -0.2278, -0.2225, -0.2174, -0.2182, -0.2222, -0.2211, -0.2118, -0.2193, -0.2095, -0.2039, -0.2158,
0:         -0.2070], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.16372917592525482; velocity_v: 0.19324567914009094; specific_humidity: 0.1564025729894638; velocity_z: 0.5466510653495789; temperature: 0.4957423210144043; total_precip: 0.5384672284126282; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14118024706840515; velocity_v: 0.14527291059494019; specific_humidity: 0.14724339544773102; velocity_z: 0.4810506999492645; temperature: 0.15126149356365204; total_precip: 0.44794055819511414; 
0: epoch: 45 [1/5 (20%)]	Loss: 0.49320 : 0.26335 :: 0.22048 (2.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16809998452663422; velocity_v: 0.19235576689243317; specific_humidity: 0.13437598943710327; velocity_z: 0.59913170337677; temperature: 0.14064788818359375; total_precip: 0.6869191527366638; 
0: epoch: 45 [2/5 (40%)]	Loss: 0.68692 : 0.28416 :: 0.21340 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16330063343048096; velocity_v: 0.17972789704799652; specific_humidity: 0.1308206021785736; velocity_z: 0.5633282661437988; temperature: 0.16668853163719177; total_precip: 0.6383351683616638; 
0: epoch: 45 [3/5 (60%)]	Loss: 0.63834 : 0.27037 :: 0.21887 (15.32 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15394572913646698; velocity_v: 0.16822338104248047; specific_humidity: 0.11835791170597076; velocity_z: 0.5232905149459839; temperature: 0.14207659661769867; total_precip: 0.5641714930534363; 
0: epoch: 45 [4/5 (80%)]	Loss: 0.56417 : 0.24305 :: 0.21725 (15.38 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 45 : 0.2630731761455536
0: validation loss for velocity_u : 0.10270854830741882
0: validation loss for velocity_v : 0.11215930432081223
0: validation loss for specific_humidity : 0.12356553226709366
0: validation loss for velocity_z : 0.529902458190918
0: validation loss for temperature : 0.1248183324933052
0: validation loss for total_precip : 0.5852847695350647
0: 46 : 22:28:37 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 46, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.5442, -0.5282, -0.5090, -0.4867, -0.4628, -0.4352, -0.4057, -0.3732, -0.3400, -0.3067, -0.2722, -0.2383,
0:         -0.2027, -0.1693, -0.1396, -0.1117, -0.0844, -0.0568, -0.5001, -0.4921, -0.4795, -0.4642, -0.4468, -0.4253,
0:         -0.4010], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.1183, -0.1325, -0.1487, -0.1659, -0.1872, -0.2107, -0.2337, -0.2609, -0.2912, -0.3252, -0.3603, -0.3964,
0:         -0.4332, -0.4691, -0.5073, -0.5462, -0.5874, -0.6261, -0.0968, -0.1046, -0.1149, -0.1282, -0.1441, -0.1606,
0:         -0.1798], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.4655, 1.5096, 1.5476, 1.6083, 1.6779, 1.7512, 1.8383, 1.9089, 1.9691, 2.0210, 2.0509, 2.0751, 2.0783, 2.0819,
0:         2.0858, 2.0880, 2.0951, 2.0996, 1.3786, 1.4346, 1.4514, 1.4720, 1.5250, 1.5692, 1.6325], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4319, -0.4473, -0.5377, -0.5972, -0.6545, -0.7250, -0.7383, -0.6920, -0.6038, -0.4473, -0.2864, -0.1961,
0:         -0.1145, -0.1079, -0.1366, -0.1366, -0.1013, -0.0286, -0.3327, -0.3437, -0.3966, -0.4848, -0.5245, -0.5245,
0:         -0.5443], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-2.7044, -2.8067, -2.9159, -3.0186, -3.1132, -3.1967, -3.2580, -3.2959, -3.3049, -3.3089, -3.2889, -3.2507,
0:         -3.2120, -3.1596, -3.1221, -3.0856, -3.0615, -3.0450, -3.0366, -3.0473, -3.0491, -3.0600, -3.0670, -3.0732,
0:         -3.0822], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2508, -0.2463, -0.2463, -0.2463, -0.2463, -0.2463, -0.2486, -0.2486, -0.2508, -0.2508, -0.2463, -0.2486,
0:         -0.2508, -0.2508, -0.2508, -0.2508, -0.2508, -0.2508, -0.2508, -0.2486, -0.2486, -0.2508, -0.2508, -0.2508,
0:         -0.2463], device='cuda:0')
0: [DEBUG] Epoch 46, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2291,     nan,
0:         -0.2314,     nan,     nan,     nan,     nan,     nan,     nan, -0.2326,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2189,     nan,     nan,
0:             nan,     nan,     nan, -0.2383,     nan,     nan,     nan,     nan, -0.1972,     nan, -0.2269,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2508,     nan,     nan,     nan, -0.2508,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2474,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2508,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2497,     nan,     nan,
0:             nan,     nan, -0.2508,     nan,     nan,     nan,     nan,     nan, -0.2463,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2463,     nan, -0.2508,     nan,     nan,     nan, -0.2508,     nan,
0:             nan,     nan,     nan, -0.2508,     nan, -0.2497,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2463,     nan,     nan, -0.2394,     nan, -0.2429,     nan,     nan, -0.2486,     nan,
0:         -0.2451,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan])
0: [DEBUG] Epoch 46, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.2044, -0.1567, -0.0877, -0.0388, -0.0191, -0.0287, -0.0245, -0.0162,  0.0010,  0.0325,  0.0443,  0.0385,
0:          0.0183,  0.0077,  0.0138,  0.0577,  0.1174,  0.1813, -0.1249, -0.1137, -0.0700, -0.0525, -0.0501, -0.0576,
0:         -0.0524], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0278,  0.0564,  0.0380, -0.0147, -0.0756, -0.1367, -0.1885, -0.2123, -0.2118, -0.1943, -0.1532, -0.1141,
0:         -0.0924, -0.1298, -0.1903, -0.2307, -0.2129, -0.1504, -0.0738, -0.0160, -0.0069, -0.0364, -0.0852, -0.1316,
0:         -0.1661], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6245, 0.6587, 0.6830, 0.7006, 0.6969, 0.6736, 0.6455, 0.6022, 0.5583, 0.5189, 0.4996, 0.4898, 0.4946, 0.5311,
0:         0.5593, 0.6071, 0.6544, 0.7011, 0.4526, 0.4961, 0.5324, 0.5611, 0.5632, 0.5437, 0.5187], device='cuda:2',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.0554,  0.1612,  0.3371,  0.2614,  0.1037,  0.2353,  0.2973,  0.2586,  0.2460,  0.1061,  0.0164,  0.0058,
0:         -0.0294, -0.0347, -0.1017, -0.0062,  0.1943,  0.0509,  0.1599,  0.1341,  0.2152,  0.1639,  0.0656,  0.2173,
0:          0.2969], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-1.6802, -1.7026, -1.7357, -1.7995, -1.8663, -1.9228, -1.9526, -1.9486, -1.9215, -1.8801, -1.8391, -1.8077,
0:         -1.7878, -1.7678, -1.7494, -1.7353, -1.7397, -1.7585, -1.7871, -1.7993, -1.7711, -1.6847, -1.5558, -1.4162,
0:         -1.2907], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2479, -0.2545, -0.2601, -0.2582, -0.2500, -0.2532, -0.2621, -0.2577, -0.2546, -0.2596, -0.2592, -0.2513,
0:         -0.2529, -0.2524, -0.2509, -0.2531, -0.2588, -0.2713, -0.2603, -0.2524, -0.2528, -0.2434, -0.2330, -0.2449,
0:         -0.2460], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15804918110370636; velocity_v: 0.19026395678520203; specific_humidity: 0.13727876543998718; velocity_z: 0.582747757434845; temperature: 0.13937143981456757; total_precip: 0.5997031331062317; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1490262895822525; velocity_v: 0.15890878438949585; specific_humidity: 0.14269791543483734; velocity_z: 0.6276326179504395; temperature: 0.14515776932239532; total_precip: 0.607814371585846; 
0: epoch: 46 [1/5 (20%)]	Loss: 0.60376 : 0.26670 :: 0.21755 (1.89 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1690729558467865; velocity_v: 0.21893124282360077; specific_humidity: 0.15239618718624115; velocity_z: 0.6224378943443298; temperature: 0.16203612089157104; total_precip: 0.6449474692344666; 
0: epoch: 46 [2/5 (40%)]	Loss: 0.64495 : 0.29120 :: 0.21482 (15.16 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15613140165805817; velocity_v: 0.17050987482070923; specific_humidity: 0.14841346442699432; velocity_z: 0.5501328706741333; temperature: 0.14280195534229279; total_precip: 0.575597882270813; 
0: epoch: 46 [3/5 (60%)]	Loss: 0.57560 : 0.25404 :: 0.21428 (15.14 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15921416878700256; velocity_v: 0.16913606226444244; specific_humidity: 0.133348286151886; velocity_z: 0.536810576915741; temperature: 0.17224028706550598; total_precip: 0.7159952521324158; 
0: epoch: 46 [4/5 (80%)]	Loss: 0.71600 : 0.27737 :: 0.21772 (14.87 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 46 : 0.2690109312534332
0: validation loss for velocity_u : 0.10569653660058975
0: validation loss for velocity_v : 0.12320852279663086
0: validation loss for specific_humidity : 0.12141778320074081
0: validation loss for velocity_z : 0.5381312966346741
0: validation loss for temperature : 0.12960557639598846
0: validation loss for total_precip : 0.5960068702697754
0: 47 : 22:32:45 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 47, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.2243, 0.2474, 0.2708, 0.2958, 0.3213, 0.3483, 0.3776, 0.4072, 0.4378, 0.4682, 0.4968, 0.5236, 0.5479, 0.5700,
0:         0.5910, 0.6112, 0.6318, 0.6528, 0.2699, 0.2915, 0.3139, 0.3378, 0.3623, 0.3890, 0.4176], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.7394, 0.7575, 0.7746, 0.7917, 0.8083, 0.8232, 0.8374, 0.8501, 0.8606, 0.8701, 0.8781, 0.8847, 0.8907, 0.8950,
0:         0.8997, 0.9051, 0.9102, 0.9156, 0.7663, 0.7847, 0.8028, 0.8207, 0.8380, 0.8536, 0.8683], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.3342, -0.3332, -0.3306, -0.3266, -0.3212, -0.3155, -0.3113, -0.3092, -0.3102, -0.3142, -0.3198, -0.3280,
0:         -0.3373, -0.3474, -0.3569, -0.3660, -0.3735, -0.3784, -0.2906, -0.2918, -0.2907, -0.2882, -0.2870, -0.2862,
0:         -0.2873], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([0.8245, 0.9350, 1.0390, 1.1364, 1.1868, 1.2415, 1.2995, 1.3466, 1.4177, 1.4856, 1.5512, 1.6169, 1.6388, 1.6465,
0:         1.6497, 1.6235, 1.6049, 1.6016, 0.5935, 0.7238, 0.8748, 1.0346, 1.1134, 1.1933, 1.2765], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1915, -0.2149, -0.2369, -0.2522, -0.2575, -0.2609, -0.2650, -0.2764, -0.2966, -0.3263, -0.3660, -0.4095,
0:         -0.4501, -0.4931, -0.5384, -0.5880, -0.6428, -0.7005, -0.7614, -0.8230, -0.8863, -0.9498, -1.0088, -1.0649,
0:         -1.1175], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2460, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2413, -0.2472, -0.2472,
0:         -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2260, -0.2401, -0.2436, -0.2472, -0.2472, -0.2472,
0:         -0.2472], device='cuda:0')
0: [DEBUG] Epoch 47, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([-0.2472,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2460,     nan,     nan,
0:             nan,     nan,     nan, -0.2143,     nan,     nan,     nan,     nan, -0.2225,     nan,     nan,     nan,
0:         -0.2354,     nan, -0.2343,     nan, -0.1932,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2472,     nan,
0:             nan,     nan,     nan, -0.2472,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2472,
0:             nan, -0.2472,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2472,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2472, -0.2472,     nan,     nan,     nan,     nan, -0.2472,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2472,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2472,     nan,     nan, -0.2472,     nan,     nan,     nan,
0:             nan, -0.2472,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2472,     nan,     nan,
0:             nan,     nan, -0.2472,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2472,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2448,     nan,     nan,     nan, -0.2472,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2472,     nan])
0: [DEBUG] Epoch 47, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.5503, -0.5078, -0.4385, -0.3705, -0.3173, -0.2849, -0.2406, -0.1946, -0.1558, -0.1167, -0.0982, -0.1004,
0:         -0.0912, -0.0578, -0.0045,  0.0741,  0.1498,  0.1991, -0.4675, -0.4483, -0.3865, -0.3415, -0.3130, -0.2871,
0:         -0.2511], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2396, 0.2682, 0.2792, 0.2831, 0.2772, 0.2876, 0.3024, 0.3361, 0.3802, 0.4279, 0.4762, 0.5220, 0.5460, 0.5280,
0:         0.4997, 0.4881, 0.5238, 0.5910, 0.2085, 0.2597, 0.2916, 0.3145, 0.3310, 0.3573, 0.3900], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4796, -0.4936, -0.5076, -0.5095, -0.5065, -0.5009, -0.4973, -0.5020, -0.5117, -0.5233, -0.5315, -0.5328,
0:         -0.5236, -0.5074, -0.4967, -0.4776, -0.4645, -0.4453, -0.4996, -0.5112, -0.5217, -0.5202, -0.5166, -0.5118,
0:         -0.5038], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.2479, 0.4075, 0.6168, 0.5361, 0.3534, 0.4748, 0.5153, 0.4236, 0.4977, 0.4941, 0.4265, 0.5025, 0.5516, 0.4971,
0:         0.4101, 0.5451, 0.7906, 0.6709, 0.3226, 0.2487, 0.3111, 0.3119, 0.2673, 0.4126, 0.4681], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.0802, -0.1163, -0.1415, -0.1727, -0.2065, -0.2474, -0.2899, -0.3294, -0.3652, -0.4032, -0.4483, -0.4994,
0:         -0.5445, -0.5795, -0.6001, -0.6188, -0.6447, -0.6804, -0.7219, -0.7659, -0.8146, -0.8683, -0.9265, -0.9867,
0:         -1.0472], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2474, -0.2528, -0.2598, -0.2590, -0.2548, -0.2591, -0.2704, -0.2673, -0.2633, -0.2521, -0.2523, -0.2518,
0:         -0.2527, -0.2514, -0.2539, -0.2584, -0.2656, -0.2791, -0.2506, -0.2494, -0.2492, -0.2392, -0.2330, -0.2461,
0:         -0.2441], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.15045028924942017; velocity_v: 0.18051765859127045; specific_humidity: 0.1327802836894989; velocity_z: 0.5701692700386047; temperature: 0.13135947287082672; total_precip: 0.6312669515609741; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15161633491516113; velocity_v: 0.17924310266971588; specific_humidity: 0.13419702649116516; velocity_z: 0.6547188758850098; temperature: 0.1463565081357956; total_precip: 0.7912770509719849; 
0: epoch: 47 [1/5 (20%)]	Loss: 0.71127 : 0.28503 :: 0.21058 (2.77 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16905972361564636; velocity_v: 0.20073337852954865; specific_humidity: 0.13823822140693665; velocity_z: 0.7926526069641113; temperature: 0.1565406620502472; total_precip: 1.191430926322937; 
0: epoch: 47 [2/5 (40%)]	Loss: 1.19143 : 0.40354 :: 0.22661 (15.33 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.15205781161785126; velocity_v: 0.1807606965303421; specific_humidity: 0.14823734760284424; velocity_z: 0.647686779499054; temperature: 0.14968395233154297; total_precip: 0.7520140409469604; 
0: epoch: 47 [3/5 (60%)]	Loss: 0.75201 : 0.30062 :: 0.22337 (15.19 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.1684320718050003; velocity_v: 0.2006853073835373; specific_humidity: 0.11722704768180847; velocity_z: 0.5579298138618469; temperature: 0.14095307886600494; total_precip: 0.4090670645236969; 
0: epoch: 47 [4/5 (80%)]	Loss: 0.40907 : 0.22966 :: 0.22096 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 47 : 0.27033600211143494
0: validation loss for velocity_u : 0.1162990853190422
0: validation loss for velocity_v : 0.11917402595281601
0: validation loss for specific_humidity : 0.10393644124269485
0: validation loss for velocity_z : 0.6271685361862183
0: validation loss for temperature : 0.11940152198076248
0: validation loss for total_precip : 0.5360363721847534
0: 48 : 22:36:40 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 48, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([1.5966, 1.5926, 1.5884, 1.5846, 1.5800, 1.5744, 1.5680, 1.5604, 1.5510, 1.5401, 1.5275, 1.5133, 1.4980, 1.4817,
0:         1.4650, 1.4480, 1.4313, 1.4163, 1.6917, 1.6904, 1.6904, 1.6907, 1.6907, 1.6906, 1.6894], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0489, -0.0681, -0.0871, -0.1054, -0.1222, -0.1373, -0.1519, -0.1638, -0.1719, -0.1771, -0.1791, -0.1773,
0:         -0.1727, -0.1666, -0.1589, -0.1513, -0.1454, -0.1416, -0.0136, -0.0362, -0.0586, -0.0808, -0.1018, -0.1222,
0:         -0.1428], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4045, -0.4215, -0.4392, -0.4577, -0.4776, -0.4981, -0.5206, -0.5458, -0.5661, -0.5850, -0.6021, -0.6171,
0:         -0.6295, -0.6447, -0.6508, -0.6536, -0.6586, -0.6502, -0.2950, -0.3100, -0.3245, -0.3414, -0.3605, -0.3805,
0:         -0.4031], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.0840, -0.0518, -0.0929, -0.1406, -0.1562, -0.2006, -0.2583, -0.2828, -0.3216, -0.3849, -0.4094, -0.4105,
0:         -0.4216, -0.4005, -0.3472, -0.3105, -0.2850, -0.2539, -0.0207,  0.0060,  0.0060,  0.0093, -0.0129, -0.0784,
0:         -0.1273], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.2605, -0.2679, -0.2771, -0.2886, -0.3018, -0.3147, -0.3264, -0.3377, -0.3482, -0.3582, -0.3690, -0.3791,
0:         -0.3865, -0.3911, -0.3920, -0.3879, -0.3810, -0.3734, -0.3633, -0.3511, -0.3417, -0.3362, -0.3321, -0.3319,
0:         -0.3340], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533,
0:         -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533, -0.2533,
0:         -0.2533], device='cuda:0')
0: [DEBUG] Epoch 48, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2533,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2533,     nan,
0:             nan, -0.2533,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2533,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,
0:             nan, -0.2493,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.2493,     nan,     nan, -0.2493,     nan,     nan,     nan, -0.2493, -0.2493, -0.2493,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2493,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.2493,     nan,     nan])
0: [DEBUG] Epoch 48, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.6963, 0.6850, 0.6933, 0.6948, 0.6901, 0.6688, 0.6476, 0.6124, 0.5625, 0.5329, 0.4948, 0.4795, 0.4661, 0.4660,
0:         0.4757, 0.4927, 0.5386, 0.5766, 0.6951, 0.6637, 0.6691, 0.6751, 0.6773, 0.6593, 0.6451], device='cuda:0',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([1.0394, 1.0571, 1.0618, 1.0749, 1.0929, 1.1382, 1.1838, 1.2334, 1.2658, 1.2849, 1.2612, 1.2167, 1.1584, 1.0902,
0:         1.0412, 1.0166, 1.0194, 1.0216, 1.0171, 1.0480, 1.0653, 1.0873, 1.1240, 1.1780, 1.2429], device='cuda:1',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.1092,  0.1229,  0.1214,  0.1163,  0.1050,  0.0873,  0.0550,  0.0240, -0.0212, -0.0650, -0.1094, -0.1526,
0:         -0.1922, -0.2257, -0.2629, -0.2836, -0.3053, -0.3012,  0.0963,  0.1081,  0.1005,  0.0905,  0.0611,  0.0241,
0:         -0.0040], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([0.3111, 0.4285, 0.7128, 0.5193, 0.2559, 0.5082, 0.5786, 0.3758, 0.3863, 0.4132, 0.3859, 0.4652, 0.5619, 0.5356,
0:         0.4114, 0.5320, 0.7421, 0.5684, 0.5098, 0.3567, 0.5091, 0.3906, 0.2551, 0.5683, 0.6941], device='cuda:3',
0:        grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([-0.1357, -0.1889, -0.2168, -0.2438, -0.2674, -0.3087, -0.3504, -0.3962, -0.4331, -0.4687, -0.5187, -0.5821,
0:         -0.6307, -0.6404, -0.6061, -0.5516, -0.5147, -0.5202, -0.5558, -0.6081, -0.6393, -0.6356, -0.5985, -0.5631,
0:         -0.5510], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.2537, -0.2618, -0.2679, -0.2628, -0.2545, -0.2549, -0.2642, -0.2599, -0.2566, -0.2582, -0.2571, -0.2485,
0:         -0.2504, -0.2483, -0.2470, -0.2500, -0.2569, -0.2695, -0.2509, -0.2485, -0.2453, -0.2353, -0.2277, -0.2399,
0:         -0.2352], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1755106896162033; velocity_v: 0.20077858865261078; specific_humidity: 0.13686582446098328; velocity_z: 0.6565530300140381; temperature: 0.8047325611114502; total_precip: 0.538913369178772; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16212715208530426; velocity_v: 0.20403490960597992; specific_humidity: 0.14575469493865967; velocity_z: 0.5974820256233215; temperature: 0.17153926193714142; total_precip: 0.6469159126281738; 
0: epoch: 48 [1/5 (20%)]	Loss: 0.59291 : 0.33179 :: 0.22227 (2.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16627061367034912; velocity_v: 0.19130288064479828; specific_humidity: 0.14396622776985168; velocity_z: 0.6952837109565735; temperature: 0.16328033804893494; total_precip: 0.7011261582374573; 
0: epoch: 48 [2/5 (40%)]	Loss: 0.70113 : 0.30514 :: 0.23381 (15.31 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14498773217201233; velocity_v: 0.18882809579372406; specific_humidity: 0.13100343942642212; velocity_z: 0.7307597994804382; temperature: 0.16319487988948822; total_precip: 0.8707141876220703; 
0: epoch: 48 [3/5 (60%)]	Loss: 0.87071 : 0.33497 :: 0.21315 (15.27 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17903989553451538; velocity_v: 0.1846695840358734; specific_humidity: 0.12515470385551453; velocity_z: 0.6104101538658142; temperature: 0.1800917536020279; total_precip: 0.6160130500793457; 
0: epoch: 48 [4/5 (80%)]	Loss: 0.61601 : 0.27781 :: 0.22678 (15.12 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 48 : 0.2513104975223541
0: validation loss for velocity_u : 0.10087572783231735
0: validation loss for velocity_v : 0.10189840942621231
0: validation loss for specific_humidity : 0.11761379987001419
0: validation loss for velocity_z : 0.47126659750938416
0: validation loss for temperature : 0.11042195558547974
0: validation loss for total_precip : 0.6057865023612976
0: 49 : 22:40:40 :: batch_size = 96, lr = 1e-05
0: Created sparse mask for total_precip with 10.0% data retained
0: [DEBUG] Epoch 49, first input batch shapes / sample data:
0:   └─ Field: 'velocity_u' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2635, -0.2542, -0.2422, -0.2229, -0.1992, -0.1751, -0.1552, -0.1425, -0.1358, -0.1351, -0.1427, -0.1544,
0:         -0.1664, -0.1765, -0.1810, -0.1817, -0.1841, -0.1908, -0.2696, -0.2611, -0.2495, -0.2311, -0.2061, -0.1788,
0:         -0.1523], device='cuda:0')
0:   └─ Field: 'velocity_v' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.4041, -0.4315, -0.4640, -0.4926, -0.5153, -0.5290, -0.5363, -0.5390, -0.5379, -0.5400, -0.5450, -0.5491,
0:         -0.5501, -0.5421, -0.5288, -0.5185, -0.5151, -0.5276, -0.4087, -0.4143, -0.4319, -0.4532, -0.4733, -0.4857,
0:         -0.4928], device='cuda:1')
0:   └─ Field: 'specific_humidity' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2389, -0.2090, -0.1629, -0.1192, -0.0783, -0.0449, -0.0208, -0.0121, -0.0105, -0.0225, -0.0504, -0.0836,
0:         -0.1192, -0.1551, -0.1871, -0.2091, -0.2302, -0.2367, -0.2452, -0.2357, -0.2114, -0.1694, -0.1295, -0.0950,
0:         -0.0692], device='cuda:2')
0:   └─ Field: 'velocity_z' shape: torch.Size([96, 5, 12, 3, 6, 3, 18, 18])
0:      first 25 values: tensor([-0.2489, -0.2205, -0.1680, -0.0761, -0.0235,  0.0968,  0.2391,  0.2741,  0.3069,  0.2829,  0.1712,  0.0881,
0:          0.0071, -0.0432, -0.0476, -0.0629, -0.0126,  0.0596, -0.0454, -0.1133, -0.1373, -0.0979, -0.0782, -0.0454,
0:          0.0706], device='cuda:3')
0:   └─ Field: 'temperature' shape: torch.Size([96, 5, 12, 2, 4, 3, 27, 27])
0:      first 25 values: tensor([-0.1409, -0.1223, -0.1105, -0.1001, -0.0907, -0.0842, -0.0808, -0.0788, -0.0712, -0.0531, -0.0229,  0.0155,
0:          0.0522,  0.0826,  0.1019,  0.1072,  0.1087,  0.1075,  0.1039,  0.1045,  0.1083,  0.1165,  0.1328,  0.1501,
0:          0.1612], device='cuda:3')
0:   └─ Field: 'total_precip' shape: torch.Size([96, 1, 12, 6, 12, 3, 9, 9])
0:      first 25 values: tensor([-0.2295, -0.2184, -0.1721, -0.1765, -0.2295, -0.2295, -0.2295, -0.2295, -0.2295, -0.2295, -0.1964, -0.1279,
0:         -0.1323, -0.2295, -0.2295, -0.2295, -0.2295, -0.2295, -0.2295, -0.2140, -0.1566, -0.1566, -0.2295, -0.2295,
0:         -0.2295], device='cuda:0')
0: [DEBUG] Epoch 49, batch 0 - Sparse-masked 'total_precip' shape target data: torch.Size([13824, 243])
0: [DEBUG] First 243 batch values:
0: tensor([    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1610, -0.1301,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0251,     nan,
0:             nan,     nan, -0.1721, -0.2173,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1897,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:         -0.0207,     nan,     nan,     nan, -0.0881,     nan,     nan,     nan,     nan,     nan,     nan,  0.0257,
0:             nan, -0.0020,     nan,     nan,     nan,  0.1317,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2273,     nan,     nan,     nan,     nan,     nan,     nan, -0.2184,     nan,     nan,
0:             nan, -0.2295,     nan, -0.2251,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1787,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan, -0.1478,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1014,     nan,
0:         -0.1455,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2295,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan, -0.2052,     nan,     nan,     nan,     nan,     nan,     nan,
0:             nan,     nan, -0.2229,     nan,     nan,     nan,     nan, -0.2273,     nan,     nan,     nan,     nan,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.2074,
0:             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.1875,     nan,     nan,
0:         -0.0594,     nan,     nan,     nan,     nan,     nan,     nan,     nan, -0.0174,     nan,     nan,  0.2455,
0:             nan, -0.0528,     nan])
0: [DEBUG] Epoch 49, first predictions sample:
0:   └─ Predictions for 'velocity_u' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.4211, -0.4177, -0.4023, -0.3882, -0.3922, -0.4021, -0.3808, -0.3361, -0.2757, -0.2126, -0.1786, -0.1841,
0:         -0.2029, -0.2035, -0.1769, -0.1148, -0.0554, -0.0094, -0.3081, -0.3306, -0.3364, -0.3492, -0.3861, -0.4081,
0:         -0.4045], device='cuda:0', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_v' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([-0.0150,  0.0577,  0.0398, -0.0396, -0.1547, -0.2651, -0.3749, -0.4453, -0.4952, -0.5323, -0.5482, -0.5566,
0:         -0.5699, -0.6081, -0.6536, -0.6637, -0.6211, -0.5392, -0.0398,  0.0594,  0.0482, -0.0458, -0.1823, -0.3329,
0:         -0.4454], device='cuda:1', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'specific_humidity' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.9886,  0.8990,  0.8069,  0.7028,  0.5684,  0.3978,  0.2032, -0.0071, -0.2000, -0.3453, -0.4338, -0.4806,
0:         -0.4751, -0.4561, -0.4480, -0.4213, -0.3826, -0.3437,  1.0800,  0.9385,  0.8123,  0.6903,  0.5734,  0.4325,
0:          0.2612], device='cuda:2', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'velocity_z' shape: torch.Size([17280, 972])
0:      first 25 pred values: tensor([ 0.2835,  0.3301,  0.5387,  0.5033,  0.3716,  0.4979,  0.4939,  0.3094,  0.3159,  0.3929,  0.3353,  0.2865,
0:          0.3232,  0.2301, -0.0023,  0.0289,  0.1503, -0.0006,  0.2745,  0.1506,  0.2131,  0.2338,  0.2810,  0.4667,
0:          0.4894], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'temperature' shape: torch.Size([7680, 2187])
0:      first 25 pred values: tensor([ 0.4466,  0.4038,  0.3621,  0.3300,  0.3163,  0.3181,  0.3290,  0.3350,  0.3271,  0.3049,  0.2654,  0.2173,
0:          0.1593,  0.0983,  0.0298, -0.0363, -0.0967, -0.1362, -0.1546, -0.1543, -0.1459, -0.1336, -0.1163, -0.0944,
0:         -0.0735], device='cuda:3', grad_fn=<SliceBackward0>)
0:   └─ Predictions for 'total_precip' shape: torch.Size([13824, 243])
0:      first 25 pred values: tensor([-0.0833, -0.1024, -0.1328, -0.1519, -0.1577, -0.1621, -0.1495, -0.1497, -0.1256, -0.0846, -0.1108, -0.1397,
0:         -0.1435, -0.1649, -0.1661, -0.1547, -0.1463, -0.1390, -0.0969, -0.1113, -0.1170, -0.1281, -0.1338, -0.1439,
0:         -0.1385], device='cuda:0', grad_fn=<SliceBackward0>)
0: velocity_u: 0.1534600555896759; velocity_v: 0.19649028778076172; specific_humidity: 0.1217794194817543; velocity_z: 0.7287558913230896; temperature: 0.1661405712366104; total_precip: 0.7200565338134766; 
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14698706567287445; velocity_v: 0.168464794754982; specific_humidity: 0.14153863489627838; velocity_z: 0.684814989566803; temperature: 0.16538985073566437; total_precip: 0.9639013409614563; 
0: epoch: 49 [1/5 (20%)]	Loss: 0.84198 : 0.32584 :: 0.22100 (2.50 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.16947780549526215; velocity_v: 0.20174384117126465; specific_humidity: 0.12121247500181198; velocity_z: 0.6356962323188782; temperature: 0.1873021423816681; total_precip: 0.5284231305122375; 
0: epoch: 49 [2/5 (40%)]	Loss: 0.52842 : 0.27004 :: 0.21825 (15.22 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.17145289480686188; velocity_v: 0.1842658519744873; specific_humidity: 0.14829635620117188; velocity_z: 0.544588565826416; temperature: 0.1311468482017517; total_precip: 0.5078737735748291; 
0: epoch: 49 [3/5 (60%)]	Loss: 0.50787 : 0.24472 :: 0.22121 (15.18 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: velocity_u: 0.14500686526298523; velocity_v: 0.18360905349254608; specific_humidity: 0.13837116956710815; velocity_z: 0.5263738036155701; temperature: 0.14196930825710297; total_precip: 0.44453272223472595; 
0: epoch: 49 [4/5 (80%)]	Loss: 0.44453 : 0.22728 :: 0.21416 (15.17 s/sec)
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: Created sparse mask for total_precip with 10.0% data retained
0: validation loss for strategy=forecast at epoch 49 : 0.27742186188697815
0: validation loss for velocity_u : 0.10757520794868469
0: validation loss for velocity_v : 0.11244222521781921
0: validation loss for specific_humidity : 0.1253800243139267
0: validation loss for velocity_z : 0.576427161693573
0: validation loss for temperature : 0.12721851468086243
0: validation loss for total_precip : 0.6154879927635193
0: Finished training at 22:44:36 with test loss = 0.27742186188697815.
0: [1;34mwandb[0m:
0: [1;34mwandb[0m: You can sync this run to the cloud by running:
0: [1;34mwandb[0m: [1mwandb sync /work/ab1412/atmorep/wandb/offline-run-20250603_192835-horob4h4[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250603_192835-horob4h4/logs[0m
0: l40369:2258724:2259001 [2] NCCL INFO [Service thread] Connection closed by localRank 0
0: l40369:2258724:2271165 [2] NCCL INFO comm 0x55557ec1ff60 rank 0 nranks 1 cudaDev 2 busId 84000 - Abort COMPLETE
0: l40369:2258724:2259006 [3] NCCL INFO [Service thread] Connection closed by localRank 0
0: l40369:2258724:2271165 [3] NCCL INFO comm 0x5555877f0560 rank 0 nranks 1 cudaDev 3 busId c4000 - Abort COMPLETE
0: l40369:2258724:2258996 [1] NCCL INFO [Service thread] Connection closed by localRank 0
0: l40369:2258724:2271165 [1] NCCL INFO comm 0x555570d10410 rank 0 nranks 1 cudaDev 1 busId 44000 - Abort COMPLETE
0: l40369:2258724:2258969 [0] NCCL INFO [Service thread] Connection closed by localRank 0
0: l40369:2258724:2271165 [0] NCCL INFO comm 0x55555f2619d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Abort COMPLETE
